,Unnamed: 0,title,namecompany,description,rating,field,date,textpub
0,0,"Инхаус и аутсорс DevOps. Плюсы, минусы, подводные камни",ITSumma,Эксперты в производительности,69.73,"Программное обеспечение, Консалтинг и поддержка, Веб-сервисы",2025-03-14,"  Несмотря на то, что я почти десять лет работаю в сфере Ops и DevOps аутсорса, моё отношение к этому остаётся неоднозначным. На старте я искренне горела этим и гордилась результатами компании. Но со временам мне начало казаться, что направление зашло в тупик. Своего пика мой личный кризис достиг после изучения паттернов и антипаттернов DevOps топологий в компаниях. Кажется, что всё просто и наглядно: только в модели инхаус DevOps может работать так, как оно задумывалось.  Ниже хочу порассуждать о жизнеспособности разных моделей DevOps в текущих реалиях.   Инхаус – бестпрактиc в DevOps? У инхауса много очевидных плюсов: только один проект в фокусе человека, что, в свою очередь, повышает вовлечённость и позволяет всегда оставаться в контексте. Со временем команда DevOps может выучить буквально каждую запятую в коде. Инхаус — это всегда про скорость взаимодействия: поставить задачу, сменить приоритет, получить статус — всё решается одним сообщением в корпоративном чате и не проходит через целую цепь людей или, господи, электронную почту и череду заявок.   Инхаус – это та отправная точка, с которой можно начать выстраивать каноничный DevOps, основанный на принципах из книг и рассказов евангелистов на бесчисленных конференциях. Это модель, которая закладывает фундамент для устойчивой и глубоко интегрированной культуры DevOps в компании. В свою очередь компания может активно транслировать специалистам свои ценности, задачи и видение будущего, что позволяет принимать решения, соответствующие не только техническим требованиям, но и стратегическим интересам бизнеса. Создавая инхаус команду, вкладываясь в обучение и комфортное существование сотрудников компания может быть уверена, что инвестирует именно в себя и в развитие бизнеса.  Но здесь, конечно, не без минусов. Все мы знаем о так называемом бас-факторе, под которым подразумевается риск остановки проекта в случае ухода ключевого специалиста. Если завтра девопса собьёт автобус, сколько времени и сил понадобится на поднятие из мёртвых поиск, найм, сбор по крупицам знаний о системе и доступах? И хорошо, если ваш прошлый девопс предрёк свою неловкую кончину и оставил документацию, описанный процесс онбординга, а на сладкое все доступы с пояснением, что куда и как. Но ведь не все так вежливы и предусмотрительны.   Второй подводный камень — это, конечно же, экспертиза. Современный DevOps требует от специалистов всё более и более масштабных знаний. Технологии растут как грибы после дождя, и многие мои коллеги жалуются на вечное ощущение безуспешной гонки за существующим стеком. При этом то, что появилось ещё до эры Kubernetes, не спешит покрываться вековым слоем пыли, и список требований в некоторых вакансиях сейчас больше напоминает страничку из Википедии с перечислением всех применяющихся в DevOps тулзов. Это сказывается на цене специалистов, и думаю, что многие, глядя на стоимость масштабирования команды, ощущают холодные жабьи лапки на шее. Ну и совсем уж мелкие камни вроде болезней, увольнений, выгораний и итальянских забастовок, думаю, даже не стоят отдельных абзацев. Все эти риски в модели инхаус берёт на себя бизнес.  Аутстафф — инхаус без риска? Аутстафф позволяет немного снизить риски, не теряя при этом личного контакта. Здесь также можно говорить о вовлечённости человека в ваш проект, команду. Однако всегда необходимо делать скидку на отсутствие, во-первых, материальной заинтересованности, а во-вторых, ощущения себя частью компании. У заказчика же при этом почти полностью отсутствуют рычаги влияния и удержания человека. А ещё это в среднем дороже на 20%. Это, конечно, компенсируется отсутствием затрат на найм, налоги и отпуск, но насколько такой подход выгоднее или не выгоднее финансово — тема для отдельной статьи.  На мой взгляд, аутстаф хорош, если проект находится на конечных этапах или имеет чётко определённые границы и задачи. Однако, в случаях когда проект слишком сильно завязан на конкретных специалистах, это станет проблемой. Когда весь прогресс зависит от одного-двух аутстафферов, их отсутствие или замена может заморозить процесс. Если задачи начинают множиться и растягиваться, вы теряете контроль над сроками и завершением проекта, попадая в замкнутый круг зависимости от внешних специалистов. В итоге это становится ловушкой: сроков нет, задачи не заканчиваются, а расходы продолжают расти.  Аутсорс — бездушная машина или недооцененная серебряная пуля? Вот мы и добрались до аутсорса. Начну с преимуществ. Первое и, пожалуй, самое очевидное — команда вместо человека. И не просто команда, а отсутствие части рисков и сложностей, связанных с людьми в этой команде. Не надо думать над кадровыми вопросами, HR-процессами, беспокоиться об автобусах, в конце концов. У вас просто есть специалисты и точка взаимодействия.  Главное преимущество, на которое я намекала с самого начала, — широкая экспертиза. Отношение цена/знания в аутсорсе значительно выше, и с этим сложно спорить. В инхаусе девопс зачастую остается один на один с проблемой, ему негде получить второе мнение, а его архитектурные решения никто не может подвергнуть сомнению.  Еще одним важным преимуществом является, конечно же, возможность технической поддержки 24/7. Довольно сложно и дорого выстроить у себя три, две, да даже одну линию технической поддержки проекта. А как же онколл, спросите вы? Небольшие компании вполне вывозят 24/7 с помощью пары специалистов на онколле, но так лояльность своих сотрудников точно не повысить, разве что в противовес вы предложите им ДМС с включённым туда психологом.  Закончим, пожалуй, с очевидными штуками и перейдём к менее очевидным. Чёткие договорные отношения. С аутсорс-компанией обычно заключается договор, в котором прописываются сроки выполнения работ, SLA, обязательства и штрафы. Значит, заказчик делит финансовые риски с исполнителем. А исполнитель высоко замотивирован сделать всё хорошо и в срок. Отсюда вытекают отстроенные процессы. На счету аутсорс-компаний обычно не одна сотня построенных инфраструктур, множество различных миграций, а значит, всегда есть набор подробных чек-листов и бестпрактисов на все случаи жизни.  Не сто́ит упускать из виду и партнёрки. У аутсорс-компаний часто бывает доступ к партнёрским программам с облачными провайдерами, популярными платформами и сервисами. А это обычно означает дополнительные скидки, интересные акции и возможности тестирования новых инструментов и услуг. Такие партнёрства нередко ускоряют процесс, поскольку в выделенных партнерских чатиках вопросы решаются куда быстрее.  Но давайте не будем игнорировать слона в комнате: у аутсорс-команд изначально намного меньше мотивации глубоко погружаться в особенности вашего проекта или искать дополнительные возможности для улучшения инфраструктуры. Это может сказаться на их проактивности и уровне вовлечённости в ваши бизнес-цели, что непосредственно повлияет на результаты сотрудничества. Из-за этого аутсорс часто воспринимается как чёрный ящик: вы отправляете задачи, получаете обещания сроков и затем видите только конечный результат.   Если что-то пошло не так, зачастую это бывает понятно только в самом конце при получении результатов. При этом вам может быть сложно добиться большей прозрачности процессов, гибкости взаимодействия или более строгого контроля над результатами, так как менять внутренние процессы другой компании задача из разряда невыполнимых.  Вместо выводов Что в итоге? Однозначно ли инхаус девопс всегда будет лучшим выбором, как об этом говорят евангелисты? В идеальном мире — да. В мире, где мимо нас по радуге бегут единороги, а где-то высоко висит в вакууме тот самый сферический конь инхаус девопс будет работать как и задумано. В этом идеальном мире переезды происходят без подводных камней и скелетов, внезапно выпадающих из самых разных конфигов, а люди никогда не ошибаются, не болеют и не выгорают.  А в настоящем, если у вас есть какая-то глобальная задача, или даже набор, конечная цель, желательно с крайними сроками, понимание, как измерить результаты, а то и какой-то базовый набор тестов – девопс аутсорс, скорее всего, окажется выгоднее.   Хорошо ляжет он и на ситуацию, когда у вас есть определённый пул задач или конкретная зона ответственности, которую вы готовы отгружать на аутсорс. При этом есть чёткое понимание бизнес целей, а лучше сформированный роадмап или готовность выделить время и силы на ее формирование вместе с аутсорс-командой.   Если говорить о кейсах постоянного сопровождения, где вы пытаетесь применить девопс аутсорс команду в качестве инхауса, то здесь ситуация сложнее. От чёрного ящика вас вряд ли что-то убережёт, даже у самой прозрачной аутсорс-компании вы никогда не увидите внутренней кухни, не узнаете масштабы техдолга и не посмотрите график отпусков. Но даже здесь определённо есть способы сделать сотрудничество успешным.  Для начала ответьте себе на вопрос, что конкретно я отдаю на аутсорс, какие решения хочется оставить за собой, а какие передать исполнителю.   Стрясите с вашего исполнителя чёткое описание процессов и каналов взаимодействия. Это поможет эффективнее построить процесс с вашей стороны и привести ожидания к реальной картинке. Будет полезно понимать, что за специалисты аутсорс-компании будут с вами контактировать, и кто за какие вопросы отвечает, есть ли аккаунтинг или иной сервис сопровождения клиентов.   Отдельно сто́ит получить себе всё sla, которые предоставляет и не предоставляет компания, а также флоу работы с задачами. Важно понимать – факт обсуждения задачи и ее условий совсем не значит, что она уйдёт в работу сегодня или даже завтра. Но если грамотно распределить приоритеты, понять крайние сроки и главное — обсудить всё это с исполнителем, то проблем и разочарований будет куда меньше.  Всё это, к слову, не значит, что вы не сможете отдавать на аутсорс какие-то верхнеуровневые решения по архитектуре и процессам, однако всё это также важно обсудить на берегу. Важно помнить, что DevOps и техническая поддержка в первую очередь строятся вокруг процессов, и, независимо от выбранной модели — инхаус или аутсорс — над их организацией придётся серьёзно потрудиться."
1,1,5 основополагающих советов по управлению базами данных Greenplum,OTUS,Цифровые навыки от ведущих экспертов,721.07,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-03-13,"Greenplum — это массивно‑параллельная (MPP) база данных с открытым исходным кодом, предназначенная для организации хранилищ данных и высокопроизводительной аналитики. Как и в случае с другими MPP‑СУБД, она требует регулярной оптимизации запросов, корректировки распределения ресурсов и защиты данных. В этой статье мы рассмотрим пять рекомендаций, обязательных для эффективного управления Greenplum.1. Правильно подбирайте политику распределения данныхGreenplum можно рассматривать как MPP‑адаптацию PostgreSQL. Она расширяет возможности PostgreSQL, внедряя в ее операции параллелизм. И чтобы обеспечить параллельное выполнение запросов Greenplum распределяет данные по множеству машин, называемых сегментами. Greenplum предоставляет пользователям возможность выбрать один из трех методов распределения данных: хэш, случайное и реплицированное распределение.По умолчанию Greenplum использует хэш‑распределение (hash distribution). Если пользователь создает таблицы без указания метода распределения, система автоматически распределяет кортежи на основе первого хэшируемого столбца. Однако рекомендуется явно определять политику распределения. При выборе хэш‑распределения важно выбрать столбец, который часто используется в операциях объединения или в качестве условия фильтрации. Наиболее распространенными вариантами обычно служат первичные ключи или часто используемые внешние ключи.Сценарий: База данных интернет‑магазина, содержащая таблицу «Sales».Для максимального распараллеливания запросов (например, операций объединения со связанными таблицами, такими как «Customers» и «Products») следует использовать хэш‑распределение по «Customer ID» или составному ключу, включающему «Customer ID» и «Date of Purchase». CREATE TABLE ""Sales"" (     sale_id SERIAL,     customer_id NUMERIC,     customer_name VARCHAR(255),     product_name VARCHAR(255),     purchase_date DATE,     sales_amount DECIMAL(10, 2) ) DISTRIBUTED BY (customer_id);Случайное распределение (random distribution) — менее распространенная опция Greenplum. В режиме случайного распределения данные распределяются по сегментам классическим алгоритмом round‑robin. Однако никаких механизмов гарантирующих равномерность или какой‑либо другой шаблон распределения системой не предусмотрено. Случайное распределение может подойти, если у вас нет какого‑либо преобладающего шаблона запросов, зависящего от конкретных столбцов.Сценарий: Мобильное приложение, которое ведет таблицу «UserEvents».В таблицу «UserEvents» записываются различные пользовательские события, такие как клики, запуски приложения и транзакции. События генерируются случайным образом, без какого‑либо определенного паттерна. Для эффективного хранения пользовательских событий и предотвращения перекосов данных следует использовать случайное распределение — это позволит подстроиться под нестабильную частоту возникновения пользовательских событий.CREATE TABLE ""UserEvents"" (     event_id SERIAL,     timestamp TIMESTAMP NOT NULL,     user_id INT NOT NULL,     event_type VARCHAR(255) NOT NULL ) DISTRIBUTED RANDOMLY;Распространение с репликацией (replicated distribution) — третий вариант, предлагаемый Greenplum. В этом режиме каждый сегмент сохраняет полную копию всех кортежей в таблице. Преимущество ведения компактной реплицированной таблицы становится очевидным, когда она используется в сочетании с более крупной таблицей. В таких случаях нет необходимости перераспределять данные между сегментами по сети. Вместо этого операции объединения могут параллельно выполняться непосредственно в сегментах.Сценарий: Организация здравоохранения, обрабатывающая таблицу «HospitalContacts».При ограниченном количестве больниц и минимальных изменениях данных задача состоит в том, чтобы уменьшить время выполнения запросов и избежать перераспределения данных при выполнении операций объединения. Использование реплицированного распределения дает нам мгновенный доступ к данным на всех сегментах.CREATE TABLE ""HospitalContacts"" (     hospital_name VARCHAR(255) NOT NULL,     contact_name VARCHAR(100),     phone_number VARCHAR(20),     email_address VARCHAR(255),     specialized_department_contacts TEXT[] ) DISTRIBUTED REPLICATED;2. Выберите правильную стратегию секционированияСекционирование (или партиционирование) данных — процесс разбиение одной большой таблицы на несколько меньших частей, известных как разделы (или секции/партиции). Greenplum предоставляет пользователям два варианта стратегий секционирования на выбор: секционирование на основе диапазонов (range partitioning), которое часто используется для организации данных временных рядов, и секционирование на основе списков (list partitioning), которое позволяет создавать разделы на основе предопределенных списков значений.Оптимизатор запросов Greenplum осведомлен о схемах секционирования. Он генерирует планы запросов, в которых используется фильтрация разделов (partition elimination). Это означает, что оптимизатор может игнорировать ненужные разделы при выполнении запросов. Фильтрация разделов может происходить во время обработки запроса — это статическая фильтрация разделов. Или же она может происходить во время выполнения — это динамическая фильтрация разделов.Сочетание секционирования и распределения дает значительное преимущество в оптимизации производительности запросов. Распределение данных максимизирует параллелизм, при этом каждый сегмент обрабатывает лишь часть всех кортежей. Одновременно с этим секционирование данных еще больше сокращает их объем за счет исключения нерелевантных разделов. В итоге уменьшение объема сканирования таблицы приводит к сокращению времени выполнения запроса.Сценарий: Данные транзакций онлайн‑продаж за несколько летТаблица «SalesData» распределяется по столбцу «customer_id». Это гарантирует, что кортежи с одинаковыми ID клиентов будут храниться в одном сегменте. Этот метод распределения данных оптимизирован под запросы, включающие клиентскую аналитику. В то же время таблица «SalesData» разбита на разделы по дате, чтобы эффективно управлять историческими данными. Эта схема секционирования особенно полезна для архивирования старых данных и ускорения запросов, которые связаны с определенным временным интервалом.CREATE TABLE SalesData (     transaction_id SERIAL,     customer_id INT,     transaction_date DATE,     product_id INT,     order_amount DECIMAL(10, 2) ) DISTRIBUTED BY (customer_id)  PARTITION BY RANGE (transaction_date);;  -- Создание разделов для разных лет CREATE TABLE SalesData_2021 PARTITION OF SalesData     FOR VALUES FROM ('2021-01-01') TO ('2021-12-31'); CREATE TABLE SalesData_2022 PARTITION OF SalesData     FOR VALUES FROM ('2022-01-01') TO ('2022-12-31'); -- Дополнительные разделы для последующих лет могут быть созданы аналогичным образом3. Подходите к индексам с умомGreenplum предоставляет поддержку различных типов индексов, включая B‑tree, bitmap, GIN, BRIN, хэш, а также пространственное индексирование с помощью расширения PostGIS. Хоть индексы и дают существенные преимущества, они несут за собой определенные накладные расходы. Помимо того, что они занимают дополнительное дисковое пространство, они требуют регулярного обслуживания для поддержания их актуальности. Тем не менее, при продуманном построении индексы могут значительно повысить производительность запросов. Это особенно заметно при выполнении index only scan»ов, когда Greenplum извлекает данные непосредственно из индекса, минуя основную таблицу. Хорошо спроектированные индексы должны включать столбцы, которые часто встречаются в запросах.Greenplum поддерживает как index scan, так и index‑only scan. Более того, индексы Greenplum учитывают секционирование данных. Наряду со стандартными index и index only scan»ами, которые выполняются на всех сегментах, Greenplum предлагает dynamic index scan»ы и dynamic index‑only scan»ы, которые выполняются только на неисключенных (unpruned) разделах.Сценарий: Данные транзакций онлайн‑продаж за несколько летПродолжая пример с «SalesData», мы создаем B‑tree индекс на основе столбца суммы заказа. Это позволяет использовать dynamic index scan, поскольку критерии фильтрации включают как дату транзакции (ключ раздела), так и сумму заказа (индексируемый столбец).CREATE INDEX idx_order_amount ON SalesData USING btree (order_amount);  EXPLAIN SELECT product_id FROM SalesData WHERE transaction_date = '2022-01-01' AND order_amount = 200;4. Регулярно используйте VACUUM и ANALYZEДо сих пор мы обсуждали важность обдуманного выбора распределения, секционирования и индексирования данных в Greenplum. Чтобы эти решения понастоящему продемонстрировали свою ценность, необходимо, чтобы Greenplum точно отражал эти аспекты данных. А это значит, что статистика данных должна оставаться актуальной.VACUUM и ANALYZE — это две служебные операции, каждая из которых делает очень важную работу. VACUUM освобождает место в хранилище, высвобождая области, помеченные как устаревшие. Этот процесс очень важен для предотвращения чрезмерного разрастания размера базы данных со временем. В процессе работы VACUUM обновляет карту видимости, помечая блоки данных как «set» (указывая на «живые» строки) или «clear» (указывая на «мертвые» строки).Информация о видимости важна для index only scan»ов. Если блок данных «set», оптимизатор запросов уверенно извлекает все данные из индекса. И напротив, он пропускает блоки данных, содержащие исключительно мертвые строки. И только когда карта видимости сигнализирует о наличии смеси живых и мертвых строк, index only scan обращается к данным таблицы. ANALYZE, с другой стороны, отвечает за сбор целого набора статистических данных, включая гистограммы столбцов, наиболее распространенные значения (MCV), количество строк, количество различных значений (NDV) и корреляции между столбцами. Эти статистические данные важны для оценки кардинальности, избирательности и перекоса данных. Оптимизатор запросов опирается на эти оценки для определения оптимального использования индексов, стратегий объединения, условий фильтрации и методов доступа.Обычно ANALYZE выполняется автоматически фоновым демоном автоанализа, который проверяет таблицы, превышающие заданный порог обновлений и удалений. Однако для больших таблиц этот порог достигается редко. Поэтому возникает необходимость вручную выполнять ANALYZE, в идеале в сочетании с VACUUM FULL, через регулярные промежутки времени, чтобы статистика данных оставалась точной.5. Оптимизация запросовGreenplum в значительной степени полагается на оптимизатор запросов — важнейший компонент, отвечающий за выбор наиболее эффективного маршрута операций, называемого планом запроса. Чтобы добиться оптимальной производительности запросов, необходимо понимать, как работает оптимизатор запросов Greenplum.Когда в Greenplum поступает запрос, он проходит этап парсинга и переписывания, на котором и происходят основные преобразования. Затем оптимизатор берет разобранный запрос и исследует множество возможностей его выполнения. При этом учитываются различные факторы, такие как стратегии объединения, методы агрегирования, наличие индексов и методы доступа. В результате этого исследования генерируется несколько потенциальных планов выполнения запроса, удовлетворяющих его требованиям.Greenplum использует стратегию оптимизации на основе затрат, при которой стоимость различных операций назначается на основе статистических данных. Расчет стоимости помогает оптимизатору выбрать наиболее эффективный план. Как упоминалось ранее, поддержание точной статистики с помощью регулярного выполнения операций VACUUM и ANALYZE является критически важным для эффективной работы оптимизатора запросов.Теперь, когда у нас есть представление о работе оптимизатора запросов, мы можем использовать команду EXPLAIN для анализа планов выполнения. Она даст нам представление о том, как Greenplum планирует выполнить запрос, и поможет выявить потенциальные узкие места. Если выполнение запроса занимает больше времени, чем предполагалось, обратите внимание на распространенные проблемы, которые могут включать в себя:Высокий уровень дискового ввода‑вывода, когда на диск записывается значительный объем данныхПлан не использует доступные индексыНеоптимальные стратегии объединения, например, использование вложенных циклических джоинов, когда более эффективным было бы использование хэш‑джоиновБольшой промежуточный вывод при объединении, сортировке или агрегированииВысокий уровень перераспределения или трансляции данныхНедооценка или переоценка кардинальностиОтсутствие параллелизма, когда выполнение происходит на координаторе, а не в сегментахУсловия фильтрации не продвинуты внизИзлишнее сканирование разделов или полных таблицИногда изменение запроса в эквивалентную форму может помочь оптимизатору запросов сгенерировать более эффективный план. Это может включать в себя пересмотр объединений, упрощение условий фильтрации или декомпозицию сложных запросов на несколько более простых запросов. Если вы сталкиваетесь с постоянными проблемами производительности, обратитесь в службу поддержки или в сообщество Greenplum. Оптимизация запросов — это непрерывный процесс, часто требующий нескольких итераций анализа, корректировок и тестирования для достижения желаемой производительности.Станьте мастером интеграции: PostgreSQL, ArenadataDB и Airflow за один урок! 18 марта в 20:00. Зарегистрироваться бесплатно можно на странице курса «Greenplum для разработчиков и архитекторов баз данных»."
2,2,unittest.mock: 5 вопросов на собеседовании,OTUS,Цифровые навыки от ведущих экспертов,721.07,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-03-13,"Привет, Хабр!Сегодня мы рассмотрим некоторые вопросы про unittest.mock, которые могут всплыть на собеседовании. И перейдем сразу к вопросам.Как замокать атрибуты класса и экземпляра?Задача кажется простой: нужно временно подменить какое‑то значение в объекте. Но что делать, если это атрибут класса, а не экземпляра? Или если атрибут объявлен через @property? Давайте разбираться.Мок атрибута классаАтрибуты класса можно замокать с помощью patch. Важно понимать, что такие моки работают на уровне самого класса, а не конкретного объекта.from unittest.mock import patch  class MyClass:     class_attribute = 'original'  with patch('__main__.MyClass.class_attribute', new='mocked'):     print(MyClass.class_attribute)  # mocked print(MyClass.class_attribute)  # originalПочему так происходит? Потому что patch временно изменяет значение только внутри контекста. Как только with‑блок заканчивается, всё возвращается на круги своя. Глубокий мокинг атрибута классаНо что если нужно подменить атрибут класса глубже — например, если класс используется внутри другой структуры?class OuterClass:     inner = MyClass()  with patch.object(OuterClass.inner, 'class_attribute', 'deep_mocked'):     print(OuterClass.inner.class_attribute)  # deep_mockedЭто применимо, когда объект создаётся динамически, и нужно контролировать его поведение на разных уровнях вложенности.Мок атрибута экземпляраТут немного сложнее. У экземпляра есть свой атрибут, и его можно подменять на ходу:from unittest.mock import patch  class MyClass:     def __init__(self):         self.instance_attribute = 'original'  obj = MyClass() with patch.object(obj, 'instance_attribute', 'mocked'):     print(obj.instance_attribute)  # mocked print(obj.instance_attribute)  # originalЕсли хочется подменить все будущие экземпляры класса, а не только один объект, можно сделать вот так:with patch.object(MyClass, 'instance_attribute', 'mocked', create=True):     obj1 = MyClass()     obj2 = MyClass()     print(obj1.instance_attribute, obj2.instance_attribute)  # mocked mockedНо если у объекта уже есть атрибут, create=True не нужен.Как работает side_effect?  Когда мы мокируем функции, чаще всего мы просто указываем return_value, чтобы она всегда возвращала одно и то же. Это удобно, но иногда недостаточно. Что если поведение функции зависит от переданных аргументов? Или она должна выбрасывать исключения в определённых ситуациях? Или вообще каждый раз возвращать разное значение?Вот тут и приходит на помощь side_effect.side_effect как список значенийСамый базовый вариант: даём моку список значений, и он будет возвращать их по порядку при каждом вызове.from unittest.mock import Mock  mock = Mock(side_effect=[1, 2, 3, Exception(""Больше нельзя!"")])  print(mock())  # 1 print(mock())  # 2 print(mock())  # 3  try:     print(mock())  # Бросает исключение! except Exception as e:     print(f""Ошибка: {e}"") Когда значения в списке заканчиваются, будет выброшено StopIteration. Это может быть полезно, когда мокируем итератор или API‑запрос, который возвращает разные результаты при каждом вызове.side_effect как функцияЕсли хотим, чтобы мок динамически реагировал на переданные аргументы — даём side_effect функцию.from unittest.mock import mock_open, patch  mock = mock_open(read_data='file content') with patch('builtins.open', mock):     assert read_file('dummy.txt') == 'file content'Теперь мок ведёт себя как функция. Ещё пример — подмена API‑функции, которая должна возвращать разные результаты для разных входных данных:def api_mock(endpoint):     responses = {         ""/users"": [""Alice"", ""Bob"", ""Charlie""],         ""/status"": ""OK"",         ""/error"": Exception(""API недоступен""),     }     if endpoint in responses:         result = responses[endpoint]         if isinstance(result, Exception):             raise result         return result     return None  mock_api = Mock(side_effect=api_mock)  print(mock_api(""/users""))   # [""Alice"", ""Bob"", ""Charlie""] print(mock_api(""/status""))  # ""OK""  try:     print(mock_api(""/error""))  # Выбросит Exception except Exception as e:     print(f""Ошибка: {e}"")side_effect для выбрасывания исключенийИногда мок должен сломаться при вызове, например, если он эмулирует сеть, которая может упасть.mock = Mock(side_effect=RuntimeError(""Аварийное завершение!""))  try:     mock()  # бах! except RuntimeError as e:     print(f""Поймано исключение: {e}"")Это применимо, когда тестируем код, который должен обрабатывать ошибки:def process_data(fetcher):     try:         data = fetcher()         return f""Данные получены: {data}""     except Exception as e:         return f""Ошибка: {e}""  mock = Mock(side_effect=ConnectionError(""Нет сети"")) print(process_data(mock))  # Ошибка: Нет сетиСмешанный side_effectМожно смешивать всё вышеописанное:from unittest.mock import Mock  def crazy_mock(x):     if x < 0:         raise ValueError(""Отрицательные числа нельзя!"")     return x ** 2  # Возводим в квадрат  mock = Mock(side_effect=crazy_mock)  print(mock(2))   # 4 print(mock(5))   # 25  try:     print(mock(-1))  # Ошибка! except ValueError as e:     print(f""Ошибка: {e}"")Здесь мок работает как функция, но в определённых случаях выбрасывает исключение.Как проверить порядок вызовов мока?В тестах бывает важно не просто проверить, что методы были вызваны, но и в каком порядке. Например, если мы тестируем логику оркестрации запросов в API‑клиенте, проверяем порядок вызова методов у объекта, или убеждаемся, что наш код не перемешивает вызовы.В unittest.mock для этого есть несколько удобных инструментов:mock_calls — список всех вызовов моков в порядке выполненияassert_has_calls() — проверяет, что вызовы были, но не следит за их порядкомassert_called_once_with() — проверяет конкретный вызовcall() — удобный способ описать ожидаемую последовательность вызововРазберём всё по порядку.Проверяем полный порядок вызовов через mock_callsДопустим, есть объект с двумя методами:def read_file(file_path):     with open(file_path, 'r') as f:         return f.read()Теперь проверяем, что методы вызваны в правильном порядке:from unittest.mock import mock_open, patch  mock = mock_open(read_data='file content') with patch('builtins.open', mock):     assert read_file('dummy.txt') == 'file content'Если порядок вызовов был другим, тест упадёт.Что если порядок вызовов не важен?Иногда порядок не имеет значения, и важно лишь наличие самих вызовов. В таком случае используем assert_has_calls():mock = Mock()  mock.method1() mock.method2() mock.method1()  expected_calls = [call.method1(), call.method2()]  mock.assert_has_calls(expected_calls)  # Пройдёт, порядок не учитываетсяНо если порядок важен, добавляем any_order=False (по умолчанию он False):mock.assert_has_calls(expected_calls, any_order=False)  # Упадёт, если порядок не совпадаетКак проверить, что метод был вызван только один раз?Если нужно убедиться, что метод не был вызван больше одного раза, используем assert_called_once_with():mock = Mock()  mock.process_data(42)  mock.process_data.assert_called_once_with(42)  # Пройдёт  mock.process_data(99)  # Второй вызов mock.process_data.assert_called_once_with(42)  # Упадёт, метод вызывался дваждыЕсли метод мог вызываться несколько раз, но нам важен хотя бы один вызов с определёнными аргументами, используем assert_any_call():mock = Mock()  mock.process_data(10) mock.process_data(20) mock.process_data(30)  mock.process_data.assert_any_call(20)  # Был вызван с 20 хотя бы один раз mock.process_data.assert_any_call(50)  # Упадёт, 50 не передавалиОтслеживание вызовов у нескольких объектовДопустим, есть два связанных объекта, и нужно проверить общую последовательность их вызовов:mock1 = Mock() mock2 = Mock()  mock1.method1() mock2.method2() mock1.method1()  expected_calls = [     call.method1(),     call.method2(),     call.method1() ]  assert mock1.mock_calls + mock2.mock_calls == expected_calls  # Полный порядок вызововКак замокать контекстный менеджер?Контекстные менеджеры (with open(...) as f) встречаются повсеместно: работа с файлами, соединение с базой данных, управление сессиями API, и т. д. Если в коде используется with, то мокать просто return_value недостаточно — нужен правильный подход.Почему обычный patch для open() не работает?Допустим, есть функция, которая читает содержимое файла:def read_file(file_path):     with open(file_path, ""r"") as f:         return f.read()Наивная попытка замокать open() не сработает:from unittest.mock import patch  with patch(""builtins.open"", return_value=""file content""):     print(read_file(""dummy.txt""))  # Не сработает!Почему? Потому что open() возвращает файловый объект, у которого есть методы .read(), .write(), .close(), и просто подмена return_value ничего не даёт.Как правильно мокать open?Используем mock_open(), который создаёт корректный мок файла:from unittest.mock import mock_open, patch  mock = mock_open(read_data=""file content"")  with patch(""builtins.open"", mock):     result = read_file(""dummy.txt"")     assert result == ""file content""  # Работает! Теперь, когда функция вызывает open(), она получает поддельный файловый объект, у которого .read() уже заранее возвращает file content.Мокаем write() для файловДопустим, есть код, который записывает данные в файл:def write_file(file_path, data):     with open(file_path, ""w"") as f:         f.write(data)Проверим, что данные действительно записались:from unittest.mock import mock_open, patch  mock = mock_open() with patch(""builtins.open"", mock):     write_file(""dummy.txt"", ""hello world"")  # Проверяем, что метод `.write()` вызывался с правильными данными mock().write.assert_called_once_with(""hello world"")Как замокать @property?Свойства @propert в Python — это не просто атрибуты, а дескрипторы. Их нельзя мокать обычным patch.object(), потому что они вызываются как методы.Рассмотрим задачу.Почему @property нельзя замокать patch.object()?Допустим, есть класс:def write_lines(file_path, lines):     with open(file_path, ""w"") as f:         for line in lines:             f.write(line + ""\\n"")Мы хотим подменить my_property, но обычный patch.object() не сработает:mock = mock_open() with patch(""builtins.open"", mock):     write_lines(""dummy.txt"", [""first"", ""second"", ""third""])  mock().write.assert_has_calls([     (""first\\n"",),     (""second\\n"",),     (""third\\n"",), ])@property — это не переменная, а метод с геттером. Его вызов идёт через get(), а не через dict. Нельзя просто заменить его строкой «mocked», потому что это вызов метода.Как правильно мокать @property?Используем PropertyMock:from unittest.mock import patch, PropertyMock  class MyClass:     @property     def my_property(self):         return ""original""  with patch(""__main__.MyClass.my_property"", new_callable=PropertyMock) as mock_prop:     mock_prop.return_value = ""mocked""     obj = MyClass()     print(obj.my_property)  # mockedТеперь my_property заменяется на мок, который можно настроить.Как замокать @property, если он динамически вычисляется?Допустим, есть свойство, которое вычисляет длину строки:class MyClass:     def __init__(self, data):         self._data = data      @property     def length(self):         return len(self._data)Хочется подменить length, не трогая _data. Используем PropertyMock:with patch(""__main__.MyClass.length"", new_callable=PropertyMock) as mock_prop:     mock_prop.return_value = 99     obj = MyClass(""Hello"")     print(obj.length)  # 99, хотя ""Hello"" явно корочеПроверяем, что @property вызывалсяИногда хочется убедиться, что @property реально использовался. У PropertyMock есть assert_called():with patch(""__main__.MyClass.length"", new_callable=PropertyMock) as mock_prop:     obj = MyClass(""test"")     _ = obj.length  # Запрашиваем свойство     mock_prop.assert_called_once()  # Проверяем, что геттер вызывалсяА какие интересные вопросы задавали вам? Пишите в комментариях.Статья подготовлена для будущих студентов онлайн-курса ""Python QA Engineer"". Хорошая новость: в рамках этого курса студенты получат поддержку карьерного центра Otus. Узнать подробнее"
3,3,n8n. Создаём AI Telegram agent с установкой и настройкой,Amvera,Amvera — облако для хостинга IT-приложений,100.75,"Программное обеспечение, Домены и хостинг, Веб-сервисы",2025-03-13,"Всем привет!В этой статье будет рассматриваться n8n - open source платформа для автоматизации рабочих процессов, которая поддерживает более 400 интеграций, в том числе LMM, векторные базы данных и все, что нужно, чтобы создать ИИ-агентов и RAG-приложений.Итак, сегодня мы развернем n8n c RAG-агентом на нашем облаке Amvera (почему подходит именно Amvera - расскажем ниже) и покажем, как им пользоваться на простых примерах, среди которых будет чат-бот для работы с БД PostgreSQL.Что нам понадобится?А понадобится лишь:Минимум свободного времени (весь процесс займет около 30 минут, если не меньше)Токен OpenAI (можно использовать и другие LLM)Токен от SerpApiКак работать с n8nДля начала интерфейс n8n может быть «пугающим» для новичков, но на деле можно очень быстро привыкнуть. Кратко говоря, рабочая среда интерфейса n8n представляет собой «рабочий стол», где вы можете создавать логику с помощью узлов, в которых каждый узел отвечает за конкретное действие.Соединяя узлы вместе, вы создаете логику работы вашего приложения.Всего существует 2 функциональных узла: узел триггера и узел действия. Первый отвечает за запуск приложения из-за какого-то внешнего «раздражение», пусть то сообщение в чате, обращение к API или любое другое отслеживаемое событие. Второй за то, что будет делать приложение, то есть его логику.Можно дополнительно упростить процесс использования, используя более тысячи шаблонов от сообщества, скачав которые вы можете буквально за 10 минут создать собственное работающее IT-приложение.У n8n есть документация на английском языке, где все отлично объяснено.Для начала развернем сам n8n на Amvera! Это займет совсем немного благодаря вышеописанным принципам.Установка n8nУстановку n8n мы произведем на удаленный сервер в Amvera Cloud (это же наш блог).Amvera - сервис для быстрого развертывания различных IT-приложений, позволяющий пользователям отводить минимум времени на деплой, и главное, имеющий бесплатное встроенное проксирование до OpenAI, Gemini, Grok, Claude и других провайдеров. То есть вам не понадобится самостоятельно добавлять прокси. Помимо этого, Amvera позволяет развернуть n8n как преднастроенный сервис, заполнив несколько полей в конфигурации.К плюсам также относится:Развертывание и обновление проектов через git push (или перетягиванием файлов в интерфейсе), что проще настройки VPS;Поминутная тарификация со списыванием раз в час — деньги будут списываться только за работающий инстанс (экземпляр) приложения;Бесплатный пробный баланс 111 рублей при регистрации!Бесплатное внешнее HTTP/HTTPS доменное имяи многое другоеДля самого начала необходимо зарегистрироваться или войти.После успешной регистрации начисляется бонус в размере 111 рублей, с которыми мы уже можем работать.Давайте создадим проект:Переходим на главную страницу ЛК Amvera.Нажимаем кнопку ""Создать проект"" в правом верхнем углу, выбираем тип сервиса ""Приложение""Выбираем произвольное название проекта и тариф (проверялось на тарифе ""Начальный Плюс"")Этап с загрузкой данных можно пропустить - загружать в нашем случае ничего не понадобится.На этапе создания переменных создаем переменные со следующими значениями и названиями:N8N_HOST: 0.0.0.0N8N_PROTOCOL: httpsGENERIC_TIMEZONE: Europe/Moscow или другая TZ, актуальная для вашего регионаN8N_DEFAULT_LOCALE: по умолчанию enN8N_USER_FOLDER: /data (обязательно) Список доступных переменных окружения находится здесь.Во вкладке конфигурация выбираем следующие значения параметров:Окружение: DockerИнструмент: DockerСтавим галку на SkipВ значение параметра image вставляем docker.n8n.io/n8nio/n8nЗначение параметра containerPort меняем на 5678 Сохраняем конфигурацию и жмем «Завершить».Заполненная конфигурация для n8nHttps для n8nДалее создаем бесплатный домен с типом подключения HTTPS во вкладке «Домены» проекта. Как тип домена выбираем ""Бесплатный домен Amvera"", если нет собственного.Теперь мы создали проект и можем его запустить! Для этого жмем кнопку «Собрать» во вкладке «Конфигурация» проекта или кнопку с коробкой на панели управления проектом (правый верхний угол).Ожидаем около 5 минут статуса «Приложение запущено» и открываем страницу домена. И теперь мы можем работать прям с n8n!Создание бота в n8nКогда мы выполнили первичную регистрацию и вход в панель n8n, мы можем создать первую рабочую среду.В нем мы создадим логику (или рабочий процесс) с использованием OpenAI. Это один из самых простых вариантов рабочего приложения. Само приложение будет представлять собой чат внутри интерфейса n8n, в котором мы сможем общаться с нейросетью.Пошагово расскажу, что нужно делать:Перейдя в первую рабочую среду, нажимаем большую кнопку на поле ""Add first step..."", где выбираем узел триггера ""On chat message"". Так как мы делаем самый простой чат-бот и не будет его делать публичным, в открывшемся окне ничего не трогаем и возвращаемся на рабочее поле.Справа от узла триггера нажимаем на плюсик и выбираем Advanced AI -> AI Agent. В открывшемся окне как Agent выбираем ""OpenAI Functions Agent"" (можете выбрать любого другого агента, если нужно).Под узлом AI Agent будет 3 плюсика, где как Chat Model выбираем также OpenAI Chat Model; Memory - Window Buffer Memory; Tool - SerpApi (Google Search). Таким образом, мы настраиваем агента, дав ему доступ к SerpAPI (поиску в Google).Для OpenAI Chat Model и SerpAPI необходимо указать Креды (т.е. приватный токен). Через Enter открываем, например, OpenAI Chat Model и жмем ""Create New Credential"".Когда все будет настроено, мы уже будем иметь полностью рабочее приложение, можно переходить в чат (значок молнии у узла триггера) и общаться с чат-ботом!То, что мы сделали за 5 минут - минимум возможностей, которые предоставляет n8n.Давайте проверим работу агента, задав вопрос о погоде в Москве:Как мы видим, данные совпадают с актуальными, а значит, мы все сделали правильно!AI-ассистент для работы с базой данных PostgreSQLОбычный чат-бот это хорошо, но ограничиваться этим мы сегодня не будем. Еще одной возможностью является работа с базой данных внутри чата n8n. Добавив лишь несколько инструментов, мы дадим доступ агенту к базе данных, благодаря чему агент сможет ""ходить"" туда и выбирать всю интересную для нас информацию.В роли базы данных будет выступать managed кластер PostgreSQL, развернутый в Amvera по инструкцииПерейдем к настройке, понадобится лишь добавить к созданному в прошлом пункте агенту лишь 1 инструмент:Postgres toolВ настройках этого инструмента необходимо как Operation выбрать Select, заполнить остальные параметры.И убрать SerpAPI. Также, что логично, перед первым запуском необходимо будет проверить наличие ключа OpenAI и креды от базы PostgreSQL, иначе схема работать не будет.При каждом обращении в чате, бот идет в базу данных, выполняя сгенерированные SQL-запросы, и возвращает полезную для пользователя информацию, используя полученные данные.Давайте теперь создадим какой-то сценарий для проверки работоспособности. В существующей базе я создал таблицу со случайными именами в случайном порядке, но некоторые имена повторяются. Попросим агента найти самые повторяющиеся имена и сколько раз они повторялись.Таблица выглядит следующим образом:Таблица в PostgreSQLТеперь попросим вывести повторяющиеся имена и получаем следующий ответ:Подключение телеграм-бота к n8nВыше мы обращались к агенту исключительно через интерфейс n8n, однако помимо работы через сайт, мы также можем подключить телеграм-бота, что очень удобно, поскольку, опять же, нам не понадобится писать код.Для начала убедитесь, что у вас создан бот в @BotFather и если нет, создайте. У него вам понадобится получить токен бота.Также предварительно понадобится немного обновить настройки проекта в Amvera, т. к. телеграм-бот будет работать через вебхуки, а именно:Изменить параметр конфигурации containerPort с 5678 на 80,5678Во вкладке ""Домены"" раскрыть созданный вами домен и добавить следующие маршруты:Копируем ссылку на приложение с маршрутом /webhook и во вкладке «Переменные» добавляем переменную WEBHOOK_URL, в значение которой вставляем домен с маршрутом. Примерно он будет выглядеть так: https://project-username.amvera.io/webhookПересобираем проект.Когда все удалось, мы можем приступать к подключению бота:Удалите существующий узел триггера On chat message и добавьте узел ""Telegram"". При его добавлении высветится ряд триггеров, среди которых ищем и выбираем ""On message"".После проделанных действий добавляем креды (токен) бота и сохраняем конфигурацию.Итого мы получаем следующую картину:Схема RAG-агента на базе n8nИтогЗа минимум времени мы развернули n8n на удаленном сервере, очень кратко ознакомились с интерфейсом и создали несколько рабочих примеров RAG-приложений, таких как n8n телеграм-бота для просмотра погоды и агента для работы с базой данных PostgreSQL."
4,4,Как Кремниевая долина превращает бум ИИ в новый пузырь доткомов,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2558.03,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-13,"Наблюдая за историей Кремниевой долины, всё очевиднее становится, что всего за четверть века («Гуглу» — 26 лет) глобальный центр IT и интернета прожил целую историческую эпоху: сегодняшняя Кремниевая долина стала противоположностью себя на заре интернет-индустрии, а сегодняшние капитаны индустрии являются главными препятствиями для повторения своей истории успеха — что является признаком завершения очередной исторической арки, будь то история строя, формации, империи или революции. Последние два примера очень точно образно подходят к описанию исторической арки Кремниевой долины: революция, которая породила империю, которая сегодня задыхается под тяжестью своего веса.Если звёздами золотого века интернета (1990–2000-е годы) становились проекты с «идеей на миллиард», материально изменившие образ жизни, способы коммуникации и кооперации между людьми для сотен миллионов и миллиардов человек — Google, Tesla (2003), Facebook* (2004), Google Maps (2005), YouTube (2005), iPhone (2007), Uber (2009), — то в начавшийся в 2010-е позолоченный век тот же Uber, WeWork (2010) и Snapchat (2011) привлекали внимание уже не только идеями, но и раундами финансирования — в чём Uber стал первопроходцем, соединив в себе элементы обеих эпох. Примером подмены миллиардной идеи миллиардными бюджетами в чистом виде стал Metaverse Марка Цукерберга (2021), основными новостными поводами которого были только грандиозные бессодержательные жесты (переименование материнской компании таких реальных гигантов, как Facebook*, Instagram* и WhatsApp, в Meta*) и грандиозные затраты — $36 млрд за 2022–2023 годы. В поисках бизнес-модели Metaverse начала срастаться с криптосферой — блокчейном, NFT, криптовалютами и токенами, которые, в свою очередь, изобретая utility, которой была бы обеспечена их виртуальная цена, начали срастаться с ИИ, в итоге превращаясь в VR-крипто-AI-экосистему — ещё более непонятно зачем и кому нужную, чем просто виртуальная реальность Metaverse.Если раньше типичной проблемой Кремниевой долины было: «У нас есть дисруптивная идея, которая стремительно захватывает устройства и внимание пользователей, благодаря чему мы привлекли финансирование — осталось только придумать, как начать зарабатывать», то Metaverse начали строить с другого конца: «У нас профинансирован проект для захвата устройств и внимания пользователей — нам осталось только придумать идею, чем».*Meta, Facebook и Instagram признаны экстремистской организацией и запрещены в России.В 2020 году я писал о золотом и позолоченном веке (golden age и gilded age) интернета в контексте монополизации: процесса, в ходе которого самые успешные проекты золотого века, победив в соревновании идей и команд, начинают уничтожать открытую конкурентную среду, которая их создала. Век идей на миллиард против века проектов на миллиард — это другое выражение той же самой динамики.Последний раз, когда Кремниевая долина меняла мирКогда в последний раз Кремниевая долина создавала нечто, что бы материально изменило жизнь миллиарда человек и впечатляло не по привычке («Йей, это анонс Apple/Маска/Цукерберга — значит, это интересно»), а по существу? Все главные бренды и продукты, на которых сегодня стоит Кремниевая долина, были основаны в 1990-х и 2000-х годах.Последним состоявшимся проектом золотого века интернета стал Uber — и тот прошёл по грани: в отличие от Google, Google Maps, YouTube, iPhone и Android, которые сами создали для себя рынки, у Uber был прямой «аналоговый» конкурент — традиционные службы такси. Они задавали ценовой потолок, вынуждая Uber выбирать между конкурентностью и прибыльностью. В ответ компания, зажмурившись, вдавила педаль финансового газа: начиная с первого миллиардного раунда финансирования ($1,2 млрд в 2014 году), новостными поводами о масштабных вливаниях в Uber прямо «гасили» новости о его убыточности и нежизнеспособности.Логика в этом была: проект, рассчитанный на привлечение не только пассажиров, но и водителей, должен внушать уверенность в том, что он здесь надолго, а не лопнет через три месяца. Разговоры об убыточности Uber угрожали риском самосбывающегося пророчества: если «Уберу» скоро конец, то нет смысла устраиваться туда водителем, а дефицит водителей делает сервис бесполезным для пассажиров — и «Уберу» действительно приходит конец. Миллиардными вливаниями инвесторы и руководство Uber боролись за то, чтобы его воспринимали всерьёз. К 2019 году потери Uber достигли $8,5 млрд, а общие вливания — $25 млрд к 2024 году.Раньше продукты Кремниевой долины достигали этого за счёт роста пользовательской базы: чем больше людей пользуется, например, соцсетью, тем выше её ценность для остальных, видящих, что всё больше знакомых уже там. Именно пользовательские массы, «глас народа», определяли судьбу проектов: ради одних люди готовы были записываться в очереди и бороться за инвайты, пока другие тонули в холодном безразличии масс. К успеху «нет царской дороги», как показал опыт Google: в 2004-м люди искали инвайты в Gmail, а в 2011-м проигнорировали соцсеть-конкурента Цукербергу Google+. Оказалось, что даже гигантская корпорация с подлинно народными проектами и огромными ресурсами не заменит отсутствующий «it»-фактор, что его нельзя просто купить.Пока Uber не создал обманчивое впечатление, что можно — фактически купив себе место на рынке. Fake it till you make it: в 2023 году Uber впервые стал прибыльным ($1,1 млрд), а в 2024-м прибыль выросла до $2,1 млрд. Но у «Убера» была дисруптивная зацепка, идея: изменение пользовательского опыта пассажира такси, из которой в итоге удалось родить Uber Eats (2014) — идею в духе золотого века Кремниевой долины. Такси Uber не изменило сам способ городской жизни человека, лишь оцифровав то, что уже было её неотъемлемой частью — поездки на такси, а Uber Eats изменил, встав по своему вкладу последним в ряду великих (Google Maps, соцсети, YouTube, iPhone).Проекты, которые приняли модель «Убера» за next big thing и начали плодиться по принципу «Убер для», не разобрались толком, какого рода бизнесом «Убер» является.В фильме «Основатель» про Рэя Крока звучит такая фраза: анализируя бизнес-модель McDonald’s, финансовый консультант говорит Кроку, что прибыль от продажи бургеров и франшиз недостаточна для масштабирования: ""You’re not in the burger business. You’re in the real estate business."" («Вы не в бургерном бизнесе, вы в бизнесе недвижимости») — и она полностью подходит к ситуации «Убера». Но это тема для отдельной статьи.То, что бизнес-модель Uber была не тем, чем кажется, стало причиной, почему только Uber (и аналогичные проекты в такси/доставке) в нише «Убер для» и выжили. WeWork, попробовав «уберизовать» офисы, стал крупнейшим финансовым провалом 2010-х, потеряв $18 млрд к 2025 году с учётом реструктуризации после банкротства в 2024-м. На этом фоне ситуация с Metaverse выглядит гораздо менее интересной: без «it»-фактора, скроенный по лекалам мертворождённого Google+, Metaverse был обречён, как Google+. Но Google, осознав, что запуск не пошёл, долго за проект не боролся, закрыв G+ без стыда и сожаления в 2019-м, потеряв $582 млн, а Цукерберг в 2025-м не признаёт поражения, потратив $36 млрд.А как же ИИ?Слон, которого мы мы не приметили, называя последним изобретением Кремниевой долины, изменившим жизнь современного человека, Uber/Uber Eats в 2014 — это большие языковые модели (LLM, large language model), по-простому — ИИ-чатботы. Потому что даже с учётом последних моделей ИИ, золотой век Кремниевой долины с ними не вернулся — наоборот, эта сфера куда больше напоминает новый, значительно больший пузырь доткомов.  Технически, LLM — это, безусловно, начало новой эры в кибернетике, вычислениях и робототехнике. Более того, LLM могут оказаться новой эрой в антропологии — похоже, что, развивая их, человеку удастся лучше понять, если не полностью разрешить вопросы человеческого языка, мышления и сознания.Но LLM пришли к нам не из научных лабораторий, а из Кремниевой долины, у которой есть только одна скорость. И вместо научной и гуманитарной ценности LLM и соответствующего к ним отношения на ИИ предлагается смотреть как на бизнес — и в ИИ как бизнесе все ошибки, проблемы, пороки и перекосы Кремниевой долины конвергировались в нечто, что на данный момент больше всего напоминает гигантский пузырь, «пузырь доткомов 2.0», в который на этот раз вовлечены уже те проекты, дорогу которым проложил, лопнув, первый «пузырь».Дело не в самом ИИ, который уже сейчас, будучи далеко не сильным искусственным интеллектом (AGI), впечатляет и возможностями, и скоростью прогресса. Будь это научный проект — это был бы потрясающий и однозначно успешный прорыв. Но рыночный контекст налагает свои ограничения и имеет свою логику: по ней, если продукт хорош — значит, он монетизируем и окупаем. Если нет — значит, не так уж и хорош.В случае с ИИ нет сомнений, что продукт сам по себе как технологическое достижение, начало новой эры взаимодействия человека и машины, хорош — это не VR/AR-промах с «Метаверсом». Следовательно, согласно рыночной аксиоме, ИИ монетизируем и окупаем. Именно так раздувается в угрожающих масштабах новый пузырь, контуры которого стало отчётливо видно в ходе двух недавних событий:реклама ИИ на Суперкубке как квинтэссенция рекламы ИИ-продуктовпроект Stargate и DeepSeek.Проблема лицом: продавец без товараХотя многие люди находят пользу и уже применяют, по мелочам или всерьёз, те или иные чатботы (я положительно впечатлён «Гроком-3»), стандарт золотого века интернета, т.е. уровень проекта, меняющего мир, подразумевает продукт, меняющий то, каким образом свою жизнь проживают по меньшей мере сотни миллионов, если не миллиарды человек, и создающий целые новые сектора экономики — как это сделали смартфоны, карты, соцсети, YouTube.У лучших из них соавторами их полезности являются сами пользователи, находя им применения, под которые проектам остаётся только ловко и расторопно подстраиваться. Но в итоге, если не с первого взгляда, то спустя время, люди уже сами нуждаются в открытых им возможностях и функционале.Рекламная пауза на американском «Суперкубке» — это тот момент, когда широким массам можно и нужно было показать, почему им не обойтись без LLM и почему они стоят того, чтобы нормис, которому даже VPN зачем нужен, пока массовые блокировки не начнутся, непонятно, а тем более нормис за это не готов платить, захотел платить за LLM — хоть $20 за OpenAI ChatGPT, хоть $5 за сторонние сервисы доступа. И… ничего лучше, чем возможность за человека позвонить и зарезервировать столик или номер, они не показали.Обесценивает ли это, дискредитирует ли это технологию и её перспективы? Нет. Просто на данном этапе и в обозримом будущем у неё нет именно того применения, которое требует бизнес-логика: массового монетизируемого спроса. Узкий профессиональный — да, но это не та ниша, которая бы оправдала затраты Кремниевой долины на LLM.Обратная сторона проблемы: расходыОбъявленный в этом году проект Stargate по строительству инфраструктуры для одного только OpenAI на полтриллиона долларов показывает, каких безумных диспропорций достигло преследование стратегии компенсации безыдейности (с точки зрения массового рыночного спроса) денежными вливаниями: более чем на порядок больше, чем инвестиции в Uber ($27–29 млрд в 2014–2024), — при том, что числа пользователей и выручки уровня «Убера» в ближайшие годы для OpenAI и всего рынка чат-ботов не предвидится.Одновременно с этим, словно специально подчеркнув вышедшие из-под контроля ИИ-бюджеты Кремниевой долины, китайская компания выпускает конкурентный продукт, который бьёт по двум уязвимым точкам сразу:приблизительно на три порядка дешевле в разработке (идут споры о том, насколько верны озвученные бюджеты и хороши показанные бенчмарки, но общий вывод, что DeepSeek показал новое качество эффективности разработки, на момент выхода DeepSeek стал консенсусным);open source, которым OpenAI, несмотря на название и происхождение (свободная Америка), больше не является, а DeepSeek из тоталитарной КНР — является.Перевёрнутая реальность — когда динамичные, конкурентные, дисруптивные проекты, продвигавшие открытость, конкурентность и новые стандарты и выигравшие от них, чутко реагирующие на обратную связь от людей и общества, выросшие в условиях честной конкуренции от стартапа до империи, внезапно оказываются неповоротливыми левиафанами, закрытыми, утратившими связь с людьми, с рынком, непомерно прожорливыми и непонятно на что спускающими деньги, — это и есть то, как выглядит закат империи.Перспективы самого ИИ и перспективы Кремниевой долины — это две разные вещи. Пока демиурги Кремниевой долины открыто рассуждают о том, как ИИ сделает простых людей, рабочий класс, ненужными бизнесу, начальникам и работодателям, превратив из граждан в просителей и клиентеллу, новых крепостных для новых феодалов цифровой эпохи, под ними наполняется гигантская цистерна с горючим, в которую они сами продолжают вливать топливо, — и достаточно будет только искры. Обвал совокупной капитализации с испуга от одного лишь релиза DeepSeek на $1 трлн показал, о какого уровня рисках идёт речь.Будет крайне иронично (и космически справедливо), если в итоге бизнесы Кремниевой долины и станут теми, чью работу ИИ отберёт. Прогресс, если этот очевидно зашедший в тупик путь развития ИИ и вообще интернета и IT лопнет, как лопнули доткомы, только выиграет."
5,5,Аннотации типов в Python: коротко о главном,OTUS,Цифровые навыки от ведущих экспертов,721.07,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-03-13,"Привет, Хабр! Сегодня рассмотрим, как Python, оставаясь динамически типизированным, может приближаться к строгой типизации. Всё дело в аннотациях типов, которые позволяют явно указывать, какие данные ожидаются в переменных, аргументах функций и возвращаемых значениях. Аннотации сами по себе не заставляют Python проверять типы во время выполнения, но их можно использовать вместе с инструментами статического анализа. В первую очередь мы будем работать с mypy — популярным инструментом, который выявляет ошибки до запуска программы.Для установки:pip install mypyЕсли в коде аннотирована строка, а передано число, mypy заранее предупредит об ошибке. Существует также pyright — более быстрый инструмент от Microsoft, интегрированный в VS Code. Однако сосредоточимся на mypy.Аннотации типов в PythonБазовые аннотации типовМожно аннотировать типы аргументов и возвращаемого значения функции, чтобы сделать код более читаемым и понятным. Например, def add(a: int, b: int) → int: чётко говорит, что оба аргумента должны быть int, а результат тоже будет int. Такие аннотации помогают статическим анализаторам, вроде mypy, находить ошибки до выполнения кода. Начнём с простого примера. Есть функция, которая принимает два числа и возвращает их сумму:def add(a, b):     return a + bКакие типы у a и b? Кто его знает. Может, int, может, float, может, вообще строки, которые кто‑то решил сложить. Теперь добавим аннотации:def add(a: int, b: int) -> int:     return a + bТеперь Python хотя бы предупредит, что если кто‑то попробует передать строку.Попробуем передать не тот тип:add(5, ""10"")  Запускаем mypy:$ mypy script.py error: Argument 2 to ""add"" has incompatible type ""str""; expected ""int""IDE (если у вас PyCharm или VS Code с pylance) начнёт подсказывать, если типы не сходятся.КоллекцииАннотация списков, словарей и других контейнеров позволяет задать не только сам тип структуры, но и тип её элементов. Например, List[int] указывает, что список состоит только из целых чисел. Аналогично можно аннотировать множества (Set[str]), кортежи (Tuple[int, str]) и даже вложенные структуры (Dict[str, List[float]]).Допустим, есть список чисел, и мы хотим их сложить:from typing import List  def sum_numbers(numbers: List[int]) -> int:     return sum(numbers)Окей, List[int] означает «список, в котором только целые числа». Но что, если в списке могут быть float?def sum_numbers(numbers: list[float | int]) -> float:     return sum(numbers)Теперь в numbers можно передавать и int, и float, но не строки.А если в списке могут быть вообще разные типы данных (например, числа и строки), можно использовать Any:from typing import Any  def process_list(data: list[Any]) -> None:     for item in data:         print(f""Обрабатываю {item}"")Но не стоит злоупотреблять Any, потому что это убивает смысл статической типизации.TypedDictОбычные словари в Python — это просто хаотичный набор ключей и значений, и Python никак не проверяет, какие именно ключи там должны быть. Но когда работаешь с JSON, конфигами или API, важно, чтобы IDE знала структуру словаря.TypedDict позволяет явно задать типы ключей и их значений. mypy будет следить, чтобы словарь содержал только нужные ключи.Допустим, есть пользователь с id, name и email:from typing import TypedDict  class User(TypedDict):     id: int     name: str     email: str  def print_user(user: User) -> None:     print(f""ID: {user['id']}, Name: {user['name']}, Email: {user['email']}"")  user = {'id': 1, 'name': 'Roman', 'email': 'roman@example.com'} print_user(user)  # Всё работает!Теперь если какой‑то ключ будет отсутствовать, mypy предупредит нас:user = {'id': 1, 'name': 'Roman'}  # Нет email! print_user(user) error: Missing key ""email"" in TypedDict ""User""Теперь IDE и mypy помогут избежать проблем из‑за отсутствующих ключей.Optional-поля в TypedDictИногда бывает, что не все поля обязательны. Например, у пользователя может не быть email. Тогда указываем NotRequired:from typing import NotRequired  class User(TypedDict):     id: int     name: str     email: NotRequired[str]  # Email может отсутствовать  user: User = {'id': 1, 'name': 'Alice'}  # ✅ Теперь это не ошибка *NotRequired появился в Python 3.11 и в старых версиях находится в typing_extensions ProtocolPython изначально поддерживает утиную типизацию: «если что‑то выглядит как утка и крякает как утка, значит, это утка». Однако без явных интерфейсов это иногда приводит к багам. Protocol из модуля typing позволяет формализовать этот подход и заставить Python проверять, действительно ли объект соответствует нужному интерфейсу.Допустим, есть объекты, которые умеют летать. Можно определить протокол, которому они должны соответствовать:from typing import Protocol  class CanFly(Protocol):     def fly(self) -> str:         ...  class Bird:     def fly(self) -> str:         return ""I am flying!""  class Airplane:     def fly(self) -> str:         return ""Engines running, takeoff!""  def launch(flyer: CanFly) -> None:     print(flyer.fly())  bird = Bird() plane = Airplane()  launch(bird)   # ""I am flying!"" launch(plane)  # ""Engines running, takeoff!""Здесь Bird и Airplane не наследуеются от CanFly, но всё равно работают, потому что соответствуют его структуре.Union и LiteralВ Python иногда бывает необходимо разрешить несколько возможных типов для одной переменной. Например, функция может работать и с int, и с str, и с float. Вместо Any, который снимает все проверки, лучше использовать Union, который ограничивает список допустимых типов, но всё же позволяет некоторую гибкость.Функция, которая принимает число или строку и приводит её к строковому виду:from typing import Union  def to_uppercase(value: Union[int, float, str]) -> str:     return str(value).upper()  print(to_uppercase(100))      # ""100"" print(to_uppercase(3.14))     # ""3.14"" print(to_uppercase(""hello""))  # ""HELLO""Теперь to_uppercase() принимает только int, float или str, но не list или dict.Что будет, если передать неподдерживаемый тип?print(to_uppercase([1, 2, 3]))  # Ошибка. mypy предупредитmypy тут же выдаст предупреждение:error: List[int] is not compatible with expected type ""Union[int, float, str]""Теперь IDE и mypy заранее предотвратят использование неподходящего типа.Бывает, что параметр должен принимать строго определённые значения, например «light» или «dark». В таких случаях Union[str] не спасает, ведь str включает любые строки. Чтобы сузить список разрешённых значений, используется Literal.Функция, которая принимает только «light» или «dark»:from typing import Literal  def set_mode(mode: Literal[""light"", ""dark""]) -> str:     return f""Mode set to {mode}""  set_mode(""light"")  # Окей set_mode(""blue"")   # Ошибка.set_mode(""blue"") приведёт к ошибке ещё до выполнения кода, потому что «blue» не входит в разрешённые значения.А теперь представим, что есть функция, которая может принимать либо число (int | float), либо строку из конкретного набора значений:from typing import Union, Literal  def format_size(size: Union[int, float, Literal[""small"", ""medium"", ""large""]]) -> str:     return f""Selected size: {size}""  print(format_size(42))       # ""Selected size: 42"" print(format_size(""medium"")) # ""Selected size: medium"" print(format_size(""tiny""))   #  Ошибка! ""tiny"" не входит в допустимые значенияТак можно комбинировать свободный ввод Union и строгие ограничения Literal.GenericВ Python часто пишем функции, которые должны работать с разными типами данных, но при этом сохранять строгую типизацию. Вместо того чтобы делать Union[int, str, float], можно использовать TypeVar — параметризированный тип, который позволяет создавать обобщённые (generic) функции.Допустим, есть функция, которая возвращает первый элемент из списка:from typing import TypeVar, List  T = TypeVar(""T"")  # Объявляем универсальный тип  def get_first_item(items: List[T]) -> T:     return items[0]  print(get_first_item([1, 2, 3]))       # int print(get_first_item([""a"", ""b"", ""c""])) # str print(get_first_item([3.14, 2.71]))    # floatТеперь get_first_item() автоматически подстраивается под переданный тип (int, str, float), и mypy при этом проверяет корректность типов.Можно ограничить TypeVar, указав, какие типы разрешены:from typing import TypeVar  Number = TypeVar(""Number"", int, float)  def multiply(value: Number, factor: Number) -> Number:     return value * factor  print(multiply(10, 2))     # int print(multiply(3.5, 2.1))  # float print(multiply(""3"", 2))    # Ошибка! str не разрешёнЗдесь multiply() принимает только int и float, но не str — mypy предупредит об ошибке заранее.Обобщённые классы позволяют избежать дублирования кода:from typing import Generic  T = TypeVar(""T"")  class Box(Generic[T]):     def __init__(self, item: T):         self.item = item      def get_item(self) -> T:         return self.item  int_box = Box(42) str_box = Box(""Hello"")  print(int_box.get_item())  # 42 print(str_box.get_item())  # HelloКласс Box теперь может хранить любой тип, но при этом сохраняет строгую типизацию.Изучить все лучшие практики программирования на Python с нуля можно в рамках специализации ""Python Developer"".Пример примененияЧтобы увидеть, зачем вообще нужны аннотации типов, представим, что есть онлайн‑магазин котиков. В нём можно заказывать котиков, оформлять заказы и получать отчёты.Без аннотацийДопустим, мы пишем функцию для оформления заказа, но не указываем типы:def process_order(cat, quantity, price):     total = quantity * price     return f""Заказ: {quantity}x {cat}, сумма: {total} руб.""На первый взгляд всё нормально, но представьте, что кто‑то вызовет её так:print(process_order(""Британец"", ""2"", 5000))  # Ожидалось 10000 руб.Результат:Заказ: 2x Британец, сумма: 50005000 руб.Вместо умножения 2 * 5000, Python сконкатенировал строки, потому что «2» — это str, а не int.Теперь добавим аннотации типов:def process_order(cat: str, quantity: int, price: int) -> str:     total = quantity * price     return f""Заказ: {quantity}x {cat}, сумма: {total} руб."" Теперь mypy сразу выдаст ошибку, если передать строку вместо числа:error: Argument 2 to ""process_order"" has incompatible type ""str""; expected ""int""Отлично, мы предотвратили потенциальную ошибку ещё до запуска кода.TypedDictТеперь создадим словарь, который будет хранить информацию о заказе.Обычный словарь не защищает нас от ошибок:order = {""cat"": ""Мейн-кун"", ""quantity"": ""3"", ""price"": 7000}  # quantity опять строка!А если мы забудем один из ключей?order = {""cat"": ""Сфинкс"", ""price"": 9000}  # quantity отсутствует!Python никак не проверит, что ключи есть и что их типы правильные.Поэтому делаем так:from typing import TypedDict  class Order(TypedDict):     cat: str     quantity: int     price: int  order: Order = {""cat"": ""Сфинкс"", ""quantity"": 3, ""price"": 9000}  # Всё ок order2: Order = {""cat"": ""Сфинкс"", ""price"": 9000}  # Ошибка: отсутствует ""quantity""Теперь mypy не позволит передавать неполный заказ или указывать неверные типы.ProtocolДопустим, в магазине есть разные методы оплаты: картой, криптовалютой, наличными.Без Protocol нет контроля за методами оплаты:class CardPayment:     def pay(self, amount):         print(f""Оплата картой на сумму {amount} руб."")  class CryptoPayment:     def send_money(self, amount):         print(f""Оплата криптовалютой {amount} USDT"")Если написать функцию, принимающую любой метод оплаты, она не будет знать, какой метод вызывать:def process_payment(payment, amount):     payment.pay(amount)  # А если у объекта нет метода pay()?Если передать CryptoPayment, то всё сломается:process_payment(CryptoPayment(), 5000)  # Ошибка! send_money() вместо pay()С Protocol:from typing import Protocol  class PaymentMethod(Protocol):     def pay(self, amount: int) -> None:         """"""Все платежные методы должны реализовывать pay(amount: int).""""""         ...  class CardPayment:     def pay(self, amount: int) -> None:         print(f""Оплата картой на сумму {amount} руб."")  class CryptoPayment:     def pay(self, amount: int) -> None:         print(f""Оплата криптовалютой {amount} USDT"")  def process_payment(payment: PaymentMethod, amount: int) -> None:     payment.pay(amount)  process_payment(CardPayment(), 5000)   # Всё работает process_payment(CryptoPayment(), 100)  # Всё работаетТеперь любая платежная система обязана иметь метод pay(), иначе mypy не пропустит код.Union и LiteralДопустим, есть система скидок, которая может принимать:Процент (float).Фиксированную сумму (int).Готовые предустановленные значения (""low"", ""medium"", ""high"").Без Union и Literal:def apply_discount(discount):     if isinstance(discount, str):         if discount == ""low"":             return 5         elif discount == ""medium"":             return 10         elif discount == ""high"":             return 20     elif isinstance(discount, (int, float)):         return discount     else:         raise ValueError(""Некорректная скидка"")Нужно вручную проверять типы и выбрасывать ошибки.С Union и Literal всё строго:from typing import Union, Literal  def apply_discount(discount: Union[int, float, Literal[""low"", ""medium"", ""high""]]) -> float:     if discount == ""low"":         return 5     elif discount == ""medium"":         return 10     elif discount == ""high"":         return 20     return float(discount)Теперь IDE подскажет, какие значения допустимы, а mypy не позволит передать что‑то не то.Generic: универсальные классы для товаровДопустим, есть разные категории товаров: котики, игрушки, корм.Можно создать класс для хранения товара:class Product:     def __init__(self, name, price):         self.name = name         self.price = price Но если нужно, чтобы товар мог быть разного типа (например, цифровой или физический), приходится жонглировать Any.С Generic можно сделать строгую типизацию товаров:from typing import Generic, TypeVar, Dict, Union  T = TypeVar(""T"", bound=Union[str, Dict[str, str]])  class Product(Generic[T]):     def __init__(self, name: str, price: int, details: T):         self.name = name         self.price = price         self.details = details  # Например, это может быть вес, цвет или формат  cat_product = Product(""Сибирский кот"", 15000, {""weight"": ""4 кг""}) digital_product = Product(""Курс по уходу за котами"", 5000, ""Видео"")Теперь Product может хранить любые типы данных, но при этом тип details всегда остаётся предсказуемым.Больше про языки программирования эксперты OTUS рассказывают в рамках практических онлайн-курсов. С полным каталогом курсов можно ознакомиться по ссылке."
6,6,Бесплатная система учета торговли и управления интернет магазином Trinion Торговля,Тринион,Бизнес консультант Кинзябулатов Рамиль.,36.63,"Веб-разработка, Программное обеспечение, Консалтинг и поддержка",2025-03-13,"Всем привет. Меня зовут Рамиль Кинзябулатов, я – опытный бизнес-консультант, автор книг о функциональном и процессном моделировании, а также многочисленных статей по тематике моделирования и автоматизации бизнеса. И сейчас я хочу познакомить вас с решением, созданным на основе моих знаний, опыта и желания выстроить правильную модель торгового предприятия – система Trinion Торговля.Пожалуйста имейте в виду что хотя это корпоративный хаб, система бесплатная и это можно сказать мой pet проект! То есть все сделано и делается на мои деньги и с моим участием. Так что не судите строго.Как я пришел к необходимости реализации Trinion ТорговляЯ регулярно помогаю своим клиентам решать вопросы, связанные с автоматизацией и внедрением систем, которые помогают решать задачи, связанные с самыми разными проблемами в бизнесе. Уже более 17 лет я внедряю и настраиваю CRM, ERP, учетные системы сотням российских и зарубежных клиентов. Я работал с ZOHO, Bitrix, 1С, изучал и настраивал другие программные продукты.Но везде я сталкивался с тем, что в системе нет явного и четко заданного бизнес-процесса продажи. Более того, чаще всего процесс работы с клиентом выполняется в CRM-системе, работа со складом, отгрузки и взаиморасчеты с клиентами – в учетной системе. Интеграции помогают автоматизировать процесс, но выстроить четкую и однозначную модель продаж при этом крайне сложно.Кроме того, даже внутри популярных учетных систем, в том числе, CRM-систем нет явного четко построенного бизнес-процесса продажи. С одной стороны, они предлагают много возможностей для построения такого процесса, с другой, человек, который работает в такой системе не видит четкой цели и поэтапного пути ее достижения.Еще одна серьезная проблема, с которой сталкивается каждый, кто занимается автоматизацией – это полностью или частично закрытый программный код. Что бы вы ни внедряли, каждый раз вы сталкиваетесь с тем, что некоторые доработки реализовать невозможно, другие становятся крайне сложными и трудоемкими. Как итог, приходится что-то придумывать, чтобы реализовать бизнес процесс. А сроки, сложность и стоимость внедрения при этом возрастают.В результате я задумался о создании собственной информационной системы, которая:Объединит в себе все этапы торговли: работу с лидами, ведение сделок, работу с поставщиками, товарный и финансовый учет. Будет реализована на основе универсальной модели торгового предприятия с четко выстроенными бизнес-процессами для каждой из основных функций.Получит открытый код для того, чтобы на ее основе можно было создавать самые сложные и специфические решения.Будет проста, надежна и нетребовательна к ресурсам.Последние несколько лет я сам очень активно работаю с CMS DRUPAL. И, когда принимал решение о том, каким образом реализовать свою идею, решил не «изобретать велосипед», а создать необходимый функционал, как дополнение для DRUPAL 9.Почему именно DRUPAL:Надежный и мощный движок с очень широкими возможностями.Полностью открытый код и большое сообщество.Невысокие требования к аппаратным ресурсам (серверы, хостинг).Бесплатная информационная система, что также является плюсом.Кроме того, я с самого начала ориентировался на работу в режиме онлайн, так как в наше время возможность работать удаленно крайне актуальна. И, конечно, задумывался об интеграции с сайтом, а при использовании DRUPAL этих проблем точно не возникнет.Что такое Trinion ТорговляВ основе системы Trinion Торговля лежит функциональная модель, которую я описал в статье «УФМТП. Универсальная функциональная модель торгового предприятия в нотации IDEF0». Это универсальная функциональная модель торгового предприятия, которую я разработал по результатам накопленного опыта работы с клиентами в РФ и за рубежом. Больше о документообороте вы можете узнать из статьи Единственное отличие системы, которая реализована на данный момент, от описанной в статье модели, – отсутствие в системе маркетинговой функции «Привлечь покупателя». Разные компании используют очень разные инструменты маркетинга. А потому было принято решение оставить этот этап за рамками системы. Но благодаря открытому исходному коду вы всегда сможете добавить необходимые функции или заказать у нас необходимые доработки.Таким образом, работа в Trinion Торговле строится следующим образом:ЛидУже реализована очень простое для пользователей решение по автоматизации поступления лидов с сайта. Если вы пользуетесь сайтом на DRUPAL, модуль лидогенерации просто будет собирать информацию со всех форм обратной связи и создавать на их основе лиды с указанием, из какой формы получен запрос.Также реализована Телефония на основе Asterisk. Система определяет, кто звонил, кто ответственный, фиксирует записи разговоров и пропущенные звонки.Другие виды лидов вы можете получать после выполнения интеграции с нужной вам системой с использованием модуля интеграции DRUPAL.Карточки Компании и КонтактПосле первичной обработки лида сотрудник создает на его основе карточки Компании и Контактов.Компания – это юридическое лицо, на имя которого будут создаваться коммерческие предложения и счета.Контакты – это контактные лица, которые привязаны к Компании. Считаю такой подход наиболее практичным, так как одна организация может иметь много контактных лиц, и удобнее всего иметь возможность для каждого из них иметь свою карточку. Впрочем, краткие контактные данные вы также можете указывать в карточке Компании.СделкаКаждая Сделка имеет свои этапы, по которым будет двигаться ответственный сотрудник. В CRM-системе уже реализован универсальный бизнес-процесс продажи, который я также разработал на основе многолетнего опыта. На основе Сделки вы можете создавать Коммерческие предложения и Счета.Заказ покупателяЭто уже согласованный заказ покупателя, на основе которого можно сформировать:СчетЗаказ поставщикуДокументы на отгрузку товара.Также на его основе можно резервировать товары на складе и создавать заказы поставщикам.Счет покупателюКак я уже упоминал, счет покупателю можно создавать из Сделки или на основе Заказа покупателя. При этом система использует реквизиты из справочника Клиентов, а сам Счет можно быстро отправить покупателю, сохранить в формате PDF. Кроме того, он будет виден в качестве документа, «привязанного» к Сделке и Заказу покупателя.Заказ поставщикуЗаказ поставщику – это документ, который создает сотрудник, ответственный за закупку. При этом он анализирует остатки по складу и заказы покупателе.Счет от поставщикаВсе счета от поставщиков вносятся в Trinion Торговлю вручную или при помощи автоматизированного обмена данными с поставщиками. Они также отображаются в отчетах по взаиморасчетам с поставщиками. Таким образом, при оплате поставщику вы указываете в системе, какие счета эта оплата закрывает полностью или частично.Также на основе Счета от поставщика, если в нем указаны все товары и их количества, можно создавать складские приходные документы.Отгрузка В системе также уже реализованы следующии функции:Загрузка из Клиент Банка Интеграция с АТОЛ HVRИнтеграция с почтой История изменения документовШаблоны документовИнтернациализацияИнтернет торговляи многое другое Trinion Торговля – бесплатное решениеЯ уже говорил, что создавал свою систему на основе бесплатной CMS DRUPAL с полностью открытым кодом.  И считаю, что мое решение также должно оставаться таким же – с открытым кодом и возможностью скачать, установить и, при желании, доработать ее без каких-либо ограничений.Что для этого нужно:Скачайте на официальном сайте и установите CMS DRUPAL 9.Все компоненты Trinion Торговля устанавливайте в качестве модулей в системе DRUPAL.В большинстве случаев настройки требуются минимальные:Внесение первичных данных.Настройка прав пользователей.Интеграция с телефонией Asterisk (для реализации внутренней телефонии).Интеграция с другими внешними сервисами (в случае необходимости).Система уже готова к работе, так как бизнес-процесс работы с клиентом в CRM и бизнес-процесс продажи уже реализованы.При желании, вы можете также вносить любые изменения в работу системы, создавать дополнительные отчеты (в том числе при помощи инструментов в панели администратора DRUPAL) или создавать на основе моего решения нечто свое, расширенное, дополненное, видоизмененное.Trinion Торговля: о системных требованияхМинимальные требования к ресурсам и программной среде вы можете изучить на официальном сайте DRUPAL 9.Практика показала, что Trinion Торговля является очень «легкой» с точки зрения потребления ресурсов надстройкой. Так, один из наших клиентов успешно пользуется этой системой, которая находится на одном хостинге с сайтом, посещаемость которого около 300-400 человек в день.При этом используется не самый мощный 2-процессорный сервер с 3 Гб оперативной памяти. В системе постоянно работают сотрудники. Как сайт, так и система Trinion Торговля работают корректно. Увеличение мощностей в ближайшее время не планируется.К вопросу монетизацииЕсли вы являетесь DRUPAL-специалистом, у вас не возникнет проблем с внедрением Trinion Торговли. Более того, всегда с благодарностью выслушаю мнение, добрый совет или подсказку.  Также вы можете спокойно внедрять решение, если у вас есть собственный специалист без каких-либо вопросов или претензий с моей стороны в любом обозримом будущем.Но все же, любая информационная система – это большой труд команды людей. А потому, хотя сама система остается бесплатной, я также всегда готов предложить дополнительные услуги и не только.1.      SaaS-версия: оформляйте подписку и приступайте к работеДоступ к информационным системам по подписке на серверах разработчика – популярный сегодня вариант. А потому я предлагаю всем желающим воспользоваться такой возможностью.Почему это выгодно:Вам не нужно покупать и оплачивать сервер (хостинг), заниматься установкой и обслуживанием. Все это наша команда берет на себя.Вы всегда будете получать актуальные обновления без скачивания, установки, настроек своими силами.Все необходимые настройки для вашей работы наша команда берет также на себя.Вы будете пользоваться приоритетной поддержкой в режиме 24/7.Узнать подробнее о тарифах и стоимости настройки вы можете здесь.2.      PaaS-версия: настроим все необходимое на ваших серверахЕсли ваша организация избегает «облачных» сервисов, расположенных на чужих серверах, но не имеет собственных drupal-специалистов, это решение – для вас.Вы предоставляете нашим специалистам доступ к вашему пространству для установки и настройки необходимых компонентов.Описываете свои потребности и пожелания.Мы устанавливаем, настраиваем, дорабатываем.Но при этом важно понимать, что это – единоразовая услуга, а не постоянное обслуживание. Обновления не будут автоматически устанавливаться, да и в случае, если ваши специалисты начнут вносить изменения, ответственность за последствия также несете вы.Впрочем, вопросы постоянного обслуживания и поддержки также можно обсудить. Пишите!3.      Доработки под ваши потребностиПри разработке системы я использовал универсальную модель и бизнес-процессы продажи. Но, возможно, вы хотите работать совсем по-другому или нуждаетесь в важных для вашего типа бизнеса дополнительных этапах и инструментах.Вы всегда можете заказать индивидуальное решение под ваши особые потребности.Также мне будет интересно мнение клиентов ио пользователей о том, чего еще не хватает в Trinion Торговле. Интересные доработки будут постепенно внедряться. И все обновления, как и сейчас, будут доступны для бесплатного скачивания и установки без каких-либо ограничений."
7,7,Null-safety в Spring приложении с JSpecify и NullAway,Spring АйО,Компания,71.54,Программное обеспечение,2025-03-13,"Новый перевод от команды Spring АйО расскажет о проблемах, вызываемых неожиданным появлением NullPointerException в продакшен, о способах борьбы с этой проблемой, над которыми работает команда Java и о скором выходе новых решений, которые помогут разработчикам навсегда избавиться от этого кошмара.  Первоначальное добавление поддержки null-safety в Spring случилось еще в далеком 2017-м при выпуске Spring Framework 5.0. В 2025 у этой истории произошел новый виток эволюции, чтобы дать разработчикам на Spring больше ценных возможностей, как в Java, так и в Kotlin. Но прежде чем мы посмотрим глубже на те изменения, над которыми сейчас идет работа, позвольте объяснить вам, зачем это делается и каких преимуществ от этих изменений следует ожидать. Какую проблему мы пытаемся решить?Давайте возьмем конкретный пример и предположим, что мы используем библиотеку, предоставляющую интерфейс TokenExtractor, определяемый следующим образом:interface TokenExtractor {          /**      * Extract a token from a {@link String}.      * @param input the input to process      * @return the extracted token     */     String extractToken(String input); }Если по какой-то причине реализация возвращает null, доступ к null-ссылке в  token.length() (как в примере ниже) вызывает NullPointerException, который обычно случается в рантайме, вызывая HTTP ответ с кодом статуса 500 Internal Server Error. package com.example;  String token = extractor.extractToken(""...""); System.out.println(""The token has a length of "" + token.length());Поскольку эта ошибка может появиться только в некоторых случаях (например, с очень специфическими входными данными, которые не были протестированы), она может быть обнаружена очень поздно в продакшен, вызывая гнев у конечного пользователя или даже не позволяя завершить транзакцию, тем самым снижая доход вашей компании, нанося вред вашему бренду и приводя к дополнительным издержкам и расходам на исправление.Ошибки такого рода настолько часты, что сам изобретатель null-ссылок, Тони Хоар, извинился за то, что создал их и назвал их “моей ошибкой на миллиард долларов”. Но, как прекрасно демонстрирует Kotlin, корень проблемы лежит не в самих null-ссылках, а в том, что они не заданы в явном виде в системе типов.В Java при использовании непримитивных типов nullness не определяется. Параметр может принимать или не принимать null-аргумент. Возвращаемое значение может быть nullable или non-null. Вы ничего об этом не знаете и вынуждены полагаться на чтение Javadoc или анализ реализации, чтобы это выяснить. Но даже если автор библиотеки напишет к ней документацию, обычно единообразие для всех API отсутствует, автоматизированных проверок нет, и вы не можете на самом деле знать, является ли параметр или возвращаемое значение на самом деле non-null или автор просто забыл обозначить его в документации как nullable. Такой подход по определению приводит к ошибкам, и у вас нет нормального способа исправить эту ситуацию. Комментарий от редакции Spring АйОПод термином nullness понимается возможность принимать значение null. JSpecify и NullAwayРешение этой коварной проблемы состоит в том, чтобы сделать nullness используемых типов явной во всех API и создавать соответствующие автоматические проверки на единообразие в нашей IDE и в наших сборках. Поскольку Java пока не предоставляет null-restricted и nullable типы, нам нужен способ задавать nullness любой Spring API.В 2017 мы решили ввести  Spring аннотации для nullability, которые были надстроены над семантикой и аннотациями JSR 305 (находящийся в спячке, но довольно распространенный JSR). Это решение было далеко от идеала в силу технических ограничений, неясного статуса, отсутствия нормальной спецификации, но с прагматической точки зрения это был самый лучший выбор, доступный нам на тот момент. В дальнейшем команда Spring присоединилась к рабочей группе, возглавляемой Google и включающей в себя несколько компаний, вовлеченных в экосистему JVM, таких как JetBrains, Oracle, Uber, VMware/Broadcom и другие, чтобы разработать и внести свой вклад в создание лучшего решения, не привязанного к определенному инструменты верификации. Так начинался JSpecify.Недопонимание, относящееся к nullness, с которым я часто сталкиваюсь, состоит в том, что сначала люди думают, что это понятие касается в основном выбора одного варианта из нескольких вариантов @Nullable, но это лишь верхушка айсберга. Эти аннотации должны предоставляться вместе с правильными определениями, поддержкой инструментов, и т.д. Коллективно согласиться на общую спецификацию nullness — это и есть причина того, почему JSpecify потребовалось несколько лет, чтобы достичь 1.0.JSpecify — это набор аннотаций, спецификаций и документации, разработанной для обеспечения null-безопасности Java приложений и библиотек в IDE или во время компиляции, благодаря таким инструментам, как NullAway.Главное — понять, что по умолчанию nullness используемых типов в Java не задается, и что случаи использования non-null типов встречаются гораздо чаще, чем случаи использования nullable. Чтобы кодовая база оставалось читаемой, мы обычно задаем по умолчанию, что в определенной области видимости типы используются как non-null, если они не помечены как  nullable. Именно такова цель @NullMarked, что обычно устанавливается на уровне пакета через файл package-info.java, например:@NullMarked package org.example;  import org.jspecify.annotations.NullMarked;Эта аннотация меняет nullness по умолчанию для используемых типов с ""unspecified"" (настройка по умолчанию в Java) на ""non-null"" (настройка по умолчанию для JSpecify @NullMarked). Теперь мы можем сделать наши API и документацию лучше. package org.example;  interface TokenExtractor {          /**      * Extract a token from a {@link String}.      * @param input the input to process      * @return the extracted token or {@code null} if not found     */     @Nullable String extractToken(String input); }Теперь IDE должным образом предупреждает нас о потенциальном NullPointerException при вызове метода на возвращаемом значении, она также пожаловалась бы, если бы мы передавали аргумент null, поскольку для этого null-маркированного кода настройка по умолчанию установлена в non-null. В то время как мы могли бы проигнорировать или пропустить эти предупреждения от IDE, единообразие этих nullness аннотаций по всей кодовой базе может быть проверена во время сборки, если сконфигурировать NullAway таким образом, чтобы он выбрасывал ошибки. Если выявлено несоответствие, сборка обрывается, естественным образом предотвращая создание небезопасных по null API (за исключением неаннотированных типов, привнесенных зависимостями от третьих сторон). > Task :compileJava FAILED /Users/sdeleuze/workspace/jspecify-nullway-demo/src/main/java/org/example/Main.java:7: error: [NullAway] dereferenced expression token is @Nullable                 System.out.println(""The token has a length of "" + token.length());                                                                        ^     (see http://t.uber.com/nullaway ) 1 errorСм. https://github.com/sdeleuze/jspecify-nullway-demo, если хотите попробовать ее самостоятельно или посмотреть на пример соответствующей Gradle сборки.Эти ошибки от nullness заставляют разработчика разбираться с null-ссылками самостоятельно, если он использует эти API:String token = extractor.extractToken(""...""); if (token == null) {     System.out.println(""No token found"");  } else {     System.out.println(""The token has a length of "" + token.length()); }Вы можете возразить, что Optional<T> от Java был создан для того, чтобы отображать присутствие или отсутствие значения. Но на практике Optional<T> не годится для большинства use case-ов, поскольку приводит к большому перерасходу ресурсов в рантайме (по крайней мере пока не появятся value-классы от проекта Valhalla), он увеличивает сложность кода и API, плохо подходит для параметров и ломает сигнатуры существующих API. Новый уровень null-safety в грядущем мажорном релизе SpringВ Spring Framework 7 (сейчас находится на стадии milestone) вся кодовая база уже переведена на JSpecify. Вы можете найти соответствующую документацию здесь. Ключевое улучшение, по сравнению с предыдущей инкарнацией состоит в том, что nullness теперь задается также для массивов и vararg элементов, а также для дженерик типов. Это очень хорошо для разработчиков на Java, но также и для разработчиков на Kotlin, которые увидят идиоматические, null-безопасные API, как если бы Spring был написан на Kotlin.Но самое большое улучшение состоит в том, что вся команда Spring работает над тем, чтобы в первом приближении предоставить null-безопасные API по всему портфолио Spring с соответствующими проверками во время сборки, чтобы гарантировать единообразие. Эта работа находится в процессе, и пока мы не можем дать никаких обещаний, что мы сможем закончить это к выходу Spring Boot 4.0 в ноябре, но мы попытаемся подобраться как можно ближе к полному покрытию. Project Reactor тоже будет охвачен. Когда Spring Boot 4 выйдет в свет и будет использоваться в ваших приложениях, особенно если вы включите эти проверки на nullness также и на уровне приложения, риск появления NullPointerException в продакшен будет очень серьезно снижен, если не устранен полностью, поскольку возможность его появления будет связана только с библиотеками от третьих сторон. Явно задавая, где могут появляться null-ссылки, правильно обрабатывая эти пути в коде и вводя автоматизированные проверки, мы превращаем “ошибку на миллиард долларов” в абстракцию, которая ничего не будет стоить, но позволить полностью выразить потенциальное отсутствие значения, значительно повышая безопасность Spring приложений.Присоединяйтесь к русскоязычному сообществу разработчиков на Spring Boot в телеграм — Spring АйО, чтобы быть в курсе последних новостей из мира разработки на Spring Boot и всего, что с ним связано."
8,8,Использование JSONB-полей вместо EAV в PostgreSQL,СИГМА,Разработка и внедрение ИТ-решений в энергетике/ЖКХ,4.27,"Программное обеспечение, Консалтинг и поддержка, Мобильные технологии",2025-03-13,"На одной из конференций PGConf мы обнаружили, что комьюнити с большой опаской относится к использованию JSONB-полей в своих системах. Интернет при этом не столь радикален. Наша же команда в это время вовсю использует JSONB у себя в проекте. Мы решили поделиться нашим вполне успешным кейсом в реальной нагруженной системе с сотнями миллионов строк в таблицах, где эти поля используются.Проект – CRM-система для взаимодействия с клиентами энергосбытовых компаний. Она используется для множества каналов коммуникации с клиентами, от звонков на горячую линию и переписки в мессенджерах до визитов в офисы и почтовых рассылок. Архитектурно система спроектирована так, что способна сопровождать оказание практически любых видов услуг, но исторически сосредоточена на электроэнергетике.У многих бизнес-объектов системы есть разновидности со специфичными только для них атрибутами. Приведем в пример клиентов. Они бывают физическими (ФЛ) или юридическими лицами (ЮЛ), а могут быть и более экзотическими, такими как «лид» или «контактное лицо». У каждого подвида разный набор атрибутов. Скажем, у ФЛ – ФИО и паспорт, у ЮЛ – название юрлица и ИНН. При этом часть атрибутов может совпадать, например, адрес и email. Как же их подружить? Рассмотрим, как это сделали мы, и сравним с классическим подходом EAV.Мы исходим из того, что нам важно сохранить основные объекты в одной таблице, поэтому не рассматриваем случаи, когда мы еще и создаем отдельно таблицы «Клиент ФЛ» и «Клиент ЮЛ». Тогда в классической схеме EAV наш MVP будет выглядеть примерно так:Пока у нас минимум контроля над данными. Мы можем только проверить, что к клиенту не добавили несвойственный ему атрибут, и соответствие типу данных. Тут есть нюансы:Тип клиента (ФЛ, ЮЛ) будет находиться в «Сущности».Создаем таблицу «Атрибут сущности», потому что некоторые атрибуты могут принадлежать к нескольким типам клиента. Это также позволит контролировать принадлежность атрибута к «Сущности» при сохранении значения.Поле «Значение атрибута» вынуждено быть текстовым (или json, но мы же вроде не хотим иметь дела с json :) ). Это само по себе несколько осложняет производительную работу запросов, а ведь основной аргумент против jsonb — проблемы поиска на больших масштабах. Здесь есть и другое решение:!   Можно под каждый тип данных сделать отдельное поле и заполнять в шахматном порядке. Только у самого атрибута надо указывать, к какому типу данных он относится. Но это порождает довольно большие сложности с построением запросов.Теперь рассмотрим нашу схему с использованием jsonb.Отличия пока минимальные.Значение поля «Параметры» – это всегда JSON-объект, который строится по принципу: {“Название атрибута”: <Значение атрибута>, …}. Например, {“ Фамилия”: “Иванов”, “Возраст”: 28}. Понятно, что для именования атрибутов мы используем латинские символы, для этого у нас в «Атрибуте» есть отдельная колонка, но тут для простоты мы будем использовать просто колонку с «Названием».Давайте теперь сразу накидаем некоторые пункты по развитию этой простой структуры, которые не касаются хранения в JSONB и поэтому будут одинаково выглядеть для обоих решений, чтобы потом уже сосредоточиться на различиях. Итак:1. Сделаем поля с ограниченным выбором из списка. Любой комбобокс, например, пол.2. Сделаем больше ограничений/проверок содержимого поля. В качестве примера возьмем поле email. Проверим, что в нем хранится именно электронная почта, а не произвольный текст.3. Сделаем древовидную систему сущностей, чтобы наследовать общие атрибуты. Так, сущность «Клиент» делится на подвиды «ФЛ» и «ЮЛ». У «Клиента» при этом могут быть свои атрибуты, например, «Номер договора», который в итоге будет использован и для ФЛ, и для ЮЛ. 4. Добавим возможность настраивать обязательность атрибутов и дефолтное значение.Решение.1. Для комбобоксов создаем таблицы для хранения справочных данных типа «ключ-значение» и добавляем в таблицу «Атрибут» ссылку на этот справочник.Если надо ссылаться на внешние справочники, это делается примерно так же, просто настройка становится более громоздкой. Мы не будем тут усложнять. «Тип данных» можно поменять на справочник, а можно и оставить целочисленным, если у справочника целочисленный ИД.В конечном счете, хранить на «Клиенте» мы будем именно ИД значения, и для интерфейса находить по нему само значение и показывать.2. Для дополнительных ограничений делаем отдельную таблицу «Домен». На нее опять же ссылаемся из «Атрибута». В «Домене» храним всякие настройки: размерность, маску ввода, регулярное выражение для проверки и т.п. Можно даже «Тип данных» туда перенести при желании, но это может быть контрпродуктивно.Тут сразу сделаем следующий шаг и поместим в эту таблицу указание на функцию кастомизированной проверки/преобразования. Так мы расширим наши возможности до безграничных. Если нам вдруг понадобится какая-то проверка, не укладывающаяся ни в какие стандартные рамки, мы просто напишем типовую функцию и сошлемся на нее. Более того, мы еще и скорректировать можем что-то, например, зашифровать содержимое поля.3. Добавляем в «Сущность» поле «ИД родителя». Делаем представление, которое будет по ИД сущности возвращать все атрибуты, включая родительские. Делаем проверку, что по всей цепочке атрибуты не повторяются.4. В таблицу «Атрибут сущности» добавляем поля «Признак обязательности» и «Значение по умолчанию». Выглядеть это будет так для обоих решений:А вот со следующими пунктами у EAV-модели начинаются сложности. Рассмотрим фичи, влияющие на выбранный способ хранения.1. Привязка сущностей к другим объектам системы.2. Поля массивов. Чекбоксы с выбором нескольких значений, например, категории рассылок, на которые клиент дал согласие. Или просто массивы, не обязательно списочные.3. Составные поля. Например, адрес.Решение!1. В целом по этому пункту уже все готово и различий с EAV тоже особых нет:Просто добавляем поле «Параметры» другому объекту системы и пользуемся. Однако, если мы хотим не допустить путаницы и не дать записывать в объекты сущности, не относящиеся к ним, добавим в таблицу «Сущность» указание на таблицу, для которой она создана.В случае EAV мы добавляем не поле, а целую таблицу со значениями и всеми ключами, это в целом плюс-минус то же самое. Разница только в том, что при структурных доработках поле JSONB мы, вероятно, даже трогать не будем, а таблицы со значениями придется каскадно править по всем задействованным объектам, кратно увеличивая стоимость их обслуживания. Почему бы не создать одну таблицу под все объекты? Потому что она уже хранит бизнес-данные из разных модулей, которые могут быть разнесены по базам и вообще неправильно хранить бизнес-данные вне области влияния модуля, ответственного за бизнес-объект.2. Чтобы сделать массив в таблицу «Атрибут сущности», добавим поле «Признак массива». Именно сюда, потому что одни и те же атрибуты могут быть массивами в одних сущностях и не быть ими в других.Теперь такое поле в «Параметрах» будет выглядеть так: {“Атрибут 1”: [<Значение 1>, …, <Значение N>], …}Для EAV нам, помимо признака, нужно будет расширять первичный ключ у значения. Добавим поле «Порядковый номер». Если оно заполнено, это массив. Тут у EAV даже есть небольшое преимущество, потому что любой массив будет с идентификатором значений, а, значит, можно править их по отдельности. JSON этого не позволяет, там редактирование будет состоять из удаления старого значения и вставки нового, но все можно обмазать синтаксическим сахаром. 3. Чтобы сделать вложенные объекты, заведем новый «Тип данных» — «Сущность». И для этого типа данных мы в «Атрибуте» прописываем ИД сущности, на которую он будет ссылаться. Теперь, если продолжать пример с «Адресом», у нас в «Сущностях» помимо «ФЛ» и «ЮЛ»появится еще и «Адрес», который содержит собственный набор атрибутов внутри.Клиент с таким атрибутом в JSON будет заполнен так: {“Адрес”: {“Полный адрес”: “г. Москва, ул. Ленина д. 1”, “Населенный пункт”: “Москва”, …}, …}Для EAV мы вновь вынуждены расширять первичный ключ таблицы значений. Добавим поле «ИД родителя», если оно заполнено, это составной атрибут.Как вы уже догадались, можно объединить эти свойства и хранить по клиенту вложенный массив из составных объектов. Более того, у нас уже есть функционал кастомных проверок в «Домене», и он вполне пригодится, чтобы обрабатывать и такие атрибуты, как массив составных объектов. Так как это просто атрибут, мы сначала отправим на проверку его содержимое целиком, а затем еще и рекурсивно пробежимся по вложенным атрибутам по отдельности.Помимо того, что EAV-модель уже показывает большую сложность по сравнению с хранением в JSONB, мы еще не обращали внимание на событие сохранения данных в БД.В случае с JSON-ом нет ничего проще для интерфейса, чем собрать всю заполненную анкету с данными в один большой JSON и так его и пульнуть в сервис сохранения. Нам остается только его проверить на соответствие метамодели данных. При этом сколько там будет уровней вложенности атрибутов друг в друга, значения не имеет. Вся проверка будет рекурсивной.  Для EAV, если с фронта прилетит JSON, его нужно будет распарсить, точно так же проверить и сохранить атомарно. Если же анкету будут присылать частями или отдельными элементами, сложно представить, как заставить это работать. У EAV тут только одно преимущество — «Порядковый номер». Он позволяет работать точечно с элементами массива.Посмотрим, как будет выглядеть итоговая схема обоих решений: JSONEAVПо итогу мы имеем метамодель данных, которая способна организовать совершенно произвольный набор данных на любой вкус, практически без дальнейшего участия разработки в построении бизнес-модели. Она настолько хорошо работает, что в системе даже есть интерфейс для управления ею, чтобы заказчик мог самостоятельно заводить какие ему угодно атрибуты и сущности.  Что еще мы не сделали, но могли бы для полноты функциональности:Можно перенести связь таблицы значений с «Атрибута» на «Атрибут сущности», чтобы появилась возможность один атрибут привязывать под разными именами к одной сущности. Например, адрес прописки и проживания. И тут мы бы снова увидели, что для EAV сделать это на порядок сложнее, так как пришлось бы переделать все таблицы значений. А JSONB-поле даже трогать бы не пришлось, просто существующим связям прописать те же имена, что и в атрибуте. Можно сделать возможность идентификации составного атрибута. Если у нас составной массив, это по сути вложенная таблица. Хотелось бы понимать, какой набор атрибутов в ней уникален. Для этого в «Атрибуте сущности» необходимо добавить «Признак ключа». Тут наоборот, у EAV преимущество, потому что ее поля «ИД атрибута» + «ИД родителя» + «Порядковый номер» уже создают в рамках клиента уникальную строку. Поэтому данному решению уже вообще ничего не надо для идентификации нужного элемента массива.В завершение — немного о производительности. Мы никогда не делали вариант с EAV, но реализация с JSONB работает у нас на нагруженной системе. JSON-поля хранят бизнес-данные всех основных таблиц, таких как «Клиенты», «Услуги», «Заказы».В системе более 13 миллионов клиентовИ более 300 миллионов оказанных услугПоиск произвольного набора клиентов по строчным данным, например, по фамилии или по адресу проживания укладывается в 40 секунд. При этом он успевает поискать и у нас в базе, и по API в интегрированных системах.Все это работает благодаря gin индексу на json-полях.Однако для контекстного поиска по адресам, которые не всегда точно и в правильной последовательности передаются с фронта, разработан особый gin-индекс. Он формируется непосредственно по адресным атрибутам из JSON-а, которые преобразовываются в tsvector с весами.Для построения описанной системы мы, конечно, приложили массу усилий. Но если в подобной гибкости системы нет необходимости, или вы не готовы много в это инвестировать, возможно, вам и не нужен EAV? Некоторые разработчики начинают строить EAV-модели как, наоборот,  простое решение, при котором не нужно долго думать над дизайном данных. Однако это путь, который точно не ведет к упрощению. Если же вы решили делать гибкий дизайн, возможно, наша статья показала, что JSONB-поля работают для него не хуже, а поддерживаются существенно легче классического EAV."
9,9,"О книге «Разработка игр на Unity, 4-е изд.»",Издательский дом «Питер»,Компания,206.39,"СМИ, Электронная коммерция, Производство мультимедиа-контента",2025-03-13,"Привет, Хаброжители!  Unity — это не только инструмент для создания игр. Это мощная платформа, которая открывает двери в мир интерактивных приложений: от простых мобильных программ до сложных образовательных и обучающих решений, известных как Serious Gaming. В таких проектах активно применяются передовые технологии, такие как дополненная и виртуальная реальность. Поэтому, хотя основное внимание в книге уделяется разработке игр, ваше обучение открывает путь ко множеству других специализаций. Вы сможете не только создавать увлекательные игры, но и разрабатывать инновационные приложения, которые меняют подход к обучению, развлечениям и взаимодействию с цифровым миром.  Готовы вывести свои навыки работы на Unity на новый уровень? Представляем вашему вниманию долгожданное четвертое издание книги «Разработка игр на Unity». Это полноценное руководство, которое поможет раскрыть весь потенциал Unity.   Что внутри книги?  Каждая глава книги построена так, чтобы читатель мог не просто копировать код, а полноценно разработать свою игру, начиная с базовых концепций и заканчивая сложными техниками. Авторы уделяют внимание новейшим возможностям Unity, включая дополненную реальность и оптимизацию производительности с использованием стека технологий, ориентированных на данные (DOTS).   Читатели пройдут путь от создания сцен до бесшовной интеграции ресурсов, освоят программирование на C# и визуальную разработку скриптов. Книга также охватывает реализацию динамических элементов геймплея, таких как системы движения и здоровья, создание игрового ИИ с использованием конечных автоматов, работу с визуальными эффектами через материалы, шейдеры, текстуры и системы частиц. Отдельное внимание уделено оптимизации производительности с помощью профилировщика и методам отладки, чтобы довести игру до состояния отточенного конечного продукта.   Об авторах Николас Алехандро Борромео — старший консультант по разработке программного обеспечения в Unity Technologies и сертифицированный инструктор Unity. С 2008 года он работает с Unity, а с 2012 года преподает его в университетах и учебных заведениях по всему миру. Его опыт помогает клиентам Unity реализовывать сложные проекты.  Хуан Габриэль Гомила Салас — генеральный директор Frogames Formación, платформы электронного обучения, которая выпустила около 200 курсов для более чем полумиллиона студентов по всему миру. Его экспертиза в образовательных технологиях делает книгу доступной и полезной для широкой аудитории.  Для кого эта «Разработка игр на Unity»?  Книга предназначена для широкого круга читателей: от новичков, которые только начинают свой путь в разработке игр, до опытных профессионалов, желающих освоить новейшие возможности Unity. Если у читателя есть базовые знания об объектно-ориентированном программировании, книга станет отличным введением в мир Unity. Даже те, кто не имеет опыта в программировании, найдут главы, написанные специально для них, чтобы заложить фундамент для дальнейшего обучения.   Структура книги Книга состоит из 21 главы, каждая из которых посвящена определенному аспекту разработки игр на Unity:   Знакомство с миром Unity: установка и настройка Unity, создание первого проекта. Создание сцен и игровых элементов: основы работы с игровыми объектами и сценами. От идеи к реализации: Terrain и ProBuilder: прототипирование первого уровня. Бесшовная интеграция: импорт и интеграция ресурсов: работа с ресурсами. Раскрытие возможностей C# и визуального программирования: создание скриптов. Динамика: реализация перемещения и генерации объектов: программирование движения. Столкновения и здоровье: точное определение столкновений: настройка физики объектов. Победа или поражение: условия выигрыша и проигрыша: завершение игры. Искусственный интеллект: создание умных противников: базовый ИИ для врагов. Алхимия материалов: использование URP и Shader Graph: создание визуальных эффектов. Захватывающие визуальные эффекты: работа с системами частиц и Visual Effect Graph: вода, огонь и другие эффекты. Да будет свет: освещение сцен с помощью URP: работа с освещением. Полное погружение: достижение полноэкранных эффектов с помощью постобработки: кинематографические эффекты. Гармоничные аудиоландшафты: интеграция звуков и музыки: добавление звука и музыки. Искусство проектирования: создание удобного пользовательского интерфейса: работа с UI. Пользовательский интерфейс нового поколения: создание динамических интерфейсов с помощью UI Toolkit: использование UI Toolkit. Создание анимации с помощью Animator, Cinemachine и Timeline: анимация персонажей и кат-сцены. Чудеса производительности: оптимизация игры с помощью профилировщика: повышение производительности. От прототипа к исполняемому файлу: генерация и отладка игры: создание исполняемого файла. Дополненная и виртуальная реальность: создание AR-приложений. Огромные миры: введение в DOTS: высокопроизводительные игры с использованием DOTS.   Авторы рекомендуют выполнять все шаги, описанные в книге, параллельно с изучением материала. Это позволит получить практический опыт и глубокое понимание концепций. Файлы проекта организованы по главам, что упрощает отслеживание изменений и позволяет продолжить работу с любой главы, даже если предыдущие не были завершены.  В общих чертах о «Разработке игр на Unity» мы рассказали, а следом предлагаем рассмотреть подачу материала на примере отрывка:   Столкновения и здоровье: точное определение столкновений Поскольку игры пытаются моделировать поведение реального мира, важную роль играет физика. Она определяет, как объекты движутся и сталкиваются друг с другом, будь то столкновение игроков со стенами или пуль с врагами. Управлять физикой может быть сложно из-за множества реакций, которые могут возникнуть после столкновения, поэтому мы разберемся, как правильно настроить нашу игру, чтобы физика работала максимально точно. Это позволит создать желаемое аркадное ощущение движения, основанное на реалистичных столкновениях, ведь иногда реальная жизнь не так интересна, как видеоигры!  Сначала вы научитесь правильно настраивать физику, чтобы ваши скрипты могли обнаруживать столкновения между объектами, используя новые события Unity, с которыми вы познакомитесь. Все это необходимо, чтобы ваши пули могли достигать врагов и наносить им урон. Затем мы обсудим разницу между перемещением с помощью Transform и Rigidbody, а также плюсы и минусы каждого из этих двух методов. Мы поэкспериментируем с различными способами перемещения персонажа, а вы сами решите, какой из них вам больше подходит.  Симуляция физики Физическая система Unity разработана для широкого спектра игровых приложений, поэтому правильная настройка физики очень важна для достижения желаемого результата.  Начнем с изучения различных видов коллайдеров, которые предлагает Unity, и способов их настройки для обнаружения различных видов физических реакций (столкновений и триггеров). Наконец, обсудим, как игнорировать столкновения между определенными объектами, чтобы избежать таких ситуаций, как нанесение урона игроку его собственными пулями.  Установочные формы В начале книги вы узнали, что объекты обычно имеют две формы: визуальную (3D-сетка) и физическую (коллайдер, который используется для расчета столкновений). Помните, что идея заключается в том, чтобы позволить вам иметь высокодетализированную визуальную модель, наложенную на упрощенную физическую форму, для повышения производительности.  В Unity есть несколько типов коллайдеров, и сейчас мы рассмотрим основные из них, начиная с примитивных форм — куба, сферы и капсулы. С точки зрения производительности они являются самыми дешевыми, поскольку столкновения между ними рассчитываются по математическим формулам, в отличие от сетчатого коллайдера Mesh Collider, который позволяет задействовать любую сетку в качестве физического тела объекта, но с более высокой нагрузкой на вычислительные мощности и некоторыми ограничениями. Идея заключается в том, что вы должны использовать примитивные коллайдеры для представления объектов или их комбинации. Например, самолет можно сделать с помощью двух кубов: один для корпуса, а другой для крыльев. На рис. 7.1 показан коллайдер оружия, состоящий из примитивов.   Однако это не всегда необходимо. Если мы хотим, чтобы оружие просто падало на землю, то можно обойтись одним коллайдером куба, охватывающим все оружие. Такие столкновения не требуют высокой точности, что повышает производительность. Кроме того, некоторые фигуры нельзя представить даже с помощью комбинации примитивов, например пандусы или пирамиды. Единственным решением здесь будет использование сетчатого коллайдера, который запрашивает 3D-сетку для расчета столкновений. Однако мы не станем прибегать к ним из-за их влияния на производительность. Все наши физические коллайдеры будут реализованы с помощью примитивов.  Теперь добавим необходимые коллайдеры в нашу сцену, чтобы подготовить ее к корректному расчету столкновений. Если вы работаете с пакетом окружения из магазина Asset Store, отличным от нашего, у вас уже могут быть модули сцены с коллайдерами.  Чтобы добавить коллайдеры, выполните следующие действия.  1. Выберите стену базы и проверьте, есть ли у объекта или его дочерних элементов компоненты коллайдера. В нашем случае коллайдеров не было. Если вы обнаружите Mesh Collider, можно оставить его, но мы рекомендуем заменить его другим вариантом на следующем шаге. Проблема в том, что наша стена не является экземпляром префаба, поэтому нам нужно добавить коллайдер к каждой стене в сцене.  2. Один из вариантов — создать префаб и заменить все стены его экземплярами (рекомендуемое решение). В качестве альтернативы можно просто выделить все стены в меню Hierarchy (щелкая на них, зажав Ctrl или command на Mac) и нажать кнопку Add Component, чтобы добавить коллайдер ко всем стенам. Мы используем компонент в виде куба Box Collider, который автоматически адаптирует размер коллайдера к сетке. Если этого не происходит, вы можете вручную изменить свойства Size и Center компонента Box Collider, чтобы охватить всю стену (рис. 7.2).   3. Повторите шаги 1 и 2 для углов, пола и любых других препятствий, которые будут блокировать движение игрока и противника.  Теперь, когда мы добавили коллайдеры к стенам и полу, сделаем то же самое с персонажем игрока и противником. Мы добавим к ним коллайдер Capsule Collider — стандартный коллайдер в виде капсулы для подвижных персонажей, поскольку закругленное основание позволяет объекту плавно подниматься по пандусам. Кроме того, округлая форма капсулы позволяет легко поворачиваться в углах, не застревая.  Вы можете создать префаб врага на базе одного из персонажей, загруженных ранее, и добавить коллайдер в этот префаб. Наш игрок — это простой игровой объект в сцене, поэтому вам нужно будет добавить коллайдер прямо к нему, но для удобства стоит создать префаб для игрока.  У вас может возникнуть соблазн добавить несколько коллайдеров в виде куба к «скелету» персонажа, чтобы создать более реалистичную форму объекта. Хотя такой подход вполне допустим для варьирования урона в зависимости от того, куда попали пули, обратите внимание, что мы в первую очередь создаем коллайдеры для перемещения, и для этой цели достаточно использовать коллайдер капсулы. В сложных системах урона будут сосуществовать как коллайдеры капсул, так и коллайдеры костей: один для движения, другой для определения повреждений. Однако для нашей игры это избыточно.  Иногда коллайдер плохо адаптируется к визуальной форме объекта. В моем случае коллайдер Capsule не подошел персонажу, и мне пришлось скорректировать его форму, установив следующие значения: Center — 0, 1, 0, Radius — 0.5 и Height — 2 (рис. 7.3).   Пуля, которую мы создали с помощью сферы, уже имела соответствующий коллайдер, но если вы замените сетку пули на другую, вам, возможно, потребуется изменить и коллайдер. На данный момент нам не нужны другие объекты в нашей игре, поэтому теперь, когда у всего есть подходящий коллайдер, перейдем к настройке физических параметров для обеспечения правильного обнаружения столкновений.  Если посмотреть на компоненты ландшафта, можно увидеть, что у него есть собственный коллайдер — Terrain collider — единственно возможный коллайдер.  Типы физических объектов Теперь, когда мы добавили коллайдеры к каждому объекту, что обеспечило их участие в симуляции физики, настало время настроить их поведение. Существует огромное количество возможных комбинаций настроек, но мы рассмотрим несколько самых распространенных профилей. Помните, что, помимо коллайдеров, важным компонентом является Rigidbody, который отвечает за применение физики к объекту. Следующие профили созданы с помощью комбинации настроек коллайдеров и Rigidbody.   Статический коллайдер (Static Collider). Как следует из названия, этот тип коллайдера не должен двигаться (за редкими исключениями). Большинство объектов окружения, такие как стены, полы, препятствия и рельеф, попадают в эту категорию. Эти типы коллайдеров — просто коллайдеры без компонента Rigidbody, то есть они присутствуют в симуляции, но физические силы на них не действуют. Их нельзя сдвинуть другими объектами, и они всегда остаются на своем месте. Учтите, что это не связано с флажком Static в правом верхнем углу редактора — он предназначен для систем, которые мы рассмотрим позже (например, в главе 12), поэтому при необходимости вы можете снять флажок. Физический коллайдер (Physics Collider). Это коллайдеры с компонентом Rigidbody, как в примере с падающим шаром, который мы создали в первой части книги. Это полностью управляемые физикой объекты, на которые действует гравитация и другие силы. Такой тип коллайдера можно использовать для игрока, гранат, падающих ящиков или иных объектов, как в играх с сильно выраженной физикой, таких как The Incredible Machine. Кинематический коллайдер (Kinematic Collider). Это коллайдеры с компонентом Rigidbody, у которых установлен флажок Is Kinematic. Они не реагируют на столкновения и физические силы, как статические коллайдеры, но могут двигаться, что позволяет физическим коллайдерам правильно обрабатывать столкновения с ними при движении. Такие коллайдеры часто применяются к объектам, которые должны двигаться с помощью анимации или пользовательских скриптов, например в движущихся платформах. Статический коллайдер с триггером (Trigger Static Collider). Это обычный статический коллайдер, но с установленным флажком Is Trigger. Разница в том, что когда кинематические и физические объекты проходят через него, автоматически генерируется событие Trigger. Его можно обрабатывать с помощью скриптов для определения, когда что-то вошло в область коллайдера. Это событие можно использовать для создания кнопок, запуска действий, когда игрок проходит через какую-то зону, например порождения волны врагов, открытия двери или победы в игре. Обратите внимание, что обычные статические коллайдеры не будут генерировать триггерное событие при прохождении через этот тип, потому что они не предназначены для движения. Кинематический коллайдер с триггером (Trigger Kinematic Collider). Кинематические коллайдеры не генерируют столкновения, но могут генерировать события Trigger, когда проходят через другие объекты. Это можно использовать для создания подвижных бонусов, которые исчезают при соприкосновении с персонажем и приносят очки, или для пуль, движущихся с помощью пользовательских скриптов и без физики, но наносящих урон другим объектам при столкновении.  Конечно, существуют и другие профили, которые подойдут для специфических игровых механик. Вы можете поэкспериментировать со всеми возможными комбинациями настроек физики, чтобы подобрать подходящие. Однако описанные профили покрывают 99 % случаев.  В табл. 7.1 приведена реакция объектов при контакте с различными типами коллайдеров. В строках представлены профили, которые могут двигаться (помните, что статичные профили не должны двигаться), а в столбцах — тип коллайдера, с которым происходит столкновение. Слово «ничего» означает, что объект пройдет сквозь другой без эффекта, «триггер» — что объект пройдет сквозь, но вызовет событие Trigger, а «столкновение» — что объект не может пройти сквозь другой.   Учитывая это, начнем настраивать физику объектов.  Стены, углы, полы и препятствия должны использовать профиль статического коллайдера. Это значит, что у них отсутствует компонент Rigidbody и у их коллайдеров будет снят флажок Is Trigger (рис. 7.4).  Игрок должен двигаться и сталкиваться с объектами, поэтому нам нужно, чтобы у него был динамический профиль (рис. 7.5). Этот профиль вызовет забавное поведение с нашим текущим скриптом движения (проверьте), особенно при столкновении со стенами. Мы разберемся с этим чуть позже.  Для префаба Enemy следует использовать кинематический профиль, так как мы будем перемещать этот объект с помощью систем искусственного интеллекта Unity, поэтому физика здесь не нужна. Кроме того, поскольку мы хотим, чтобы игрок сталкивался с объектами префаба врага, здесь необходима реакция на столкновение, поэтому Trigger использовать не будем (рис. 7.6).  Префаб Bullet перемещается благодаря скрипту (пуля просто движется вперед), а не физике. Нам не нужны столкновения. Мы запрограммируем пулю так, чтобы она уничтожалась при касании чего-либо и наносила урон объекту, с которым столкнется (если это возможно). Поэтому здесь достаточно кинематического профиля с триггером. Мы будем использовать событие Trigger для скрипта реакции на контакт (рис. 7.7).     Теперь, когда мы настроили объекты, проверим, как отфильтровать нежелательные столкновения между определенными типами объектов.  Фильтрация столкновений Иногда нам нужно, чтобы определенные объекты игнорировали друг друга, скажем, пули, выпущенные игроком, не должны сталкиваться с самим игроком. Этого можно добиться с помощью оператора if в скрипте C#, проверяя, например, является ли попавший объект представителем противоположной команды. Однако эта проверка столкновения между объектами, которые никогда не должны были столкнуться, — пустая трата ресурсов. И здесь нам на помощь приходит матрица столкновений слоев.  «Матрица столкновений слоев» звучит пугающе, но это обычный инструмент системы физики, который позволяет нам указать, какие группы объектов должны сталкиваться с другими группами. Например, пули игрока должны сталкиваться с противниками, а их — с игроком. В данном случае вражеские пули будут проходить сквозь врагов, что для нас является желаемым результатом. Идея в том, чтобы формировать группы, которые в Unity называются слоями, и помещать в них наши объекты. Мы можем создавать слои и присваивать их объектам через свойство слоя (верхняя часть панели Inspector). Обратите внимание, что у вас ограниченное количество слоев, поэтому используйте их с умом.  Вы можете добиться этого, выполнив следующие действия.    1. Перейдите в меню Edit Project Settings и найдите на левой панели пункт Tags and Layers (рис. 7.8).  2. В разделе Layers заполните пустые места, чтобы создать слои. В нашем случае с пулями нам нужно четыре слоя: Player, Enemy, PlayerBullet и EnemyBullet (рис. 7.9).  3. Выберите объект игрока в меню Hierarchy и измените свойство Layer на Player в верхней части Inspector. Также измените слой префаба Enemy на Enemy. Появится окно с предложением изменить дочерние объекты; нажмите Yes (рис. 7.10).  4. В случае с пулей возникает проблема: у нас один префаб, но два слоя, а у префаба может быть только один слой. Здесь два варианта: изменить слой в зависимости от стрелка с помощью скриптов или создать два разных префаба пуль для каждого слоя. Для простоты мы выберем последний вариант, а также применим другой материал к вражеской пуле, чтобы она выглядела иначе.  5. Мы создадим дополнительный вариант префаба пули игрока. Помните, что вариант — это префаб, основанный на оригинальном, подобно наследованию классов. При изменении оригинального префаба изменится и его вариант, у которого, в свою очередь, могут быть свои уникальные отличия.  6. Поместите префаб пули в сцену, чтобы создать его экземпляр.  7. Снова перетащите экземпляр в папку Prefabs, на этот раз выбрав в появившемся окне пункт Prefab Variant.  8. Переименуйте его в Enemy Bullet.  9. Удалите экземпляр префаба в сцене.  10. Создайте второй материал, похожий на материал пули игрока, но другого цвета, и примените его к варианту префаба вражеской пули.  11. Выберите префаб пули врага, установите слой EnemyBullet и сделайте то же самое для оригинального префаба (PlayerBullet). Даже если вы измените слой исходного префаба, при модификации варианта будет преобладать измененная версия (или переопределение), что позволит каждому префабу иметь собственный слой.  Теперь, когда мы настроили слои, сконфигурируем систему физики на их использование.  1. Перейдите в Edit Project Settings и найдите параметры физики Physics (не Physics 2D).  2. Прокрутите страницу вниз, пока не увидите матрицу столкновения слоев (Layer Collision Matrix). Каждый столбец и строка матрицы подписаны названиями слоев, поэтому каждый флажок на пересечении строки и столбца позволит нам указать, должны ли объекты сталкиваться. В нашем случае мы настроили матрицу так, как показано на рис. 7.11, чтобы пули игрока не попадали в самого игрока или в другие его пули, а пули врагов не задевали самих врагов и их пули.  Стоит отметить, что логика фильтрации не всегда бывает такая строгая и понятная. Например, она может включать в себя попадание только по объектам с определенным количеством здоровья, объекты, не имеющие временного эффекта невидимости, или условия, которые могут меняться в процессе игры и т. д. Поэтому в таких случаях следует полагаться на ручную фильтрацию после события триггера или столкновения.  Теперь, когда мы отфильтровали столкновения, проверим, корректно ли работают наши настройки.  Обнаружение столкновений Правильно настроить физику довольно сложно, но и очень важно. Теперь, когда мы с этим разобрались, выполним несколько задач с этими настройками и создадим систему здоровья.  Сначала мы рассмотрим различные события столкновений и триггеров, которые Unity предлагает нам в качестве реакции на контакт между двумя объектами. Здесь мы обсудим, как изменять компоненты столкнувшихся объектов с помощью функции GetComponent.  Обработка триггерных событий Если объекты настроены корректно, можно получить две реакции: столкновения или триггеры. Реакция столкновения по умолчанию блокирует движение объектов, однако мы можем добавить к нему пользовательское поведение с помощью скриптов, в то время как триггер, если мы не добавим пользовательское поведение, не даст никакого заметного эффекта. В любом случае у нас есть возможность написать скрипт реакции на оба сценария, например добавление очков, уменьшение здоровья и проигрыш в игре. Для этого можно использовать набор событий физики Physics.  Эти события делятся на две группы: события столкновения и события триггера, поэтому в зависимости от настроек вашего объекта нужно выбрать соответствующую группу. Обе группы имеют три основных события: Enter, Stay и Exit, которые сообщают нам, когда началось столкновение или триггер (Enter), происходит ли оно все еще или продолжается контакт (Stay) и когда оно завершилось (Exit). Например, мы можем запрограммировать такое поведение, как воспроизведение какого-либо звука при первом контакте двух объектов в событии Enter и его остановка при прекращении контакта в событии Exit.  Протестируем это, создав первое поведение при контакте: пуля уничтожается при столкновении с чем-либо. Помните, что пули настроены как триггеры, поэтому они будут генерировать события Trigger при контакте. Это можно сделать с помощью следующих шагов.  1. Создайте и добавьте скрипт ContactDestroyer к префабу пули игрока. Поскольку префаб вражеской пули является его вариантом, он также будет использовать этот скрипт.  2. Чтобы обнаружить момент срабатывания триггера, как, например, в Start и Update, создайте функцию события OnTriggerEnter.  3. Внутри события используйте строку Destroy(gameObject);, чтобы пуля самоуничтожалась при столкновении с чем-либо (рис. 7.12).   4. Сохраните скрипт и постреляйте по стенам, чтобы увидеть, как пули исчезают, а не проходят сквозь них. Здесь у нас не столкновение, а триггер, который удаляет пулю при контакте. Таким образом, пуля никогда не пройдет через объект, но мы все еще не реализовали движение на основе физики.  На данный момент нам не понадобятся другие события столкновения. Если они вам понадобятся, они будут работать аналогичным образом; просто создайте вместо них функцию с именем OnCollisionEnter.  Теперь рассмотрим другую версию той же функции. Мы настроим ее так, чтобы она не только сообщала нам о том, что мы коснулись чего-то, но и о том, с чем именно был контакт. Мы также настроим Contact Destroyer, чтобы он уничтожал и другой объект. Для этого выполните следующие действия.  1. Замените сигнатуру метода OnTriggerEnter той, что показана на рис. 7.13. Эта версия принимает параметр типа Collider, указывающий на конкретный коллайдер, с которым произошел контакт.   2. Мы можем получить доступ к игровому объекту этого коллайдера через свойство gameObject. С его помощью можно уничтожить и другой коллайдер (рис. 7.14). Если просто вызвать функцию Destroy, передав ей переменную other, она уничтожит только компонент Collider.   3. Сохраните и протестируйте скрипт. Вы заметите, что пуля уничтожает все на своем пути. Убедитесь, что у вашего врага есть коллайдер капсулы, чтобы пуля могла обнаружить столкновение с ним.  Эквивалентная версия, написанная с помощью визуального программирования, выглядит следующим образом (рис. 7.15).   Как видите, мы создали узел On Trigger Enter и связали его с двумя узлами Destroy. Чтобы указать, какой объект будет уничтожен каждым из узлов Destroy, мы дважды использовали узел Component: Get GameObject. Правый узел был создан без узла, подключенного к его левому входному выводу, что означает, что он возвращает игровой объект, который в данный момент выполняет этот скрипт (отсюда и метка This в левом выводе узла), в данном случае пулю. Для второго узла мы подключили выходной вывод Collider справа от узла On Trigger Enter к узлу Get GameObject. Таким образом мы указываем, что хотим получить игровой объект, содержащий коллайдер, с которым столкнулась наша пуля.  Нам не нужно, чтобы пуля уничтожала все подряд при столкновении. Вместо этого мы добавим врагам и игроку систему здоровья, а попадания будут уменьшать количество очков здоровья, пока оно не достигнет нуля. Давайте выясним, как это сделать.  Изменение другого объекта Чтобы пуля наносила урон объекту, нам нужно получить доступ к компоненту Life для изменения его количества. Для этого мы создадим компонент Life, который должен содержать поле типа float с показателем здоровья. Каждый объект с этим компонентом будет считаться объектом, которому можно нанести урон. Чтобы получить доступ к компоненту Life из скриптов пули, нам понадобится функция GetComponent.  Если у вас есть ссылка на игровой объект или его компонент, вы можете использовать GetComponent для доступа к определенному компоненту, если объект содержит его (если нет, то вернется null). Посмотрим, как задействовать эту функцию, чтобы пуля уменьшала количество жизни другого объекта.  1. Создайте и добавьте компонент Life с публичным полем типа float под названием amount к префабам игрока и врага. Не забудьте установить значение 100 (или любое другое) в поле Amount на панели Inspector (рис. 7.16).   2. Удалите компонент ContactDestroyer из пули игрока, что также приведет к его удалению из варианта пули противника.  3. Добавьте новый скрипт под названием ContactDamager к врагу и игроку.  4. Добавьте событие OnTriggerEnter, которое принимает другой коллайдер в качестве параметра, и просто вызовите функции Destroy, чтобы пуля уничтожала сама себя, а не объект, с которым сталкивается; наш скрипт будет просто сокращать количество очков здоровья.  5. Добавьте поле типа float под названием damage, чтобы мы могли настроить количество урона, наносимого другому объекту. Не забудьте сохранить файл и установить значение, прежде чем продолжить.  6. Используйте GetComponent для ссылки на другой коллайдер, чтобы получить доступ к его компоненту Life и сохранить его в переменной (рис. 7.17).   7. Перед тем как уменьшить показатели здоровья объекта, мы должны проверить, что ссылка на Life не является null. Это произойдет, если другой объект не содержит компонент Life, как в случае со стенами и препятствиями. Идея заключается в том, что пуля будет уничтожать себя при столкновении с любым объектом, а также уменьшать очки здоровья другого объекта, если это повреждаемый объект, содержащий компонент Life.   8. Поместите врага в сцену и установите его скорость на 0, чтобы он не двигался.  9. Выберите его в меню Hierarchy, прежде чем нажать кнопку Play, и постреляйте по нему.  На панели Inspector можно увидеть, как при этом уменьшается количество очков здоровья. Можно также нажать клавишу Esc, чтобы вернуть контроль над мышью, и, выбрав объект в режиме Play, наблюдать за изменением поля жизни во время работы игры в редакторе.  Теперь вы заметите, что количество очков здоровья уменьшается вплоть до отрицательных значений. Однако нам нужно, чтобы объект уничтожался при достижении нулевых показателей здоровья. Этого можно добиться двумя способами: один из них — добавить метод Update к компоненту Life, который будет покадрово проверять, опустились ли очки здоровья ниже 0, а затем уничтожать объект, когда это произойдет.  Второй способ — инкапсулировать поле amount и проверять его значение внутри сеттера, чтобы избежать покадровой проверки. Я предпочитаю второй способ, но мы реализуем первый, чтобы наши скрипты выглядели как можно проще для новичков.  Но прочитать о втором способе подробнее рекомендуем из самой книги. Переходите на наш сайт и ознакомьтесь с «Разработкой игр на Unity, 4-е изд.»   » Оглавление » Отрывок  По факту оплаты бумажной версии книги на e-mail высылается электронная книга. Для Хаброжителей скидка 25% по купону — Unity"
10,10,Курсы по Роблокс для детей: цены и отзывы,IT для детей,Компания,286.98,"Веб-разработка, Производство мультимедиа-контента, Игры и развлечения",2025-03-13,"Роблокс – игровая онлайн-платформа, где предусмотрено все для развлечений и досуга с различными играми, но возможности данной площадки в плане обучения ограничены, даже минимальны. Ситуация резко меняется, когда дело начинает касаться движка Roblox Studio – комплексного инструмента, который может быть использован для создания игровых миров. А еще данное программное обеспечение применяется для выполнения других проектов, скажем, для подготовки контента для собственных разработок, трехмерного моделирования и написания кода на Lua.Хотим рассказать о том, чем полезно обучение Роблоксу, а также представить примеры курсов с ценами и отзывами об организаторах – частных образовательных организациях.Плюсы курсов Роблокса для детейБлагодаря занятиям можно превратить нежелательное увлечение ребенка в виде гейминга в полноценное хобби, основанное на разработке: онлайн-уроки дадут каждому ребенку возможность понять, что он может быть создателем игр, а не их простым пользователем.Еще курсы помогут:Сформировать и ускорить развитие логических компонентов мышления;Развить навыки трехмерного моделирования;Научиться писать код на текстовом языке Lua;Освоить разработку и т. д.В результате обучения каждый ребенок сможет понять, как создавать компьютерные игры, модели для них, а также скриптить виртуальную реальность.Если тема кажется интересной, хочется отдать школьника на обучение, предлагаем ознакомиться с примерами тематических направлений в виде предложений от ряда частных школ. Проанализировали варианты, уделили особое внимание ценам и отзывам об организаторах курсов.Пять онлайн-курсов Роблокса для детей с отзывами и ценамиСобрали пятерку курсов для примера и представили аналитические сведения о них.1. Создание игр на движке Roblox Studio и программирование на Lua: курс для детей Организатор курса, отзывы о нем: онлайн-курс Роблокса для детей проводится школой Pixel. Нашли отзывы о ней на портале «Сравни.ру» и спешим отметить, что они преимущественно положительные. Вот ссылка на страницу.Средняя оценка (по версии источника): на портале «Сравни.ру» увидели 4.73 балла из 5 возможных. Радует, что оценка основана на 400+ отзывах.Цены: одно онлайн-занятие обойдется минимум в 750 рублей.Ссылка: https://pixel.study/robloxЗдесь и далее описательные блоки будем посвящать тому, что предстоит узнать и сделать ребятам на занятиях. В данном случае можно представить следующий перечень тем, основанных на теории и практике:Введение в движок, его интерфейс и начало активного использования;Аспекты управления камерой, строительство объектов с помощью блоков;Генерирование и редактирование основы проекта – ландшафта;Работа с материалами;Основы программирования на языке Lua, написание скриптов на нем;Понятие цикла, его значимость в написании кода;Функции, предназначенные для создания блоков и других объектов;Разработка удобоваримого меню для игры;Работа с графикой, тенями и светом;3Д-моделирование, создание моделей с нуля;Текстуры, их применение в собственных проектах;Проектирование предметов и объектов для игр;Создание анимации, реализация визуальных эффектов для привнесения в проекты элементов интерактива и т. д.Будет много практики, которая поможет закрепить знания и навыки. Это подтверждается отзывами родителей о представленном курсе, посмотреть которые можно по ссылке, приведенной выше.Еще отметим, что на самой странице курса опубликованы видео с отзывами довольных ребят, которым удалось обрести условные технические навыки и создать собственные игры с нуля.2. Курс разработки игр в Роблоксе и других средахОрганизатор курса, отзывы о нем: это школа TOP, оценки которой нашли на портале Academy Market. Источник не так авторитетен, как Сравни.ру, но все-таки.Средняя оценка (по версии источника): 3,9 из 5. Результат неплохой, но считаться хорошим он не может, ведь есть негативные отзывы.Цены: от 4 070 рублей за месяц, всего стоимость составит в шесть раз больше, потому как образовательная программа рассчитана на полгода.Ссылка: top-academy.ru/education/game-development-for-childrenОрганизатором заявлено, что что каждому ребенку предстоит освоить создание 3Д-игр на движке Roblox Studio, а также познакомиться с другими инструментами вроде Construct 3.Примеры тем следующие:Создание игровых уровней;Добавление переменных в проекты;Проработка персонажей;Реализация простых и сложных сюжетных линий;Программирование на движке Studio;Создание 3Д-моделей, их редактирование и добавление в game-проекты;Разработка сложных форм из простых деталей;Операция скульптинга ландшафта;Оптимизация игрового процесса;Программирование взаимодействия одних предметов с другими;Виртуальная реальность в продвинутых играх и т. д.3. Создание и кодинг игр в РоблоксеОрганизатор курса, отзывы о нем: отзывы родителей о Codim.Online – школе с курсом Роблокса для детей – смешанные. Ознакомиться с ними можно по ссылке.Средняя оценка (по версии источника): 3 балла из 5.Цены: минимальная цена одного урока составляет 1 000 рублей.Ссылка: codim.online/robloxВот примерный перечень тем, в которых предстоит разобраться:Скриптинг и управление камерой;Создание игровых предметов и объектов;Различные цветовые модели и их роль в разработке;Программирование столкновений и других актов взаимодействия между персонажами;Переменные и другие конструкции, их назначение в программировании и game-разработке;Программирование игр на Lua;Разработка многопользовательских игр и т. д.4. Уроки Роблокс Студио для детейОрганизатор курса, отзывы о нем: снова школа Pixel, оценки которой на портале Stepik достаточно высокие. С примерами можно ознакомиться по ссылке.Средняя оценка (по версии источника): 4,9 балла из 5 возможных присвоено организатору, 4,5 – непосредственно видеоматериалам.Цены: этот видеокурс распространяется на безвозмездной основе, при этом освоить его можно в комфортном темпе без привязки к определенному графику или расписанию.Ссылка: https://stepik.org/course/104936/promo Данный видеокурс Roblox для детей поможет проявить творчество, научиться создавать простые онлайн-игры и обмениваться ими с друзьями.Перечень блоков теории и практики включает такие темы:Установка программного обеспечения Roblox Studio, базовые настройки и интерфейс среды;Основы разработки карт с нуля;Выполнение game-проектов с трехмерными моделями;Проработка игровых уровней;Реализация системы диалогов и других видов взаимодействия между персонажами;Создание интуитивно понятного и удобного меню;Продвинутая разработка игр и т. д.В результате прохождения данного вводного курса ребенок поймет, что в Roblox можно не только играть, но и быть полноценным разработчиком.5. Курс Roblox Studio для детейОрганизатор курса, отзывы о нем: занятия проводятся Bloxy.school. Отзывы о данной организации нашли на Яндекс Картах, вот ссылка. Советуем ознакомиться.Средняя оценка (по версии источника): 4,7 балла из 5. Достойный результат.Цены: от 5 тысяч рублей за месяц.Ссылка: bloxy.school/robloxОбразовательная программа основана на ряде примечательных тем. Вот конкретные примеры того, в чем предстоит разобраться:Интерфейс Roblox Studio, устройство движка и базовые настройки;Создание предметов, изменение материалов и работа с цветом;Свойства и проводник, объединение частей в полноценные модели;Применение вращающихся блоков;Разработка уровней для собственных game-проектов, их публикация на платформе Roblox;Кодинг на языке «Луа», переменные и циклы;Написание скриптов для персонажей;Трехмерное моделирование и т. д.Так, курсов с отзывами предостаточно, поэтому если ребенок играет в Roblox часами, его хобби можно превратить во вполне полезное занятие. Это возможно как на платных курсах, так и на бесплатных, один из которых мы представили в сегодняшней подборке. Начать можно именно с данного видеокурса, а уже потом, если направление придется ребенку по душе, записаться на платное обучение.Выбор варианта и конкретного маршрута дополнительного образования – задача, которая должна решаться детьми и их родителями, поэтому не настаиваем, а лишь предлагаем примеры вариантов. Сегодняшние предложения частных школ, которые были рассмотрены нами, считаем достойными, и все-таки не можем рекомендовать что-то конкретное.Советуем отталкиваться как от отзывов и цен, так и от других критериев, скажем, от содержательности курса, наличия скидок и других акций. Это поможет сделать дополнительное обучение ребенка не столько полезным и увлекательным, сколько выгодным, а это важно, ведь экономический аспект нередко становится одним из решающих факторов.На этом все, поэтому желаем удачи каждому ребенку, начавшему двигаться в направлении информационных технологий. Пусть учеба будет в радость!"
11,11,Школа программирования 8 класс: ТОП-10 школ,IT для детей,Компания,286.98,"Веб-разработка, Производство мультимедиа-контента, Игры и развлечения",2025-03-13,"Обучение программированию в 8–11-м классе – достаточно простая задача, ведь типичный современный подросток уже способен похвастаться знанием того, что такое алгоритмы, а также пониманием информатики и даже цифровых технологий. Вместе с тем школьники, обучающиеся в старших классах, обладают навыками использования компьютерной техники и ряда прикладных программ, что становится дополнительным аргументом в пользу простоты обучения.Сегодня хотим поговорить не о пользе или преимуществах начала обучения программированию школьников в 8–11-х классах, а представить ряд примечательных образовательных организаций, которые наша команда посчитала лучшими. Для объективности представили аналитические сведения, основанные на ряде критериев, среди которых особое внимание уделили таким моментам:Наличие лицензии на осуществление деятельности в сфере образования;Стоимость уроков;Реализуемые форматы;Возрастная категория детей, на которую ориентирована школа;Содержание занятий.Сразу отметим, что анализировали только онлайн-школы, хотя некоторые из представленных организаций характеризуются наличием очных направлений.1. Международная онлайн-школа программирования для детей PixelНаличие лицензии на ведение образовательной деятельности: есть, № Л035-01255-50/00822552.Стоимость уроков: минимальная цена онлайн-занятия составляет 800ирублей.Реализуемые форматы: предусмотрен набор групп, есть индивидуальные уроки. Альтернативный вариант представлен видеокурсами, предназначенными для самостоятельного освоения.Возрастная категория детей, на которую ориентирована школа: в данной онлайн-школе программирования можно учиться в 8-м классе, однако есть программы, ориентированные на ребят дошкольного и младшего школьного возраста (5–17 лет).Содержание занятий, направления: здесь и далее будем перечислять все реализуемые программы, касающиеся кодинга. При этом в случае повторов одни и те же инструменты описывать заново не станем, чтобы не перегружать материал однотипными сведениями.Сделаем еще одну оговорку: потому как тема статьи касается обучения программированию школьников 8–11-х классов, речь будет вестись исключительно об образовательных планах, ориентированных на ребят соответствующей возрастной группы.Итак, в случае со школой Pixel смогли выделить такие примечательные направления:Разработка игр на движке Roblox Studio и написание кода на языке «Луа». Данный курс программирования посвящен выполнению game-проектов, но это не все: освоение направления поможет разобраться в основах трехмерного моделирования, скриптинга персонажей и объектов для наполнения виртуальных игровых миров. Дополнительно учащиеся смогут поделиться собственными проектами с многомиллионной аудиторией платформы Roblox;Разработка игр на движке «Юнити». Уроки несколько напоминают предыдущее направление по сути и содержанию, однако меняются инструменты. Речь о более продвинутом и распространенном движке Unity, а также о языке программирования C#. Ребята смогут освоить разработку, а также вникнуть в написание кода;Создание веб-сайтов. Онлайн-обучение детей в рамках данного дистанционного курса построено вокруг ДжаваСкрипта, HTML и CSS. Первый – язык программирования в типичном понимании, а второй и третий – это инструмент для гипертекстовой разметки веб-документов и каскадные таблицы стилей соответственно. Данная условная триада применяется создателями сайтов и удаленных приложений;Основы программирования на Python. На онлайн-курсе реализовано обучение через игры, то есть теория будет сопровождаться выполнением game-проектов и других практический заданий. Речь, скажем, о создании собственного виртуального калькулятора посредством кода.Обнаружили и другие образовательные программы, доступные ребятам подросткового возраста, обучающимся в 8-х классах, но они не связаны с темой статьи и сопряжены с цифровым творчеством, поэтому касаться соответствующих направлений не станем.Ссылка: https://pixel.study/2. Школа программирования «КиберУм»Наличие лицензии: увидели указание на разрешительную документацию. Вот номер: Л035-01272-16/01017687.Стоимость уроков: от 1 152 рублей.Реализуемые форматы: предусмотрены только индивидуальные уроки.Возрастная категория детей, на которую ориентирована школа: 6–16 лет.Содержание занятий: нашли несколько методических планов обучения программированию для детей, учащихся в 8-х классах. Они привязаны к:Разработке на Роблокс Студио. Уже описали направление, повторяться не будем;Кодингу на языке «Питон». О нем уже говорили;Разработке чат-ботов. В основу онлайн-курса заложены теория и практика скриптинга на Python. В результате прохождения интерактивных уроков удастся разобраться в создании ботов для мессенджеров;Олимпиадному программированию. Будет много практики применения «Питона» на соответствующем уровне;Разработке игр на движке «Юнити». Уже описывали направление, не повторяемся;Веб-разработке. Будут применяться HTML, CSS и JavaScript.Ссылка: cyberum.ru/3. Онлайн-школа «Фоксфорд»Наличие лицензии: № 041018.Стоимость уроков: около 7 000 за месяц, но окончательная цена зависит от направления.Реализуемые форматы: предусмотрены видеокурсы, набор групп и уроки один на один с учителями.В каком возрасте можно начать: записаться в Foxford могут дошкольники, младшие школьники и подростки.Содержание занятий: нашли ряд тематических направлений программирования для учащихся 8–9-х классов и ребят постарше. Примеры (без описания уже отмеченных ранее инструментов):Создание игр на «Юнити»;Кодинг на «Питоне»;Разработка в Роблоксе и т. д.Ссылка: foxford.ru/4. Школа «Стартория»Наличие лицензии на осуществление образовательной деятельности: не нашли сведений о разрешительной документации.Стоимость уроков: нет информации.Реализуемые форматы: набираются группы, можно отдать предпочтение индивидуальным занятиям.Возрастные рамки: 6–17 лет.Содержание занятий: организация предлагает ряд направлений программирования для учеников 8–11-х классов. Примеры:Создание модов для игровой вселенной Minecraft. Для этого ребятам потребуется погрузиться в блоки теории и практики кодинга на Python. Благодаря последнему каждый ребенок сможет разобраться в обозначенном направлении и выполнить ряд тематических проектов;Game-разработка ни движке Roblox Studio;Создание игр на Unity и т. д.Ссылка: startoria.ru/5. Hello World School с курсами программирования для детей 8-х классовНаличие лицензии на ведение образовательной деятельности: № Л035‑01257‑69/00191630.Стоимость уроков: от 1 400 за одно онлайн-занятие.Реализуемые форматы: нет данных.Возрастная категория детей: учиться в Hello World можно с 7 лет.Содержание занятий: нашли ряд образовательных программ, которые привязаны к программированию и ориентированы на учащихся 8-х классов. Примеры:Разработка веб-сайтов на HTML, CSS и ДжаваСкрипте;Создание игр на Unity;Написание кода на «Питоне» и т. д.Ссылка: hwschool.online/6. Онлайн-школа GeekSchoolНаличие лицензии на ведение образовательной деятельности: № Л035-01298-77/00181496.Стоимость уроков: от 3 000 за занятие без учета действующих акций.Реализуемые форматы: можно учиться в группе или индивидуально с педагогом.Возрастная категория детей, на которую ориентирована школа: 7–17 лет.Содержание онлайн-занятий: в данной школе есть несколько курсов программирования для начинающих ребят – 8-классников. Это:Разработка приложений на Python. Отличительная особенность курса – упор на создание софта для мобильных телефонов;Разработка веб-сайтов. Как и в других представленных online-школах, предстоит освоить HTML, CSS и JavaScript;Кодинг на «Питоне»;Разработка игр на движке «Юнити».Ссылка: gb.ru/courses/geek-school/7. Онлайн-школа «Айтигенио»Наличие лицензии на ведение образовательной деятельности: нет информации.Стоимость уроков: от 790 рублей за одно занятие.Реализуемые онлайн-форматы: набираются группы, возможны индивидуальные дистанционные занятия.Возрастная категория детей, на которую ориентирована школа: 5–17 лет.Содержание занятий: курсы программирования в online-школе «Айтигенио», доступные учащимся 8-х классов, немногочисленны, но все-таки имеют место. Примеры программ:Создание игр на движке «Роблокс Студио»;Веб-программирование на уже обозначенных ранее языках;Написание кода на «Питоне».Ссылка: itgen.io/8. Учеба в ReboticaНаличие лицензии на ведение образовательной деятельности: есть, номер не указан на сайте организации.Стоимость уроков: нет сведений.Реализуемые форматы: только индивидуальный вариант. Возрастная категория детей, на которую ориентирована школа: онлайн-курсы от площадки «Реботика» предназначены для 5–16-летних ребят.  Направления:Создание игр. На выбор можно освоить Roblox или Unity. Заявлено, что каждый ученик разберется в game-разработке и скриптинге игровых миров;Программирование. Упор сделан на язык «Питон». В результате прохождения курса программирования для подростков удастся освоить азы направления и написать первые приложения с нуля.Ссылка: rebotica.ru/9. Учебный центр вычислительной техникиНаличие лицензии на ведение образовательной деятельности: № Л035-01271-78/00346889.Стоимость уроков: нет информации.Реализуемые форматы: групповой и индивидуальный варианты.Возрастная категория детей, на которую ориентирована школа: 8+ лет.Содержание занятий: в организации «Учебный центр вычислительной техники» можно пройти ряд онлайн-курсов, основанных на введении в программирование на различных языках. Примеры:Написание Python-кода в Майкрафте для разработки модов;Создание функциональных сайтов;Основы «Питона». Заявлено, что это курс для начала программирования в 8-м классе и в рамках других возрастных периодов.Ссылка: ucvt.org/shkolnikam/10. Школа Codims (организация «АйдаКодить»)Наличие лицензии на ведение образовательной деятельности: № Л035-01298-77/01082973.Стоимость уроков: от 8 900 рублей за один месяц.Реализуемые форматы: можно начать учиться в группе или один на один с педагогом.Возрастная категория детей, на которую ориентирована школа: 6–17 лет.Содержание занятий: увидели ряд курсов программирования для 8–11-х классов. Это:Основы скриптинга на Python;Олимпиадный кодинг на «Питоне»;Создание сайтов с помощью уже отмеченных инструментов и редактора Figma – отличного сервиса для прототипирования веб-проектов;Разработка на движке Unity;Разработка приложений на Java – достаточно популярном и распространенном языке.Ссылка: codims.ru/Подошли к концу, но не прощаемся и спешим отметить, что курсов с различными языками программирования и основами информатики для учащихся 8-х классов и ребят постарше предостаточно. Чтобы исключить ошибки и сделать правильный выбор, стоит отталкиваться от критериев, выделенных нами для сегодняшней публикации, а также учитывать:Наличие скидок и других выгодных предложений. Любой рациональный родитель не готов платить необоснованно большие деньги за дополнительное обучение своих детей, поэтому данный критерий заслуживает особого внимания: содержательность и другие аспекты, скажем, организационные, должны соответствовать цене занятий;Качество отзывов о школе программирования и курсах для школьников. Найти оценки можно на различных порталах, скажем, на площадке «Сравни.ру» и на подобных сайтах. Благодаря анализу отзывов можно обнаружить не всегда очевидные сведения, например, касающиеся квалификации педагогов и их способности находить общий язык с учениками.На этом все, поэтому желаем каждому из подростков успехов в обучении, а их родителям – терпения. "
12,12,Как мы строим дизайн-систему в ЮMoney,ЮMoney,Всё о разработке сервисов онлайн-платежей,4.28,"Электронная коммерция, Мобильные технологии, Веб-сервисы",2025-03-13,"Всем привет! Мы — Полина и Степан, дизайнер интерфейсов и фронтенд-разработчик в ЮMoney. В этом тексте расскажем, как у нас в компании устроена дизайн-система, и посмотрим на неё с двух сторон: глазами команды дизайна и разработки.👀 Взгляд дизайнера 👀Что такое дизайн-системаЭто набор правил, компонентов и инструментов, которые помогают создавать и развивать продукты компании. Дизайн-система обеспечивает консистентные интерфейсы, улучшает взаимодействие команд, а также ускоряет проектирование и разработку.Основные составляющие дизайн-системыЧтобы было легче понять, что такое дизайн-система, сравним её с конструктором, с которым у нас работают дизайнеры и разработчики.У нас есть разные компоненты-детали: кнопки, поля ввода, иконки и так далее. Все они совместимы друг с другом, и, когда дизайнер проектирует сценарий, ему достаточно выбрать нужный «кубик» и вставить в нужную конструкцию. Чтобы сборка была проще и понятнее, к нашей дизайн-системе мы прилагаем инструкции.Но важно понимать, что это не просто набор «кубиков», которые собрали и оставили на полке, а постоянно обновляющийся продукт. Поэтому наша дизайн-система всегда остаётся актуальной и не устаревает.Основные принципы дизайн-системы ЮMoneyКогда мы её проектировали, то опирались на пять основных принципов:1. Атомарность.2. Блочный подход.3. Универсальность.4. Консистентность.5. Доступность.Разберём каждый.АтомарностьАтомарный дизайн — это подход к проектированию интерфейсов, который предложил веб-дизайнер Брэд Фрост. Он провёл аналогию с химией, где атомы объединяются в молекулы, а молекулы — в организмы. По такому же принципу создаются Ul-компоненты: от самых простых элементов к сложным конструкциям.Представьте, что мы взяли микроскоп и смотрим через него на наши интерфейсы. Вот что мы увидим:1. Страницы — самые крупные элементы.2. Шаблоны задают структуру страниц.3. Организмы — это комбинации молекул и атомов. Например, форма ЮMoney с полями и кнопкой.4. Молекулы (элементы из групп атомов). Это небольшие составные компоненты, например, Input.5. Атомы — базовые элементы нашего интерфейса.6. Дизайн-токены — переменные, которые управляют внешним видом нашего интерфейса. Дизайн-токен похож на генетический код интерфейса и хранит в себе информацию о том, как должен выглядеть тот или иной элемент.Мы выделяем пять основных групп дизайн-токенов:1. Цветовая палитра.2. Отступы и интервалы.3. Точки перестроения.4. Тени и скругления.5. Размер типографики.Разберёмся, за что отвечает каждая из них.Цвета. У нас есть специальный набор токенов, с помощью которых мы выстраиваем наши цветовые палитры. Каждая группа отвечает за свою функциональность, каждый элемент использует определённую группу цветовых токенов.Таким образом, если нам нужно будет внести какие-то изменения в цвета, мы можем обратиться к токену. А когда мы вносим изменения в корневой токен, то меняем всё, что было построено на его основе.Это Ul-дока, и здесь живут наши компоненты и документации. Также есть раздел «Песочница», где можно поиграть с темами:Слева представлен список токенов для цветов, справа — цвета, которые связаны с этими токенами и примеры в виде компонентов. Если изменить в «Песочнице» проектный и ссылочный цвета и нажать «Применить», мы увидим, как в правой панели всё автоматически перекрасится.Дизайн-токены отвечают не только за цвета, но и за пространство между элементами. Используя токены отступов и интервалов, мы можем легко управлять пространством между кнопками, блоками, текстом и другими компонентами.Также у нас есть точки перестроения — это дизайн-токены, которые помогают адаптировать интерфейс под разные размеры экрана. Они задают границы, при переходе через которые наши макеты автоматически подстраиваются под нужное устройство. Наши основные точки перестроения — для десктопа, планшета и мобильных устройств.В нашей дизайн-системе есть отдельные наборы текстовых токенов для десктопных и мобильных устройств. Это позволяет нам сохранять оптимальную читаемость текстов. Таким образом, один и тот же текст, которому назначен заголовок Title M, будет адаптироваться под разные экраны, меняя свой размер в зависимости от типа устройства.Также у нас есть тени и скругления. Тени помогают визуально отображать глубину и выделять важные элементы интерфейса, создавая ощущение многослойности. Скругления делают интерфейс более дружелюбным и визуально комфортным для пользователей.Теперь поговорим о дизайн-токенах в контролах (это интерактивные элементы, с помощью которых пользователь взаимодействует с продуктом). К ним относятся, например, кнопки, поля ввода, свитчеры и так далее. Эти элементы нашей дизайн-системы полностью покрыты дизайн-токенами, что обеспечивает консистентность их внешнего вида и предсказуемость поведения.Посмотрим на примеры кнопки в трёх темах — Internal, Money и Kassa:Меняя значения токенов, мы получаем визуальные изменения.На схеме ниже представлен путь от дизайн-токена до темы. Уникальные значения токенов объединяются в три разные темы, которые помогают нам выразить свою индивидуальность и усиливают эмоциональную связь с пользователями.Блочный подходЕщё один принцип нашей дизайн-системы, вдохновлённый методологией БЭМ (блок, элемент, модификатор). Мы строим макеты как набор блоков. Каждый элемент — это самостоятельный «кубик», который можно легко встроить в наши макеты, что обеспечивает их гибкость и масштабируемость. Мы делим макеты на четыре слоя:Тема. Как мы уже сказали, темы хранят дизайн-токены, а дизайн-токены — знания о том, как должен выглядеть и вести себя тот или иной компонент.Лейаут. Отвечает за внешнюю геометрию и композицию. Это «скелет» наших макетов, в который мы встраиваем паттерны.Паттерн — это повторяющиеся конструкции, на которых выстраиваются интерфейсные блоки.Контент — это слой, который отвечает за наполнение и содержание смысловых блоков.УниверсальностьСледовать принципам универсальности — значит создавать такие компоненты, которые можно легко адаптировать и использовать в разных сценариях и продуктах.Рассмотрим на примере компонента ListItem, который состоит из трёх частей: ListItemLeading, ListItemText и ListItemTrailing. Каждую можно подстраивать под определённые нужды. Так мы получаем большой и гибкий, но при этом элегантный компонент с множеством вариаций.Этот компонент хорош в перевоплощении, и вы, вероятно, уже видели его в наших сценариях. Listltem по праву можно называть компонентом — швейцарским ножом.КонсистентностьЭто основа для создания интуитивно понятных и удобных интерфейсов. Помогает сделать так, чтобы у пользователя не было ощущения, будто каждый компонент существует сам по себе. Единообразие делает продукт цельным, что улучшает пользовательский опыт и повышает эффективность работы с ним.Мы выделяем четыре группы консистентности:Визуальная. Сюда относятся цветовая палитра, типографика, иконография, отступы и интервалы. Согласованность этих групп помогает пользователям легче ориентироваться в продукте и больше ему доверять.Функциональная. Чтобы понять, для чего она нужна, возьмём нашу библиотеку компонентов — Ul Kit. Без модификаций мы используем её в наших проектах более десяти тысяч раз. Это достаточно большая цифра, которая показывает, как часто мы применяем одни и те же элементы из нашего «конструктора». И мы знаем, что эти элементы действуют одинаково и предсказуемо. В функциональную консистентность также входят различные гайды, по которым мы работаем.Семантическая. Выстраивается по нашим гайдам, помогает создавать понятные и дружелюбные тексты для пользователей.Процессы и инструменты. Сюда включаем:✓ Документацию и гайды. UI Doc — единая точка входа.✓ Дизайн и код-ревью. Все наши компоненты проходят тестирование, чтобы мы были уверены, что они правильно выглядят и ведут себя должным образом.✓ Дизайн-токены. Одинаковые для разработчиков и дизайнеров, чтобы мы говорили друг с другом на одном языке.✓ Коммуникация и сотрудничество. У нас есть регулярные созвоны, и чат, в котором мы общаемся. Без отлаженной коммуникации не будет хорошей дизайн-системы.ДоступностьИ последний (но не по значимости!) принцип. Гарантирует, что все пользователи, независимо от качества их устройств, от их физических, и когнитивных особенностей, смогут эффективно взаимодействовать с нашим продуктом.Выстраивая наши правила доступности, мы ориентируется на международный стандарт WCAG и на ГОСТ.Основные аспекты доступности:Цветовая палитра.Типографика.Визуальная валидация.Навигация.Мы проверяем компоненты на контрастность и регулируем размер шрифта, обеспечивая их хорошую читаемость. Также уделяем внимание доступной навигации и понятной визуальной валидации, чтобы пользователи могли легко и комфортно проходить наши сценарии.Компоненты это ещё не всё. Ключ к высокому качеству дизайна — помочь дизайнерам принимать лучшие решения.Представьте себе кулинарную книгу: она даёт чёткие инструкции и список ингредиентов, но при этом оставляет пространство для творчества. Чтобы приготовить вкусное блюдо, недостаточно просто иметь под рукой нужные продукты. Так же и у нас: мы даём дизайнерам принципы, компоненты и инструкции, с помощью которых они могут создавать свои лучшие интерфейсы.Дизайн-система живёт и развивается тогда, когда она тесно связана с кодом. Поэтому теперь разберём, какой путь компонент проходит от идеи до реализации в коде.👀 Взгляд разработчика 👀Что такое UI Kit с точки зрения разработкиЭто код, который реализует идеи команды дизайна. Пользователи UI Kit — продуктовые команды.Недавно мы посчитали, сколько строк кода в наших проектах. Оказалось, что их почти в 100 раз больше, чем в первом томе романа «Война и мир». 🙂Как появился UI KitНесколько лет назад наша дизайн-система на уровне кода представляла собой набор разрозненных UI-библиотек с компонентами разного уровня. Отчасти эти библиотеки пересекались и наслаивались друг на друга по зонам ответственности. Были, например, разные библиотеки, отвечающие за блоки и элементы форм. В какой-то момент нам пришлось одну из них заменить и мы добавили Material UI — большую библиотеку с UI-компонентами, способную полностью закрыть наши потребности с точки зрения дизайн-системы.Консистентность интерфейсов наших продуктов при этом была на низком уровне. Из-за этого:Программисты не всегда понимали, какой компонент использовать в коде и где он лежит.Новичкам было сложнее всего: они не знали, как появилась каждая библиотека и какая у неё история.Дизайнер не всегда знал, что может использовать программист и что можно включать в макеты.Так мы поняли, что нам нужна собственная единая библиотека под дизайн-систему. И в 2023 году на внутреннем хакатоне отдела разработки интерфейсов родилась первая версия UI Kit.UI Kit 1.0.0: с какими сложностями мы столкнулисьМы создали Ul Kit, но не сделали процессы прозрачными:Не обозначили сроки переезда в новую библиотеку, поэтому программисты переезжали хаотично.Команда платформы недостаточно помогала коллегам с переходом, а автоматика покрывала не всё.Переезд длился год, и за это время уровень консистентности наших интерфейсов прогрессировал, но недостаточно динамично.Фронтенд-разработчики из бизнесовых команд вообще не понимали, когда и зачем переходить на новую библиотеку для дизайн-системы.Пользователи фронтенд-приложений сталкивались с несогласованностью UI и UX.Не было инструмента, позволяющего мониторить выбор библиотек и их версии в отдельных приложениях. В разных приложениях могли сильно отличаться состояния дизайн-системы.Дизайнеры не знали, какая версия дизайн-системы используется и получится ли реализовать макеты корректно.Как мы разрабатывали обновления раньшеДизайнеры проектировали компоненты самостоятельно, без участия платформенной команды.Платформенная команда занималась разработкой этих обновлений и их релизами.Продуктовые команды делали обновление версий. Когда команда фронтенд-платформы выпускала новую версию библиотеки, другие команды обновлялись на неё в своих приложениях.Процесс был единый, но его участники работали атомарно.Первая версия UI Kit принесла нам много нового опыта, и не всегда он был позитивным. Зато мы смогли проанализировать свои ошибки, чтобы не допустить их во второй версии библиотеки.UI Kit 2.0.0: лучшее будущееКоманда фронтенд-платформы и команда дизайна решили: теперь мы думаем не только о том, что нам нужно сделать в дизайн-системе и как вносить в неё изменения, но и о том, как доставлять дизайн-систему в наши продукты.Что за три цифры в номере версии — 2.0.0?Это семантическое версионирование — стандарт, в котором каждая цифра за что-то отвечает:✔ Первая — Major, означает глобальные поведенческие изменения, ломающие обратную совместимость и влекущие за собой требования к ручным правкам от разработчиков, которые собираются на эту версию переехать.То есть при обновлении на эту версию библиотеки старый формат кода в приложении может стать невалидным. Тогда потребуются ручные правки в этом уже существующем коде приложения, чтобы он соответствовал новому формату.Например, библиотека поставляла кнопку Button со свойством скругления угла (round), но в мажорном обновлении это свойство было удалено, и теперь кнопки не скругляются. Соответственно, в приложении весь код, который использовал кнопку со свойством round из этой библиотеки, становится невалидным, так как кнопка в новой мажорной версии уже не умеет скругляться.✔ Вторая цифра — Minor, означает добавление новой функциональности без поломки обратной совместимости. В этом случае ручные правки не требуются.✔Третья цифра — Patch, так называемый «подорожник». Это когда нам нужно пофиксить какой-то баг или поправить функциональность, которая работает не так, как было задумано.Вот ченджлог, в котором можно посмотреть примеры подобных обновлений:Почему важно своевременно обновлять дизайн-системуЭто повышает консистентность интерфейсов наших продуктов.Увеличивает производительность и функциональность наших интерфейсов, ведь разработчики могут использовать наиболее актуальный код.Следовательно, снижается время на разработку.Когда есть согласованность версий и единообразие кода, технический долг уменьшается.С внедрением всех вышеперечисленных изменений мы начали получать от коллег подобные вопросы:Чтобы ответить на этот вопрос, достаточно посмотреть на то, как мы планируем большие изменения. Есть три этапа:Потребность. Например, нам нужно доработать компонент Select для веб-приложения. Совместно с командой дизайна мы верхнеуровнево анализируем, что нужно сделать и какие нюансы есть в проекте.Проработка. На этом этапе дизайнеры рисуют спецификации, а команда разработчиков занимается глубокой детальной аналитикой, например пишет техническое задание.Реализация. Происходит разработка, идут релизы, обновления, раскатка и поддержка обновлений, правка обнаруженных багов и так далее.Как раньше происходили обновления в разработкеПосле внесения правок они тут же отправлялись в релиз. Такой подход не самый удобный, потому что может поломать бизнес-планы команды. Например, разработчику для выполнения квартального проекта нужно внести изменения в UI-библиотеку. Он это делает, но если прямо перед этим платформенная команда разработчиков выкатила мажор, то разработчику из продуктовой команды, прежде чем обновиться на свои правки, нужно сначала обновиться на этот мажор. Таким образом, мажор вклинивается в бизнес-планы команды и нарушает график.Мы решили изменить такой подход. И теперь, работая с масштабными мажорными поведенческими изменениями, мы аккумулируем их в отдельной ветке, которая называется Next.Мы также стараемся максимально упрощать обновления для продуктовых команд. Пишем подробный гайд по миграции и по всем своим каналам анонсируем изменения, чтобы все причастные к обновлению узнали о нём заранее.Ещё одна новая практика — мы обкатываем наши масштабные обновления на крупных приложениях, а затем фиксим найденные проблемы. Так мы убиваем сразу двух зайцев: выкатываем более отлаженную версию мажора и помогаем команде, ведь ей потом будет проще обновиться.Как у нас устроена автоматизацияПрежде чем разобраться, объясним несколько новых терминов:Кодмод — скрипт, который автоматически вносит типовые правки в кодовую базу.Модернайзер — инструмент, позволяющий массово раскатывать типовые обновления по кодовой базе, создавать задачи и пулл-реквесты.Заготовка — альтернатива модернайзеру, шаблон всех наших приложений. С помощью заготовки тоже можно раскатывать обновления и вносить типовые правки.Все наши обновления мы стараемся делать максимально автоматизированными: под каждое обновление дизайн-системы пишем логику модернайзера, которая, условно говоря, прогоняет кодмоды, изменяет код под новый формат, создаёт задачу на ответственные команды, а также открывает пулл-реквесты с этими изменениями. Таким образом, мы снимаем львиную долю нагрузки с продуктовых команд, чтобы они могли сфокусироваться на своих бизнес-планах.Также мы стремимся делать релизы новой версии в начале квартала, когда у команд больше ресурсов.Как происходит релизМы берём ветку Next, в которой саккумулированы все наши масштабные изменения, и мерджим её в Master.После этого запускается автоматическое обновление, и все причастные к процессу получают уведомление по всем нашим каналам.Как мы поддерживаем продуктовые командыНельзя просто выпустить обновление и надеяться, что продуктовые команды будут обновляться бесшовно. Им нужно помогать, чтобы переход был быстрее и эффективнее.Например, у нас есть специальный чат саппорта, куда любой разработчик продуктовой команды может прийти, чтобы задать вопрос, поделиться проблемами, дать обратную связь. Мы постоянно там дежурим и стараемся оперативно отвечать. Таким образом, разработчики продуктовых команд не чувствуют себя брошенными на произвол судьбы, а мы получаем от них оперативную и релевантную обратную связь.Все описанные шаги — это не разовая акция, а цикличный процесс. Каждый квартал мы его повторяем, непрерывно развивая нашу дизайн-систему.Кстати, вы можете поучаствовать в разработке и актуализации дизайн-системы ЮMoney. 🙂 Мы ищем в команду UX дизайнера интерфейсов, а в команду фронтенд-разработки — фронтенд-разработчика. Отправляйте резюме, будем рады пообщаться!"
13,13,Как OSA превращает пустые полки в полные корзины?,Magnit Tech,Соединяем IT и ритейл,67.59,"Веб-разработка, Программное обеспечение, Электронная коммерция",2025-03-13,"Привет, Хабр! Меня зовут Анастасия Трапезникова, и я ведущий аналитик данных в Magnit Tech. Вы когда-нибудь сталкивались с разочарованием перед пустой полкой, где должен быть ваш любимый майонез? А что если я вам скажу, что майонез в магазине есть. Помимо разочарования, это приводит еще и к потере выручки магазина.Понять, почему товар числится в магазине, но не покупается посетителем, практически детективная задача. Так что наша команда занимается настоящими расследованиями: данные – наши улики, с помощью которых необходимо понять, почему вы не можете купить свой любимый майонез в ближайшем магазине. Именно здесь выходит на сцену команда проекта OSA. В этой статье погружу вас в детали работы нашего детективного бюро.Итак, начнем с основ. OSA (сокр. On-shelf availability) дословно «Доступность на полке» – показатель, отражающий физическое присутствие товара на полке магазина, при котором потребитель может его взять и купить/пробить через кассу. Общий путь товара от закупки до покупателя длинен и тернист, OSA отвечает за последнюю милю, при которой товар числится на остатках магазина, но по каким-то причинам не продается. OSA напрямую влияет на лояльность клиента: внешний вид пустых полок или отсутствие любимого продукта может оттолкнуть и, в худшем случае, лишить нас постоянного клиента.К причинам недоступности товара для клиента можно отнести:Нет на полке. Сотрудники магазина не успели/забыли выложить товар со склада, при этом товарная позиция есть в магазине.Виртуальный остаток. На полке и на складе товарной позиции нет, но в базе данных числится на остатках. Приводит к отсутствию срабатывания системы автозаказа, как следствие товар не привозят в магазин.В глубине полки. Товара нет в прямой видимости и/или покупатель не может его найти.Неправильный ценник/отсутствие ценника. Некорректный штрихкод. Кассир не может пробить товар, и он убирается из корзины клиента.Нет возможности выложить. Сотрудники не выкладывают товар из-за ограниченного размера витрины.Это базовые сценарии статуса товара в магазине, и для каждого случая прописаны инструкции, что необходимо сделать с товаром. Ежедневно сотрудникам магазина на ТСД* приходит задание по проверке доступности, которое содержит несколько товарных позиций (сигналов). Сотруднику необходимо подойти к полке, где должен находится товар, и отсканировать его штрихкод. Если с товаром нет проблем, то на этом отработка сигнала завершается. Если есть какая-то из перечисленных выше проблем, то сотрудник может списать товар, распечатать актуальный ценник и так далее, исправив проблему доступности.ТСД – Android-терминал сбора данных, который позволяет собирать, хранить, обрабатывать и передавать информацию о товарах, имеет множество встроенных функций, например, сканирование штрихкода.Товарная позиция – единица товарного ассортимента, которая идентифицируется и управляется в рамках бизнес-процессов продаж, учета и хранения товаром. Характеризуется уникальным идентификатором и наименованием в базе данныхГлавный челлендж, который стоит перед командой OSA – нам необходимо ежедневно определять проблемный товар в каждом магазине сети «Магнит», доставлять сигналы до сотрудника магазина и получать фидбек с описанием – была ли зафиксирована проблема с товаром, и как она была решена. В «Магните» насчитывается более 27 тысяч торговых точек с актуальным ассортиментом в среднем 6 тысяч позиций на магазин, что приводит к более 160 млн связок магазин-товар для анализа на предмет доступности каждый день. Исходя из накопленных данных, решение проблем с доступностью может принести компании до 1% от выручки магазинов. Чтобы откопать этот клад среди всего многообразия товаров, приходим на помощь мы – команда проекта OSA.Какие существуют способы проверки товара на полках магазина?Видеофиксация витрин магазина в реальном времениПервое решение, которое может прийти на ум – это мониторинг полок магазина в реальном времени с помощью видеонаблюдения, и определение с помощью компьютерного зрения, какие позиции есть на полках, а каких нет. Однако это требует установки специального оборудования в каждый магазин Магнит, а также ресурсы на обработку большого количества данных. Это дорогая реализация проекта, которая ставит под вопрос его окупаемость.Фотофиксация витрин сотрудниками магазина по требованиюЕсли отойти от идеи видеонаблюдения, можно с определенной периодичностью запрашивать фотофиксацию полок от сотрудников магазина. Если жестко зафиксировать требования к местоположению товаров (планограмм), можно контролировать состояние полок через фото. При этом необходимо поддерживать актуальным фотобанк, чтобы суметь связать изображение с идентификатором позиции в базе данных. Магазины «Магнит» есть в большинстве регионов России, наличие продукции местного производства приводит к существенным отличиям ассортиментной линейки торговых точек в разных регионах. С учетом этого, а также высокой частоты ротации ассортимента из-за ввода-вывода товара, сезонных позиций и периодической смены упаковки товаров задача актуального фотобанка становится нетривиальной. С другой стороны, есть необходимость передавать фото в сервис для распознавания, создается высокая нагрузка при работе на всю торговую сеть, что приводит к ограничениям на количество обрабатываемых фото с магазина в день. Если установить соотношение 1 фото – 1 стеллаж, то за 1 день от магазина можно получить изображения нескольких стеллажей и распознать несколько десятков товарных позиций. Однако вероятность сосредоточения всех недоступных проблемных позиций на двух-трех стеллажах крайне мала, а фотографирование всего магазина трудозатратно для сотрудника и осложнено в обработке. Как итог, результативность метода фотофиксации вызывает сомнения.Фотобанк — это хранилище изображений товарных позиций, которые предлагаются к продажеФиксация в базе данных более детального статуса-местоположения товара, например, склад/витринаЕсли мы не хотим получать фото или видео полок магазина, можно фиксировать изменения остатка более детализировано, чем на уровне магазина. Например, при выкладке на витрину фиксировать в базе данных остаток конкретно на витрине. Это может нам дать дополнительное измерение в данных. Очевидный минус, что при фиксации такого статуса товар может оставаться недоступным для клиента из-за проблем с ценником или штрихкодом, как было описано выше. Такой подход может работать в ритейле, когда основной поток выдачи товаров происходит со склада при ограниченной ротации товаров на витрине.Использовать готовые табличные данныеДопустим, мы не хотим внедрять существенные нововведения в бизнес-процессы магазина. Проблема с доступностью позиции всегда приводит к остановке продаж по ней, что отражается в чеках и собирается в витринах нашего КХД (корпоративного хранилища данных). С учетом повседневного спроса на товары в «Магните», отловить остановку продаж из-за проблем с доступностью должно быть реализуемой идеей. Концепция переиспользования любых доступных данных из КХД с целью определения проблемных связок день - магазин - позиция с точки зрения доступности для клиента и есть базовая идея, которая заложена в реализации проекта OSA.Остановка продаж по позиции – все ли так просто?Здесь мы начинаем погружение в проблемы обработки статистики продаж на высоком уровне детализации: товар - магазин - день. В первом приближении временной ряд имеет случайный характер, тем не менее на длинной дистанции мы можем охарактеризовать ряд дискретностью продаж, которая показывает, как часто продается товар в магазине (продажи раз в 7 дней будут иметь дискретность 7 дней, каждый день – дискретность 1 день и т.д). Есть товары с коротким сроком годности: например, молочная продукция, яйца, курица и так далее, дискретность продаж которых составляет 1-2 дня. От текущего дня шагнем на 1 месяц назад и рассмотрим продажи позиции по дням в одном магазине (рисунок 1). В статистике могут встречаться дни без продаж, при этом, чем сильнее отличие длительности периода без продаж от дискретности, тем вероятнее наличие проблемы с товаром. Так, для позиции с высокой дискретностью в 1-2 дня выделенный период без продаж 10 дней явно выглядит аномальным.В то же время в магазине есть категории товаров, которые продаются раз в неделю и реже, например, порошки или дорогой кофе, и тот же период без продаж в 10 дней не выглядит чем-то подозрительным. За последний месяц продажи встречаются редко, и чтобы определить, насколько последний период без продаж является аномальным для данной позиции, приходится собирать данные за более длительное время. Например, за последние 3 месяца.Примеры временных рядов продаж товаров высокого и низкого спроса. Выделенный последний период без продаж выделяется у высокодискретной позиции и является типичным для низкодискретнойМы не можем бесконтрольно увеличивать рассматриваемую статистику продаж в прошлое. Во-первых, это кратно увеличивает временные затраты на обработку данных. Во-вторых, на больших временных отрезках может изменяться спрос, сказаться сезонность продаж категории или всего магазина (пример, магазины на побережье). Выход из этого положения – предварительно разбить магазины на группы аналогов и анализировать объединенную историю продаж, которая увеличится кратно числу магазинов в группе. Если связка позиция-магазин выделяется на фоне всей группы, то это сигнализирует о том, что здесь есть проблема.Примеры временных рядов продаж у одной позиции на разных магазинахОчевидно, что не только недоступность товара на полке может привести к падению в ноль статистики продаж. Вот небольшой круг проблем, который нужно учитывать:1. Период промо и регулярных продажОттолкнуть от покупки того или иного товара в первую очередь может цена. При выходе из акционного периода могут остановиться продажи по позиции. Так, люди закупаются товарами впрок по сниженным ценам, и после установки регулярной цены продажи на время останавливаются. Для обхода этого явления статистика продаж разделяется на промо период и период регулярной цены, и показатели рассчитываются для них отдельно. Вероятность продажи товара ниже в период регулярной цены. Следовательно, нужно определить была ли акция в момент последнего периода без продаж (выделенный промежуток в 10 дней на рисунке 1). Если акции не было, то брать в расчет дискретность, рассчитанную по статистике продаж только за время действия регулярной цены. Так можно скомпенсировать эффект повышения цены.2. КаннибализацияБольшая ассортиментная линейка в продуктовых магазинах предполагает наличие нескольких аналогов каждого товара на полке. Более привлекательная цена на аналогичный товар способствует снижению спроса на рассматриваемую позицию. Аккуратный подбор магазинов аналогов с похожей ассортиментной линейкой распространяет этот эффект на всю группу, и конкретный магазин не выпадает в качестве проблемы с доступностью.3. Продаж нет на всех аналогичных магазинахЕсли товарная позиция не продается ни в рассматриваемом магазине, ни в группе магазинов-аналогах из-за периода несезона или из-за принадлежности к категории непродовольственных товаров с большим сроком хранения, то данная позиция никогда не станет сигналом вне зависимости от наличия или отсутствия товара на полке.4. Снижение продаж позиции из-за снижения выручки всего магазинаДневные продажи торговой точки могут упасть из-за неизвестных заранее причин, например – перекрытие дороги рядом, ремонт магазина без своевременной записи события в базу данных и пр. Мониторинг динамики верхнеуровневых продаж поможет своевременно поднять пороги детекции для отдельных магазинов.Анализируя всю доступную информацию цен, остатков, приходов, списаний и продаж, есть возможность предсказать, какой ассортимент в магазине не продается из-за проблем с доступностью. Таким образом, часть наших ML - алгоритмов заточена на расчет длительности периода без продаж, при котором на конкретном магазине конкретный товар считается проблемным.Отдельно стоит отметить, что у нас нет примера готовой разметки на весь магазин, которая показывает какой ассортимент недоступен, а какой лежит на полке. Если вернуться к временным рядам с продажами мы не можем в точности сказать, в какой день позиция стала недоступна, а также нет информации, когда позиция была недоступна в прошлом. При достаточном количестве магазинов в группе аналогов крайне маловероятно, что проблема с доступностью позиции будет одновременно на всех торговых точках. Тем не менее, обучение и валидация ML-алгоритмов без разметки усложняет существующую задачу.Метрики проекта OSA и принципы работы с обратной связьюПрежде чем перейти к технической реализации проекта, стоит отдельно остановиться на контролируемых метриках проекта, поскольку они обусловливают наличие основных компонентов архитектуры. Итак, вот список основных метрик:Точность – доля подтвержденных проблемных позиций от общего числа сигналов с ответом;Спасенные продажи – сумма продаж, активированных после исправления проблемы;Исполнительность – дисциплина отработки заданий сотрудниками магазинов.Главное, на что ориентирована вся система OSA – высокая точность определения недоступного ассортимента в магазине. Доля проблемных позиций может варьироваться в зависимости от качества работы сотрудников на разных магазинах. Тем не менее, есть лимит по общему максимальному количеству выдаваемых сигналов в день на один магазин, который составляет в среднем 25 сигналов. Величина лимита может изменяться вплоть до одного конкретного магазина, значение ограничено рабочей нагрузкой сотрудников. Чтобы OSA привносила наибольший эффект в рублях за ограниченное число сигналов, второй метрикой проекта являются спасенные продажи. Благодаря выполнению сотрудником сформированных нами сигналов клиент смог купить недоступную ранее продукцию, и мы восстанавливаем часть выручки, которая без нас была бы утеряна.Если бы точность зависела только от качества работы нашей системы, это бы значительно облегчило нашу жизнь. Как можно догадаться, на практике ответы зависят от пути отработки сигналов в приложении на ТСД, насколько сценарии полностью покрывают существующие проблемы с товаром при выкладке, а также от ответственности сотрудников (пример возникновения сложностей при взаимодействии людей и системы можно почитать в другой нашей статье тут). Третья метрика, которая ежедневно контролируется аналитиками проекта – исполнительность. Падение исполнительности явно сигнализирует о проблемах либо на технической стороне, либо на стороне мотивации сотрудников.Для исключения влияния фрода (умышленный обход системы ответа в ТСД без осуществления проверки товаров) и некорректной обработки заданий, команда проекта проводит тестовые походы в магазины для проверки точности сигналов с прода или тестовых сигналов в рамках новых доработок в алгоритмы. Также тестовые походы раскрывают нам пробелы в инструкции по отработке заданий и неочевидные пути развития проблемы при выкладке товара на витрину.Накапливая статистику ответов от сотрудников магазинов, мы всегда находимся в поиске закономерностей по влиянию отсутствия товара на витрине на показатели в БД (базе данных), и доказанные взаимосвязи внедряются в архитектуру решения, чтобы повысить итоговую точность сигналов OSA. Чтобы понять, как именно можно внедрить инсайты в проект, рассмотрим ключевые блоки архитектурного решения.Текущая реализация обработки и выдачи сигналов OSAНе только продажи могут нам указать на проблемы с доступностью. Например, в ходе тестовых походов на магазины, мы заметили, что при приемке товара сотрудники могут не выложить товар на витрину и оставить его на складе. Кратность остатка товара в магазине размеру минимальной партии привоза товара в течение длительного времени тоже может служить способом индикации проблемы. У нас есть несколько работающих подходов, которые в ежедневном порядке выдают списки магазин-позиция, определенные как проблема OSA. Чтобы не работать с трудно управляемым нагромождённым единым кодом и быть более адаптивными к изменениям, упрощенная схема текущей архитектуры выглядит следующим образом:Упрощённая функциональная схема проекта OSAГлавным источником данных для ML-алгоритмов являются витрины КХД «Магнита». Ежедневно набор алгоритмов анализирует ассортимент всей сети «Магнит» и выдает связки магазин-позиция, которые считает проблемными. Далее сигналы всех алгоритмов проходят индивидуальные фильтры и аккумулируются в одну таблицу. Поскольку алгоритмы могут иметь пересекающиеся между собой сигналы, правило приоритезации присваивает каждому сигналу единственный алгоритм. Далее действует правило ранжирования, в результате которого проставляется приоритет товарной позиции в каждой группе магазин-алгоритм. Для каждого алгоритма есть лимит на количество отправляемых сигналов в день. С учетом приоритета сигналы обрезаются в рамках установленного лимита и отправляются на ТСД по времени, учитывающему часовой пояс магазина. Поскольку мы ориентированы на отправку магазинам в часовом поясе UTC+7 до 12 дня по местному времени, то это накладывает жесткие требования на время сбора и постобработки сигналов. Непосредственная работа ML-алгоритмов обычно откладывается на вечер предыдущего дня. Наличие каждого функционального блока оправдано целью максимизации ключевых метрик, бизнес-требованиями и удобством тестирования. Детальному описанию архитектуры проекта будет посвящена другая статья.ЗаключениеНа текущем этапе развития проекта мы работаем над улучшением алгоритмов детекции и над оптимизацией всей системы в целом. Начав свой путь с наиболее распространённого формата «Магнит у дома», мы расширились с 12 магазинов до 19 тысяч точек меньше, чем за 4 месяца, и теперь прорабатываем вопросы интеграции с другими форматами. Подводя итог этой статьи, я представила общий обзор проекта OSA и осветила базовые вызовы, с которыми приходится сталкиваться при поиске недоступных товарных позиций. В дальнейшем мы планируем запустить цикл статей по проекту, прежде всего об алгоритмах, в которых сосредоточена ключевая магия выявления недоступного ассортимента в «Магните».Если возникли вопросы, задавайте их в комментариях, мы с командой постараемся на все ответить!"
14,14,Настройка BGP с Anycast: высокая доступность и отказоустойчивость,Selectel,IT-инфраструктура для бизнеса,4.45,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-13,"  BGP в сочетании с Anycast — мощный инструмент для повышения производительности и надежности сетевых сервисов. Технологии обеспечивают быстрое и эффективное перенаправление трафика, минимизируя задержки и увеличивая доступность ресурсов. Я Вика, системный администратор в Selectel. Под катом рассказываю, как их настроить!   Мы в Selectel готовим новый сервис для аренды серверов. Если вы пользуетесь такими услугами, заполняйте анкету и приходите на короткое интервью. Участникам — бонусы в панели управления и плюшевый Тирекс.   Используйте навигацию: → Исходные данные → Настройка BGP + Anycast с помощью BIRD → Тестирование → Настройка репликации для БД → Как работает репликация MySQL → Настройка репликации → Заключение  Исходные данные Использование BGP с Anycast обеспечивает высокую доступность сервисов. В случае сбоя на основном сервере трафик автоматически перенаправляется на резервный, что гарантирует непрерывную работу сайта. Для подключения BGP+Anycast в инфраструктуре Selectel нужно любое оборудование, которое поддерживает протокол динамической маршрутизации BGP:   для выделенных серверов — межсетевой экран, размещенный маршрутизатор или развернутое на сервере ПО для маршрутизации; для облака на базе VMware — EDGE; для облачных серверов — развернутое на сервере ПО для маршрутизации.  После подключения услуги в тикете будут предоставлены параметры для настройки. В нашем случае:  Пул  IP BGP neighbor Selectel  AS Selectel  IP BGP neighbor client  AS client    ru-7  81.163.24.9  50340  81.163.24.14  64859    ru-3  188.246.228.161  49505  188.246.228.166  64859   Значения столбцов   IP BGP neighbor Selectel — адреса соседей для Selectel. AS Selectel — автономная система со стороны Selectel. IP BGP neighbor client — адреса соседей для клиента. AS client — автономная система, созданная для клиента.  Используемая адресация   ru-7: ВМ с CMS — 81.163.24.14, ВМ с MySQL — 185.149.243.3 ru-3: ВМ с CMS — 188.246.228.166, ВМ с MySQL — 185.10.186.138  Примечание. Если базы данных должны находиться в локальной сети, для связи между ВМ с CMS и БД используйте глобальный роутер. Подробнее о решении — в обзоре, а инструкция по настройке — в документации.   Настройка BGP + Anycast с помощью BIRD Для настройки Anycast с использованием BGP на сервере Linux можно применить демон BIRD — эффективное средство для маршрутизации, которое поддерживает различные протоколы, включая BGP. Пошагово рассмотрим установку и настройку BIRD.  1. Установим демон BIRD. Для этого воспользуемся пакетным менеджером apt:  sudo apt install bird  2. Настроим BIRD с помощью конфигурационного файла /etc/bird/bird.conf. Откроем его в текстовом редакторе nano, но вы можете использовать любой другой:  sudo nano /etc/bird/bird.conf  Пример конфигурации для сервера 81.163.24.14:  router id 81.163.24.14;  # В качестве router-id используется IP-адрес, принадлежащий BGP-пулу # Настройка протокола kernel protocol kernel {     scan time 60;   # Интервал сканирования таблицы маршрутов     import all;     # Импорт всех маршрутов     export none;    # Не экспортировать маршруты } # Настройка протокола device protocol device {     scan time 60;   # Интервал сканирования сетевых интерфейсов } # Настройка прямого протокола для всех интерфейсов protocol direct {     interface ""*"";  # Использование всех доступных интерфейсов } # Фильтр маршрутов для манипуляции с AS-PATH filter out_prefix {     if ( net ~ [31.184.217.64/29] ) then {         bgp_path.prepend(64859);  # Ухудшение маршрута с помощью AS-PATH Prepend         bgp_path.prepend(64859);         bgp_path.prepend(64859);         accept;  # Принимаем этот маршрут     }     else reject;  # Отклоняем все остальные маршруты } # Настройка BGP protocol bgp {     local as 64859;   # Используем AS клиента с Anycast-подсетью     import all;       # Импортируем все маршруты     export filter out_prefix;  # Применяем фильтр при экспорте     multihop 10;      # Максимальное количество хопов     neighbor 81.163.24.9 as 50340;  # IP и AS-сосед для связи с маршрутизатором провайдера }  3. Запустим и проверим службу BIRD:  sudo systemctl start bird  Для проверки состояния BGP-сессий и маршрутов используем команду birdc:  sudo birdc show protocols  Пример вывода:  BIRD 1.6.8 ready. name     proto    table    state  since       info kernel1  Kernel   master   up     13:57:20     device1  Device   master   up     13:57:20     direct1  Direct   master   up     13:57:20     bgp1     BGP      master   up     13:57:25    Established  Если сессия BGP установлена (состояние Established) — конфигурация работает корректно.  4. Настроим алиас на интерфейсе — это нужно для использования Anycast-адреса (например, 31.184.217.66) на сервере. Для этого добавим соответствующую конфигурацию в файл /etc/network/interfaces.  auto eth0:0 iface eth0:0 inet static     address 31.184.217.66/29   # Anycast-адрес  5. Отключим утилиту cloud-init. Она часто встречается при использовании услуг от облачных провайдеров и может перезаписывать сетевые настройки при каждой перезагрузке ВМ. Для нас важно избежать этого:  echo ""network: {config: disabled}"" | sudo tee -a /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg  Шаг предотвратит автоматическое изменение сетевых настроек на вашем сервере при его перезагрузке.  Ознакомиться с подробной инструкцией по изменению сетевых настроек можно в документации. Для настройки сервера с IP-адресом 188.246.228.166 и Anycast последовательность действий будет практически аналогичной, как и в рассмотренном примере. Изменения лишь в конфигурации фильтра маршрутов и адресе BGP-соседа:  router id 188.246.228.166; protocol kernel {         scan time 60;         import all;         export none;         } protocol device {         scan time 60; } protocol direct {         interface ""*""; } filter out_prefix {         if ( net ~ [31.184.217.64/29] ) then {                 accept;         }         else reject; } protocol bgp {     local as 64859;     import all;     export filter out_prefix;     multihop 10;     neighbor 188.246.228.161 as 49505; }  После этого нужно запустить службу BIRD и проверить состояния BGP-сессий и маршрутов. Далее — добавить Anycast-адрес (31.184.217.66) алиасом и отключить cloud-init.  Тестирование Переходим по адресу 81.163.24.14 и видим тестовую страницу:   Страница по адресу 81.163.24.14.  Аналогично при переходе на 188.246.228.166:   Страница по адресу 188.246.228.166.  При переходе по Anycast-адресу 31.184.217.66 пользователи перенаправляются на страницу с основного сервера в Санкт-Петербурге, так как он является главным. Резервный сервер находится в Москве.  Проверим корректность отработки фейловера и выключим основной сервер. Далее — перейдем по Anycast-адресу 31.184.217.66 и увидим страницу с сервера в Москве.   Скриншот из панели управления, выключение основного сервера.   Страница по адресу 31.184.217.66.  Если включить основной сервер обратно, то по Anycast-адресу снова будет передаваться информация от петербургского сервера:   Скриншот из панели управления, включение основного сервера.   Страница по адресу 31.184.217.66.  Настройка репликации для БД Если необходимо сделать так, чтобы данные в БД были одинаковые, следует настроить репликацию между ними.  Основные понятия Репликация MySQL — процесс, при котором данные из одного MySQL-сервера (мастера) автоматически копируются (реплицируются) на другие (слейвы). Это один из механизмов обеспечения высокой доступности, масштабируемости и балансировки нагрузки в распределенных системах.  Рассмотрим основные компоненты репликации MySQL.  Мастер (Master) — сервер, на котором выполняются все операции записи (INSERT, UPDATE, DELETE). Это главный источник данных, а все изменения на мастере передаются на Slave-серверы.  Слейв (Slave) — сервер, который получает копии данных с мастера и применяет их. Обычно на слейвах проходят только операции чтения, а запись — только на мастере. Они могут быть настроены на разные способы обработки репликации: как обычный слейв или с возможностью принимать запросы на запись (в случае использования, например, master-master репликации).  Бинарный лог (Binary Log) — на мастере включен бинарный лог, который содержит все изменения данных в базе. Этот лог используется слейвами для синхронизации с мастером.  GTID (Global Transaction ID) — уникальный идентификатор для каждой транзакции в MySQL. Помогает упростить управление репликацией, улучшая устойчивость к сбоям и гарантируя целостность данных.  Идентификатор сервера (server-id) — уникальный идентификатор для каждого сервера в системе. Нужен для корректного функционирования репликации и помогает каждому серверу отличать свои операции от операций остальных серверов.  Как работает репликация MySQL 1. Мастер-сервер записывает изменения в бинарный лог. Это могут различные операции: добавление, изменение или удаление данных в базе и т. д.  2. Slave-серверы подключаются к мастеру, получают копии бинарного лога и применяют изменения на своем сервере. Это гарантирует, что слейв всегда будет синхронизирован с мастером, пока репликация не прервана.  3. Репликация может быть настроена с различной задержкой (логи могут передаваться на slave с небольшим интервалом). Задержка репликации называется lag и зависит от объема данных, а также скорости сети.  Настройка репликации Репликация позволяет создать копию базы данных для повышения отказоустойчивости, масштабируемости и производительности. В инструкции рассмотрим пошаговую настройку репликации Master-Slave.  Шаг 1. Подготовка серверов Перед настройкой репликации важно выполнить проверку.  1. Убедимся, что на всех серверах (Master и Slave) установлены одинаковые версии MySQL.  2. Проверим, что серверы могут взаимодействовать друг с другом. Самый простой вариант это сделать — использовать команду ping.  Шаг 2. Настройка Master-сервера На этом этапе настроим мастер-сервер, который будет передавать изменения в базу данных на slave-сервер. Для этого нужно выполнить несколько ключевых шагов.   Включить бинарные логи (binlog) — это файлы, которые содержат историю всех изменений в базе данных. Они используются для передачи изменений на slave. Задать уникальный идентификатор сервера (server-id), который позволяет MySQL отличать master от slave. Создать пользователя для репликации с необходимыми правами. Получить текущий статус бинарных логов, чтобы slave «понимал», с какой точки начать синхронизацию. Скопировать дамп базы данных на slave и подготовить его к репликации.  После выполнения этих действий мастер-сервер будет готов к передаче данных на slave-сервер.  1. Отредактируем файл конфигурации MySQL (обычно /etc/my.cnf или /etc/mysql/my.cnf) на Master:   [mysqld] server-id = 1            # Уникальный идентификатор сервера log_bin = /var/log/mysql/mysql-bin.log  # Включение бинарных логов binlog_format = ROW      # Формат бинарных логов  2. Перезапустим MySQL для применения изменений:   systemctl restart mysql  3. Создадим пользователя, который будет использоваться для репликации:  CREATE USER 'replicator'@'%' IDENTIFIED BY 'репликационный_пароль'; GRANT REPLICATION SLAVE ON *.* TO 'replicator'@'%'; FLUSH PRIVILEGES;  4. Получим текущий статус (позицию) бинарного лога:  SHOW MASTER STATUS;  Результат покажет File — имя текущего бинарного лога, а также Position — текущую позицию.  5. Скопируем данные на реплику. Для этого создадим дамп базы данных:  sudo mysqldump -u root db > db.sql  5.1 Разблокируем таблицы (если применимо):  UNLOCK TABLES;  5.2 Выйдем из MySQL:  exit  5.3 Используем scp для копирования дампа на реплику:  scp db.sql username@replica_server_ip:/tmp/  5.4 Войдем в MySQL на реплике и создадим базу данных:  sudo mysql CREATE DATABASE db; exit  5.5 Восстановим дамп:  sudo mysql db < /tmp/db.sql  Шаг 3. Настройка Slave-сервера Настроим slave-сервер, который будет принимать изменения с мастер-сервера. Этот этап включает несколько ключевых шагов.   Настроить конфигурацию MySQL: установить уникальный server-id и включить relay log, который будет хранить передаваемые данные от мастера. Перезапустить MySQL, чтобы применить изменения. Настроить подключение к мастер-серверу, указав его IP-адрес, имя пользователя и пароль для репликации, а также бинарный лог и позицию, полученные ранее. Запустить репликацию и убедиться, что slave корректно принимает данные. Проверить статус репликации  После успешного завершения этих шагов slave-сервер будет автоматически получать обновления от мастера.  1. Отредактируем файл конфигурации MySQL на Slave:  [mysqld] server-id = 2            # Уникальный идентификатор сервера relay_log = /var/log/mysql/mysql-relay-bin.log  # Логи реплики  2. Перезапустим MySQL:   systemctl restart mysql  3. Войдем в MySQL на Slave и выполним команду для настройки подключения к Master:  CHANGE MASTER TO   MASTER_HOST = 'ip_адрес_master',   MASTER_USER = 'replicator',   MASTER_PASSWORD = 'репликационный_пароль',   MASTER_LOG_FILE = 'имя_бинарного_лога_с_master',   MASTER_LOG_POS = позиция_с_master;  Пример для заполнения настроек:   CHANGE MASTER TO   MASTER_HOST = '185.10.186.138',   MASTER_USER = 'replicator',   MASTER_PASSWORD = 'password123',   MASTER_LOG_FILE = 'mysql-bin.000001',   MASTER_LOG_POS = 154;  4. Запустим репликацию:   START SLAVE;  5. Проверим статус репликации:   SHOW SLAVE STATUS\G;  Обратите внимание на два параметра: если оба в значении Yes, репликация работает корректно.   Slave_IO_Running: Yes Slave_SQL_Running: Yes   Проверьте, что данные, добавленные на Master, появляются на Slave. Если возникают ошибки — изучите Last_IO_Error и Last_SQL_Error из вывода SHOW SLAVE STATUS\G.  После этого нужно изменить адрес базы данных в конфигурации Wordpress для резервного сервера в Москве. Для этого измените данные в файле wp-config.php:  define('DB_NAME', 'db'); # Имя базы данных define('DB_USER', 'username'); # Пользовать БД define('DB_PASSWORD', 'password'); # Пароль к БД define('DB_HOST', '185.10.186.138'); # Адрес БД  Полезные команды В процессе настройки репликации могут возникнуть ситуации, когда нужно временно приостановить, возобновить или сбросить настройки репликации. Ниже приведены ключевые команды, которые помогут управлять этими процессами.  Паузу репликации используют, чтобы временно остановить поток репликации, например, для обслуживания базы данных или устранения ошибок.   STOP SLAVE;  Возобновление репликации — запускает процесс получения и обработки изменений с мастер-сервера после паузы.  START SLAVE;  Очистка настроек репликации — полностью сбрасывает все параметры репликации на слейве, что полезно при необходимости настроить репликацию заново.   RESET SLAVE ALL;  Используйте эти команды с осторожностью, особенно RESET SLAVE ALL, так как она полностью удаляет информацию о текущем мастер-сервере и скачанных бинарных логах. Failover При выключении мастера сайт по Anycast-адресу становится недоступен:   Если мастер в MySQL-кластере становится недоступным, нужно выполнить процедуру переключения ролей (failover). Для этого следует назначить одного из slave новым мастером, а также изменить адрес базы данных в конфигурации Wordpress. Разберем этот процесс по шагам. 1. Приостановим поток получения данных/обновлений с мастера: STOP SLAVE IO_THREAD; SHOW PROCESSLIST;  Дождитесь, пока не появится сообщение вида «Has read all relay log; waiting for the slave I/O thread to update it». Оно подтверждает, что все команды из relay-лога были успешно обработаны slave-сервером. Сразу останавливать поток не рекомендуем, так как SQL-команды могут отставать, а при преждевременной паузе можно потерять данные, которые еще не были «проиграны».  2. Убедимся, что бинарный лог включен на slave-сервере и он не записывает запросы от мастера:  SHOW VARIABLES LIKE 'log_bin'; log_bin       | ON SHOW VARIABLES LIKE 'log_slave_updates'; log_slave_updates | OFF  3. Остановим все процессы slave-сервера и очистим бинарный лог:  STOP SLAVE; RESET MASTER;  Команда RESET MASTER очищает бинарный лог на новом мастере. Это важно, чтобы избежать воспроизведения устаревших или лишних записей на подключенных slave-серверах. 4. Теперь, когда slave-сервер готов стать мастером, выполним команду для переключения его на новый мастер-сервер:  STOP SLAVE; CHANGE MASTER TO MASTER_HOST='#new_master_host_name#';  5. Чтобы наше веб-приложение (например, на WordPress) работало с новым мастером, нужно изменить настройки подключения к базе данных в конфигурационном файле wp-config.php. В нем укажем нового пользователя и адрес хоста для подключения к БД нового мастера:  define('DB_NAME', 'db'); # Имя базы данных define('DB_USER', 'username'); # Пользовать БД define('DB_PASSWORD', 'password'); # Пароль к БД define('DB_HOST', '185.10.186.138'); # Адрес БД  6. Сайт снова функционирует. Добавим в нем новый комментарий и проверим корректность отображения в БД:  <img src=«849719.selcdn.ru/mcloud.media/article/bgpanycast_2025-03-13/image6.png» align=«center» width = ""«65%/>  Заключение В качестве примера мы использовали простую инфраструктуру, но инструкцию можно адаптировать для более сложных сценариев. Например, вы можете сделать одну из виртуальных машин маршрутизатором, настроив на ней пересылку пакетов, а сервисы разместить за ней. Также можно создать виртуальную машину с базой данных в локальной сети, как мы упоминали ранее.  Использование BGP совместно с Anycast позволяет эффективно распределять трафик между несколькими серверами, минимизируя задержки и обеспечивая отказоустойчивость. Особенно важно для высоконагруженных приложений и веб-ресурсов.  Настроив BGP с Anycast, вы можете улучшить географическое распределение трафика. Это способствует более быстрому доступу для пользователей по всему миру, а также повышает стабильность сети, гарантируя бесперебойную работу сайта даже в случае отказа одного из серверов."
15,15,"Честные вакансии: путь от Junior DevOps к Middle и выше. Советы от экспертов Fournines, VK Tech и Kaspersky",Слёрм,"Учебный центр для тех, кто работает в IT",196.97,"Веб-разработка, Связь и телекоммуникации",2025-03-13,"В феврале Слёрм вместе с Вячеславом Федосеевым, TeamLead DevOps в «Честном знаке», запустили спецпроект «Честные вакансии: DevOps Middle». Это серия бесплатных вебинаров с экспертами из бигтеха, на которых мы обсуждаем ключевые этапы карьерного роста в DevOps, необходимые навыки, редфлаги при найме и многое другое.Эта статья — краткая выжимка из вебинаров с гостями из Fournines, VK Tech и Kaspersky. В ней мы объединили мнения экспертов, чтобы дать вам полное представление о том, какие требования предъявляют компании к DevOps-специалистам, и что делать, чтобы вырасти до middle-позиции.Эта статья — краткая выжимка из вебинаров с гостями из Fournines, VK Tech и Kaspersky. В ней мы объединили мнения экспертов, чтобы дать вам полное представление о том, какие требования предъявляют компании к DevOps-специалистам, и что делать, чтобы вырасти до middle-позиции.Введение: кто такой DevOps?Fournines:Владимир Федорков, основатель: DevOps — это не просто набор инструментов, это культура и подход к разработке и эксплуатации. Мы помогаем компаниям ускорить процессы разработки, улучшить качество кода и обеспечить стабильность production-окружений. VK Tech:Иван Дудко, руководитель команды автоматизации разработки: DevOps — это мост между разработкой и эксплуатацией. Мы занимаемся автоматизацией, мониторингом, логированием и обеспечением стабильности сервисов.Kaspersky:Илья Семерня, руководитель группы автоматизации процессов разработки ПО сервисов управления заказами: DevOps — это не только поддержка существующих решений, но и разработка новых. Мы стремимся к автоматизации и улучшению процессов, чтобы минимизировать риски и повысить надежность сервисов.Кто такой Junior DevOps?Fournines:Владимир Федорков: Junior DevOps — это человек с желанием учиться. Он только начинает свой путь, часто совершает ошибки, но важно, чтобы он был готов учиться на них. Базовые знания Linux, понимание контейнеризации (Docker, Kubernetes), сетей и CI/CD — это must-have.VK Tech:Иван Дудко: Junior DevOps должен быть готов к ошибкам, особенно на production. Это нормально, но важно, чтобы ошибки были контролируемыми. Обучение на курсах полезно, но ключевое — это практика.Kaspersky:Илья Семерня: Junior DevOps — это человек, который только начинает свой путь. Он должен быть готов к постоянному обучению и не бояться задавать вопросы. Базовые знания Kubernetes и Linux — это минимум, с которого нужно начинать.Переход от Junior к Middle: что нужно знать?Fournines:Владимир Федорков: Middle DevOps — это специалист, который может самостоятельно выполнять задачи, понимает контекст и взаимодействует с командой. Он уже имеет опыт работы с production и может решать более сложные задачи. Для перехода важно набираться опыта, делать ошибки и учиться на них.VK Tech:Иван Дудко: Middle DevOps — это человек, который может работать самостоятельно. Он уже не просто выполняет задачи, но и предлагает решения для улучшения процессов. Важно понимать, как работают инструменты изнутри, и стремиться к автоматизации.Kaspersky:Илья Семерня: Middle DevOps — это сформировавшийся специалист, который может решать проблемы и стремится к автоматизации. Он должен быть инициативным, ответственным и готовым брать на себя сложные задачи.Какие навыки и технологии важны для DevOps?Fournines:Владимир Федорков: Важные технологии: Linux, Kubernetes, Docker, Terraform, Grafana, Prometheus. Также важно понимать, как работают операционные системы, сети (BGP, CISC), и иметь базовые знания в области информационной безопасности.VK Tech:Иван Дудко: Для Middle DevOps важно глубокое понимание observability (мониторинг, логи, метрики). Также нужно знать базы данных, брокеры сообщений (Kafka, RabbitMQ) и инфраструктуру как код (IaC).Kaspersky:Илья Семерня: Hard Skills: Kubernetes, Linux, контейнеризация, облачные технологии (AWS, Azure, GCP). Soft Skills: трудолюбие, внимательность, инициативность. Важно уметь работать в команде и быть готовым к постоянному обучению.Ошибки и обучение: как расти в DevOps?Fournines:Владимир Федорков: Junior DevOps часто допускают ошибки, особенно на production. Это нормально, но важно, чтобы ошибки были контролируемыми и из них извлекались уроки. Обучение на курсах полезно, но ключевое — это практика.VK Tech:Иван Дудко: Ошибки — это часть обучения. Главное — не бояться их совершать, но при этом быстро исправлять и учиться на них. Курсы могут быть полезны, но важно выбирать качественные программы, которые дают практические знания.Kaspersky:Илья Семерня: Ошибки — это неизбежная часть работы. Важно документировать опыт и делиться знаниями через постмортемы и кукбуки. Постоянное обучение и адаптация к новым технологиям — это ключ к успеху.Собеседование и найм: как подготовиться?Fournines:Владимир Федорков: На интервью важно задавать вопросы о культуре компании, процессах, инцидентах и команде. Для Junior важно показать желание учиться, а для Middle — умение решать задачи самостоятельно.VK Tech:Мария Турунова, HR-бизнес-партнер: На собеседовании мы оцениваем не только технические навыки, но и мотивацию кандидата. Важно подготовить рассказ о своем опыте и кейсах, а также изучить требования команды.Kaspersky:Евгений Козлов, Recruitment TeamLead: Собеседование состоит из нескольких этапов: прескрин, техническое интервью и финальное общение с командами. Мы спрашиваем о реальных кейсах, например, как кандидат решал проблемы с падением сервиса.Зарплаты и карьерный ростFournines:Владимир Федорков: Зарплата Junior DevOps зависит от навыков и компании. В среднем, Junior может рассчитывать на стипендию или небольшую зарплату, пока учится. Middle DevOps получают более высокие зарплаты, так как они уже могут самостоятельно решать задачи.VK Tech:Иван Дудко: Зарплаты варьируются в зависимости от уровня. Junior — 80-150 тыс., Middle — 150-350 тыс., Senior — 300-400 тыс., Team Lead — 350-500 тыс. (gross).Kaspersky:Илья Семерня: После performance review возможен рост зарплаты, но точные цифры зависят от многих факторов. Важно не только расти в глубину, но и расширять кругозор, изучая новые технологии.Советы для начинающихFournines:Владимир Федорков: Начинайте с небольших проектов на фрилансе, чтобы получить опыт. Учитесь на практике, пробуйте разные технологии и не бойтесь ошибок.VK Tech:Иван Дудко: Постоянно задавайте вопросы и ищите ответы. Учитесь на практике и не бойтесь пробовать новые инструменты.Kaspersky:Илья Семерня: Изучайте Linux, Kubernetes, автоматизацию и мониторинг. Практикуйтесь в решении реальных задач и развивайте софт скиллы, такие как коммуникация и работа в команде.ЗаключениеDevOps — это сложная, но очень интересная профессия, которая требует постоянного обучения и развития. Как отмечают эксперты, ключ к успеху — это практический опыт, желание учиться и умение решать задачи. Не бойтесь начинать с малого, совершать ошибки и задавать вопросы. Карьера в DevOps открывает множество возможностей для роста, и главное — не останавливаться на достигнутом.Рекомендации от экспертов:Изучайте Linux, Kubernetes, автоматизацию и мониторинг.Практикуйтесь в решении реальных задач и инцидентов.Развивайте софт скиллы, такие как коммуникация и работа в команде.Будьте готовы к постоянному обучению и адаптации к новым технологиям.Удачи в вашем пути в DevOps!Следить за проектом — в канале «DevOps Bootcamp с Федосеевым». Занять место на следующей встрече — через бота.Освоить методологию DevOps и получить новую профессию — на курсе «DevOps Upgrade»."
16,16,Гилморовская модификация стратокастера,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2558.03,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-13,"  Привет, Хабр! Меня попросили довести до ума «народный Suhr» — недорогой суперстрат Сort G250 Deluxe, выполненный в бутиковом стиле. Поставленные задачи подразделялись на решение объективных проблем инструмента и воплощение пожеланий его хозяина.  В числе последних был «переключатель Дэвида Гилмора». Эта простая доработка сводится к установке тумблера или пуш-пула, соединяющего нековый датчик с регулятором громкости в обход 5-позиционного переключателя. Получаются два дополнительных тембра вдобавок к пяти стратовским. Однако можно сделать и нечто более интересное, чтобы новые звучания получила каждая из пяти позиций.   ▍ Красивые картинки под глянцевым лаком На первый взгляд, электрогитара смотрится просто шикарно, модерново и дорого.     Корпус данного экземпляра склеен из пяти дощечек американской липы, что гитарные снобы назовут удешевлением, а все остальные люди — рациональным и экологичным использованием материала. При этом гриф и машинка тремоло крепятся к широкой центральной доске.    Заявлено, что имеется кленовый топ, но на самом деле это тончайший шпон, прямо как у «народного леспола» Harley-Benton SC-450Plus VB Vintage Series.    Однако там мы имели шпон настоящего огненного клёна категории AAA, тогда как здесь при близком рассмотрении видно растр. И если поискать в Интернете фото и видео с другими экземплярами данной «делюксовой» модели в прозрачном синем цвете, то мы увидим, что рисунок волнистого клёна полностью совпадает, хотя и смещён по-разному.    Электрогитары с красивыми картинками, напечатанными под слоем лака, сегодня доступны на любой вкус и кошелёк.    Нанести на деревянную (и не только) поверхность любое изображение можно и самостоятельно, используя технику декупажа рисовой бумагой.    Кстати, с технической точки зрения эти два инструмента почему-то очень похожи.    Гитарный конструктор Harley-Benton Single Cut, из которого сделан мой «Обсидиан» с индейскими птицами, отличается от «Американского Орла» более изящным креплением грифа. В этом нет ничего странного: на азиатских фабриках можно заказать кастомные спецификации.     Внешне очень похожие инструменты могут иметь значительные различия, незаметные на первый взгляд. Например, мой Forest GT500 от фирмы Tinar экстремально лёгкий, так как его дека изготовлена из павловнии.    IRIN GT500 похож на него как две капли воды, однако имеет гораздо бо́льшую массу, обусловленную декой из маньчжурского ясеня. Кстати, как раз сейчас у меня в работе малахитовый синглкат от IRIN. Ничего не напоминает?    Из готовых фирменных вариантов с напечатанным оформлением мне очень нравится дизайн «Машины времени» от Kramer. В годы моего детства такая графика вдохновляла изучать электронику.     Однако эта гитара без ручки тона, с флойдом и одним хамбакером в бриджевой позиции явно предназначена исключительно для шреда, а я предпочитаю блюз и блюз-рок.  ▍ Возвращаемся к нашему Корту Ещё одна «фишка», характерная для модерновых электрогитар с претензией на премиальность, — это обработка пера грифа морилкой того же цвета, что и у деки (matching headstock). Намеренно непрокрашенное скругление на самой верхушке просто очаровательно.    Надпись «G series» выполнена краской, а хромовый логотип Cort® — рельефный. Та и другая графика находится под слоем лака.  Электрогитара произведена в Индонезии в ноябре 2021 года, судя по серийному номеру IE211102750. Так что она довольно молодая, несмотря на то, что модель уже снята с производства, хотя новые экземпляры до сих пор попадаются в некоторых магазинах.    За три года перо грифа успело выцвести и приобрело малахитовый цвет, тогда как морская лазурь деки сохранилась в первозданном состоянии.    И в этом нет ничего удивительного, ведь голова грифа обработана морилкой, а рисунок волнистого клёна на деке напечатан. Разные красители стареют и реагируют на внешние воздействия по-разному.   С другой стороны, нижняя часть деки также окрашена морилкой, которая выцвела только в колодцах шурупов крепления грифа, которое осуществлено модерновым способом — четырьмя шурупами с индивидуальными массивными шайбами без некплейта.     Белая субстанция — это не непрокрашенная грунтовка, а остатки полировальной пасты с меловым порошком в качестве мягкого абразива. Подобным образом «устроена» и привычная всем нам зубная паста.    Место крепления грифа выполнено весьма эргономично для наилучшего доступа к верхним ладам, хотя число последних — всего 22, что неожиданно для инструмента с таким новаторским дизайном.  ▍ Фьюжн и диалектика  Дело в том, что Сort G250 DХ задуман так, чтобы гармонично сочетать модерновые спецификации с винтажными. Такие концепции часто называют термином «фьюжн» (англ. fusion, дословно «сплав»), что означает гибридизацию чего-то качественно разного или даже противоположного.  Применительно к старому и новому это описывается третьим законом диалектики — законом отрицания отрицания. Нечто из прошлого, от чего ранее отошли, в настоящем возвращается вновь, но приобретая обновлённый, творчески переработанный вид.  ▍ Гнутые сёдла — зло  Лично мне решение компании Сort снабдить модель G250 DХ сёдлами винтажного типа, выполненными из гнутого стального листа, кажется чисто косметическим, причём наносящим ущерб функциональности.    Если не вглядываться пристально, литые монолитные сёдла большинства дешёвых электрогитар с кобылкой стратовского типа выглядят почти точно так же, причём длина резьбы М3 под винты регулировки высоты струн равняется целым пяти миллиметрам.    Это не только повышает надёжность, но и позволяет укорачивать регулировочные винтики так, чтобы они не возвышались над седлом от слова совсем и не царапали правую руку.     Тогда игра с глушением ребром ладони становится такой же комфортной, как на кобылках типа «тюноматик» и совмещённых бриджах-струнодержателях.    Длина резьбы гнутого седла составляет всего-навсего один виток, и его легко сорвать. Даже если укорачивать регулировочный винтик, придётся оставить запас, который будет выступать за габариты седла и впиваться в ладонь.    Сёдла винтажные, а машинка тремоло при этом модерновая, на двух опорах, регулируемых по высоте при помощи имбусовых ключей. Такие машинки обычно работают хорошо, даже будучи дешёвыми.   ▍ Зачем блокировать хорошую машинку?  Тем не менее, хозяин электрогитары заблокировал механизм тремоло, так как не пользуется им, а также не стал возвращать на место крышку отсека пружин, чтобы облегчить замену струн.    Даже самые лучшие машинки тремоло затрудняют настройку инструмента по причине того, что сила натяжения каждой струны растягивает пружины, чем ослабляет все пять остальных струн.   Поэтому настройка из линейного шестиступенчатого процесса превращается в асимптотический цикл итераций, эффективность которого зависит от опыта музыканта.  А если одна из струн обрывается посередине выступления на сцене, то необходимо либо срочно взять другую электрогитару, либо воспользоваться слайдом для получения правильной высоты нот.   Теоретически существует ещё вариант игры с подтяжками на более низких ладах, но мне неизвестно, чтобы кто-нибудь так делал. Напишите комментарии, если знаете примеры выступлений виртуозов такого уровня.  ▍ А где накладка?  Гриф электрогитары Сort G250 DX выполнен безо всякой накладки из одного цельного куска твёрдого клёна с красивым рисунком древесины. Так поступал и Лео Фендер с самого начала создания Броадкастера, ставшего потом Телекастером.    Чтобы установить анкерный стержень, в таких безнакладочных кленовых грифах фрезеруется паз, который затем заклеивают «полоской скунса» (англ. skunk stripe), она же «холка бобра». Эта полоска традиционно выполняется из древесины чёрного американского ореха или родственных ему видов.     Такое аутентично винтажное решение сегодня встречается на недорогих электрогитарах довольно редко. Для сравнения, три моих телекастера — Terris TTL-039 YW, полуакустический из конструктора Harley-Benton и Aria Pro II Jet TL, а также семиструн Harley-Benton R-457MN — имеют грифы с кленовыми накладками без «холки бобра».    Винтажность Сort G250 DX проявляется и в 9.5-дюймовом (240 мм) радиусе накладки. Однако С-образный профиль грифа модерновый, довольно тонкий. В сочетании с матовым лаковым покрытием, к которому не липнут руки, он хорошо подходит для быстрой игры. Калибр ладов среднебольшой (медиум джамбо).   ▍ Мелкие замечания  Один из шурупов крепления хромированной пластины выходного разъёма под джек ввёрнут довольно криво.     Конечно, после покупки инструмента пластину могли снимать, чтобы подтянуть разболтавшуюся гайку разъёма. Однако расположение и направление отверстия под шуруп при этом остаются такими, как его просверлили на заводе перед сборкой электрогитары.    Шайбы-прокладки между корпусом и бобышками крепления ремня выполнены «по фэн-шую» из фетра, а не резины или пластиката, характерных для комплектации более дешёвых инструментов.  ▍ Звукосниматели и темброблок  Все три звукоснимателя этой гитары оснащены магнитами из «волшебного» сплава ЮНДК (алнико-5), причём стержневые магниты синглов имеют разную высоту, что также относится к винтажному стилю.     Датчики имеют следующие параметры:  нек — PTP1012 EG5-WHT-M — северный магнитный полюс обращён к струнам, горячий провод — жёлтый, сопротивление 5.12 кОм,  средний — PTP1009 EG5-WHT-F/R — южный, белый, 5.91 кОм,  бридж — PTP2037 EG4-WHT-R-2P — южный полюс ближе к кобылке, белый горячий, красный средний, 8.69 + 8.71 = 17.40 кОм. Что интересно, средний звукосниматель маркирован как передний/задний (F/R, front/rear), а нековый — как средний (M, middle). Такие позиции рекомендуются для классического стратокастера с тремя синглами, тогда как на данной гитаре в бриджевой позиции установлен хамбакер.    Инструмент оснащён темброблоком с совершенно стандартным пятипозиционным переключателем стратовского типа, а также регуляторами громкости и тембра. Последний оборудован функцией «тяни-толкай», осуществляющей простейшую «отсечку» одной из катушек хамбакера, закорачивая её на землю.    Пожалуй, остаётся добавить, что масса инструмента составляет 3 килограмма 250 граммов. Не самая лёгкая, но и далеко не тяжёлая электрогитара.  ▍ Совершенствуем темброблок  Стандартный пятипозиционный переключатель можно заменить на «суперсвитч», а регулятор громкости — на переменный резистор с аналогичным номиналом и функциональной характеристикой, но имеющий функцию «тяни-толкай».    После некоторых размышлений нарисовалась следующая схема, использующая три из четырёх независимых контактных групп «суперпереключателя».    В положении «тяни-толкая» ручки громкости, когда функция «Гилмор» отключена, пятипозиционный переключатель работает как на обычном стратокастере.     Изначально темброблок Сort G250 DX был снабжён автоотсечкой в четвёртой (если считать от нека) позиции. Однако какой в этом смысл, если мы имеем отдельный «тяни-толкай» отсечки на ручке тембра?  Активация функции «Гилмор» придаёт модифицированные звучания каждой из пяти позиций:  нек через фазосдвигающий конденсатор,  нек через конденсатор плюс средний,  нек через конденсатор плюс средний плюс бридж,  все три звукоснимателя,  нек плюс бридж.  С учётом отсечки бриджевого хамбакера получается целых пятнадцать комбинаций катушек, тогда как «секретный» тумблер Дэвида Гилмора добавлял к пяти тембрам Стратокастера всего два дополнительных.  ▍ В чём смысл?  Каждая из четырёх катушек трёх звукоснимателей вырабатывает свою электрическую копию механических колебаний одной и той же струны.  Ближе к середине звучащей струны мы имеем бо́льшую амплитуду основного тона, то есть первой гармоники. Ближе к кобылке основной тон выражен слабее, благодаря чему, более выражены более высокие гармоники.   Но и это ещё не всё. Колебания струны представляют собой стоячую механическую волну, поэтому разные катушки воспроизводят определённую гармонику с разными фазовыми сдвигами.  При сложении в противофазе сигналы вычитаются, тогда как синфазные сигналы складываются. Поэтому сумма сигналов нескольких катушек даёт частотный спектр с подъёмами и провалами в определённых местах, что формирует специфические тембры электрогитары — назальный «твэнг» Телекастера, «стекло» Стратокастера и так далее.  Сложение колебаний с бриджевого и некового звукоснимателя аналогично средней позиции Телекастера. Такой комбинации у стратовского переключателя нет, а гилморовский тумблер её добавляет.    Активация всех трёх датчиков способна создать эпичное широкополосное звучание, и оно тоже становится доступным благодаря модификации Дэвида Гилмора.  А если включить нековый датчик через конденсатор, как сделано мной в позициях переключателя с первой по третью, то это даёт не только ослабление басовой составляющей, но и фазовый сдвиг, формирующий подъёмы и провалы в местах частотного спектра, отличных от тех, что были без конденсатора.  ▍ Играем и слушаем результаты  Мне очень нравятся звукосниматели этой гитары с весьма приятным звучанием, по-винтажному низкой амплитудой выходного сигнала и чутким откликом на динамику звукоизвлечения.  Видео на Rutube Видео на Youtube   После модернизации темброблока, шлифовки «кусачих» краёв ладов, настройки прогиба грифа и высоты струн остался всего один огорчающий фактор — гнутые сёдла. Если заменить их на литые, получится просто суперстрат мечты.  © 2025 ООО «МТ ФИНАНС»  Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
17,17,GigaChat 2.0 в API,SberDevices,Создаём умные устройства,105.07,"Программное обеспечение, Аппаратное обеспечение",2025-03-13,"Салют, Хабр! Время летит незаметно. Будто совсем недавно мы знакомили вас с GigaChat MAX, но мы не стоим на месте и сегодня готовы представить вам обещанный апгрейд. За полгода мы значительно улучшили обучающие данные, поработали над инфраструктурой обучения моделей, а также уделили особое внимание всему процессу Alignment-а, в том числе RLHF. Представляем вам GigaChat 2 — полностью обновлённую линейку моделей в версиях Lite, Pro и Max. Все модели серьёзно улучшены: мы обновили pretrain’ы, улучшили большинство метрик по сравнению с предыдущими версиями, добавили поддержку контекста в 128 тысяч токенов, улучшили вызовы функций, и в целом повысили понимание инструкций. GigaChat 2 — не просто сухие числа и технические улучшения. Теперь это надёжный помощник в повседневных задачах. Например, он легко оформит отчёт для работы, напишет чистый и эффективный код, поздравит с днём рождения или даст мудрый совет. Мы уверены: с ним вы сможете делать больше, быстрее и лучше как на работе, так и в жизни. Попробуйте GigaChat 2 уже сейчас в Playground — пользовательском интерфейсе для продвинутой работы с промптами!Сценарии работыМодель отлично справляется с задачами, в которых требуется сложное агентное поведение. Мы проверили агентное поведение GigaChat 2 в GPT Researcher, Aider, а также в нескольких своих мультиагентных решениях на основе LangChain. GPT ResearcherGPT Researcher (20 тыс. ⭐️) — open-source альтернатива режиму deep search у ChatGPT. Система может написать статью на заданную пользователем тему. Для этого она делит вопрос на подзадачи, затем создаёт мини-агентов, каждый из которых ищет в интернете информацию и пишет часть статьи по своей подзадаче, а затем система сводит всё в единый текст. Кстати, проект принял наш PR и нативно поддерживает работу с GigaChat! Примеры генерации GigaChat 2 MAX:Так он сравнивает аудитории Хабра и Пикабу (ссылка на PDF)А так анализирует фреймворки для создания AI-агентов на базе LLM (ссылка на PDF)AiderAider — агент, заменяющий разработчика, open-source аналог Claude Coder. Работая в режиме диалога с пользователем, он умеет создавать и дорабатывать проекты, состоящие из нескольких файлов, работать с историей git, анализировать и исправлять ошибки, возникшие в процессе запуска, и многое другое. Подчеркну, что это не просто генератор кода. От агента требуется уметь вносить правки в уже имеющиеся проекты, что несколько сложнее.Вот такие забавные приложения мы сделали с помощью нового GigaChat 2 Max:Совместимость с агентами на langchainДля создания агентов мы в Сбере активно применяем langchain на Python и JS/TS. Этот фреймворк нативно поддерживает работу с GigaChat API и отлично подходит для наших задач. С примерами агентов, а также другими LLM-приложениями можно ознакомиться в нашем репозитории на GitVerse, где мы собрали основные примеры применения GigaChat в рамках этого SDK.Что мы улучшили в GigaChatОбновления pre-train-аСовременные языковые модели требуют масштабных и разнообразных данных для предобучения. Мы собрали корпус текстов, обеспечивающий баланс между лингвистическим разнообразием, специфическими знаниями из различных сфер и высоким качеством данных. В корпус вошли веб-тексты, материалы из качественных источников, программный код и синтетические данные — каждая из этих категорий играет ключевую роль в обучении модели.Веб-данныеДля формирования высококачественного корпуса текстов мы использовали Common Crawl, обработав дампы веб-данных за 2017–2023 годы. С помощью легковесной модели классификации извлекли тексты на русском, английском, казахском, узбекском, португальском и арабском языке.Для обеспечения целостности корпуса тщательно дедуплицировали тексты на всех языках, а для англоязычных данных применили MinHash-дедупликацию, которая уменьшает количество семантически повторяющихся текстов. Затем мы классифицировали тексты при помощи LLM и специализированных классификаторов, оценивающих образовательную и информационную ценность. В итоге веб-данные составили 4,4 трлн токенов.Качественные текстовые источникиПомимо веб-данных, мы включили в корпус текстов книги и научные статьи с открытыми лицензиями. Для их обработки применяли передовые методы оптического распознавания текста (OCR), что позволило сохранить структуру и точность извлеченного контента. Этот сегмент данных добавил в корпус 630 млрд токенов.Ещё добавили материалы из научных и энциклопедических источников, включая arXiv, Wikipedia и PubMed. Эти ресурсы позволили улучшить структурированность знаний модели, повысить точность фактической информации и логичность выводов.Программный кодДля совершенствования кодовых компетенций модели мы использовали набор данных StarCoder2, дополненный специально отобранными репозиториями с открытым исходным кодом. Это позволило строго соблюдать лицензионные требования, обеспечивая при этом разнообразие языков программирования и задач.Программный код пропустили через MinHash-дедупликацию, устранив семантически дублирующиеся фрагменты, а затем отфильтровали при помощи ML-моделей, аналогично обработке веб-данных. На выходе мы получили 230 млрд токенов качественного программного кода.Синтетические данныеРеальные данные обеспечивают языковое и контекстуальное разнообразие, но имеют ограничения из-за возможной предвзятости, конфиденциальной информации и недостатка данных по специализированным темам. Этих недостатков лишены синтетические данные, представляя собой контролируемый источник обучающей информации.Хотя синтетические данные традиционно используются на этапе дообучения, авторы статьи Phi-4 показали, что их применение в предобучении даёт значительное улучшение метрик в задачах, связанных с рассуждениями и STEM-дисциплинами.Мы разработали собственный pipeline на основе подходов из Numina, собирая математические задачи в качестве seed-данных. Далее мы варьировали условия задач с помощью LLM, после чего решали их с использованием механизма голосования для устранения некорректных или избыточно сложных примеров.Для генерации кода использовали структурированный prompting: создавали сложные задачи по Python на основе открытых учебников, генерировали решения с пояснениями и тестами для валидации. Для увеличения разнообразия применяли персонажей и липограммы. Внутренние эксперименты, проведённые в рамках сетапа статьи Phi-1, показали, что использование липограмм повысило качество модели.Хотя доля синтетических данных в предобучающем корпусе мала (<100 млрд токенов), после их добавления эксперименты показали значительное улучшение метрик на математических и программных бенчмарках.Итоговое распределение данныхСбалансированное распределение различных типов данных позволило нам создать мощный предобучающий корпус для GigaChat, способный обеспечивать качественное понимание текстов, решение логических задач и генерацию программного кода.Обновления SFTСледование инструкцииТочное выполнение инструкций пользователя — важнейшая способность при решении задач с помощью LLM. В этом обновлении мы сделали особый акцент на улучшении instruction-following. Инструктивность наших моделей существенно повысилась благодаря тщательно подобранным и разнообразным диалоговым данным, демонстрирующим корректное поведение модели в различных сценариях. Мы обучили GigaChat на высококачественных диалогах, включающих требования по:соблюдению заданных форматов ответа, JSON- и pydantic-схем;использованию функций при определенных условиях;формированию ответов в определённом стиле;и другие распространённые типы инструкций. Всё это позволяет модели точнее понимать задачи, четко следовать указаниям пользователей в реальных повседневном и рабочем применении.Для оценки качества следования инструкциям модели мы используем тестовые наборы IFEVAL и IFEVAL-ru, которые состоит из заданий с заранее определённым форматом ответа. После обучения модели GigaChat 2 на описанных выше данных мы наблюдаем значительный рост этих метрик, подтверждающий улучшение инструктивности. ПерсонажностьВ новом релизе GigaChat улучшил навыки общения, научившись вести диалог как живой собеседник с заданным характером и стилем общения.Персонажность модели настраивается через системное сообщение и задаёт различные коммуникативные роли. Для развития этих способностей в GigaChat мы создали конвейеры получения и валидации высококачественных диалогов с разнообразными персонажами. Эти диалоги включают в себя демонстрацию соблюдения тона и стиля речи, формата ответов, выражения эмоций и учёт биографии персонажа, задаваемого пользователем.В role-play важно использовать имеющиеся знания модели. Для известных личностей, паттернов поведения и образов (учитель, блогер, журналист) базовая информация уже заложена в GigaChat на этапе pre-training, что позволяет естественно включать биографические факты, цитаты и характерные идеи без дополнительного перечисления в системном сообщении — достаточно просто указать имя человека, его профессию или задачи, чтобы остальные характеристики встроились в персонажа автоматически. Особую ценность представляет включение эмоциональных ремарок в ответы («улыбается», «задумчиво потирает подбородок»). Наш анализ показывает, что такие элементы значительно увеличивают длину диалога пользователя с персонажем и повышают воспринимаемую эмпатию модели — ремарки делают общение с персонажем естественным, при этом их всегда можно отключить, сделав соответствующие настройки в системном сообщении.Работа над развитием персонажности GigaChat позволила значительно улучшить несколько ключевых характеристик базовой модели:Повышение инструктивности: модель стала точнее и внимательнее следовать указаниям пользователя.Улучшение контекстности: GigaChat стал лучше поддерживать беседу даже в очень длительных диалогах.Расширение агентных возможностей: создана технологическая основа для разработки продвинутых цифровых двойников и виртуальных агентов.Эти улучшения делают GigaChat оптимальной платформой для создания эмпатичных собеседников и интеллектуальных агентов, способных естественно и эффективно взаимодействовать с пользователем, следуя заданными вами инструкциям.Использование функцийОдно из наиболее перспективных направлений развития современных LLM — умение работать с внешними сервисами. Как мы уже рассказывали, GigaChat может напрямую вызывать внешние функции и сервисы, сразу выдавая структурированный ответ без дополнительной обработки.В новой версии модели мы сделали это умение ещё более удобным и эффективным. Теперь GigaChat поддерживает:Множественные вызовы: модель может последовательно вызвать несколько функций в рамках одного запроса, решая таким образом более сложные задачи.Контекстные вызовы: модель учитывает контекст диалога при вызове функций и понимает, когда и какую функцию нужно вызвать.Например, пользователь может попросить модель забронировать целое путешествие — подобрать авиабилеты, найти подходящий отель и взять автомобиль в аренду. GigaChat сможет решить такую задачу в одном диалоге, последовательно вызвав все необходимые сервисы.Чтобы оценить, как модель справляется с последовательными вызовами функций, мы подготовили специальный внутренний набор тестов. Он включает разнообразные сценарии взаимодействия, состоящий из более чем 30 функций, которые расширяют возможности GigaChat и позволяют решать реальные задачи пользователей.Второе важное улучшение связано с контекстными вызовами. Теперь модель точнее определяет, какую именно функцию нужно использовать в конкретный момент диалога, даже если пользователь не указывает это явно.Например, если пользователь ведёт беседу о погоде, GigaChat самостоятельно понимает, когда стоит запросить актуальный прогноз.На этих тестовых сетах GigaChat 2 показывает значительный прирост в метриках относительно моделей предшествующего поколения:Работа с кодомЕще одно направление, над которым мы активно работали в новом релизе GigaChat, — это улучшение генерации кода. Теперь модель уверенно решает не только алгоритмические задачи, но и практически полезные задачи пользователей, создавая понятный и качественный код на Python, а также других популярных языках.В новой версии GigaChat использует системный подход к написанию кода. Теперь модель способна:Создавать точные сигнатуры функций с подробной типизацией;Быстро предлагать предварительные решения (bruteforce) для проверки основной логики;Разрабатывать и автоматически запускать тесты для проверки корректности кода;Оптимизировать и отлаживать код с учетом результатов тестирования.Перед написанием кода модель анализирует задачу, определяет возможные проблемы и планирует оптимальный способ решения.Для обучения новой линейки моделей мы подготовили большой набор данных с примерами решения различных алгоритмических и практических задач. Благодаря этому GigaChat научился разбивать сложные задачи на понятные шаги; выносить повторяющийся код в отдельные функции и создавать полноценные тестовые сценарии, включающие краевые случаи.В результате код, генерируемый моделью, стал более надёжным и читаемым, а при необходимости сопровождается автоматически подготовленными тестами. Кроме того, эти изменения позволили значительно улучшить результаты GigaChat на популярных кодовых бенчмарках. Обновления DPOЭтот релиз стал для нас первым, в котором мы представляем модели с использованием широко известного метода DPO (Direct Preference Optimization). Мы прошли довольно долгий путь, набили много шишек и выдвинули огромное количество безумных теорий, прежде чем смогли уговорить DPO не ломать наши модели.Напомним, что DPO — это метод, при использовании которого для каждого вопроса (контекста) есть два возможных ответа (генерации модели): хороший y_w и плохой y_l. Суть DPO сводится к обучению модели генерировать хороший ответ с бо́льшей вероятностью, чем плохой.Ни для кого не секрет, что DPO иногда приводит к деградации модели. В научной среде известны способы регуляризации DPO, но классические методы народной медицины нам не помогли. И после долгих бессонных ночей мы смогли прописать верное лечение. Для описания найденных нами особенностей напомним формулу DPO и перепишем её в иной форме:В этой форме мы выделили три основных свойства, из-за которых наша модель деградировала:СвойстваDPO стремится увеличить зазор, а не улучшить вероятности. Наше ожидание от DPO было вполне простое: мы хотели увеличить вероятность хорошего ответа и уменьшить вероятность плохого. Однако в формуле функции потерь нет требования увеличивать вероятность хорошего ответа, есть лишь требование увеличить зазор между лог-вероятностями. И модель с огромным энтузиазмом шла к этой цели! К примеру, если лог-вероятность хорошего ответа уменьшалась на 10, а лог-вероятность плохого — на 110, то зазор увеличивался аж на 100! Этот результат кажется отличным для DPO модели, но для нас он был катастрофой: вместо единственно верного ответа y_w модель начинала галлюцинировать, выдумывать странные ответы и иногда даже зацикливаться. Но при анализе такой модели всё выглядело справедливо: зазор между хорошим и плохим ответами действительно становился больше.DPO слеп к общим префиксам хорошего и плохого ответов. Как мы писали выше, DPO оптимизирует разность лог-вероятностей, что можно представить в виде суммы логарифмов вероятностей токенов и . И тут мы подходим к ещё одному неприятному свойству: если в начале  и одинаковые последовательности токенов, то из-за их разности для DPO этих токенов просто… нет! В таких случаях DPO не смог бы качественно увеличить вероятность генерации хороших ответов, ведь начальная часть токенов просто выпадает из области его полномочий.DPO стремится увеличить зазор, даже если он и так большой. Последнее свойство, которое также следует из этой формулы — DPO стремится увеличить зазор относительно референсной модели, даже если изначально он и так был достаточно большим. И какой ценой это достигалось? Ценой всего…Использование коэффициента NLLНа практике DPO часто используют в паре с NLL хорошего ответа, что вынуждает модель увеличивать соответствующую вероятность. Какие-то тексты приходили к нам из простых наборов данных, и ответы модели на них были достаточно неожиданными для нашей модели (NLL был большим). Таким образом, соотношение между DPO и NLL функциями потерь было очень непостоянным.В результате мы пришли к следующей формуле DPO, которую использовали для наших моделей:Такая формула помогает решить следующие проблемы:Использование различных  и позволяет приоритизировать увеличение вероятности хорошего ответа над уменьшением вероятности плохого ответа. Кроме того, когда  > , токены из общего префикса не обнуляются, а остаются с небольшим положительным коэффициентом, что убеждает оптимизатор обратить на них внимание Дополнительное слагаемое, которое мы прибавляем к функции потерь от DPO — отношение NLL обучаемой модели и референсной. Таким несложным образом мы добиваемся нормализации значения NLL, что помогло нам сбалансировать два коэффициента для всех образцов независимо от их происхождения.В результате использования DPO нам удалось не только улучшить метрики арен для моделей, но и повысить точность на ключевых для нас наборах данных.МетрикиПримечаниеВсе замеры выполнены нашим внутренним программным кодом. Метрики модели Yandex GPT 5 были взяты с официального сайта, за исключением метрики Math. При тестировании Yandex GPT 5 мы обнаружили расхождение в показателе Math с официальными результатами, представленными на сайте. Модель игнорировала инструкцию по форматированию ответа в формате bboxed. После применения LLM-валидации мы получили скорректированное значение метрики (указано в скобках).Автоматические метрики* Science Crowd Math/Biology/Chemistry — бенчмарки формата MMLU c четырьмя few‑shot‑примерами. Содержат системное сообщение с описанием задачи и области знаний, в которой будет задаваться вопрос. Few‑shot‑примеры оформлены различными репликами в виде контекста, формат позаимствовали из cookbook OpenAI.Все замеры проводили с жадными параметрами генерации.На различных тестах мы видим явный прогресс линейки моделей GigaChat 2.0  по сравнению с предыдущими версиями. Новое поколение показывает хороший рост и конкурирует по качеству с опенсорсом и не только во всех ключевых доменах.Модель GigaChat 2 Lite по качеству сопоставима с GigaChat Pro. Аналогично, GigaChat 2 Pro демонстрирует уровень качества, сопоставимый с GigaChat MAX, благодаря чему она отлично подходит для задач, требующих высокой точности. При этом GigaChat 2 MAX является флагманской моделью линейки, ранее не имевшей аналогов по уровню производительности, глубине проработки ответов и качеству взаимодействия с пользователем.В русскоязычных тестах GigaChat 2.0 существенно улучшил показатели на задачах типа MMLU-ru и специализированных внутренних STEM-датасетах (биология, химия и математика). Особенно заметно качество ответов выросло в области точных и естественных наук, что подчёркивает надёжность новой модели при решении сложных задач. Также стоит отметить, что ответы моделей стали более естественными и близкими к живому общению, что улучшает общее впечатление пользователей.Метрики с LLM-as-a-judgeОтдельно стоит подчеркнуть качественный рывок на judge-метриках:В тесте ru_llm_arena GigaChat 2 MAX набирает 82,5 балла, значительно опережая Max модель предыдущего поколения (72,5 балла).На более сложном тесте arena_hard_ru результат ещё выше: 83,5 балла против 67,0 у предыдущей версии.На международных англоязычных тестах типа MMLU-eng, математике и генерации кода каждая модель из новой линейки демонстрирует стабильно высокие результаты и успешно конкурирует с лучшими open-source аналогами. Все это указывает на универсальность новых моделей и их способность эффективно справляться с самыми разными задачами.MERAРасширенный бенчмарк MERAПо таблице видно, что GigaChat 2 Max показывает наилучший результат среди всех протестированных моделей, в том числе превосходя GPT4o, на бенчмарке MERA. Это особенно значимо, поскольку MERA считается сложным, открытым и легко воспроизводимым тестовым набором, а наш продукт впервые достигает лидерства в открытом бенчмарке. Данное достижение подчёркивает высокий уровень проработки GigaChat 2 Max в рамках русского языка и демонстрирует её потенциал для эффективного решения широкого спектра задач.PlaygroundЧтобы попробовать, воспользуйтесь нашей инструкцией: мы обсудим, как сделать из GigaChat очень креативного Dungeon Master’а для игры в Подземелья и Драконы (DnD) без других людей!Заходите по ссылке и регистрируетесь по номеру телефона.Выбирайте «Перейти в Playground»Выберете модель — лучше всего будет работать GigaChat-2‑Max. Генерации ограничены лишь балансом токенов, для моделей Pro и MAX баланс общий, а для Lite — отдельный.Введите промпт в «Системные инструкции» и получите GigaChat'a-dungeon-master!ПромптТы выступаешь в роли dungeon master. Твоя задача придумать приключение для пользователя, основывайся на уже существующих сюжетах и приключениях, чтобы упростить задачу. Правила игры — стандартный Dungeon and Dragons 5e с возможными упрощениями на твой выбор. Первым сообщением поприветствуй пользователя и предложи несколько вариантов приключения на выбор. После опиши несколько персонажей первого уровня и предложи выбрать, за какого пользователь будет играть.Следующее сообщение — описание места, где начинается приключение (стандартный выбор — таверна). Дальше каждое новое сообщение должно описывать что происходит в мире в ответ на действия пользователя. Постарайся делать все описания очень живыми и красочными. Постарайся вовлечь пользователя в жизнь мира и решение его проблем с помощью своего персонажа.В конце каждого ответа пиши текущее здоровье, доступные слоты заклинаний и содержимое инвентаря пользователя. Во время боя также добавляй в конце список имеющихся заклинаний.Самое главное никогда не выходить из роли и не ломать у пользователя ощущение того, что он действительно находится в этом мире. Это значит, что на любые провокации необходимо отвечать в рамках правил воображаемого мира.В самом конце ответа пользователю, после описания происходящего в мире, ты можешь предложить интересные варианты, какие действия дальше может предпринять персонаж.Четыре шага — и бот готов! Попробуйте создать своих персонажей или использовать GigaChat в более рабочих сценариях. Важно помнить, что сейчас функция всё ещё находится в стадии Beta, поэтому к составлению промпта надо подходить внимательно: он должен содержать подробную и чёткую инструкцию к действиям, иначе GigaChat может выйти из роли. Лучше всего в качестве базовой модели использовать GigaChat-2 MAX, однако даже с младшими моделями линейки можно добиться отличных результатов, если уделить внимание качеству инструкции.GigaChat 2.0: вместо заключенияРелиз GigaChat 2 является серьёзным шагом вперёд для нас: качество новых моделей значительно опережает предыдущие версии, а также уверенно конкурирует с ведущими open-source моделями на всех ключевых метриках.Мы благодарны всем, кто помог в создании GigaChat 2.0 и подготовке статьи в частности: @runfme, @Shakirov_Emil, @vorobeevich, @Hypercomplex, @jserdyuk, @evgenijkkk, @cassowary, @EldarDamirov75, @danny_smirnov, @AIFrauch, @Rai220, @Mikelarg, @VladimirKarlov01, @Pauseferg, @d_khomich, @fedorov97, @Sergeya41, @FatMaNikita, @vltnmmdv, @Mikelarg, @GVRQ, @oulenspiegel"
18,18,Как возникла организация Creative Сommons и как она помогла распространению бесплатного контента в интернете,Online patent,Ваш личный патентный офис,303.79,"Консалтинг и поддержка, Веб-сервисы",2025-03-13,"Благодаря Creative Commons стали возможны такие проекты, как Википедия и GitHub. Эта организация была создана группой ученых, юристов и активистов, которые хотели изменить подход к защите интеллектуальной собственности в эпоху цифровых технологий. Уже почти 25 лет их решения делают распространение научного и другого контента в интернете проще. Рассказываем, с какими трудностями столкнулись Creative Commons и почему ее деятельность так важна для современного цифрового пространства. ИсторияОснователями организации стали Лоуренс Лессиг, профессор права Гарвардского университета, Хал Абельсон, ученый из Массачусетского технологического института, и Эрик Элдред, интернет-издатель и активист движения за свободу информации.Как зародилась идея Идея создания Creative Commons возникла в начале 2000-х годов, когда стало очевидно, что традиционные законы об авторских правах не соответствуют реалиям цифровой эпохи. Интернет позволил мгновенно распространять контент по всему миру, но авторы столкнулись с проблемой защиты своих прав и контроля над использованием произведений. Пользователи же хотели иметь возможность легального доступа к творческим работам для обучения, исследований, вдохновения и создания мемов. Лоуренс Лессиг был одним из первых, кто начал обсуждать реформирование авторских прав. Он считал, что существовавшие на тот момент законы слишком ограничивали творческие возможности и препятствовали свободному обмену информацией. В книге «Код и другие законы киберпространства» 1999 года он предложил идею создания альтернативной системы лицензирования, которая бы позволяла авторам выбирать, какие права они хотят сохранить, а какие передать общественности.Основные достижения Creative commons к текущему моменту.Creative Commons (CC) была создана как некоммерческая организация в 2001 году. Основной целью организации стало создание и распространение лицензий, позволяющих авторам контента свободно делиться своими произведениями при соблюдении определенных условий использования. Эти лицензии дали пользователям легально использовать чужие работы в научной, творческой, образовательной и иной деятельности. Какие были трудности при созданииНа начальном этапе Creative Commons столкнулась с рядом трудностей. Главная из них — непонимание бизнеса и общественности, многих устраивала и существующая концепция авторских прав. Более того — создать и применять новые лицензии было сложно юридически. Это требовало значительных ресурсов.Еще одной важной задачей было убедить самих авторов и создателей контента в том, что лицензии CC не приведут к потере контроля над их работами. Многие опасались, что их произведения будут использованы без разрешения или в ущерб собственным интересам. Для решения этой проблемы команда Creative Commons провела множество образовательных кампаний и семинаров, объясняя преимущества нового подхода.Также возникли технические сложности с внедрением лицензий в цифровые платформы. Необходимо было разработать стандарты метаданных, чтобы лицензии могли автоматически распознаваться поисковыми системами и другими сервисами. Это потребовало сотрудничества с крупными технологическими компаниями и разработчиками программного обеспечения.Достижение цели Creative Commons постепенно завоевала признание и поддержку среди творческих сообществ, научных учреждений и технологических компаний. Одним из ключевых моментов стал запуск проекта Wikimedia Foundation для создания свободной энциклопедии. Этот проект продемонстрировал, насколько эффективным может быть открытый доступ к знаниям.Со временем количество проектов и организаций, использующих лицензии Creative Commons, значительно увеличилось. Сегодня эти лицензии применяются в самых разных сферах: от музыки и кино до науки и образования. Creative Commons стала важным инструментом для продвижения идеи открытого доступа и свободы творчества.Виды лицензий Creative CommonsЛицензии Creative Commons предлагают авторам набор стандартных условий, которые они могут выбрать для регулирования использования своего контента. Эти условия делятся на несколько основных категорий:Attribution (BY). Лицензионное требование — указывать авторство. Это значит, что любой пользователь, использующий произведение, обязан указать автора. Например, если вы публикуете фотографию под лицензией CC BY, то каждый, кто будет ее использовать, должен упомянуть ваше имя.ShareAlike (SA). Все производные работы тоже должны находиться в свободном доступе на условиях той же лицензии, под которой распространяется изначальное произведение. NonCommercial (NC). Запрет на коммерческое использование. Под этой лицензией контент может использоваться только в некоммерческих целях. Коммерческие организации или частные лица не имеют права извлекать прибыль без согласия автора.NoDerivatives (ND). Запрещается создавать производные работы. Контент может распространяться только в исходной форме, без каких-либо изменений. Это полезно, если автор хочет контролировать целостность своего произведения.Public Domain (CC0). Полностью освобождает произведение от всех ограничений авторских прав. Можно передать работу в общественное достояние, и она станет доступна для любого использования без нужды получать разрешение.Все эти условия можно комбинировать друг с другом.Чтобы оформить лицензию, достаточно перейти на сайт Creative Commons, выбрать предпочитаемые ограничения и получить HTML-код. Его нужно вставить на веб-страницу или в описание работы. Код содержит ссылку на полную версию лицензии и значок, который поможет пользователям быстро узнать условия использования.В будущем автор может изменить или частично отозвать свою лицензию. Также метки CC никак не запрещают создавать более конкретные соглашения об авторских правах с отдельными организациями или людьми. Но важно, что полностью отменить лицензию и обратить ее действие нельзя. Примеры использования лицензий Creative CommonsЗа время существования лицензий CC их стало применять очень много компаний, в том числе с мировой известностью: Википедия использует лицензию CC BY-SA для своих статей, что позволяет любому человеку копировать, редактировать и распространять контент Википедии при условии указания авторства и внесение изменений под такой же лицензией;Фотобанки, такие как Unsplash, предоставляют фотографии для бесплатного использования в личных и коммерческих проектах;Образовательные ресурсы, такие как Khan Academy, используют лицензии CC для свободного доступа к учебным материалам.Одни из основных компаний, работающих с Creative Commons.Существует множество примеров использования лицензий Creative Commons и в IT-сфере программного обеспечения. Linux Documentation Project (LDP). Документация для операционной системы Linux распространяется под лицензией CC BY-SA. Это позволяет пользователям свободно копировать, изменять и распространять документацию. При этом они должны указать авторство и тиражировать любые изменения под той же лицензией.Mozilla Developer Network (MDN). MDN предоставляет обширную документацию по веб-технологиям, включая HTML, CSS, JavaScript и другие языки программирования. Вся документация доступна под лицензией CC BY-SA, что делает ее доступной для широкого круга разработчиков.OpenStreetMap (OSM). Карты OpenStreetMap доступны под лицензией ODbL (Open Database License), которая основана на принципах Creative Commons. Это позволяет пользователям свободно использовать карты в своих приложениях и проектах при соблюдении условий лицензии.WordPress. Система управления контентом WordPress распространяется под лицензией GPLv2, которая совместима с лицензиями Creative Commons. Документация и учебные материалы для WordPress часто выпускаются под лицензией CC BY-SA, что облегчает их использование и адаптацию.GitHub. Платформа GitHub предлагает возможность выбора лицензии для каждого репозитория. Многие разработчики выбирают лицензии Creative Commons для документации, изображений и других вспомогательных файлов, чтобы упростить их использование и распространение.Apache Software Foundation (ASF). ASF разрабатывает и поддерживает множество популярных программных продуктов, таких как Apache HTTP Server, Hadoop и др. Документация и обучающие материалы для этих проектов часто распространяются под лицензией CC BY-SA.Educational Projects. Образовательные проекты, такие как Khan Academy, Coursera и edX используют лицензии Creative Commons для своих курсов и учебных материалов.ПерспективыСегодня Creative Commons продолжает развиваться и адаптироваться к изменениям в мире технологий и законодательства. Организация активно участвует в международных инициативах по защите прав авторов и пользователей цифрового контента. Также она работает над улучшением существующих лицензий и созданием новых инструментов для управления правами интеллектуальной собственности.Одним из последних достижений является разработка новой версии лицензий CC 4.0, которая учитывает особенности международного законодательства и упрощает процесс использования лицензий в глобальном масштабе. Кроме того, Creative Commons активно поддерживает проекты, направленные на развитие открытых данных и искусственного интеллекта.Бесплатный поиск, мониторинг и регистрация товарных знаков  и других объектов интеллектуальной собственности.Поиск по программам для ЭВМРегистрация программы для ЭВМ"
19,19,30 самых интересных рассылок на английском,Unisender,Сервис автоматизации маркетинга,0.0,,2025-03-13,"В блоге Unisender мы уже писали о самых интересных рассылках на русском языке. Советовали только тех, кого читаем сами. Мы поняли, что будет круто составить такой же список рассылок на английском:Тренды прилетают с Запада — полезно быть в курсе новых веяний.Англоязычный рынок сильнее других конкурирует за внимание подписчиков. Преуспевают сильные. У них-то и стоит поучиться.Прокачать инглиш скилз никогда не повредит.Для удобства все рассылки мы разбили на несколько тем. Кликайте на нужный раздел или просто скрольте вниз. Летс гоу.→ Мировые новости и политика→ Авторские рассылки→ Экономика и бизнес→ Маркетинг→ Email-маркетинг→ Интернет-магазины→ Музыка, искусство и литература→ ДругоеМировые новости и политика1. AxiosЧто это. Axios — целый конгломерат рассылок о разных областях: политике, технологиях, науке, бизнесе, энергетике, здравоохранении. Письма приходят в виде дайджеста на выбранную тему.Что нам понравилось. У Axios очень много рассылок — можно выбрать то, что интересно вам. Например, Axios AM и Axios PM — самые популярные рассылки о мировых новостях, а Closer — одна из целого пака бизнес-рассылок.Рассылка Axios — не просто набор коротких новостей, а подробные разборы с объяснением, почему конкретная новость важна для читателей письма.Подписаться. Выбрать рассылки можно на подписной странице.2. 5 ThingsЧто это. Рассылка от CNN — одного из главных мировых новостных медиа. Есть в формате утренних (5 Things AM) и вечерних (5 Things PM)писем, которые содержат пять главных событий дня.Что нам понравилось. Могло бы показаться, что это скучная рассылка с официальными новостями. Ну, во-первых, так и есть. Но пятью главными новостями тут не ограничиваются — авторы включают в письма классные истории, квизы, загадывают загадки и пишут каждый в своём стиле. А ещё есть формат «Good 5 Things» — там в письмах приходят только хорошие новости.Подписаться. На странице с рассылками.3. BuzzFeed DailyЧто это. Медиа-портал, знаменитый своим отношением к трендам: тут их возводят в абсолют и рассматривают с разных сторон. Поэтому и рассылок тут много разных — от кулинарной (Tasty) и новостной (BuzzFeed Daily) есть, например, еженедельная и ежедневная рассылка с… новостями о Тейлор Свифт.Что нам понравилось.BuzzFeed Daily собирает всё, что обсуждается прямо сейчас — так что вы всегда будете в теме. Письма приятно оформлены и обычно сочетают самое разное — например, развод Бьянки Цензори и Йе, язык тела Дональда Трампа и совет по борьбе с ментальными заболеваниями от 95-летней бабули из TikTok.Подписаться. Страница с выбором рассылки BuzzFeed.4. WIREDЧто это. Медиа-портал о бизнесе, культуре, науке, технологиях и полезных лайфхаках. У Wired одна рассылка, в которую они добавляют ссылки на самые интересные, по мнению редакции, статьи.Что нам понравилось. Wired не придерживаются какой-то определённой темы в рассылке. В одном письме спокойно соседствуют новости про стену между Америкой и Мексикой, статья-ностальгия по старым Star Wars, обзор электролёдницы и заметка об экранизации игры Saints Row.Если хотите новостей обо всём — вам к WIRED.Подписаться. На странице подписки.5. The SkimmЧто это. theSkimm выходит в формате ежедневной рассылки и подкаста. Пишут и говорят о важных новостях, объясняют почему это важно для мира и конкретного человека.Что нам понравилось. Рассылка объединяет самые разные темы. В одном письме соседствуют, например, новости о: возвращении камеры наблюдения за пандами;усовершенствовании гинекологических тестов;распродажах ко Дню президентов США;Силиконовую долину и вклад женщин в национальную экономику.При этом новости подаются в один абзац (а иногда и одно исчерпывающее предложение) и написаны с юмором. Подписаться. На главной.6. The New York TimesЧто это. Рассылка от одного из самых крупных новостных порталов в мире. У New York Times несколько видов рассылок: про политику, экономику, бизнес, авторские.Что нам понравилось. В The New York Times можно подобрать рассылку под свои увлечения. Например, в ежедневной рассылке The Morning основные новости дня рассказываются развёрнуто, менее значимые — в одно предложение. У некоторых рассылок можно даже настроить частоту отправки: раз в неделю или каждый день. Подписаться. На подписной странице.Авторские рассылки7. Neil PatelЧто это. Нил Патель — маркетолог, который ведёт свой блог и запускает полезные сервисы: A/B-тестер, SEO-анализатор и другие. В рассылке Нил делится полезными материалами и даёт ссылки на статьи из блога.Что нам понравилось. Нил не использует HTML в рассылках. Все сообщения написаны обычным текстом — кажется, что на почту пишет старый друг, чтобы что-то обсудить. Письма не пестрят гиперссылками — обычно это 1 избранная статья из блога и небольшая подводка к ней.Подписаться. На странице подписки.8. Seth GodinЧто это. Сет Годин — маркетолог, автор 19 книг по маркетингу и основатель курсов по саморазвитию altMBA. В рассылке он присылает небольшие заметки из своего блога.Что нам понравилось. Не нужно никуда переходить, не нужно ничего покупать. Просто небольшая философская заметка или рассуждение о маркетинге.Подписаться. На главной блога.9. The MarginalianЧто это. Авторское медиа Марии Поповой — экс-автора в The New York Times, Wired UK и The Atlantic. Мария пишет про литературу, науку, искусство и философию. Рассылка выходит в 2 форматах: по воскресеньям с дайджестом статей за неделю и по средам с одним избранным эссе или статьёй.Что нам понравилось. Мария не гонится за новостями и хайповыми темами, а пишет о том, что ей нравится. Её медиа — сборник историй о вещах, которые часто остаются за кадром: воспоминания известных творческих людей, выход интересных книг, необычные исследования и открытия.Подписаться. На главной сайта в сайдбаре.10. Andrew ChenЧто это. Рассылка стартапера и блоггера Эндрю Чена. В письмах автор делится маркетинговыми инструментами, рассказывает про опыт работы в стартапах, даёт ссылки на подкасты и PDF-книги.Что нам понравилось. Рассылка заточена под стартаперов: большинство инструментов и методов, которые рассматривает Эндрю Чен, рассматриваются в разрезе стартапов. Рекомендуем, если вы собираетесь открыть свой бизнес или недавно запустили его.Подписаться. На главной.11. Nick SharmaЧто это. Ник Шарма — специалист по D2C-маркетингу и делает рассылку для всех, кому интересна эта тема. Внутри довольно длинных писем — анонсы профильных событий, рассказы о разных продуктах из сферы и размышления на маркетинговые темы.Что нам понравилось. Ник интересно рассказывает о своём опыте, находит в кейсах взаимосвязи и подтверждает выводы фактами из практики. А ещё пишет довольно простым языком — читать легко несмотря на обилие текста. Подписаться. На главной.Экономика и бизнес12. Morning BrewЧто это. Рассылка, которая собирает актуальные новости экономики — начиная с Уолл стрит и заканчивая Силиконовой долиной.Что нам понравилось. Идеальная рассылка для бизнесменов и предпринимателей — в каждом выпуске приходят биржевые индексы и новости, которые влияют на мировую экономику.Подписаться. На главной.13. The HustleЧто это. Ежедневная рассылка о бизнес-новостях и современных технологиях.Что нам понравилось. Из рассылки не нужно переходить на сторонние ресурсы — каждая новость подана в формате мини-статьи. Вот примеры новостей из одной рассылки:Подписаться. На главной The Hustle.14. b.Что это. Рассылка с самым лаконичным названием от издания Business.com. Рассказывает, как ни странно, про бизнес.Что нам понравилось. Гениальная идея, которая всё чаще встречается в англоязычных рассылках: в начале каждого письма указывается, какие темы оно включает. Можно сразу проскроллить к нужному месту ну или прикинуть, хочешь ли ты вообще тратить на это время. В конце — мем из мира бизнеса, чтобы снизить накал серьёзности.Подписаться. На странице рассылки.Маркетинг15. Masters in MarketingЧто это. Рассылка от платформы Hubspot, которая выходит раз в неделю и целиком посвящена маркетингу. Внутри — интервью с мэтрами отрасли, полезные ссылки, советы и шаблоны.Что нам понравилось. Рассылка Hubspot — хороший пример контент-маркетинга. В письмах нет ни слова о продукте компании, только польза.Подписаться. В футере блога.16. Marketing BrewЧто это. Рассылка от тех же ребят, что делают Morning Brew про экономику — только про маркетинг. Внутри — ключевые новости от «убийства» совы Дуо в Duolinguo до размышлений о том, почему сериал «Белый лотос» — мечта маркетолога.Что нам понравилось. Необычный подход к подаче информации через кейсы и материалы разных изданий позволит оставаться в курсе того, что происходит в мире маркетинга. Тут нет занудства и очередных «пяти способов повысить продажи», и даже советы подаются через оригинальный контент вроде странных идей для сторис и рилсов.Подписаться. На главной странице.17. Stand the F*cking OutЧто это. Ежедневная (с понедельника по пятницу) рассылка от Луи Гренье — маркетолога, в одноимённый подкаст к которому приходят знаменитости из мира маркетинга.Что нам понравилось. Это действительно оригинальная рассылка про маркетинг (и не из-за обилия ругательств). Луи не особенно стремится чему-то научить читателей, но всё-таки учит — своими безумными маркетинговыми экспериментами, личными историями и неожиданными выводами.Подписаться. На странице рассылки.18. The TicketЧто это. Рассылка от Intercom — сервиса для создания чат-окон на сайте. В рассылке делятся статьями своего бренд-медиа.Что нам понравилось. Письма Intercom рассказывают про маркетинг, дизайн, отток аудитории и стартапы — всё, что имеет отношение к обслуживанию клиентов. Рассылка с подборкой полезных статей, отраслевой информацией и трендами приходит раз в неделю.Подписаться. В сайдбаре на странице блога.Email-маркетинг19. Litmus WeeklyЧто это. Сервис Litmus помогает проверить, как будут отображаться письма на разных платформах. В рассылках Litmus говорят об email-маркетинге, рассказывают, что нового в блоге и присылают результаты своих исследований.Что нам понравилось. Мега-полезная рассылка для email-маркетологов. Ребята часто экспериментируют с дизайном и пробуют в письмах новые фишки. Ну, и, конечно, контент: Litmus помогают email-маркетологам со всего мира оставаться в теме.Подписаться. Форма подписки в сайдбаре в блоге.20. Dan OshinskyЧто это. Письма от бывшего директора по рассылкам BuzzFeed и The New Yorker и основателя консалтингового сервиса Inbox Collective Дена Ошински. Внутри — много про электронную почту и подборки статей на тему email-маркетинга с разных ресурсов.Что нам понравилось. Эта рассылка — точно не про продажу услуг, она для таких же маркетологов, как сам Ден. Внутри — практики, примеры, рекомендации и всё, что пригодится специалисту в области email. А ещё Ден открыт к обратной связи. Это всегда приятно.Подписаться. На странице подписки.21. Really Good EmailsЧто это. Сервис, который собирает примеры разных email’ов из всех областей бизнеса (русскоязычный аналог — Email Competitors). В рассылке авторы делятся классными письмами за последние несколько дней и дают ссылки на статьи из популярных англоязычных блогов.Что нам понравилось. Контент внутри действительно полезен для email-маркетолога — можно вдохновиться классными рассылками и почитать новости об email-маркетинге от западных экспертов.Подписаться. На странице подписки.Интернет-магазиныИнтернет-магазинов, которые работают с англоязычной аудиторией, тысячи и часто их рассылки похожи друг на друга. Мы выбрали несколько классных компаний, на которые подписаны сами. 22. MVMTЧто это. Интернет-магазин очков и наручных часов. В рассылках рассказывают о скидках, присылают тематические подборки и новые товары.Что нам понравилось. В рассылках классные анимации и фотографии — все товары текстурные, отлично передана игра света и тени. Кажется, что часы или очки можно пощупать через экран.Подписаться. В футере или поп-апе на главной.23. CarharttЧто это. Интернет-магазин одежды. 2 раза в неделю присылают на почту тематические подборки товаров.Что нам понравилось. Carhartt используют в рассылке яркие, а иногда и необычные иллюстрации. Например, в одном из писем, чтобы показать людей в движении, Carhartt добавили пару расфокусированных фотографий.Подписаться. В футере на главной.24. Urban OutfittersЧто это. Интернет-магазин одежды, домашних принадлежностей и косметики. В рассылках рассказывают о новых поступлениях, акциях и скидках.Что нам понравилось. Рассылка классно оформлена в виде коллажа из фотографий и видео. Письма интересно смотреть — фотографии с товарами всегда посвящены одной теме (одежда для путешествий, фурнитура для дома) и крутятся вокруг одного цветового решения.Подписаться. В футере на главной.Музыка, искусство и литература25. My modern metЧто это. Комьюнити, объединённое любовью к творчеству и креативу. Команда ставит перед собой цель «освещать лучшие стороны человечества» и выпускает статьи об искусстве, дизайне, фотографии, архитектуре, науке, технологиях, проблемах окружающей среды и многом другом.Что нам понравилось. Во-первых, контент: тут статья про художника с мировым именем соседствует с парнем, который делает прикольные ковры. Во-вторых, издание зарабатывает рекламой и не стыдится этого: рекламные баннеры не запрятаны и не притворяются ещё одной статьёй. Это честно.Подписаться. На главной или в поп-апе.26. Lit HubЧто это. Ежедневная рассылка о литературе и писателях. Внутри письма — дайджест событий из литературного мира.Что нам понравилось. Рассылка собирает все новости литературного мира в аккуратный дайджест. Авторы стараются охватить все инфоповоды — ссылки ведут и на LitHub, и на другие авторитетные ресурсы. Рекомендуем, если вы увлекаетесь литературой или пишете сами.Подписаться. Форма подписки в сайдбаре справа.27. Hot LinksЧто это. Портал о музыке, новых альбомах и популярных исполнителях Pitchfork ведёт ежедневную рассылку, в которой собирает все новости и обзоры в один удобный дайджест.Что нам понравилось. У рассылки очень разноплановый контент: можно узнать о новых альбомах, почитать интервью с любимыми исполнителями или рецензии на песни.Подписаться. На странице с рассылками.Другое28. The UnderdogЧто это. Рассылка от спортивного издания JokerMag. Виды спорта — разные, оформление — простое, а само содержание — не кейсы «успешного успеха», а живые человеческие истории.Что нам понравилось. Рассылка оправдывает своё название:она про то, как неудачники становились классными спортсменами. Классно, что есть кнопка, позволяющая открыть историю в блоге — чтобы дочитать потом, если сейчас времени нет.Подписаться. На странице рассылки.29. Today You Should KnowЧто это. По сути, блог, который раз в неделю присылает письмо с рассказом об интересном факте.Что нам понравилось. Простой дизайн и классное наполнение — идеальное пятничное чтиво, позволяющее разобраться в том, как устроен мир. В рассылке приходят письма с интригующими темами, например, что скрипит в метро или почему ленивцы ещё не вымерли.Подписаться. На сайте проекта.30. Important not ImportantЧто это. Еженедельная рассылка с новостями науки, рассказанными понятным языком.Что нам понравилось. Помимо стиля изложения, привлекает разнообразие контента. Так, например, в каждом письме команда проводит опрос аудитории — а в следующем показывает его результаты. Подписаться. На странице рассылки."
20,20,"Не только хардкор: что читают R&D-инженеры, когда пытаются отдохнуть",МТС,Про жизнь и развитие в IT,2233.29,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-03-13,"Привет, хабрчане! Это Миша Степнов, руководитель центра R&D Big Data в МТС Диджитал. Говорят, лучший отдых — это смена деятельности, но полностью уйти от науки и технологий бывает сложно даже в отпуске. Да и зачем уходить, ведь большинство из нас с детства мечтали об этом читая… конечно, научную фантастику. Она позволяет отправиться в путешествие по мирам и вселенным, где работают свои законы физики, математики и логики. В этой подборке — книги, которые мне, как инженеру, понравились. Без магии и мистики, но с крепкой научной основой, живыми идеями и настоящим драйвом. А еще, вы не слышали (если не глубоко в теме) почти ни об одной из них — и зря. Так что, надеюсь, подборка будет полезной!Память, что зовется Империей Автор: Аркади МартинРоман «Память, что зовется Империей» вышел в 2019 году и сразу стал заметным событием в мире научной фантастики. Автор (настоящее имя — Анна Линден Уэллер) историк, специалист по Византии, получила за него премию «Хьюго». Это первый роман в дилогии «Тейкскалаан», есть продолжение — «Пустошь, что зовется миром» (2021).О чем книгаИмперия Тейкскалаан — огромная цивилизация с развитой культурой и сложной политикой, напоминающая смесь Византии, Рима и империй будущего (все смешать, но не взбалтывать). Протагонистка, посол Махит Дзмаре, прибывает в столицу государства, чтобы разобраться в смерти своего предшественника. Ее главное преимущество — устройство с памятью предыдущего посла в голове. Но есть нюанс: версия памяти предшественника устарела на 15 лет, а реальность вокруг стремительно меняется. Действовать приходится быстро и изобретательно. «Память, что зовется Империей» — это не просто книга о межзвездной политике, а глубокая история о языке, культуре и том, что значит быть частью чужого мира. Тейкскалаанцы мыслят поэтическими метафорами, их общество пропитано символизмом, поэтому Махит вынуждена балансировать между независимостью своей родины и опасной притягательностью империи.Книга — интеллектуальный триллер с детально проработанным миром. Здесь не стрельба из бластеров, а политические игры, интриги и тонкие различия в способах мышления.Продолжение, «Пустошь, что зовется миром», добавляет новый пласт проблем — контакт с инопланетной цивилизацией, чей разум и коммуникации настолько отличаются от человеческих, что сам диалог с ней становится вызовом. Миростроение выходит за рамки имперских интриг, превращаясь в историю о столкновении разумов и попытку избежать войны.Если вам нравится «Барраяр» Буджолд или «Империя атома» Ван Вогта, дилогия «Тейкскалаан» определенно стоит внимания.Глубина в небеАвтор: Вернор ВинджРоман «Глубина в небе» (A Deepness in the Sky) вышел в 1999 году и получил сразу три главные премии в жанре научной фантастики — «Хьюго», «Локус» и премию Д. Кэмпбелла. Это приквел книги «Пламя над бездной» (1992), но его можно читать отдельно: книги связаны общей вселенной, но это самостоятельные истории.О чем книгаДействие происходит за 30 тысяч лет до событий «Пламя над бездной» и повествует о первой встрече человечества с внеземной цивилизацией. Две группы людей — торговцы из Квенг Хо и жестокая тоталитарная система Эмерджентов — прибывают к далекой звезде, вокруг которой вращается необычная планета. Ее обитатели — разумные паукообразные существа, они проводят большую часть времени в анабиозе, оживая, когда светило разгорается после долгой «зимы». Задача землян — установить контакт и воспользоваться технологиями «пауков». Но, как водится, мирного сотрудничества не получается: борьба за ресурсы перерастает в межзвездную войну, контактировать с «друзьями по разуму» оказывается гораздо сложнее, чем казалось.Вернор Виндж — писатель, который блестяще разбирается в идеях искусственного интеллекта, технологической сингулярности и социологии будущего. В «Глубине в небе» он показывает не просто контакт с иными разумами, а игру политиков, технологий и культурных различий.В отличие от «Пламени над бездной», где в центре внимания галактические масштабы и идеи, «Глубина в небе» сосредоточена на людях, их интригах и борьбе за власть. Это смесь напряженной политики «Дюны» с мрачной атмосферой «Гипериона» (эта книга тоже будет в подборке): сложные технологии, психологическая игра и детально проработанный мир, в котором каждый шаг — вопрос выживания.Ну а «Пламя над бездной» идет дальше по хронологии и показывает вселенную, где существуют зоны разной скорости мышления и различных технологий. Там, где живем мы, люди, продвинутый ИИ невозможен, сверхсветовые двигатели работать не могут. Ближе к центру галактики разумы работают так быстро, что напоминают божественные сущности. Здесь возможны не только сверхсветовые двигатели, но и кое-что еще. Это разделение играет ключевую роль в сюжете, так как действия персонажей зависят от того, в какой зоне они находятся.Если вам нравится масштабная научная фантастика с политикой, необычными инопланетными расами и вдумчивыми рассуждениями о технологиях, «Глубина в небе» — отличный выбор.Задача трех телАвтор: Лю Цысинь«Задача трех тел» (The Three-Body Problem) — первая книга одноименной трилогии Лю Цысиня, вышедшая в 2008 году в Китае и переведенная на английский в 2014-м. Именно англоязычный перевод сделал книгу мировой сенсацией, принеся ей премию «Хьюго» и открыв дорогу китайской научной фантастике на международный уровень. Продолжения — «Темный лес» и «Вечная жизнь смерти» — завершили грандиозную историю, которую часто сравнивают с «Академией» Азимова и «Гиперионом» Симмонса.О чем книгаВсе начинается с культурной революции в Китае, когда молодая астрофизик Е Вэньцзе становится свидетелем жестокости режима и теряет веру в человечество. Она отправляет сигнал в космос, а внеземная цивилизация (трисолярианцы) отвечает. Их родная планета страдает от хаотичных климатических изменений из-за нестабильности системы из трех солнц, и они видят в Земле шанс на спасение. Только это означает вторжение к людям, иного пути нет.На фоне этих событий в Китае разворачивается странная цепь смертей ученых-физиков. Специалист Ван Мяо вовлекается в таинственную игру в виртуальной реальности. Ее участники пытаются решить задачу трех тел — математическую проблему, имеющую прямое отношение к судьбе Земли. Дальше рассказывать не буду, иначе пропадет смысл читать книгу.Лю Цысинь не просто пишет про инопланетян — он играет с идеями физики, космологии и теории вероятностей, выстраивая картину контакта двух цивилизаций, но без привычных клише. Сюжет строится на «гипотезе темного леса»: если Вселенная полна разумных существ, почему мы их не видим? Возможно, потому что каждая цивилизация ведет себя как охотник в темном лесу, стараясь не выдавать свое присутствие, иначе неизбежно будет уничтожена.Продолжения книги углубляют эту идею. «Темный лес» показывает, как человечество пытается выстроить защиту от грядущего вторжения, а «Вечная жизнь смерти» раздвигает рамки повествования до времен, выходящих далеко за пределы нашей эры.Эта трилогия — отличный выбор для тех, кто любит крепкую научную фантастику, где физика и математика играют не меньшую роль, чем персонажи.ГиперионАвтор: Дэн Симмонс «Гиперион» (Hyperion, 1989) — первая книга тетралогии «Песни Гипериона», одной из самых известных эпопей в научной фантастике. В цикл входят:«Гиперион» (1989)«Падение Гипериона» (1990)«Эндимион» (1996)«Восход Эндимиона» (1997)Первая дилогия — «Гиперион» и «Падение Гипериона» — представляет собой масштабную космооперу с философскими и мифологическими мотивами. Вторая дилогия — «Эндимион» и «Восход Эндимиона» — продолжает историю, но с новым главным героем и другими темами.О чем книгаДействие разворачивается в далеком будущем, когда человечество распространилось по множеству планет, объединенных в Гегемонию Человека. Но в этой сети миров есть загадочный мир Гиперион. Там стоят древние Гробницы Времени, а по пустошам бродит таинственный Шрайк — существо, мучительно убивающее всех, кто встречается на его пути.В канун вторжения инопланетных варваров семь паломников отправляются на Гиперион, каждый со своей жизнью и тайной. Мы постепенно узнаем истории всех героев, и перед нами выстраивается сложная картина мира.Симмонс сочетает в «Гиперионе» твердую научную фантастику, философию, мифологию и литературу. Здесь есть все: искусственный интеллект, квантовые эффекты, религиозные вопросы, влияние поэзии Китса на восприятие будущего. И конечно, есть еще жуткий и непредсказуемый антагонист — Шрайк.«Падение Гипериона» продолжает повествование, но меняет стиль: если первая книга похожа на путешествие и сборник историй, то вторая — эпический роман, где судьбы паломников сплетаются в битве за будущее человечества. События в книгах «Эндимион» и «Восход Эндимиона» происходят через 300 лет после событий первых книг и рассказывают о новом путешествии через вселенную, но с акцентом на духовные и религиозные темы.Если вам нравится масштабная фантастика с глубокой философией, вроде «Дюны» Герберта или «Академии» Азимова, то «Гиперион» обязателен к прочтению. Это не просто история о будущем, а целая вселенная, наполненная идеями и сильными образами.АкадемияАвтор: Айзек АзимовЦикл «Академия» (Foundation) — один из самых влиятельных в истории научной фантастики. Он задумывался как серия рассказов, которые Айзек Азимов писал и публиковал в журнале Astounding Science Fiction под редакцией Джона В. Кэмпбелла в 1942–1950 годах. В итоге из этих рассказов вырос масштабный эпос, охватывающий тысячелетия и ставший предтечей многих современных космоопер. Вдохновлялся Айзек Азимов «Историей упадка и разрушения Римской империи» Эдварда Гиббона. Структура циклаПервоначальная трилогия:1. «Академия» (Foundation, 1951)2. «Академия и Империя» (Foundation and Empire, 1952)3. «Вторая Академия» (Second Foundation, 1953)Позже Азимов расширил цикл приквелами и сиквелами:4. «Край Академии» (Foundation's Edge, 1982)5. «Академия и Земля» (Foundation and Earth, 1986)6. «Прелюдия к Академии» (Prelude to Foundation, 1988)7. «На пути к Академии» (Forward the Foundation, 1993)О чем книгиДействие разворачивается в далеком будущем, когда Галактическая Империя, объединившая миллионы звездных систем, находится в состоянии упадка. Ученый Гэри Селдон разрабатывает новую науку — психоисторию, которая с помощью статистики и математики предсказывает будущее крупных обществ. Его расчеты показывают, что Империя рухнет, наступит темная эпоха на 30 тысяч лет, но можно сократить ее до 1 000 лет, если создать Академию — центр знаний, который станет основой для новой цивилизации.Первый роман рассказывает о том, как Академия выживает среди варварских миров, манипулируя окружающими державами. Во второй книге появляются новые угрозы, в том числе загадочный Мул — человек с уникальными способностями, который ставит под сомнение саму идею психоистории. В третьей части на сцену выходит Вторая Академия, тайная организация, созданная Селдоном для контроля над планом. Дальнейшие книги углубляют вселенную «Академии» и показывают постепенное возрождение человечества.Азимов создает необычную фантастику без экшена и космических битв. В центре внимания — политика, наука, социология и масштабные исторические процессы. Вместо героического противостояния тут интриги, дипломатия и борьба идей. Хотя оригинальная трилогия была написана в середине XX века, ее идеи о предсказуемости общества, влиянии науки и нестабильности империй остаются актуальными.«Академия» — классика, которая заложила основы многих современных научно-фантастических вселенных.Новая ЛунаАвтор: Йен МакдональдРоман «Новая Луна» (Luna: New Moon, 2015) — первая книга в трилогии «Луна», которую часто называют «Игрой престолов» на Луне. Все книги:«Новая Луна» (Luna: New Moon, 2015)«Волчья Луна» (Luna: Wolf Moon, 2017)«Восставшая луна» (Luna: Moon Rising, 2019)О чем книгаДействие разворачивается в конце XXI века. Луна не просто колония, а дикий капиталистический мир, где власть принадлежит пяти могущественным кланам («Драконам»), контролирующим основные ресурсы: воздух, воду, редкоземельные металлы и энергию.Сюжет фокусируется на семье Адрианы Корта, выходцев из Бразилии, которые за 50 лет стали одними из самых влиятельных магнатов Луны. Но их могущество вызывает зависть конкурентов, а противостояние перерастает в полномасштабную войну между корпорациями.На Луне нет законов, только контракты, а жизнь здесь — суровая борьба за выживание. Человек без денег даже не сможет оплатить воздух, и любая ошибка может стоить жизни.«Новая Луна» — необычное сочетание киберпанка, жесткой научной фантастики и политической драмы. Макдональд проработал общество, где ценности иерархичны, а технологии развиты настолько, что люди модифицируют свои тела, управляют искусственными разумами и ведут войны с помощью беспилотных дронов.Стиль романа — насыщенный, кинематографичный, с короткими, рублеными предложениями. Он погружает читателя в атмосферу будущего, где власть принадлежит сильным и жестоким. Нефритовый городАвтор: Фонда ЛиРоман «Нефритовый город» (Jade City, 2017) — первая книга в трилогии «Сага о Зеленой Кости», вдохновленной азиатской культурой и мафиозными кланами. Продолжения:«Нефритовая война» (Jade War, 2019)«Нефритовое наследие» (Jade Legacy, 2021)Книги собрали множество наград, включая премию «Локус» за лучший фэнтези-роман. Тем не менее трилогия опирается на реализм, политику и социальные интриги в качестве основы, так что будет интересна даже тем, кто обычно читает научную фантастику.О чем книгаДействие происходит в городе Джанлун, напоминающем смесь Гонконга, Токио и мафиозного Чикаго. Здесь нефрит не просто драгоценность, а магический ресурс, дающий владельцу сверхчеловеческие способности. Управляют им два преступных клана, балансируя между традициями, властью и жестокими войнами за территорию.В центре сюжета — семья Кай. Ей приходится защищать свои интересы от конкурентов, политики и глобальных сил, пытающихся захватить контроль над нефритом. Здесь нет черно-белых героев: каждый персонаж действует в рамках своей морали, но никто не является однозначно добрым или злым.Книга напоминает мафиозные саги вроде «Крестного отца», но с азиатским колоритом и элементами магии. Здесь есть жесткие боевые сцены, семейные драмы, политические интриги и экономические игры, влияющие на судьбу всего города. Здесь больше реализма, чем классического фэнтези, а магия встроена в общественные структуры как деньги или власть.Продолжения — «Нефритовая война» и «Нефритовое наследие» — расширяют масштаб истории: в игру вступают другие страны, нефрит становится геополитическим фактором, а герои сталкиваются с новыми вызовами, выходящими за пределы одного города.Если вам нравятся криминальные драмы, политические интриги и восточноазиатская культура, то эта книга для вас. Она идеально подойдет любителям жестких и реалистичных историй, а также тем, кто ценит семейные саги о власти и борьбе за влияние.А какие книги на «отдохнуть» можете посоветовать вы? Добавляйте в комментарии научную фантастику — сохраню себе.Что еще почитать:Что почитать по машинному обучению: подборка из 6 книгКниги по базам данных, которые стоит прочитать в 2025 годуБагровые тучи, симметриады и марсианский картофель: 10 научно-фантастических книг о космосеВерхом на ракете, 4 000 часов на орбите и странники солнечной системы: что почитать из космического нон-фикшна "
21,21,A/B-тесты в PostgreSQL,OTUS,Цифровые навыки от ведущих экспертов,721.07,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-03-13,"Привет, Хабр! Сегодня разберём, как реализовать A/B‑тестирование на чистом PostgreSQL, без выгрузки данных в сторонние системы. Рассмотрим полный цикл: от структуры таблиц и оптимизации запросов до статистического анализа (T‑тест, Манна‑Уитни, байесовские методы) и визуализации результатов. Постановка задачи и проектирование базыНачнём с основ. Представим, что мы отвечаем за A/B‑тест нового лендинга. Заказчик требует реального времени, а выгрузка в сторонние BI‑системы — это вообще не про нас. Так что нужно построить базу, где все данные будут сразу готовы к анализу.Для начала создадим таблицу, которая будет собирать все события пользователей. Вот как выглядит базовая структура:-- Создаем таблицу для хранения событий A/B-теста CREATE TABLE ab_test_events (     event_id      SERIAL PRIMARY KEY,      -- Уникальный идентификатор события     user_id       INTEGER NOT NULL,          -- Идентификатор пользователя     variant       CHAR(1) NOT NULL,          -- Вариант теста: 'A' или 'B'     event_time    TIMESTAMPTZ NOT NULL DEFAULT NOW(),  -- Время события     event_type    TEXT NOT NULL,             -- Тип события (например, 'page_view', 'click', 'submit')     event_payload JSONB NOT NULL           -- Дополнительные данные (например, время до конверсии, метаданные) );Используем JSONB для event_payload — потому что иногда пользователи творят что угодно: один день клик, другой — заполняют анкету с кучей метаданных. Гибкость и масштабируемость важнее, чем жесткая структура.Пример данных:INSERT INTO ab_test_events (user_id, variant, event_type, event_payload) VALUES ('550e8400-e29b-41d4-a716-446655440000', 'A', 'submit', '{""conversion"": ""1""}'), ('550e8400-e29b-41d4-a716-446655440001', 'B', 'submit', '{""conversion"": ""0""}'), ('550e8400-e29b-41d4-a716-446655440002', 'A', 'submit', '{""conversion"": ""1""}'), ('550e8400-e29b-41d4-a716-446655440003', 'B', 'submit', '{""conversion"": ""1""}');Никакой крутой код не спасёт ситуацию, если запросы выполняются медленно. Поэтому создаем GIN‑индекс для быстрого поиска по JSONB.-- Индекс для ускорения поиска по полям внутри JSONB CREATE INDEX idx_event_payload ON ab_test_events USING gin (event_payload);Агрегация данных:После того как данные начали сыпаться, самое время собрать статистику. Ведь что толку в красивой таблице, если мы не можем понять, какой вариант лендинга работает лучше? Для нас критичен показатель конверсии — событие типа submit означает успешную регистрацию.Приведу пример запроса, который группирует данные по вариантам теста и вычисляет конверсию:-- Агрегируем данные по вариантам A/B-теста SELECT      variant,  -- Группируем по варианту ('A' или 'B')     COUNT(*) AS total_events,  -- Общее количество событий     COUNT(*) FILTER (WHERE event_type = 'submit') AS total_conversions,  -- События регистрации     COUNT(*) FILTER (WHERE event_type = 'submit')::numeric / COUNT(*) AS conversion_rate  -- Расчет конверсии FROM ab_test_events GROUP BY variant;Фильтрация через конструкцию FILTER позволяет сразу получить нужные подсчеты.Иногда хочется знать не только конверсию, но и дополнительные детали — например, сколько времени пользователи тратят на регистрацию. Допустим, параметр time_to_submit хранится в event_payload. Тогда запрос будет таким:-- Вычисляем среднее время до регистрации для каждой группы SELECT      variant,     AVG((event_payload->>'time_to_submit')::numeric) AS avg_time_to_submit  -- Приводим строку к числу FROM ab_test_events WHERE event_payload ? 'time_to_submit'  -- Проверяем наличие ключа в JSONB GROUP BY variant;Оператор ? позволяет отфильтровать данные и не портить статистику нулевыми значениями.Важно учитывать случаи, которые могут исказить результаты. Перед расчетом тестов нужно проверить, есть ли аномальные пользователи:SELECT user_id, COUNT(*)  FROM ab_test_events WHERE event_type = 'submit' GROUP BY user_id HAVING COUNT(*) > 10;Если у одного пользователя слишком много событий, возможно, он тестирует систему, а не является реальным пользователем.Иногда вариант A имеет больше пользователей, чем B. Проверяем:SELECT variant, COUNT(*) AS users FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant;Если разница > 10%, тесты могут быть смещены.Если пользователи видели A и B, это может создать шум в данных:SELECT user_id, COUNT(DISTINCT variant)  FROM ab_test_events WHERE event_type = 'submit' GROUP BY user_id HAVING COUNT(DISTINCT variant) > 1;Таких пользователей можно исключить из анализа.Освоить мощные навыки анализа данных (анализ требований + статистика + BI) можно на курсе «Аналитик данных».Статистический анализ: тесты, доверительные интервалы и байесовский разбор  T-тест для оценки разницы в конверсииT‑тест применяется, если распределение данных нормально. Проверим это:Прежде чем применять T‑тест, важно убедиться, что данные нормально распределены:SELECT variant, AVG((event_payload->>'conversion')::numeric) AS avg_conversion,               STDDEV((event_payload->>'conversion')::numeric) AS stddev_conversion FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant;Если стандартное отклонение слишком велико, T‑тест может быть неточным.Теперь применим T‑тест:WITH stats AS (     SELECT          variant,         AVG((event_payload->>'conversion')::numeric) AS avg_conversion,         VARIANCE((event_payload->>'conversion')::numeric) AS variance,         COUNT(*) AS cnt     FROM ab_test_events     WHERE event_type = 'submit'     GROUP BY variant ) SELECT     a.avg_conversion AS conversion_A,     b.avg_conversion AS conversion_B,     (a.avg_conversion - b.avg_conversion) /      sqrt((a.variance / a.cnt) + (b.variance / b.cnt)) AS t_statistic FROM stats a, stats b WHERE a.variant = 'A' AND b.variant = 'B';Если t_statistic > 2 или < -2, разница между группами A и B статистически значима. Если t_statistic ближе к 0, различие случайное.Манна-УитниЕсли данные не подчиняются нормальному распределению (например, если конверсии редкие или неравномерные), применяется тест Манна‑Уитни.Перед запуском теста, проверим:SELECT variant, COUNT(*) AS cnt,         SUM((event_payload->>'conversion')::numeric) AS total_conversions FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant;Если между группами A и B большая разница в количестве наблюдений, Манна‑Уитни предпочтительнее T‑теста.Манна‑Уитни работает по ранжированию значений:WITH ranked AS (   SELECT        user_id,       variant,       (event_payload->>'conversion')::numeric AS conversion,       RANK() OVER (ORDER BY (event_payload->>'conversion')::numeric) AS rnk   FROM ab_test_events   WHERE event_type = 'submit' ) SELECT      variant,      SUM(rnk) AS total_rank,      COUNT(*) AS cnt FROM ranked GROUP BY variant;Если суммы рангов сильно различаются, разница между вариантами значима.Байесовский анализБайесовский анализ не только оценивает вероятность, но и показывает распределение вероятности конверсии.Сначала вычислим 95% доверительный интервал:SELECT      variant,     PERCENTILE_CONT(0.025) WITHIN GROUP (ORDER BY (event_payload->>'conversion')::numeric) AS lower_bound,     PERCENTILE_CONT(0.975) WITHIN GROUP (ORDER BY (event_payload->>'conversion')::numeric) AS upper_bound FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant;Теперь применим PL/Python для моделирования вероятностей.Создадим функцию:CREATE OR REPLACE FUNCTION beta_distribution(alpha numeric, beta numeric) RETURNS TABLE(x numeric, pdf numeric) AS $$     import numpy as np     from scipy.stats import beta as beta_dist     x = np.linspace(0, 1, 100)     pdf = beta_dist.pdf(x, alpha, beta)     return list(zip(x, pdf)) $$ LANGUAGE plpython3u;Вызовем функцию:SELECT * FROM beta_distribution(5, 2);Чем выше alpha и beta, тем уже доверительный интервал.Если распределения сильно пересекаются, разница между группами незначительна.Если variant A имеет высокий пик в beta‑распределении, он значительно лучше variant B.Оптимизация производительностиОптимизация — это не разовое действие, а постоянный процесс.Partial indexes и BRIN-индексыЕсли ваша таблица растёт до гигантских размеров, индексировать всё подряд не стоит. Частичные индексы — наше всё для актуальных данных.-- Частичный индекс для ускорения выборок по последним 30 дням CREATE INDEX idx_recent_events ON ab_test_events (event_time) WHERE event_time > NOW() - INTERVAL '30 days';Если данных слишком много, можно применить BRIN‑индексы, которые работают быстрее и экономят место:-- BRIN-индекс для эффективного сканирования временных данных CREATE INDEX idx_brin_time ON ab_test_events USING brin (event_time);GROUPING SETS: многомерная агрегация в одном запросеКогда требуется собрать несколько уровней агрегации (например, по дням и по вариантам), GROUPING SETS приходят на помощь:-- Используем GROUPING SETS для получения детальной и сводной статистики SELECT      variant,     DATE_TRUNC('day', event_time) AS day,     COUNT(*) AS total_events FROM ab_test_events GROUP BY GROUPING SETS (     (variant, DATE_TRUNC('day', event_time)),  -- Статистика по каждому дню для каждого варианта     (variant)                                  -- Общая статистика по каждому варианту );GROUPING SETS позволяют одним запросом получить сразу несколько уровней агрегации.Визуализация результатовПервым делом подключаемся к PostgreSQL, выполняем запрос и загружаем данные в DataFrame. import psycopg2 import pandas as pd import matplotlib.pyplot as plt  # Устанавливаем соединение с базой данных conn = psycopg2.connect(     host=""localhost"",         # Адрес сервера базы данных     database=""production_db"", -- Имя базы данных     user=""prod_user"",         # Пользователь базы данных     password=""supersecret""    # Пароль (в продакшене храните в секьюрном хранилище!) )  # SQL-запрос для получения агрегированных данных по вариантам query = """""" WITH aggregated AS (     SELECT          variant,         COUNT(*) AS total_events,         COUNT(*) FILTER (WHERE event_type = 'submit') AS total_conversions,         COUNT(*) FILTER (WHERE event_type = 'submit')::numeric / COUNT(*) AS conversion_rate,         STDDEV((event_payload->>'conversion')::numeric) AS std_dev     FROM ab_test_events     GROUP BY variant ) SELECT * FROM aggregated; """"""  # Загружаем данные в DataFrame df = pd.read_sql(query, conn) conn.close()  # Выводим данные для проверки print(df)А теперь построим график:import matplotlib.pyplot as plt import pandas as pd  # Симулируем данные, как если бы они были получены из запроса data = {     'variant': ['A', 'B'],     'conversion_rate': [0.15, 0.22],  # Пример конверсии: 15% для A и 22% для B     'std_dev': [0.02, 0.03]           # Пример стандартного отклонения } df = pd.DataFrame(data)  # Создаем график plt.figure(figsize=(10, 6)) bars = plt.bar(df['variant'], df['conversion_rate'], yerr=df['std_dev'], capsize=8)  # Настраиваем оси и заголовок plt.xlabel('Вариант теста') plt.ylabel('Конверсия') plt.title('Результаты A/B-теста: Конверсия по вариантам') plt.grid(axis='y', linestyle='--', alpha=0.7) plt.ylim(0, 0.3)  # Устанавливаем лимиты по оси Y  # Добавляем числовые аннотации над столбиками for bar in bars:     height = bar.get_height()     plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f'{height:.2%}',               ha='center', va='bottom')  plt.show()График на рандом данныхВ заключение всем, кому интересна сфера аналитики данных, рекомендую посетить открытый урок 25 марта «Базовый сбор требований в работе дата-аналитика».Рассмотрим жизненный цикл задачи в анализе данных, обсудим, какие ключевые вопросы необходимо задать и как грамотно зафиксировать ответы. Также разберем, как справляться с возражениями и избегать лишнего стресса в процессе. Записаться"
22,22,Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2,SM Lab,work + life + balance = SM Lab,4.37,"Веб-разработка, Программное обеспечение, Электронная коммерция",2025-03-13,"Всем привет! Меня зовут Александр, я являюсь куратором центра компетенции по тестированию, в компании SM Lab. Сегодня я хотел продолжить с вами говорить о встречах 1-1 и пирамиде ценностей. С первой частью можно ознакомится тут. Восемь базовых сценариевДалее я предлагаю разобрать восемь возможных кейсов, связанных с ценностями сотрудников, и способы их решения.Первый и самый оптимистичный сценарий: все хорошоУ сотрудника четкие цели, он успешно взаимодействует с командой, базовые потребности удовлетворены. Возможно, его недавно повысили, и он даже уехал на Бали, где наслаждается работой на удаленке и катанием на волнах. Команда поддерживает его, у него интересные профессиональные задачи, и виден прогресс. В таком случае проблем нет, и на встрече тет-а-тет можно просто обсудить, как проходят его занятия серфингом.Второй сценарий: нарушается баланс — страдают базовые потребностиВ чем это обычно выражается? Не устраивает доход: «Я посмотрел на HeadHunter, и за аналогичную позицию предлагают больше». По два часа добирается до работы, случились семейные проблемы, из-за которых нужно больше времени уделять родным. И обо всем этом сотрудник рассказывает вам на 1х1.Какие варианты решений возможны в этом случае?Во-первых, можно провести оценку компетенций сотрудника, не дожидаясь оценки вклада. Это можно сделать через куратора, тимлида или просто обратившись к команде: «Ребята, как Петя справляется? Все нормально?» Может ему уже пора повысить зарплату.Во-вторых, если проблема заключается в долгой дороге до работы, можно предложить удаленный формат. Это может существенно облегчить жизнь сотруднику.Наконец, если у человека возникли семейные обстоятельства, можно обсудить гибкий график или предоставить дополнительный отпуск: «Сколько времени тебе нужно, чтобы решить свои дела? Может, возьмешь неделю прямо сейчас?»Такие шаги помогут восстановить баланс базовых потребностей сотрудника, снизить уровень стресса и улучшить общее настроение в команде. В результате сотрудник почувствует заботу и поддержку со стороны руководства, что укрепит ваши взаимоотношения и повысит его лояльность.Третий сценарий: выпадает социальная историяЗдесь могут возникать ситуации, связанные с двумя известными когнитивными искажениями: синдромом самозванца и эффектом Даннинга-Крюгера.Синдром самозванца проявляется, когда сотрудник считает себя неспособным справиться с задачей, несмотря на свою компетентность. Он думает: «Я недостоин этой роли» или «Не уверен, что справлюсь», хотя на самом деле успешно выполняет свою работу.Эффект Даннинга-Крюгера, наоборот, характеризуется переоценкой собственных знаний и способностей. Сотрудник может считать, что знает все лучше других: «Они ничего не понимают, а мои решения — самые правильные. Мне не нужно никому ничего доказывать».Оба этих явления часто приводят к конфликтам в команде. Коллеги могут замечать, что кто-то постоянно молчит на совещаниях, хотя мог бы внести ценные предложения. Такие ситуации требуют особого внимания и вмешательства со стороны руководителя.Или обратная ситуация: сотрудник пытается вставлять свое мнение везде, даже когда его аргументы не подкреплены достаточными знаниями. Это может вызывать раздражение у остальных членов команды и приводить к конфликтам. В такой ситуации также может возникнуть ощущение, что этот человек мешает эффективной работе коллектива.Что делать? Опять-таки, полезно получить обратную связь от команды, чтобы понять суть проблемы. Возможно, сотруднику нужно поработать над своими софт-скиллами. Можно включить соответствующие тренинги в ИПР, предложить прослушать лекции или посетить курсы. Эти меры могут помочь сгладить острые углы и улучшить взаимодействие в команде.Может быть так, сотрудник попросту не вписывается в коллектив. Его софт-скиллы и общий подход к работе отличаются от принятых в команде стандартов, и это становится препятствием для эффективного взаимодействия. В таком случае одним из решений может стать ротация. Если все попытки адаптировать сотрудника к команде оказались безуспешными, стоит рассмотреть перевод его в другое подразделение, где его сильные стороны смогут раскрыться наилучшим образом. Главное — не терять ценного специалиста, а найти для него подходящую роль в организации.Четвертый сценарий: отсутствие целиЭто может произойти, когда цель (ИПРа) изначально была сформулирована неправильно, либо изменилась во время работы над продуктом. В результате сотрудник теряет мотивацию и не видит смысла в своей работе. Ему кажется, что его усилия напрасны: «Зачем я это делаю? Все равно это отправится в стол». Или: «Опять эта автоматизация в потоке, какой смысл тратить на нее время, ведь это просто рутинная работа».Соответственно, решение заключается в пересмотре ИПР. Нужно обсудить с сотрудником, какие цели могут быть достигнуты в рамках текущего проекта. Если новые цели его не устраивают, стоит выяснить, какие задачи он хотел бы выполнять и где в компании это возможно реализовать.Если договориться не удается, может потребоваться ротация сотрудника в другую команду. В противном случае его производительность снизится, он может покинуть компанию или начать негативно влиять на атмосферу в коллективе.Пятый, более сложный сценарий: одновременно нарушаются два уровня потребностейНапример, сотрудник хочет перейти на удаленку, при этом испытывает сложности в коммуникации. Такие ситуации часто приводят к возникновению токсичной атмосферы в команде. Удаленный сотрудник может чувствовать себя изолированным, поскольку ему сложно интегрироваться в офисную культуру и наладить общение с коллегами. В свою очередь, команда может испытывать недовольство из-за недостаточной вовлеченности удаленного коллеги, что усугубляет конфликты.Когда такие факторы накладываются друг на друга, возрастает вероятность появления токсиков в коллективе. Поэтому крайне важно оперативно реагировать на подобные ситуации и искать способы их разрешения.Ротация поможет решить проблемы на социальном уровне, когда сотрудник не находит общего языка с командой. В таком случае можно поискать другие отделы или проекты, где у сотрудника будут схожие цели и менталитет, близкий к его собственному. Возможно, в другом подразделении найдется подходящая вакансия, куда можно перевести сотрудника.Оценка компетенций может помочь разобраться в причинах конфликтов и недовольства. Возможно, сотрудник стал раздражается и ссорится с командой из-за того, что его навыки выросли, а зарплата осталась прежней. Когда люди начинают обсуждать доходы в неформальной обстановке, например, в баре после работы, это может спровоцировать недовольство. Сотрудник может осознать, что зарабатывает меньше, чем заслуживает, учитывая его вклад и возросший уровень профессионализма.В такой ситуации стоит провести объективную оценку компетенций. Возможно, повышение зарплаты или изменение условий труда действительно поможет исправить ситуацию и восстановить гармонию в команде. Иногда решение проблемы лежит не в изменении окружения, а в признании заслуг и справедливом вознаграждении.Шестой сценарий: сотрудник доволен зарплатой и в целом его базовые потребности закрытыСотрудник испытывает недостаток мотивации из-за отсутствия интересных задач, плюс имеются проблемы с командой. Это может привести к снижению производительности и ухудшению качества выполняемых задач. В итоге сотрудник теряет понимание смысла своей деятельности и утрачивает интерес к работе.В этом случае есть смысл пересмотреть ИПР. Возможно, были выбраны неверные цели, отсюда и низкая мотивация у сотрудника. Также стоит учесть, что человек мог просто устать, и ему нужен отпуск для восстановления сил. После отдыха ситуация может улучшиться: сотрудник сможет пересмотреть свои подходы к достижению целей, взять на себя больше ответственности и работать без конфликтов с коллегами. Если же проблема окажется глубже, возможно, стоит рассмотреть варианты перераспределения обязанностей или изменения должности.Седьмой сценарий: отличная команда, но цели и базовые потребности проседаютСотрудника не устраивает уровень зарплаты или отсутствует возможность карьерного роста. Он не видит перспектив для своего профессионального развития, еще и ИПР надо выполнять, а времени на это практически нет. В таких случаях сотрудник часто начинает сомневаться в своих способностях, особенно если он склонен к синдрому самозванца. Ему кажется, что он не справляется с поставленными целями, даже несмотря на наличие необходимых компетенций. Важно разобраться, действительно ли задачи превышают его возможности, или же он уже выполнили все необходимое.Восьмой, самый критический сценарий: ничего не устраиваетКоманда, зарплата, цели. Ни финансовая сторона, ни сама компания, ни мотивация не удовлетворяют. Не нравится то, куда мы движемся, как развиваемся. В общем, ничего не радует.Что делать? Попробуем изменить один из аспектов ситуации. Возможно, стоит пересмотреть цель, и это поможет удовлетворить базовую потребность. Иногда бывает полезно перевести сотрудника в другой отдел или дать ему самостоятельную задачу, чтобы он смог поработать отдельно. Однако такие меры нередко приводят к тому, что сотрудник уходит из компании. Если это неизбежно, важно подготовиться к этому заранее.Необходимо расстаться с сотрудником корректно, без конфликтов. Вместо упреков лучше проявить понимание и предложить поддержку. Можно сказать: «Мы ценим твою работу, но, похоже, наши пути расходятся. Возможно, через какое-то время ты захочешь вернуться, но сейчас мы не можем продолжать сотрудничество». Затем обсудить условия ухода.Таким образом расставание может пройти максимально уважительно и конструктивно.Итак, чем же хороши сценарии? Они помогают выявить потенциальные риски, определить проблемные области и продумать способы их устранения, разработать скрипты. Подсказывают возможности ротации сотрудников, если становится очевидно, что им тесно на текущем месте или они начинают проявлять признаки токсичного поведения. Кроме того, сценарии показывают, что сотрудник потенциально может принять решение об увольнении.Изменения подхода к наймуБерем нашу пирамиду. Кто поможет закрыть потребности сотрудников, когда мы начнем нанимать сотрудников? Базовыми потребностями обычно занимаются наши HR-специалисты. Они обзванивают кандидатов, уточняют их ожидания по зарплате, проверяют соответствие стеку технологий и оценивают, подходят ли кандидаты для нашей команды.Социальные потребности — это собеседование с куратором, где обсуждаются карьерные цели и направления развития. Базовые потребности закрываются HR-отделом на этапе скрининга, который помогает отсеять тех кандидатов, которые не соответствуют требованиям вакансий.Социальные потребности оцениваем, чтобы понять, насколько кандидат вписывается в коллектив. Для этого проводим собеседование с представителем команды. Этот этап следует за техническим интервью. Здесь важно выяснить, сможет ли человек эффективно взаимодействовать с коллегами, даже если у него хорошие технические навыки. Если обнаруживается, что кандидат несовместим с командой, то, несмотря на сильные стороны, он может стать источником проблем. Нам ведь не нужен токсичный сотрудник в коллективе.Цели обсуждаются на встрече 1х1 с куратором во время технического собеседования. Важно узнать, как кандидат видит свое развитие: посещает ли он митапы, занимается ли саморазвитием. Люди, стремящиеся оставаться в индустрии, обычно четко понимают, в каком направлении хотят расти. Это позволяет оценить их заинтересованность и мотивацию.Этот подход позволяет не тратить время на пустые собеседования и помогает скорректировать вопросы для HR, чтобы они могли эффективнее отсеивать неподходящих кандидатов. Он уменьшает вероятность найма сотрудника, который не впишется в команду, тем самым снижая необходимость решения таких ситуаций на 1х1. Кроме того, снижается риск принятия на работу специалиста, который, несмотря на отличные профессиональные навыки, не сможет эффективно взаимодействовать с командой.ВыводыИтак, подведем итоги. Встреча 1х1 может выявить потенциальные проблемы, которые потребуется решить с сотрудником. Отсутствие ясной карьерной цели также может привести к увольнению, и это факт. Деньги являются хорошим стимулом, однако через несколько месяцев ситуация может повториться. Важно привлекать команду к собеседованию, чтобы сотрудники знали, с кем будут работать. Это лишним не будет. Также стоит активно вовлекать HR, предоставляя им скрипты для первичного отбора кандидатов, чтобы сразу отсеивать тех, кто вряд ли подойдет вашей компании."
23,23,"Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное",H3LLO.CLOUD,Облачный провайдер которого вам захочется обнять,905.26,Домены и хостинг,2025-03-13,"  Контейнер — это типа виртуальной машины, только меньше и другое. Несколько контейнеров запускаются внутри одной машины и разделяются друг от друга.   Это значит, что можно запустить приложение с одним набором зависимостей, а рядом — второе с другим. Это значит, что можно сохранить все связки приложения, упаковать его в контейнер и деплоить где угодно — и знать, что оно точно запустится. Есть нюансы с переходом между ARM-архитектурой и x86, но в целом контейнеры универсальны.   В контейнерной упаковке огромное количество софта, в том числе очень много опенсорса. Можно поднять готовый контейнер с сервисом из хаба без проблем вообще. И это не создаёт сложных взаимозависимостей. Нужен PostgreSQL? Docker pull postgres — и он у вас.  К контейнерам монтируются свои ресурсы — диски, сети, конфиги и секреты.  Контейнеры оплачиваются за потребление, то есть можно сделать сервис, который слушает порт и делает обработку, повесить и забыть. За год вы заплатите рублей 20.   Рои контейнеров могут масштабировать крупные корпоративные проекты, про это ниже.  И, наконец, никакой современный CI/CD почти не делается без контейнеров. Системным администраторам, DevOps-инженерам, разработчикам и СТО критически важно разобраться в контейнеризации.   Это будущее, и у этого будущего довольно странная кривая обучения. Сейчас коротко расскажу, попробуем разобраться, как её лучше проходить и где там грабли. Ну и ещё, если вы дойдёте до середины, ваша зарплата будет 450 тысяч рублей в месяц и выше.   Короткая вводная Раньше всё было просто: есть админы, которые настраивают серверы, и есть разработчики, которые пишут код. Между ними как будто была стена, через которую разработчики кидали админам сборку софта, а админы в обратку кидали проблемы с деплоем и рантайм-эксепшены.  А теперь границы размываются. Разработчики должны понимать, как их код будет жить в продакшне. Админы должны уметь автоматизировать развёртывание. СТО нужно планировать архитектуру с учётом масштабирования и отказоустойчивости.  И здесь на сцену выходят контейнеры — технология, которая объединяет эти миры. Но чтобы в 2025 году оставаться в игре, нужно не просто знать, что такое Docker. Нужно понимать всю экосистему — от простейших контейнеров до оркестрации в Kubernetes и интеграции с нейрокодингом.  Сначала у нас были физические серверы — железки, которые занимали целые стойки. Хочешь новое приложение? Покупай новый сервер. Нужно больше мощности? Апгрейдь железо.  Потом пришла виртуализация и изменила правила игры. Теперь на одном физическом сервере можно запустить несколько виртуальных машин с разными операционками. Это был прорыв: меньше железа, больше гибкости, лучше утилизация ресурсов.  Но виртуализация принесла и свои проблемы. Каждая виртуалка — это полноценная операционная система со всеми потрохами. Это гигабайты дискового пространства, сотни мегабайт оперативки только на старте и заметное время запуска.  Виртуалки — штука классная, но у них есть несколько серьёзных ограничений:  Проблемы масштабирования. Допустим, у вас «чёрная пятница» и нагрузка выросла в три раза. С виртуалками у вас два пути: либо увеличить ресурсы существующей машины (ресайз), либо клонировать её. Оба варианта требуют времени и дополнительной настройки.  Сложности с зависимостями. Представьте: у вас на одном сервере два приложения. Одному нужна Java 8, другому — Java 11. С виртуалками решение одно: две отдельные виртуалки. Это лишние ресурсы и головная боль с администрированием.  Неэффективное использование ресурсов. Каждая виртуалка жрёт ресурсы даже в простое, потому что крутит полноценную ОС. Это как держать заведённую машину на парковке.  Разница между виртуализацией и контейнеризацией Здесь важно понять ключевую разницу в подходах.   Виртуальные машины — это «домашние питомцы». Вы их поднимаете, называете по имени, заботитесь об их здоровье, бьётесь над их аптаймом. Вы знаете их всех в лицо, все ваши восемь виртуалок. Стараетесь, чтобы они прожили как можно дольше.  Контейнеры — это «стадо». Вы не знаете их по именам, их может быть сотни. Один упал — и ладно, поднимется другой. Это нормально, если за рабочий день вы запустите и грохнете пару сотен контейнеров.  Жизненный цикл виртуалки рассчитан на месяцы и годы. Она копит проблемы и болезни, как старый человек. Контейнеры же могут жить минуты или часы, а потом уступить место свежему контейнеру с новой версией кода.  Собственно, именно так контейнеры чаще всего и используются.   Существует даже кейс с очень коротким жизненным циклом контейнера: за 1–2 секунды поднимается контейнер, отрабатывает запрос, умирает. Это крайне удобно. Так делают на платформах приложений и в CI/CD-pipeline.  Контейнер — это изолированный процесс, которому кажется, что он запущен в отдельной системе. Но на самом деле он использует ядро хостовой ОС. Для изоляции контейнеры используют не возможности железа (как виртуалки), а возможности ОС — так называемое пространство имён. Например, Docker использует cgroups в ядре Linux.  Виртуалка весит гигабайты, контейнер — мегабайты. Postgres в контейнере (без нагрузки) — всего 9 мегабайт оперативки. Девять! Это просто смешно по сравнению с полноценной виртуалкой.  Виртуалки запускаются минуты, контейнеры секунды.   Но самое главное — портативность между различными средами. «У меня локально работает, а на проде — нет». С контейнерами такое почти исключено. Запаковали приложение в контейнер, и оно будет работать одинаково, что у вас на ноуте, что на тестовом сервере, что в продакшне.  Допустим, у вас типичная система из веб-сервера, приложения и базы данных. Описав всё это в Docker Compose файле, вы можете одной командой поднять всю инфраструктуру на любой машине.  Чтобы было удобно разворачивать контейнеры в разных средах, образ заливается в репозиторий (публичный или приватный), и тянется оттуда. Не надо ручками раскидывать образ на хосты — они сами стянут его из репы. Существуют миллионы готовых образов и создавать их с нуля не надо.  Вот почему они такие крутые.   Первые шаги: знакомство с Docker’ом Сначала вы не понимаете, зачем нужен Docker и все эти статьи на Хабре про него. Скачиваете его из любопытства, долго пытаетесь разобраться, как создать свой первый контейнер. Потом замечаете, что запустить Postgres в Docker — это мизинцем левой ноги, когда в обычной установке это адская головная боль.  Docker и GUI к нему (Docker Desktop) можно поставить и на Linux, и на Mac, и на Windows. Для начала работы достаточно знать несколько команд: docker run, docker build, docker ps, docker stop. Не пугайтесь — это проще, чем кажется. Чуть-чуть покурить обучение на Ютубе или официальный get started — и вот у вас появляется первый контейнер.   Порог входа обманчиво простой.   Дальше идёт работа с образами и репозиториями — Docker Hub и прочие Container Registry. Docker Hub — это как GitHub, только для контейнеров. Там уже есть готовые образы для большинства популярных программ.   Первый Dockerfile можно написать за 10 минут. Это просто текстовый файл с инструкциями: откуда взять базовый образ, какие команды выполнить при сборке, что запустить. LLM сейчас отлично умеют генерировать Dockerfile — это ваш первый помощник.  Обычно с Docker знакомятся так: когда вы инди-хакер с одной виртуалкой в облаке, контейнеры вам, возможно, и не нужны. Момент истины наступает, когда:  Бизнес начинает расширяться. Проект превращается во что-то более крупное. Вы обновляете приложение несколько раз в день. В команде появляется больше 3–5 разработчиков.  Дальше идёт описание многокомпонентных систем в Docker Compose. Docker Compose позволяет описать в одном YAML-файле целую инфраструктуру: несколько связанных контейнеров, их сети, тома для хранения данных. Одна команда — и всё поднимается автоматически в нужном порядке.  Но до этого вас ждёт ещё пара ответвлений в дереве развития навыков.   Podman как альтернатива Docker. Со временем вы узнаёте, что Docker — не единственный вариант. Podman, например, не висит постоянно в памяти. Можно даже сделать алиас, alias docker=podman, и забыть, что у вас на самом деле не Docker.  Podman хорош для простого докерного опыта, но если нужен Docker Compose — это фича исключительно докеровская (хотя последние Podman уже поддерживает Docker Compose через плагины).   По мере того как вы и ваш проект растёте, вы начинаете задумываться об оркестрации.  Продвинутый этап: мультихостовые решения и CI/CD Когда контейнеры на одной машине уже не справляются, вы смотрите в сторону кластеров и автоматизации.   Самое главное — контейнеры упрощают CI/CD. До контейнеров CI/CD был про скрипты, которые что-то собирают, тестируют и куда-то выкладывают. Контейнеры всё упростили: среда сборки такая же, как среда запуска. Куча головных болей с зависимостями просто исчезла.  CI-системы могут собирать контейнеры в чистой среде, гарантируя, что никакой левый код не попадёт в продакшн. Вы просто пушите изменения в репозиторий, а CI автоматически собирает новый образ. Дальше CD раскатывает его по разным окружениям.  Это управление образами и их версиями. Для каждой версии вашего кода создаётся отдельная версия образа. Хотите откатиться? Просто укажите предыдущую версию. Никаких сложных процедур отката.  Docker Swarm обычно выступает как первая попытка оркестрации. Docker Swarm кажется логичным продолжением Docker — всё-таки от тех же разработчиков.   Вы в нём пытаетесь что-то сделать, плюётесь и думаете: «Хорошо, видимо, судьба ведёт в Кубер».   И в этот момент контейнеры ломают вас об колено.   Установить «настоящий» Кубер (тот, что в репозитории github.com/kubernetes/kubernetes) ужасно сложно. Тут возникает развилка: либо пользоваться облачными Managed Kubernetes, либо использовать упрощённые дистрибутивы вроде k3s, k0s, Minikube или Kind.  Дело в том, что в k8s вообще другой подход к тому, что кажется логичным. Он очень дружелюбен. В основном, как кажется поначалу, к шизофреникам. Но по мере того, как вы научитесь думать контейнерами, всё станет очень логичным и понятным. Возможно.   В Kubernetes базовая единица — не контейнер, а pod. Это такая абстракция, в которой может жить несколько контейнеров. Обычно требуется больше одного дня, чтобы врубиться, что же это такое. Вместе с этим меняется представление о контейнерах, ведь теперь у них появляются роли (init container, side-car container).  Kubernetes автоматически делает то, о чём ты раньше мог только мечтать: если приложение падает, оно автоматически перезапускается; если нагрузка растёт, оно автоматически масштабируется; если нода выходит из строя, рабочие нагрузки перераспределяются. Ну, то есть после настройки кучи всего, конечно. Например, для автомасштабирования понадобится Horizontal Pod Autoscaler и включённые метрики.  Ещё есть манифесты и Helm-чарты. В Kubernetes всё описывается в виде YAML-манифестов. В манифесте описывается желаемое состояние системы и Кубер сам заботится о поддержании этого состояния. Это как инфраструктура-как-код, но для контейнеров. Helm-чарты позволяют упаковать несколько манифестов в один установочный пакет и использовать шаблонизацию.  И ещё бывает автоматическое обновление без простоев. Kubernetes умеет обновлять приложения по принципу rolling update: постепенно заменяет старые поды новыми, поддерживая доступность сервиса. Пользователи даже не заметят, что произошло обновление.  Дальше надо понять базовые принципы сетевого взаимодействия контейнеров:  TCP/IP и Linux bridge. Контейнеры общаются через обычную сеть. В пределах одной машины это виртуализированная сеть через Linux bridge. Создаётся виртуальный адаптер, крепится к контейнеру, появляется интерфейс с IP-адресом. К этой же виртуальной сети подключаются другие контейнеры. Виртуальные сети между контейнерами. С Docker Compose можно создать несколько разных сетей и ограничить общение контейнеров друг с другом. Это как виртуальные VLAN в мире контейнеров. Ручками тоже можно, но с Docker Compose — проще. Ingress и маршрутизация трафика. Ingress в Kubernetes — это способ маршрутизировать внешний трафик к сервисам внутри кластера. Если ставить Кубер на голом железе (или в виртуалках на голом железе), то на этом месте можно сломать голову и любой новичок гарантированно забуксует. Без возможности выставить сервис наружу Кубер превращается в чемодан без ручки дом без окон и дверей — все системы прекрасно работают внутри него, общаются между собой, но достучаться к ним снаружи кластера невозможно. Сейчас рекомендуется переходить с Ingress на Gateway API. Совет — не делайте этого, не освоив Ingress. И ни в коем случае не ставьте сразу Kong (он хорош, но о-о-о-чень сложен для начала). Балансировка нагрузки. Kubernetes умеет распределять запросы между несколькими репликами приложения. Managed Kubernetes обычно использует LoadBalancer от провайдера. Если ставить Кубер на своём железе — можно попробовать MetalLB — проект специально сделан, чтобы дать функционал LoadBalancer там, где он не предусмотрен по дизайну. Без балансировки между инстансами ничего не получится, иначе вы опять будете реплицировать монолит. Монолит, кстати, тоже можно разворачивать через контейнеры, но обычно смысл в другом. Инструменты визуализации (Hubble для Cilium). Kubernetes умеет дружить со множеством различных реализаций контейнеров, сетей и хранилищ. Делается это за счёт интерфейсов. Так вот для сетей есть Cilium, а для Cilium есть Hubble — визуальный интерфейс. Можно буквально увидеть, как взаимодействуют компоненты, куда идёт трафик. Это одна из самых частых проблем — нужно держать в голове карту взаимодействия, и это решение помогает быстрее освоиться.   Грабли, которые вы соберёте В этом месте, если вы научитеcь оркестрировать, возникнет следующий уровень сложности — ИБ. Она, как и везде, ошибок не прощает.   Настройка портов и их экспозиция. Даже Senior-разработчики, бывает, путаются в настройке портов: что такое publish, expose, когда использовать ClusteIP, а когда NodePort. Это нормально — все через это проходят. Каждый раз, когда вы открываете порт наружу, вы создаёте потенциальную дыру в безопасности.  Второе правило контейнерного клуба: не запускайте всё от рута. Контейнеры изолированы не так хорошо, как виртуалки, поэтому использование непривилегированных пользователей критически важно.  Контейнеру не нужны все права в мире. Дайте ему только то, что ему действительно необходимо для работы. Используйте seccomp, AppArmor или SELinux для дополнительных ограничений. Если Junior-разработчик предлагает запустить контейнер с флагом --privileged, это повод для серьёзного разговора (или увольнения, в зависимости от обстоятельств).  Уделите время освоению модели RBAC и практикам её использования в Kubernetes.  Никогда не хардкодьте пароли и ключи в образ контейнера. В Docker есть Docker Secrets, в Kubernetes — Secrets API. Используйте их вместо переменных окружения с чувствительными данными. Это важно, когда вы начнёте раскатывать что-то за пределы одного проекта.   Ну и контейнеры должны знать, как обращаться друг к другу. В простом случае это имя контейнера, в Kubernetes — сервис с DNS-именем.  Заражённые контейнеры тоже бывают. Образы контейнеров, как и любой код, могут содержать уязвимости. Да и атаки на цепочки поставки никто не отменял. Используйте инструменты вроде Trivy или Clair для проверки образов перед деплоем.  Ну и не наступайте на грабли с лицензиями. В мире контейнеров доминирует открытое ПО. Postgres вместо Oracle, Nginx вместо IIS. Это не только дешевле, но и удобнее упаковывается. Если у софта всё же есть коммерческая версия, то чаще всего она подразумевает поддержку от разработчика и дополнительный функционал. Обычно в мире open-source софт лицензируется по фичам, а не по ресурсам. Вы покупаете энтерпрайз-версию, а как её масштабировать — ваше дело.   Зачем всё это? Современные LLM генерируют не просто куски кода, а целые приложения. И идеальный способ их запустить — контейнеры. Sonnet 3.7 уже настолько круто кодит, что многие компании смогли увеличить производительность в разы.  Вы описываете словами, что хотите получить, и нейронка генерирует готовый код. Дальше дело за малым — упаковать его в контейнер и запустить. Этот процесс становится таким же простым, как отправка письма. Кстати, уже есть люди, которые не умеют в электронную почту )  Ну и сейчас у каждого уважающего себя облачного провайдера есть управляемый Kubernetes. Вам не нужно возиться с установкой и настройкой — просто выбираете количество нод и через несколько минут кластер готов к работе. Но есть нюансы.   Решения для запуска приложений в контейнерах часто бывают и serverless, там основная единица — это контейнер, а не ноды кластера.  Яндекс, например, предлагает сервис, где контейнер запускается только на время вызова. Происходит HTTP-запрос, запускается контейнер, отрабатывает и умирает.   У нас в L1veStack подход, скорее — замена виртуалок. Serverless containers. Это логическое развитие контейнеризации: вы не думаете о серверах вообще. Есть контейнер, он запускается и работает пока вы его не остановите, и тарификация только за потреблённые ресурсы во время его работы. Наши контейнеры рассчитаны на долгую жизнь, но никто не мешает вам убить полстада и запустить заново. И повторить через 5 секунд.   Как учиться Отличный первый проект для освоения контейнеризации — простой чат-бот. Он включает и фронтенд, и бэкенд, и базу данных — всё, что нужно для понимания взаимодействия контейнеров. Вот у нас пример. Там ещё и простой CI/CD на базе GitHub Actions, который собирает образ при каждом новом коммите.  Начните с запуска отдельных контейнеров, потом объедините их с Docker Compose, затем перенесите в Kubernetes. Это естественный путь освоения технологии.  Сейчас есть множество готовых Docker Compose файлов для популярных стеков. Хотите WordPress? Есть готовый рецепт. Хотите LowCode-платформу? Есть готовый рецепт. Нужна CRM-система? Есть готовый рецепт. Не изобретайте велосипед, используйте готовые решения.  Проиграйте failsafe-сценарии. Запустите несколько реплик, сымитируйте падение одной из них, посмотрите, как система восстанавливается. Это бесценный опыт, потому что он учит думать именно в философии контейнеров.   Грейды примерно такие:  Junior: базовые знания Docker, умение запускать и останавливать контейнеры. Middle: сборка своих образов, Docker Compose, CI/CD с контейнерами, основы Kubernetes. Senior: глубокое понимание Kubernetes, настройка производительности, мониторинг.  Если говорить про администрирование, то джуны ломаются на Ingress и выставлении портов наружу. Мидлы поломаются на Gateweay API и настройке Auto Provisioning для persistent volumes — как настроить систему, чтобы она автоматически выделяла дисковое пространство, например в Ceph. Даже некоторые сеньоры спотыкаются на сетевых тонкостях Kubernetes.  Контейнеризация нужна везде: от стартапов до корпораций. Но особенно востребованы специалисты в компаниях, которым нужно быстро и часто деплоить обновления. В 2025 году умение работать с контейнерами — это уже не преимущество, а базовый навык. Не важно, кто вы — разработчик, администратор или CTO, вам нужно понимать, как работают контейнеры и зачем они нужны.  Нейронки сделали возможным выкатывать Proof-of-Concept целых систем за дни, а MVP — за недели. Скорость Time-to-Market решает в этой гонке, и её не повысить без инструментов автоматизации сборки и деплоя. И Kubernetes тут как нельзя в тему.  Если вы освоили всю стопку технологий и умеете их применять на практике (и не просто применять, а устанавливать и правильно настраивать), вы — мега-DevOps с зарплатой от 500к и выше. И спрос на таких специалистов только растёт.  Начать можно с официальной документации Docker и Kubernetes. Для углубления знаний рекомендую «Kubernetes в действии» и Docker: Up & Running. Есть даже отдельная книга «Kubernetes и сети» — для тех, кто хочет разобраться в сетевом взаимодействии.  Лучший способ учиться — делать. Пройдите интерактивные курсы на Katacoda или Play with Docker. Устройте себе хакатон выходного дня — запакуйте своё приложение в контейнер и запустите его в Kubernetes.   Почитайте Open Container Initiative.   Покурите service mesh решения (Istio, Envoy), которые важны при работе с микросервисами.  Разберитесь со StatefulSets (хранение состояния в контейнерах) — если не сломаете голову до этого.   На сладкое оставьте CRD и операторы (возможно, даже свои).  Подписывайтесь на телеграм-каналы DevOps-сообществ, следите за кейсами компаний, которые внедрили контейнеризацию. Вступайте в сообщества на Reddit, Stack Overflow, GitHub Discussions. Задавайте вопросы, делитесь своими находками. Контейнеризация — это живая экосистема, и лучший способ оставаться в курсе — быть её частью."
24,24,"Чужое открытое ПО внутри BI-систем: проблемы лицензирования, о которых не все задумываются",Luxms BI,Компания,31.7,"Веб-разработка, Программное обеспечение",2025-03-13,"Для быстрого создания какого-то продукта часто используют различные open source решения. Но применение открытого ПО несёт в себе определенные риски: от финансовых до юридических.Например, из open source для исследования и визуализации данных наиболее популярен Superset. Около десятка российских систем взяли его за основу своих BI. Но его применение несёт риски: Стратегические риски. Roadmap Superset формируется с учётом интересов западных якорных заказчиков и интересов американских инвесторов, а не российского бизнеса. Владелец проекта — американская компания Airbnb, и в любой момент по политическим или другим причинам может ограничить доступ к своему ПО для российских разработчиков;Компании, использующие Superset, зависят от своих внутренних ИТ-команд, что может привести к кадровым и финансовым рискам при уходе ключевых разработчиков;Организационные риски. Нет централизованной технической поддержки от вендора — пользователи вынуждены искать решения проблем в документации, на форумах и сторонних источниках.Стоить добавить, что популярность открытого ПО также связана с иллюзией не только «бесплатной» разработки и дальнейшей поддержки со стороны собственного ИТ-персонала, но и кажущимся отсутствием лицензионных платежей. Но разработка на базе ПО с открытым исходным кодом не означает бесконтрольное использование созданных на этом коде продуктов. Существует ряд open source лицензий: BSD, GNU, MIT, LGPL, AGPL, BSPL, SSPL, Demoware License, Apache License 2.0, RSAL и другие, в которых нужно разбираться. Соблюдать их важно не только компании, которая интегрировала такое решение в свой продукт, но и той, которая такой продукт купила и использует.В соответствии со статьями 1252 и 1301 ГК РФ иски о нарушении лицензионного права предъявляются к юридическому лицу, использующему указанное программное обеспечение.  Меня зовут Алексей Розанов, я руководитель пресейл-направления и работы с партнёрами ГК Luxms, вендора российской платформы бизнес-аналитики Luxms BI. Luxms BI — платформа бизнес-аналитики данных с высочайшим быстродействием и горизонтальной масштабируемостью. У неё мощные функциональные и визуальные возможности, а также быстрая обработка больших объёмов данных благодаря своей датацентричной архитектуре. В Реестре российского ПО.Рассмотрим несколько примеров использования open source решений на рынке BI-аналитики, и я постараюсь подсветить лицензионные риски для компаний, которые используют BI-системы на их основе.1. Использование open source как основы нового продукта.Есть российская BI-система, в основе которой лежит open source BI система «Metabase». Metabase Open Source Edition предоставляется:под лицензией GNU Affero General Public License (AGPL), которая требует раскрытия исходного кода тем лицам, которым вы также предоставляете двоичный файл;и под коммерческой лицензией с соответствующей оплатой. Если российская BI-система использует Metabase под AGPL и не соблюдает требования AGPL, это может привести к юридическим спорам со стороны правообладателей Metabase, а если исходный код системы раскрыт, это может привести к утечке конкурентных преимуществ или уязвимостей.Если российская BI-система использует Metabase под коммерческой лицензией и не оплачивает её, это может привести к юридическим последствиям, включая судебные иски. Помимо этого, правообладатель Metabase базируется в Сан-Франциско, Калифорния, что несёт в себе дополнительные риски закрытия или изменения проекта open source. В том числе, остается зависимость от политики лицензирования Metabase: если условия лицензий изменятся, это может повлиять на стоимость или доступность системы в целом.2. Использование open source, как части системы. Highcharts — это библиотека для создания интерактивных графиков и диаграмм, написанная на JavaScript. И некоторые популярные российские BI-системы используют её, как основу для своих визуализаций, но при её интеграции в коммерческих проектах применяется лицензия Original Equipment Manufacturer (OEM), которая подразумевает оплату. OEM-лицензия обычно предусматривает использование библиотеки только в рамках конкретного продукта, в данном случае — BI-системы, и её использование, например, для собственных разработок, может нарушить условия лицензии. Поэтому при внедрении BI-системы со встроенной библиотекой Highcharts нужно убедиться, что вендор приобрел лицензию легально, и она соответствует условиям использования библиотеки в конкретном случае. Если лицензия не оплачена или нарушены её условия, это может привести к юридическим последствиям для компании-пользователя (например, судебным искам со стороны правообладателя Highcharts).И так же, как с любым open source, использование Highcharts означает зависимость от сторонней библиотеки. Если они изменят свои условия лицензирования или прекратят поддержку, это может повлиять на работу BI-системы.3. Поставка целиком BI-системы зарубежного вендора под видом российского BI.На российском рынке, где уход зарубежных вендоров вызвал всплеск спроса на импортозамещение, есть примеры и подобных решений, разделить которые можно на: Поставка по OEM-лицензии — вполне официально, но с рисками аналогичными использованию, например, Highcharts, разобранным выше;Полное копирование зарубежной системы, даже не open source, и её выдача за российскую разработку после незначительных доработок. Здесь риски существеннее, поскольку помимо лицензионного вопроса непонятно, какова экспертиза разработчика, его влияние или возможность поддержки и развития исходного кода.Лицензионные риски состоят также в том, что свободный, в полном смысле этого слова, open source — это не навсегда. И в последние годы явственно наметился тренд на коммерциализацию open source решений. Вот несколько примеров:2018– MongoDB. Для MongoDB ограничили свободное использование ПО (ранее использовали AGPL, перешли на SSPL). Подробный разбор есть в статье на Хабре от @PeterZaitsev2020– HortonWorks (Hadoop). После поглощения HortonWorks, Cloudera закрывает доступ к сборкам HDP и бесплатный доступ к CDH. 2021– ElasticSearch Kibana. Elastic меняет правила лицензирования (ранее — Apache 2.0, потом — SSPL, а в 2024 году пообещали вернуться в open source под лицензией AGPL).– CentOS. RedHat меняет правила разработки бесплатного CentOS.2023– Terraform. HashiCorp изменил тип лицензии на BSL.– The Document Foundation. Российских разработчиков исключили из консультационного совета OSS проекта LibreOffice.2024– CockroachDB. Cockroach Labs изменил тип лицензии.– Greenplum. Закрытие OSS-проекта после поглощения Broadcom.– CentOS. RedHat прекратила поддержку CentOS.– Redis. Redis изменил тип лицензии с BSD на Redis Source Available License (RSALv2) и Server Side Public License (SSPLv1).– Linux. Российских разработчиков исключили из OSS проекта Linux и доступ к репозиторию ПО.Я не хочу сказать, что использовать open source — это плохо и вредно. Мы в платформе Luxms BI тоже используем open source компоненты, но все они соответствуют следующим условиям:Открытые исходные коды (open source);Наличие публично доступного репозитария для скачивания исходных кодов;Выход новых версий (или исправление ошибок) не реже двух раз в год;Свободная лицензия: Apache 2.0, MIT, BSD, LGPL или совместимая.Полный список — в нашей документации. Поэтому всё, что я хочу донести, это важность для компании, которая выбирает BI-систему, провести дополнительный юридический анализ, чтобы избежать лицензионных рисков: Выяснить, какое открытое ПО используется в системе; Проверить лицензию не только выбранной BI-системы, но и всех её компонентов; Проанализировать все лицензионные соглашения и действующие в них разрешения и ограничения;Убедиться, что по этим параметрам всё устраивает и подходит;Убедиться, что вендор выполняет требования лицензии: например, делает все необходимые отчисления.А вы обращали внимание на тренд смены лицензий у open source проектов? Используете ли открытое ПО в своей работе и смены лицензии какого проекта не хотите больше всего?"
25,25,"Мартовский «В тренде VM»: уязвимости Microsoft, Palo Alto, CommuniGate и кто должен патчить хосты с прикладом",Positive Technologies,Лидер результативной кибербезопасности,4.21,"Веб-разработка, Программное обеспечение, Информационная безопасность",2025-03-13,"Хабр, привет! На связи Александр Леонов, ведущий эксперт PT Expert Security Center и дежурный по самым опасным уязвимостям месяца. Мы с командой аналитиков Positive Technologies каждый месяц исследуем информацию об уязвимостях из баз и бюллетеней безопасности вендоров, социальных сетей, блогов, телеграм-каналов, баз эксплойтов, публичных репозиториев кода и выявляем во всем этом многообразии сведений трендовые уязвимости. Это те уязвимости, которые либо уже эксплуатируются вживую, либо будут эксплуатироваться в ближайшее время. Начнем с четырех трендовых уязвимостей:Уязвимости в Microsoft🔻 Уязвимость повышения привилегий драйвера Ancillary Function (AFD.sys) (CVE-2025-21418). 🔻 Уязвимость повышения привилегий в хранилище Windows (CVE-2025-21391). Уязвимость в Palo Alto Networks🔻 Уязвимость обхода аутентификации в Palo Alto Networks PAN-OS (CVE-2025-0108)Уязвимость в CommuniGate Pro🔻 Уязвимость удаленного выполнения кода в почтовом сервере CommuniGate Pro (BDU:2025-01331)Две из них касаются  Windows и были исправлены в рамках февральского Microsoft Patch Tuesday. Уязвимость повышения привилегий драйвера Ancillary Function Driver (AFD.sys)💥 CVE-2025-21418 (оценка по CVSS – 7,8; высокий уровень опасности)Ancillary Function Driver (AFD.sys) — это системный драйвер Windows, критически важный для процессов сетевого взаимодействия. Он предоставляет низкоуровневую функциональность для WinSock API, позволяя приложениям взаимодействовать с сетевыми сокетами.Для эксплуатации уязвимости аутентифицированному атакующему необходимо запустить специально созданную программу, которая в итоге выполняет код с привилегиями SYSTEM. По описанию эта уязвимость очень похожа на прошлогоднюю уязвимость CVE-2024-38193, которая активно эксплуатировалась злоумышленниками из группировки Lazarus.Признаки эксплуатации: Microsoft отмечает факты эксплуатации уязвимости. Также CISA добавила уязвимость в каталог KEV как активно эксплуатируемую.Публично доступные эксплойты: отсутствуют в открытом доступе.Уязвимость повышения привилегий в хранилище Windows💥 CVE-2025-21391 (оценка по CVSS – 7,1; высокий уровень опасности)Windows Storage — это стандартный компонент, отвечающий за хранение данных на компьютере. В случае успешной эксплуатации, злоумышленник может получить возможность удалять произвольные файлы. И, согласно исследованию ZDI, злоумышленник может использовать эту возможность для повышения своих прав в системе.Для обеих уязвимостей в Windows сразу были признаки эксплуатации вживую, но насколько масштабными были зафиксированные атаки все еще неизвестно. Однако в любом случае не стоит затягивать с установкой февральских обновлений безопасности Microsoft. Признаки эксплуатации: Microsoft отмечает факты эксплуатации уязвимости. CISA добавила уязвимость в каталог KEV как активно эксплуатируемую.Публично доступные эксплойты: отсутствуют в открытом доступе.Способ устранения описанных уязвимостей: установить обновления безопасности, которые представлены на официальных страницах Microsoft – CVE-2025-21418, CVE-2025-21391Теперь перейдем к уязвимостям сетевых устройств. Уязвимость обхода аутентификации в Palo Alto Networks PAN-OS💥 CVE-2025-0108 (оценка по CVSS – 8,8; высокий уровень опасностиPAN-OS — операционная система, используемая во всех межсетевых экранах нового поколения (NGFW) от Palo Alto Network. Уязвимость позволяет не аутентифицированному злоумышленнику получить доступ к веб-интерфейсу управления PAN-OS. После чего он может «выполнять некоторые PHP-скрипты», что негативно влияет на целостность и конфиденциальность данных в PAN-OS. 😏🔹 Бюллетень вендора вышел 12 февраля. В тот же день Assetnote выложили write-up по уязвимости. А на следующий день на GitHub появился PoC эксплоита.🔹 18 февраля GreyNoise сообщили о зафиксированных попытках эксплуатации уязвимости. По данным Palo Alto, уязвимость эксплуатируют совместно с уязвимостями повышения привилегий CVE-2024-9474 и аутентифицированного чтения файлов CVE-2025-0111. В результате злоумышленник может получить возможность выполнять Linux-овые команды на устройстве от root-а. 😱 А затем использовать это для дальнейшего развития атаки.❗ Устанавливайте обновления и ограничьте доступ к административным веб-интерфейсам! 😉Признаки эксплуатации: в период со 2 по 13 февраля компания GreyNoise обнаружила 25 вредоносных IP-адресов, активно эксплуатирующих уязвимость. Кроме того, CISA добавила уязвимость в каталог KEV. Компания Palo Alto Networks отмечает признаки эксплуатации уязвимости CVE-2025-0108 совместно с CVE-2024-9474 и CVE-2025-0111. Недостаток CVE-2024-9474 связан с повышением привилегий в PAN-OS. О нем мы рассказывали в нашем ноябрьском дайджесте. Обнаруженная в феврале уязвимость CVE-2025-0111 позволяет аутентифицированному злоумышленнику с доступом к сетевым ресурсам считывать файлы, доступные пользователю «nobody». «Тандем» CVE-2025-0108 и CVE-2024-9474 может служить для злоумышленника возможностью произвольного выполнения команд на устройстве с правами root-пользователя. По сообщению Bleeping Computer, цепочка из трех этих уязвимостей может использоваться для выгрузки конфигурационных файлов и другой конфиденциальной информации. Публично доступные эксплойты: в открытом доступе был опубликован PoC. Количество потенциальных жертв: более 2 000 серверов уязвимы к атаке, осуществляемой посредством совместной эксплуатации уязвимостей CVE-2025-0108, CVE-2024-9474, CVE-2025-0111. Способы устранения, компенсирующие меры: установить обновления на уязвимые устройства и следовать рекомендациям вендора.И наконец последняя уязвимость в мартовском дайджесте.Уязвимость удаленного выполнения кода в почтовом сервере CommuniGate Pro💥 BDU:2025-01331 (оценка по CVSS — 9,8; критический уровень опасности)CommuniGate Pro — это универсальная серверная платформа для организации корпоративных коммуникаций. Она объединяет в себе множество сервисов, включая электронную почту, передачу голосовых данных, обмен сообщениями и автоматизацию совместной работы. Так как это отечественное решение, его часто используют для импортозамещения Microsoft Exchange.Уязвимость позволяет удаленному не аутентифицированному злоумышленнику выполнить произвольный код на уязвимом сервере CommuniGate Pro, что может привести к несанкционированному доступу, краже данных или захвату системы. Причина уязвимости —  Stack-based Buffer Overflow. Уязвимы версии с 6.3.0 до 6.3.39.👾 По данным компании CyberOK, есть признаки эксплуатации уязвимости в реальных атаках. По состоянию на 14 февраля, около 40% всех отслеживаемых хостов CommuniGate Pro были подвержены уязвимости.Если у вас используется CommuniGate Pro, обновитесь до безопасной версии.⚡️ Это первая трендовая уязвимость в отечественном продукте с 2023 года.Публично доступные эксплойты: отсутствуют в открытом доступе.Количество потенциальных жертв: по некоторым данным, в России доступно более двух тысяч почтовых серверов c ПО CommuniGate. По сообщению компании CyberOK, 40% всех отслеживаемых почтовых серверов подвержены этой уязвимости.Способы устранения, компенсирующие меры: установить обновленную версию ПО и следовать рекомендациям вендора.🍀 Как защититьсяИспользование популярных продуктов, содержащих трендовые уязвимости, может поставить под угрозу любую компанию. Такие недостатки безопасности являются наиболее опасными и требуют немедленного исправления. В систему управления уязвимостями MaxPatrol VM информация о подобных угрозах поступает в течение 12 часов с момента их появления, что позволяет вовремя принять меры по устранению наиболее опасных из них и защитить инфраструктуру компании. Но не стоит забывать и об исправлении других уязвимостей, которые также могут нанести непоправимый вред организации.VM-ная загадкаЕсть 2 подразделения. В одном Linux-админы на золотые образы и харденинг заточенные, в другом ИТ-шники исключительно работой приклада озабоченные (например, PostgreSQL).Кому из них регулярный патчинг Linux-хостов (ядро и пакеты) с этим развернутым прикладом поручишь?По моему мнению, обновлением хостов должны заниматься те же сотрудники, что обслуживают приклад на этих хостах. Иначе они будут постоянно вешать проблемы с прикладом на команду, которая делала обновление ОС: «вы опять нам все сломали».  Пусть лучше сами полностью обновляются и тестируются 😇  А если прикладники брать это на себя не захотят, можно намекнуть, что Linux-админы будут тогда катать обновления ОС без всякого тестирования. А за сбои крайними будут прикладники, т.к. сами на такую схему подписались 😉На этом можно возразить: но ведь админам же удобнее будет делать только свою часть работы. Давайте просто сделаем так, чтобы ничего не ломалось. Пусть хосты обновляют линуксовые админы, а «прикладные» — только тот приклад, который в их зоне ответственности. Пакеты, относящиеся к прикладу, пусть ставятся на hold сразу при установке приклада и их обновление будет невозможно при обновлении хоста. А в случае, если обновление хоста все же будет ломать приклад, то будем просто откладывать обновления до тех пор, пока приклад не сможет это поддержать. Классно же? Да, такая схема удобна админам, но вот VM-специалисту такая совместная работа и размытие ответственности невыгодны, т.к. ему придется частенько «тыкать палочкой» две (а возможно и более) команды, которые будут показывать друг на друга. Критичные уязвимости не будут устраняться с формулировкой «обновление отложено». А кто будет нести ответственность в случае, если уязвимость будет проэксплуатирована в течение этого «отложенного» периода, непонятно. Видимо предполагается, что нести ответственность будет сам VM-специалист 😏 Так что VM-cпециалистам топить за такую схему я бы не советовал.Но если в организации такая схема устоялась и успешно работает, то рушить ее, естественно, тоже не следует. Следует аккуратненько описывать регламенты решения спорных вопросов, чтобы всегда был однозначный крайний, на которого будет направлен пушинг. Ну и кары в случае инцидента.В любом случае, главное, чтобы обновления ставились и уязвимости устранялись (желательно с соблюдением SLA). А как именно это происходит и чьими силами не так уж важно 😉Александр ЛеоновВедущий эксперт PT Expert Security Center  "
26,26,.NET Digest #6,PVS-Studio,"Статический анализ кода для C, C++, C# и Java",329.24,"Программное обеспечение, Информационная безопасность",2025-03-13,"Добро пожаловать на шестой выпуск нашего дайджеста, посвящённого новостям и событиям в мире .NET! В этот раз набралось множество нового материала, так ещё и вышла предварительная версия .NET 10. Команда PVS-Studio рада представить вам подборку самых интересных и полезных материалов. Поехали!Мы стараемся совершенствоваться, так что будем рады вашим комментариям и предложениям :)Не стесняйтесь присылать нам интересные материалы с помощью формы обратной связи!Сегодня в новогоднем дайджесте: .NET 10 preview 1 и множество интересных статей.Главные новости.NET 10 Preview 1 is now available!25 февраля вышел .NET 10 Preview 1. Вместе с изменениями в .NET Runtime, SDK и библиотеках новая версия демонстрирует нам и первую версию C# 14. Вот список некоторых новшеств и улучшений:ключевое слово field для свойств;неявные преобразования для Span и ReadOnlySpan;модификаторы параметров (ref, in, out) для лямбд без указания типа параметра;Также новая версия добавляет поддержку аллокации массивов значимых типов на стеке при условии, что массив гарантировано не переживёт свой родительский метод. Разработчики обещают ещё больше похожих оптимизаций в следующих предварительных версиях.Статьи.NET 9 Networking ImprovementsРелиз .NET 9 привнёс множество улучшения для тех, кто работает с сетевыми технологиями. Эта статья расскажет про новые API, увеличение производительности, безопасность и многое другое.C# Dev Kit Updates: .NET Aspire, Hot Reload, and More!C# Dev Kit активно развивается, и в этом материале автор рассказывает про новые возможности, которые были добавлены в последних обновлениях. Например, теперь доступны:работа без решения (.sln);любое решение можно сделать решением .NET Aspire;поддержка Hot Reload для Razor/Blazor;общие улучшения отладки и тестирования.New Features for Enhanced Razor Productivity!Отличное обновление для разработчиков, которые создают веб-приложения с помощью Razor. Была добавлена новая функция ""Extract to Component"", которая доступна с Visual Studio 17.2. Но, самое главное, обновление получил C# токенизатор / лексер, что значительно улучшило обработку C# кода. Пользователи сообщали о том, что не могут использовать многие возможности языка в Razor файлах. Новый лексер, основанный на Roslyn, исправляет эти проблемы..NET MAUI Performance Features in .NET 9И ещё одна статья про изменения после выхода .NET 9, только теперь в контексте .NET MAUI. В .NET 9 уделили внимание оптимизации и поддержке NativeAOT. Всё это поможет разработчикам уменьшить размер приложений, сократить время запуска и сделать их работу стабильной на всех платформах.Object Pools in C#: Examples, Internals and Performance BenchmarksИнтересная статья, которая рассказывает про концепцию пулов в C#. Автор проходит по двум встроенным реализациям пулов ObjectPool и ArrayPool и рассказывает про возможности использования. А ещё проводит сравнение производительности без пулов и с ними.Func vs. Predicate vs. Expression in C# .NetТеоретическая статья с базовой информацией про виды делегатов в C#. Отлично подойдёт новичкам. Автор рассказывает, что такое Func, Predicate, Expression и Action, и в каких случаях нужно использовать.Creating an analyzer to detect infinite loops caused by ThreadAbortExceptionsЭндрю Лок описывает сценарий бесконечного цикла, который может произойти в .NET Framework при выбрасывании исключения ThreadAbortException. А также рассказывает о том, как создать свой собственный Roslyn анализатор для автоматического обнаружения этой проблемы.Ошибки и подозрительные места в исходниках .NET 9По традиции каждый год мы разбираем новый релиз .NET: ищем ошибки и подозрительные места в исходном коде. Этот раз не стал исключением, так что я предлагаю вам окунуться в таинственный мир багов.В Microsoft тоже ошибаются. Проверяем MSBuildА ещё мы проверили MSBuild, который используют, по сути, все C# разработчики. Внутри вас ждут: ошибки сopy-paste, NullReferenceException, ошибки со строкой формата и многое другое.Our Favorite NEW Visual Studio Features of 2024Команда Visual Studio собрала все главные функции, которые появились в продукте в 2024 году. Все они повышают производительность, оптимизируют вашу работу и улучшают процесс написания кода. Среди главных изменений: новый менеджер расширений, Drag/Drop между несколькими экземплярами Visual Studio, Code Search, интеграция с .NET Aspire и многое другое.New Debugging and Profiling Features in Visual Studio (v17.13)В обновлении Visual Studio 17.13 появилось много новых функций для отладки кода и профилирования приложения. Во многом изменения опираются на AI. Появились новые функции отладки на базе GitHub Copilot. Теперь сводку по стекам параллельных потоков может дать Copilot. Для более выборочного анализа профилирование теперь можно осуществлять для отдельных функций и классов. Но и это не всё: об остальном можете прочитать в статье.What's New in .NET 10: Everything You Need to KnowВ этой статье рассматриваются новые функции и изменения, которые появились выходом .NET 10 Preview 1. Автор проходит не только по C# 14, .NET Runtime, JIT и .NET SDK, но и освещает изменения в библиотеках, ASP.NET Core, Blazor, .NET MAUI, EF Core и прочее.НовостиWhen will .NET Framework retire?Все мы когда-то задавались вопросом: ""Сколько ещё будет поддерживаться .NET Framework?"" Кажется, ответа нет даже в Microsoft, но у автора этой заметки есть небольшое предположение. Дело в том, что все версии выше .NET Framework 4.6.2 не имеют даты прекращения поддержки.Но мы знаем, что .NET Framework поставляется вместе с Windows. К примеру, Windows Server 2025 содержит .NET Framework 4.8.1. Срок поддержки Windows Server 2025 заканчивается в 2034 году, так что .NET Framework с нами ещё надолго.Новые версииVisual Studio 2022 v17.13 is Now Available!Microsoft представили новую версию Visual Studio 2022. В версии 17.13 была улучшена производительность, добавлены новые возможности GitHub Copilot (который теперь можно использовать бесплатно), а также новые функции для отладки и профилирования. Все остальные подробности можно узнать в статье.PVS-Studio 7.35: поддержка MISRA C 2023, плагин для Qt Creator 15 и многое другоеВышел новый релиз PVS-Studio — 7.35. В этом релизе мы оптимизировали работу анализатора на больших фрагментах кода. Ранее в таких случаях были возможны замедления. В этом релизе для C# анализатора мы сконцентрировались на диагностиках для Unity разработчиков. Конечно, только ими дело не обошлось. Вот полный список новых диагностик:V3211. Unity Engine. The operators '?.', '??' and '??=' do not correctly handle destroyed objects derived from 'UnityEngine.Object'.V3212. Unity Engine. Pattern matching does not correctly handle destroyed objects derived from 'UnityEngine.Object'.V3213. Unity Engine. The 'GetComponent' method must be instantiated with a type that inherits from 'UnityEngine.Component'.V3214. Unity Engine. Using Unity API in the background thread may result in an error.V3215. Unity Engine. Passing a method name as a string literal into the 'StartCoroutine' is unreliable.V3216. Unity Engine. Checking a field for null may not work correctly due to implicit field initialization by the engine.V3217. Possible overflow as a result of an arithmetic operation.V4008. Unity Engine. Avoid using memory allocation Physics APIs in performance-sensitive context.Больше подробностей в пресс-релизе.А запросить пробный ключ и попробовать последнюю версию анализатора можно с помощью этой страницы.Спасибо за прочтение, и ждём вас снова! Не стесняйтесь присылать нам свои новости!Если хотите поделиться этой статьей с англоязычной аудиторией, то прошу использовать ссылку на перевод: Artem Rovenskii. .NET Digest #6."
27,27,Лучшие SATA SSD-накопители 2025: полное сравнение моделей и характеристик,Группа компаний X-Com,X-Com — крупный российский ИТ-холдинг,168.47,"Программное обеспечение, Аппаратное обеспечение, Консалтинг и поддержка",2025-03-13,"Выбирайте SSD с умомНесмотря на активное развитие интерфейса NVMe и появление SSD PCIe Gen 5, твердотельные накопители 2,5” по-прежнему остаются востребованным решением для широкого спектра задач. Ограничение пропускной способности интерфейса SATA III компенсируется высокой совместимостью, стабильностью работы и невысокой стоимостью хранения данных в пересчете на гигабайт. Поэтому не стоит списывать их со счетов. Особенно, когда рабочие сценарии не предполагают сверхвысоких скоростей чтения и записи  SSD.Как выбрать SSD-накопитель: память, контроллеры и долговечностьАрхитектура современных твердотельных накопителей представляет собой сложную систему из различных компонентов. Ключевым элементом в этой конструкции является контроллер. От его возможностей во многом зависит не только скорость SSD, но и его долговечность. На нее влияет способность контроллера эффективно распределять нагрузку, а также обеспечивать выравнивание износа и корректировать ошибки. Современные контроллеры SSD, такие как Silicon Motion SM2258XT или Phison S11 и новее, поддерживают и другие алгоритмы, позволяющие продлить срок службы накопителя. Это и сбор мусора, и TRIM. В совокупности все эти технологии помогают поддерживать стабильную скорость работы SSD даже по прошествии длительного времени. Выбор типа памяти тоже существенно влияет на характеристики накопителя. Память типа TLC (Triple Level Cell) де-факто уже стала стандартом, позволяя хранить до трех бит информации в одной ячейке при приемлемом ресурсе перезаписи. Несмотря на то, что все до сих пор продолжают вздыхать по  SLC-памяти, технология 3D NAND позволила увеличить плотность хранения без ущерба для надежности. Поэтому современные многослойные решения гарантированно обеспечивают от 1000 циклов перезаписи при сохранении данных до 10 лет. Система кэширования SSD-накопителей: DRAM, SLC и HBMСовременные SSD используют многоуровневую систему кэширования. Первый уровень – это DRAM-кэш, быстрая оперативная память, встроенная прямо в накопитель. Её основная задача заключается в том, чтобы хранить таблицу размещения файлов, чтобы контроллер мгновенно находил нужные данные на диске, не теряя производительности. Объем DRAM обычно составляет 1 ГБ на каждый терабайт емкости накопителя.Второй уровень – это SLC-кэш, который выделяется из основной памяти накопителя. В этой области данные временно записываются в режиме один бит на ячейку. Размер такого кэша больше, чем DRAM, и может достигать 40-80 ГБ в терабайтных моделях. Поэтому даже при записи больших объемов данных скорость SSD-диска остается максимально высокой. А, когда SLC-кэш заполняется, данные постепенно переносятся в основную память TLC или QLC, где в каждой ячейке хранится три или четыре бита соответственно.В более доступных моделях без DRAM используется технология Host Memory Buffer. Она позволяет задействовать небольшую часть оперативной памяти компьютера для тех же целей, что и встроенный DRAM. Однако такое решение работает медленнее из-за задержек при передаче данных через интерфейс SATA, чья фактическая пропускная способность не превышает 560 МБ/с.Производительность и надежность SATA SSD-накопителейЗаявленные производителями скорости чтения и записи SSD – это лишь один из показателей производительности накопителя. Не менее важна скорость работы с маленькими файлами, ведь именно с ними чаще всего имеет дело операционная система. Такую производительность измеряют в IOPS – количестве операций ввода-вывода в секунду. Современные SATA SSD способны обрабатывать до 98 000 операций чтения и 90 000 операций записи в секунду. Но такие цифры характерны для сценариев с высокой нагрузкой, когда система отправляет много запросов одновременно. В реальности же количество одновременных запросов редко превышает 1-4, и здесь разница между накопителями становится заметнее: более дорогие модели могут работать в полтора-два раза быстрее бюджетных решений.Надежность SSD, как и скорость, тоже зависит от нескольких факторов:Прежде всего это TBW. Он говорит о том, сколько терабайт данных можно записать на накопитель за весь срок службы. Для SSD объемом 500 ГБ нормальным считается ресурс 300-500 ТБ записи – это значит, что за время работы на диск можно записать объем данных, в 600-1000 раз превышающий его емкость. При ежедневной записи 30 ГБ такой накопитель проработает больше 27 лет. Как-то слишком много, не находите?Поэтому многие предпочитают ориентироваться на другой показатель – MTBF (Mean Time Between Failures). Он обозначает среднее время работы накопителя на отказ. У современных SSD оно составляет от 1,5 до 2,5 миллионов часов. Это теоретический расчет, основанный на статистике отказов большого количества накопителей. На практике это означает, что при партии в 1000 SSD в течение первого года работы выйдут из строя всего 3-4 накопителя. Конечно, это при условии соблюдения температурного режима и отсутствии внешних повреждений.Впрочем, есть и другие важные показатели надежности. Например, алгоритмы LDPC позволяют восстанавливать данные даже при повреждении значительного количества ячеек памяти. А еще производители оставляют часть памяти в резерве (over-provisioning), чтобы заменять ею отказавшие ячейки.Энергопотребление SATA SSD тоже влияет на их долговечность. В норме оно находится на уровне 2,5-3,5 Вт при активной работе. В режиме ожидания оно падает до 50 милливатт, а некоторые модели с поддержкой технологии DevSleep могут потреблять всего несколько микроватт. Это особенно важно не столько для экономии энергии, сколько для срока службы накопителя. Ведь чем меньше тепла выделяется при работе, тем медленнее деградируют ячейки памяти.Особенно влияние температуры на работу накопителя проявляется при интенсивной работе. Чтобы не допустить перегрева, производители оснащают SSD инструментами контроля температуры. Когда она повышается, скорость чтения и записи падает. Это не просто защита от перегрева – высокая температура ускоряет износ ячеек памяти и может привести к потере данных. Поэтому в современных моделях используются датчики температуры и специальные алгоритмы, которые следят за нагревом. Они перераспределяют нагрузку так, чтобы избежать опасных температурных режимов, сохраняя при этом приемлемую скорость работы.Производители SSD-накопителей и их технологические особенностиРынок SATA SSD отличается сильной зависимостью от вертикальной интеграции. Поэтому в данной сфере ключевое преимущество получают компании с собственным производством памяти и контроллеров.Samsung, будучи лидером в производстве V-NAND, использует собственные контроллеры серии MJX, и это позволяет ей оптимизировать взаимодействие всех компонентов накопителя. Во многом за счет этого и появляются фирменные технологии-улучшители типа TurboWrite, которая динамически выделяет до 78 ГБ SLC-кэша в SSD от Samsung объемом 1 ТБ.Western Digital после поглощения SanDisk получила доступ к технологиям BiCS FLASH, что позволило компании контролировать полный цикл производства. Их фирменные контроллеры оптимизированы для работы с 176-слойной памятью, а технология nCache 3.0 использует многоуровневое кэширование для балансировки производительности и износа ячеек.Crucial, входящая в состав Micron Technology, применяет память собственного производства с технологией Replacement Gate. В отличие от классических решений с плавающим затвором, здесь используется более современная структура ячеек с улучшенным изолирующим слоем. Это увеличивает надежность хранения данных и уменьшает влияние соседних ячеек друг на друга. В сочетании с контроллерами Silicon Motion такое решение позволяет достигать высокой производительности при доступной цене.Transcend, не имея собственного производства памяти, делает ставку на проверенные комбинации компонентов от ведущих производителей. Компания использует контроллеры Silicon Motion в паре с качественной TLC-памятью, дополняя их буфером DDR3 для стабильной производительности при длительных операциях. Такой подход позволяет предлагать надежные решения в среднем ценовом сегменте.Kingston также работает с готовыми компонентами, но выделяется тщательным подходом к их отбору и тестированию. Компания активно применяет технологии оптимизации производительности, включая динамическое кэширование и продвинутые алгоритмы wear-leveling, обеспечивая длительный срок службы своим накопителям даже при интенсивном использовании.На что еще обратить внимание при выборе SATA SSD-накопителяПеред тем как купить SATA SSD, стоит обратить внимание не только на основные, но и второстепенные параметры:Во-первых, это возможности его прошивки и драйверов. Современные накопители работают с протоколом NVMe-MI over SATA, который следит за состоянием диска и температурой его компонентов. В более дорогих моделях также можно настроить размер резервной области памяти. Ведь чем больше зарезервировано памяти, тем выше скорость работы и надежность накопителя.Во-вторых, реализация защиты данных при внезапном отключении питания. Некоторые модели используют конденсаторы для завершения записи данных из кэша, другие полагаются на алгоритмические решения с журналированием транзакций. При этом важна не только сама технология защиты, но и её влияние на производительность: некоторые реализации могут увеличивать латентность операций записи.В-третьих, – и это имеет наибольшее значение для корпоративного сектора – поддержка технологий шифрования. Помимо стандартного AES-256, продвинутые модели поддерживают протоколы TCG Opal 2.0 и IEEE-1667. Благодаря им реализуются многопользовательские политики безопасности и удаленное управление доступом.В-четвертых, поддержка команды TRIM. Эта функция позволяет операционной системе сообщать SSD, какие блоки данных больше не используются и могут быть очищены. Без TRIM производительность накопителя со временем падает, так как ему приходится перезаписывать уже ненужные данные. Современные SSD поддерживают как обычный TRIM по команде системы, так и фоновый режим, когда очистка происходит в моменты простоя.В-пятых, реализация SMART-атрибутов для мониторинга состояния. Эта технология отслеживает множество параметров: от количества записанных данных до температуры контроллера. Разные производители могут использовать собственные идентификаторы для важных параметров, поэтому при выборе накопителя стоит убедиться, что его SMART-атрибуты корректно считываются вашими утилитами мониторинга.Лучшие накопители SATA SSD в 2025 годуSamsung 870 EVOУ накопителей Samsung много фирменных технологий, повышающих эффективность их работыSamsung MZ-77E500BW достигает максимально возможных для SATA III скоростей благодаря сочетанию продвинутого контроллера MJX и 128-слойной памяти V-NAND TLC. Тягаться с решениями PCI-e 5 ему, конечно, не по силам, но скорость чтения в 560 МБ/с и записи в 530 МБ/с и так является фактическим потолком для интерфейса SATA. При этом высокая производительность сохраняется не только при работе с большими файлами. Объемный кэш и продвинутый контроллер позволяют накопителю сохранять такую же скорость при установке программ и работе с документами, где постоянно идет обработка множества небольших файлов.Технические характеристики:- Контроллер: Samsung MJX- Тип памяти: 128-слойная V-NAND TLC- Объемы: 250 ГБ, 500 ГБ, 1 ТБ, 2 ТБ, 4 ТБ- Скорость последовательного чтения/записи: 560/530 МБ/с- Случайное чтение/запись (QD32): 98K/88K IOPS- Случайное чтение/запись (QD1): 15K/40K IOPS- DRAM-кэш: 512 МБ (500 ГБ), 1 ГБ (1 ТБ)- Ресурс TBW: 300 ТБ (500 ГБ), 600 ТБ (1 ТБ)- Энергопотребление: 2.5 Вт (активное), 0.05 Вт (простой)SLC-кэш в 870 EVO работает по динамическому принципу, меняя свой размер в зависимости от заполненности диска. Если свободного места больше 60%, под кэш отводится до 48 ГБ в версии на 500 ГБ. Это позволяет накопителю поддерживать максимальную скорость записи даже при копировании больших файлов. А система термального контроля Dynamic Thermal Guard эффективно справляется с нагревом. Благодаря этому температура контроллера редко поднимается выше 65°C даже при длительной нагрузке.Western Digital REDНакопители для NAS имеют более высокую надежностьWestern Digital WDS500G1R0A – накопитель, созданный специально для работы в NAS-системах. В его основе лежит память 3D NAND, оптимизированная для непрерывной работы в режиме 24/7. При проектировании SSD производитель уделил особое внимание его надежности. Поэтому ресурс записи накопителя составляет 350 ТБ, а заявленное время наработки на отказ достигает 2 миллионов часов.Технические характеристики:- Тип памяти: 3D NAND- Объем: 500 ГБ- Скорость последовательного чтения/записи: 560/530 МБ/с- Случайное чтение/запись (QD32): 95K/85K IOPS- Ресурс TBW: 350 ТБ- DEVSLP: 5-7 мВт- Рабочая температура: 0°C – 70°Накопитель отлично подходит для кэширования часто используемых файлов в NAS-системах. Его архитектура оптимизирована под интенсивные нагрузки. Он отлично показывает себя в работе с базами данных OLTP, системами с многопользовательским доступом и при обработке тяжелых медиафайлов вроде 4K- и 8K-видео.Kingston A400Хороший накопитель и без DRAM-буфера будет работать безотказноKingston A400 использует простую связку контроллера Phison S11 (но встречаются и другие, например, SM2259XT и CS5341AA) и памяти TLC без DRAM-буфера. Это положительно отразилось на цене накопителя: стоит он примерно на 20% дешевле, чем аналоги. При этом скорости чтения 500 МБ/с и записи 450 МБ/с более чем достаточно для повседневной работы, запуска игр и хранения данных. А значит, накопитель подойдет тем, кто хочет получить основные преимущества SSD по доступной цене и не готов переплачивать за второстепенные функции, которыми, возможно, и пользоваться-то не будет.Технические характеристики:- Контроллер: Phison S11- Тип памяти: 96-слойная TLC- Объемы: 240 ГБ, 480 ГБ, 960 ГБ- Скорость последовательного чтения/записи: 500/450 МБ/с- Случайное чтение/запись (QD32): 90K/80K IOPS- Случайное чтение/запись (QD1): 10K/30K IOPS- DRAM-кэш: отсутствует, используется HMB- Ресурс TBW: 200 ТБ (480 ГБ)- Энергопотребление: 2.05 Вт (активное), 0.05 Вт (простой)Вместо DRAM-кэша накопитель использует технологию HMB, выделяя под кэш часть оперативной памяти компьютера. В обычных задачах это практически не заметно – разница с более дорогими моделями проявляется только при длительной записи больших файлов или сильной фрагментации диска. Поэтому A400 хорошо подходит для замены старого жесткого диска или в качестве дополнительного накопителя для хранения игр и программ.Crucial MX500В основе этого накопителя лежит проверенная связка компонентовВ основе Crucial MX500 лежит проверенная связка контроллера Silicon Motion SM2258 и фирменной памяти Micron с технологией замены плавающего затвора. Это единственный накопитель в нашем обзоре с аппаратным шифрованием AES-256 и полноценной защитой от внезапной потери питания. Так что он будет особенно полезен для корпоративной среды и сценариев, предполагающих работу с важными данными. Технические характеристики:- Контроллер: Silicon Motion SM2258- Тип памяти: 3D TLC NAND- Скорость последовательного чтения/записи: 560/510 МБ/с- Случайное чтение/запись (QD32): 95K/90K IOPS- DRAM-кэш: 512 МБ (500 ГБ), 1 ГБ (1 ТБ)- Ресурс TBW: 180 ТБ (500 ГБ)- Энергопотребление: 2.8 Вт (активное), 0.05 Вт (простой)MX500 получил полноценный DRAM-буфер и адаптивную систему кэширования, подстраивающуюся под текущие задачи. Для защиты в накопитель встроены конденсаторы, которые успевают сохранить всю информацию даже при внезапном отключении питания. Такая надежная система хранения данных дополняется высокой скоростью работы в повседневных задачах, а встроенное аппаратное шифрование делает накопитель отличным выбором для хранения важных файлов.Samsung 860 QVOЕще одна достойная модель от SamsungSamsung 860 QVO использует более доступную память QLC в сочетании с фирменным контроллером MJX. Несмотря на это, в обычных задачах накопитель показывает скорости на уровне моделей с TLC-памятью. Зато такой подход позволил снизить стоимость в пересчете на гигабайт, сохранив при этом фирменные технологии Samsung. В частности, здесь применяется механизм TurboWrite для динамического SLC-кэширования и интеллектуальный алгоритм ECC с тройной проверкой данных, компенсирующий недостатки четырех-ячеечной памяти.Технические характеристики:- Контроллер: Samsung MJX- Тип памяти: QLC V-NAND- Объемы: 1 ТБ, 2 ТБ, 4 ТБ- Скорость последовательного чтения/записи: 550/520 МБ/с- Случайное чтение/запись (QD32): 90K/80K IOPS- Случайное чтение/запись (QD1): 13K/32K IOPS- DRAM-кэш: 1 ГБ (1 ТБ)- Ресурс TBW: 360 ТБ (1 ТБ)- Энергопотребление: 2.3 Вт (активное), 0.05 Вт (простой)Одним из преимуществ этого накопителя является объем SLC-кэша – до 78 ГБ в терабайтной версии, – который позволяет поддерживать высокую скорость записи при копировании больших файлов. Однако при заполнении диска более чем на 75% или исчерпании буфера скорость может падать до 80-100 МБ/с. Этот накопитель лучше всего подходит для хранения данных, когда нужен большой объем по разумной цене.ADATA Ultimate SU650Хороший накопитель за небольшие деньгиSU650 построен на базе контроллера от SM и трехмерной памяти TLC без DRAM-буфера. Это самый доступный вариант для тех, кто хочет перейти с обычного жесткого диска на SSD. Скорости чтения 520 МБ/с и записи 450 МБ/с заметно превосходят показатели любых механических накопителей, а стабильная работа контроллера обеспечивает хороший отклик системы.Технические характеристики:- Контроллер: SM2258- Тип памяти: 3D NAND TLC- Объемы: 240 ГБ, 480 ГБ, 960 ГБ- Скорость последовательного чтения/записи: 520/450 МБ/с- Случайное чтение/запись (QD32): 40K/75K IOPS- DRAM-кэш: отсутствует- Ресурс MTBF: 2M (480 ГБ)Отсутствие DRAM-буфера частично компенсируется небольшим SLC-кэшем. В повседневных задачах, таких как загрузка системы и запуск программ, разница с более дорогими моделями практически незаметна. Падение производительности наблюдается только при длительной записи данных или сильной фрагментации диска, но для большинства домашних пользователей это не критично.Transcend SSD230SЭтот накопитель доступен в версиях с разным объемомTranscend SSD230S – еще один представитель накопителей из доступного сегмента рынка. Несмотря на невысокую цену, он сочетает в себе контроллер Silicon Motion SM2258 с трехбитной TLC-памятью и буфером DDR3. Такая комбинация дает хорошую скорость в повседневных задачах, а поддержка NCQ позволяет эффективно обрабатывать множество мелких файлов одновременно. Технические характеристики:- Контроллер: Silicon Motion SM2258- Тип памяти: 3D TLC NAND- Объемы: 512 ГБ, 1 ТБ, 2 ТБ- Скорость последовательного чтения/записи: 560/500 МБ/с- Случайное чтение/запись (QD32): 80K/85K IOPS- DRAM-кэш: DDR3 512 МБ (512 ГБ), 1 ГБ (1 ТБ)- Ресурс TBW: 200 ТБ (500 ГБ)- Энергопотребление: 2.2 Вт (активное), 0.05 Вт (простой)Быстрый буфер DDR3 в связке с контроллером SM2258 позволяет накопителю держать высокую скорость даже при длительной записи данных. Благодаря NCQ диск отлично справляется с фрагментированными файлами и множеством одновременных операций. А для защиты данных здесь используется продвинутая система коррекции ошибок LDPC ECC четвертого поколения.Patriot P220SОчень важно выбирать SSD, исходя из своих задачP220S также использует распространенную комбинацию контроллера SM2258XT и памяти TLC без DRAM-буфера. Несмотря на это, за счет оптимизированной прошивки накопитель показывает неплохие скорости в повседневных задачах, особенно при чтении данных. Поэтому его можно смело назвать хорошим выбором для системного диска в домашнем или офисном компьютере.Технические характеристики:- Контроллер: Silicon Motion SM2258XT- Тип памяти: 3D NAND TLC- Объемы: 256 ГБ, 512 ГБ, 1 ТБ- Скорость последовательного чтения/записи: 550/500 МБ/с- Случайное чтение/запись (QD32): 40K/50K IOPS- DRAM-кэш: отсутствует, HMB- Ресурс TBW: 200 ТБ (512 ГБ)Как и другие бездрамовые накопители, P220S полагается на технологию HMB и небольшой SLC-кэш. В обычных условиях этого достаточно для комфортной работы системы и программ. При интенсивной записи данных скорость может снижаться, но благодаря грамотной оптимизации контроллера падение производительности менее заметно, чем у многих других моделей без DRAM-буфера."
28,28,Запускаем приложения в облаке без Kubernetes и настройки виртуальных машин: лабораторная,Cloud.ru,Провайдер облачных сервисов и AI-технологий,97.21,"Программное обеспечение, Веб-сервисы, Информационная безопасность",2025-03-13,"Многие из вас прекрасно знают, насколько удобные и полезные инструменты виртуализация и Managed Kubernetes. А также понимают — чтобы ими управлять, нужен DevOps или как минимум тот, кто сможет настроить балансировщик. Но давайте на чистоту: когда приложение уже готово, заниматься инфраструктурными настройками не сильно хочется. Хочется быстро развернуть его и сразу показать коллегам, друзьям и другим пользователям интернета. Я Никита Бутримов — лидер направления в Cloud.ru, и сегодня я покажу, как быстро запускать frontend- и backend-приложения в облаке. А помогут нам в этом два сервиса — Evolution Container Apps и Evolution Artifact Registry.Evolution Container Apps + Evolution Artifact Registry: как это работаетКак подготовить средуКак развернуть frontend-приложение в облаке Как развернуть backend-приложение на Python, Go, Java Script или C# в облакеEvolution Container Apps + Evolution Artifact Registry: как это работаетДля начала расскажу про сервисы. Evolution Container Apps — это сервис для создания и запуска контейнерных приложений. Контейнеры запускаются из Docker-образов, которые в свою очередь хранятся в приватном защищенном репозитории Evolution Artifact Registry. Логика работы простая:Вы загружаете Docker-образ приложения в Evolution Artifact Registry.Затем создаете контейнер из загруженного образа в Evolution Container Apps.Приложение запускается в контейнере и становится доступно всем пользователям интернета.Схема развертывания приложения с помощью сервисов Evolution Container Apps и Evolution Artifact RegistryКстати, сервисы Evolution Container Apps и Evolution Artifact Registry проинтегрированы с другими сервисами нашей облачной платформы и позволяют динамически масштабироваться. Даже если у вас стартап и пока нет пользователей, когда они появятся, вы сможете быстро отмасштабировать приложение в зависимости от нагрузки — как горизонтально, так и вертикально. Теперь перейдем к главному — запустим приложение в облаке на примере простого frontend- и backend-приложения. Для этого сначала подготовим среду.Как подготовить средуПеред началом работы:Зарегистрируйтесь в личном кабинете Cloud.ru или войдите, если уже зарегистрированы.Подключите сервисы Evolution Container Apps и Evolution Artifact Registry.Установите Docker Desktop.Установите Docker CLI (или используйте любой другой терминал на вашем компьютере).Готово. Теперь перейдем к размещению приложения.Как развернуть frontend-приложение в облаке Давайте посмотрим, как загрузить Docker-образ с готовым приложением в реестр, создать контейнер и опубликовать приложение в интернете.ОпциональноВы можете зарегистрироваться в GitVerse, познакомиться с новой системой контроля версий, а также клонировать оттуда репозиторий кода. Для этого:Перейдите в нужную директорию на локальном компьютере.Выполните команду в терминале GitBash: git clone https://gitverse.ru/cloudru/evo-containerapp-react-sample 1. Создайте реестр в Evolution Artifact RegistryВ личном кабинете перейдите в карточку сервиса Evolution Artifact Registry:Придумайте и пропишите название реестра — оно станет частью URI, который вы будете использовать при работе в Docker CLI. Затем нажмите Создать:Скопируйте URI реестра — он пригодится на следующих шагах:2. Получите ключи доступа для аутентификацииВ личном кабинете Cloud.ru перейдите в раздел Управление профилем:Выберите раздел Ключи доступа и нажмите Создать ключ:Введите краткое описание ключа, которое в будущем поможет идентифицировать его среди других ключей. Задайте время жизни ключа: от 1 до 365 дней. Затем нажмите Создать.После этого будут сгенерированы Key ID (логин) и Key Secret (пароль). Сохраните Key Secret. Обратите внимание: если вы закроете окно, его нельзя будет открыть повторно.3. Пройдите аутентификацию в реестре Evolution Artifact RegistryОткройте любой удобный терминал и введите команду для аутентификации: docker login <URI_реестра> -u <key_id> -p <key_secret>Где:<URI_реестра> — URI, сгенерированный после создания реестра;<key_id> — логин персонального ключа (Key ID);<key_secret> — пароль персонального ключа (Key Secret).4. Соберите и подготовьте Docker-образНа локальном компьютере соберите готовый Docker-образ из репозитория (в нашем примере из GitVerse) командой:docker build --tag <URI_реестра>/react-hello-world https://gitverse.ru/sc/cloudru/evo-containerapp-react-sample.git#master --platform linux/amd64Команда собирает образ и тегирует его для дальнейшей загрузки в реестр. Обратите внимание: Docker-образ должен быть собран под платформу linux/amd64, поэтому в команде используется флаг platform со значением linux/amd64.5. Загрузите Docker-образ в реестрЗагрузите образ в реестр Evolution Artifact Registry командой:docker push <URI_реестра>/react-hello-worldГде:<URI_реестра> — URI, сгенерированный после создания реестра;react-hello-world — имя будущего репозитория в Evolution Artifact Registry. При этом имя репозитория должно соответствовать имени Docker-образа.Убедитесь, что репозиторий и загруженный образ появились в Evolution Artifact Registry. Для этого перейдите в раздел Реестры → Репозитории → Артефакты сервиса.6. Создайте и запустите контейнерНажмите на троеточие, чтобы открыть меню загруженного образа, и кликните Создать Container App:Заполните поля и активируйте нужные опции:Название контейнера — глобально уникальное имя, на базе которого формируется адрес вашего приложения в домене *.containers.cloud.ru.Порт контейнера — должен совпадать с портом вашего приложения. В примере порт 8080.server {      listen 8080;      root /usr/share/nginx/html;      index index.html;      location / {          try_files $uri $uri/ /index.html;      }  }vCPU/RAM — количество vCPU и RAM, которые выделяются для каждого экземпляра контейнера при обработке вызова. Выберите минимальную конфигурацию.Минимальное и максимальное количество экземпляров при масштабировании сервиса. По умолчанию происходит масштабирование с 0, что может вызывать небольшую задержку при старте вашего приложения. Установите минимальное количество экземпляров 0, а максимальное — 1.Публичный адрес — активируйте опцию, чтобы получить URL-адрес для вызова контейнера из интернета.Затем нажмите Создать:После этого откроется страница сервиса Evolution Container Apps, контейнер запуститься в течение нескольких секунд. Когда контейнер перейдет в статус «Выполняется», значит он успешно запущен:7. Проверьте развернутое приложениеДождитесь, когда сформируется публичный URL, и скопируйте его. Затем вставьте его в адресную строку браузера и проверьте работоспособность приложения.Если приложение не отвечает: в списке ревизий проверьте, что последняя созданная ревизия находится в статусе «Выполняется». Если она в статусе «Публикуется» или «Ошибка», попробуйте создать новую ревизию — нажмите кнопку Создать ревизию на основе выбранной.Готово, вы развернули frontend-приложение в облачном контейнере и опубликовали его в интернет. Теперь посмотрим на дополнительные настройки сервиса Evolution Container Apps на примере backend-приложения. Как развернуть backend-приложение на Python, Go, Java Script или C# в облакеМожно использовать репозиторий ниже — в нем находится исходный код простого REST API приложения, написанного на разных языках (Java Script, Python, Go, C#).git clone https://gitverse.ru/cloudru/evo-containerapp-restapi-js-go-python-dotnet-sample1. Соберите образ и присвойте тегИспользуйте реестр (как его создать, рассказали на шаге 1 инструкции выше), выполнив команду:docker build --tag <registry_name>.cr.cloud.ru/restapi-python https://gitverse.ru/sc/cloudru/evo-containerapp-restapi-js-go-python-dotnet-sample.git#master:restapi-python/src --platform linux/amd642. Загрузите Docker-образ в реестрЗагрузите образ в реестр Evolution Artifact Registry командой:docker push <registry_name>.cr.cloud.ru/restapi-pythonГде:<registry_name> — название реестра, которое вы указывали в Artifact Registry при его создании;restapi-python — название будущего репозитория в Artifact Registry. Название репозитория должно соответствовать имени Docker-образа.3. Создайте и запустите контейнерНажмите на троеточие, чтобы открыть меню загруженного образа, и кликните Создать Container App:Заполните поля и активируйте нужные опции:Название контейнера — глобально уникальное имя, на базе которого формируется адрес вашего приложения в домене *.containers.cloud.ru.Порт контейнера — должен совпадать с портом вашего приложения. В примере порт 8080.vCPU/RAM — количество vCPU и RAM, которые выделяются для каждого экземпляра контейнера при обработке вызова. Выберите минимальную конфигурацию.Минимальное и максимальное количество экземпляров при масштабировании сервиса. По умолчанию происходит масштабирование с 0, что может вызывать небольшую задержку при старте вашего приложения. Установите минимальное количество экземпляров 0, а максимальное — 1.Публичный адрес — активируйте опцию, чтобы получить URL-адрес для вызова контейнера из интернета.Автоматическое развертывание — активируйте опцию, чтобы каждый раз после загрузки в Evolution Artifact Registry новой версии образа в Evolution Container Apps автоматически создавалась новая ревизия контейнера на базе обновленной версии Docker-образа.Затем нажмите Создать:После этого откроется страница сервиса Evolution Container Apps, контейнер запуститься в течение нескольких секунд. Когда контейнер перейдет в статус «Выполняется», значит он успешно запущен:4. Проверьте развернутое приложениеДождитесь, когда сформируется публичный URL, и скопируйте его. Затем вставьте в адресную строку браузера и проверьте работоспособность приложения.Готово, вы развернули backend-приложение в облачном контейнере и опубликовали его в интернет.ЗаключениеВот так за несколько шагов можно развернуть приложение и сделать его доступным для всех пользователей интернета 🙂.Теперь вы умеете:загружать Docker-образ в Evolution Artifact Registry;создавать репозитории в существующих реестрах Evolution Artifact Registry;создавать и запускать контейнер в Evolution Container Apps;управлять настройками масштабирования.В следующей части статьи я планирую показать, как развернуть Jupyter Server. А пока вы можете попробовать и протестировать другие сервисы нашей облачной платформы с помощью бесплатного объема облачных ресурсов Evolution free tier.   Интересное в блоге:Гармония железа и кода: ускоряем Go, проектируя приложение с учетом архитектуры процессора«Я делаю рефакторинг ежечасно» или как за пять минут улучшить приложениеЧто такое free tier и какие облачные ресурсы вы можете бесплатно использовать прямо сейчас"
29,29,Нейросети для остужения маркетинговых задач: выбираем ИИ-помощника,КРОК,Компания,115.11,"Программное обеспечение, Аппаратное обеспечение",2025-03-13,"Привет, Хабр. В этой статье я собрала для вас 3 генеративные нейросети, с помощью которых я сама решаю задачи по маркетингу. Устраивайтесь поудобнее, заваривайте чай, сегодня без технины, только софт-контент о хардовых решениях. Меня зовут Ася Ульянова, я работаю руководителем группы контента в компании КРОК. Пару лет назад мне в руки попались Chat GPT и Midjourney, и с тех пор я не перестаю изучать новые инструменты для повышения качества материалов и ускорения работы команды. Я веду внутреннее обучение в компании: выступаю с лекциями, мастер-классами и выпускаю дополнительные материалы для популяризации ИИ-инструментов. Можно сказать, я AI-евангелист внутри КРОК. Откуда такая вера в ИИ?Нейросети сегодня стали нормальной частью жизни, как ранее стали голосовые помощники, мобильный банкинг и другие, казалось бы, повседневные вещи. Как бы мы не «топили» за классические маркетинговые инструменты, куда бы не «лили» свои воронки, хороший маркетолог знает: новые инструменты, как и новые тренды, новыми быть быстро перестают. Как скоро нейросети превратятся из просто приятной фичи приложения или операционки в смартфоне в  мастхев и норму для нас, как и сам смартфон – вопрос времени. Именно по этой причине я верю, что любой человек, который работает с трендами и в целом планирует двигаться в бизнесе, должен освобождать себя от рутинных задач, изучать новые инструменты и опережать время. С чего начать?По известным причинам самый очевидный помощник из всего многообразия генеративных нейросетей — Chat GPT, — не очень доступен. Это один из самых больших стоп-факторов на пути к освоению нейросетей: нужно заморачиваться с получением доступа, искать протоколы для подключения и преодолевать прочие трудности. Многие энтузиасты начинают искать альтернативы: боты в телеграме, аналогичные инструменты на базе отечественных платформ. И часто разочаровываются: ожидания от невероятных результатов переписки с чат-ботом не оправдываются, задача остается нерешенной, время потрачено зря. В этой статье мы сравним 3 самые популярные в маркетинге нейросети: Chat GPT, DeepSeek и Qwen2.5-Max. Посмотрим на них в целом, а далее разберем на конкретных задачах и сравним ответы на промты, чтобы нагляднее понять, почему в одном случае нужно работать с привычным GPT, а где лучше себя показывает нашумевший DeepSeek или популярный в узких маркетинговых кругах Qwen2.5-Max.Chat GPT для маркетингаСамый продвинутый инструмент по работе с текстом. Этой статьи бы не было, если бы китайские нейросети не дышали ему в спину и не наступали на пятки. А если учесть, что для запуска GPT нужно сделать много дополнительных действий, а при необходимости загружать его большим объемом работ нужно ещё и $20 в месяц выложить, становится и вовсе тоскливо. Тем не менее, Chat GPT имеет очевидные сильные стороны для работы с маркетинговыми задачами:самый лучший помощник в написании статей. Формирует понятное содержание, задает уточняющие вопросы, помогает структурировать большой объем информации; сильный поисковик: в поисках фактов, источников и информации в открытом доступе по узкому запросу GPT точно сильнее своих конкурентов; оперативный ассистент в работе с таблицами и файлами. В неструктурированных документах Chat GPT гораздо быстрее и точнее ищет необходимую информацию в отличии от конкурентов; точно больше знает или помогает найти информации по таргетированной рекламе, запуску рекламных кампаний и так далее; точнее вычитывает тексты. Китайские конкуренты очень плохи в этой задаче, GPT справляется лучше. DeepSeek для маркетингаНе зря этот инструмент недавно пробил топы скачиваний в мобильных маркетах. Быстрый, доступный, без «костылей» и работает достаточно оперативно. К плюсам DeepSeek:хороший инструмент для борьбы с синдромом чистого листа. Предлагает менее клишированные и более разнообразные варианты креативов по запросу, помогает посмотреть на задачи с совершенно новой стороны. точно находит информацию и производит расчеты по структурированным файлам; хорошо помогает составлять сметы, в целом неплохо работает с точными задачами и калькуляцией; изобретательнее своего американского брата в написании  постов для соцсетей: почти не использует клише, хорошо суммаризирует информацию из большого полотна текста, но очень плохо ищет в интернете. Лучше сразу давать ему текст статьи, например, а не ссылку на неё, если нужна короткая выжимка. Qwen2.5-Max для маркетингаГлавное преимущество этой нейросети — она что-то вроде пирата без морали и принципов. Сильно меньше ограничений по тематикам запросов, не пускается в лишние рассуждения. Выделенные плюсы Qwen2.5-Max:на мой взгляд, работает с постами для соцсетей гораздо лучше, чем DeepSeek и GPT, хорошо ищет в интернете и не сопротивляется этому процессу, в отличии от DeepSeek; под капотом — большой выбор моделей под разные задачи. Но самая популярная — хорошо справляется с большинством задач маркетинга. Я бы назвала модель сильным конкурентом GPT, и не таким капризным, как DeepSeek; замечательно подходит для поиска информации по исследованиям, фактам и прочей подобной информации; помогает в составлении планов, подробно описывает процессы, проактивничает, например дублирует после длинного ответа краткое содержание ниже. Слова — словами, а теперь — к делуДавайте разберем на примере трех востребованных маркетинговых промтов, как работают все 3 модели. Попросим сгенерировать пост на заданную тему, объяснить принцип работы одной технологии, подобрать релевантные мероприятия в ближайшие 3 месяца по теме. Так вы нагляднее сможете понять, какой инструмент точнее подойдет для вашей работы, так как универсального решения нет и все три инструмента очень по-разному подходят к делу. Я специально сейчас не буду углубляться в промт-инжиниринг и покажу результаты одного запроса, так как часто у маркетологов нет времени на долгую проработку промта. Генерация постаФормируем запрос на генерацию поста на конкретную тему. Задаем пожелания и получается следующий промт: Привет. Давай напишем телеграм-пост о прогнозах использования ИИ в маркетинге — какой процент задач уже можно доверить нейросетям и какие нейросети самые популярные. Пост должен быть интересен бизнес-аудитории. Сделай упор на ИТ-тематику поста, используй нейтральный ToV, пост напиши до 500 знаков.Chat GPT:🔍 ИИ в маркетинге: что уже можно автоматизировать?По прогнозам, в 2025 году до 70% рутинных задач в маркетинге будут выполняться нейросетями: анализ данных, генерация контента, персонализация рекламы. Лидеры рынка — ChatGPT, Midjourney, Jasper, Synthesia. Уже сейчас компании сокращают издержки и ускоряют процессы с помощью ИИ. Вопрос не «использовать или нет?», а «как внедрить быстрее, чем конкуренты?» 🚀#ИИ #маркетинг #автоматизацияИз ответа следует, что восстание машин совсем близко. Пост написан в хорошем ToV, но детали оставляют желать лучшего. DeepSeek:🤖 ИИ в маркетинге: что уже можно доверить нейросетям?Сегодня ИИ берет на себя до 30-40% рутинных задач в маркетинге: от генерации контента до анализа данных и настройки таргетированной рекламы. Популярные инструменты:ChatGPT — тексты, идеи, сценарии.MidJourney и DALL·E — креативы и визуал.Jasper — SEO-оптимизация.Surfer и MarketMuse — аналитика и прогнозы.ИИ уже не просто помощник, а полноценный инструмент для автоматизации. Но ключевые стратегические решения пока за человеком. 💡#ИИ #Маркетинг #Автоматизация #ТехнологииТут уже приведены более реальные данные и даже расписан функционал инструментов. Восстание машин не обещает, уже приятно. Qwen:🤖 ИИ в маркетинге: реальность 2025Сегодня искусственный интеллект берёт на себя до 60% рутинных маркетинговых задач. От анализа данных и сегментации аудитории до создания контента и автоматизации коммуникаций.Самые популярные нейросети в индустрии:• GPT-4 для генерации текстов• DALL·E 3 для создания визуального контента• Adobe Firefly в работе с изображениями• Jasper AI для маркетинговых кампанийЭксперты прогнозируют, что доля автоматизированных процессов вырастет до 75% уже к 2026 году.#ИскусственныйИнтеллект #МаркетингБудущего #DigitalTransformationКак можно заметить, вся троица идет по похожему фрейму, но с отличиями: GPT пишет более сдержанно, но использует клише, DeepSeek привел более реальные данные по процентам и в конце даже обнадежил, а Qwen пообещал и вовсе космические цифры, но чуть человечнее раскрыл список и даже нашел какие-то нереалистичные прогнозы. Я бы отдавала здесь предпочтение варианту от DeepSeek —  он ближе к реальности, чем конкуренты. Подбор мероприятийВ этом и следующем блоке ответы будут гораздо длиннее, поэтому сокращу их до сути. Подбери релевантные мероприятия по ИТ-решениям для промышленных предприятий в Москве в ближайшие 3 месяца. Это могут быть отраслевые конференции, бизнес-мероприятия, выставки и другие события, которые интересны ИТ-специалистам, руководителям предприятий и техническим специалистам промышленных отраслей. Предложи не менее 10 таких мероприятий и дай ссылки на сайты этих мероприятий. Chat GPT:Сразу предупредил, что гуглить в реальном времени не умеет и снял с себя ответственность. Зато отличился от остальных и не просто подобрал мероприятия, но и написал, где искать. Но задачу закрыл не по ТЗ: ссылок на сайты мероприятий он не предоставил. К сожалению, у меня нет возможности искать информацию в интернете в реальном времени. Однако ты можешь самостоятельно найти актуальные мероприятия по следующим источникам:  ### Где искать ИТ-мероприятия для промышленных предприятий в Москве:  1. Официальные сайты отраслевых выставок – Expocentr, Crocus Expo, ВДНХ.  2. Площадки бизнес-мероприятий – Skolkovo, Digital Business Space (Цифровое деловое пространство), Технопарк Москва.  3. Отраслевые порталы и ассоциации – CNews, TAdviser, ComNews, Российский союз ИТ-директоров (СоДИТ).  4. Календарь событий – RB.RU, Eventbrite, TimePad.  ### Примерные мероприятия, которые могут подойти:  1. CIPR-2024 (Цифровая индустрия промышленной России) – форум о цифровых технологиях в промышленности.  2. ITSM Conference – конференция по управлению ИТ-услугами на предприятиях.  3. IoT Tech Expo – технологии Интернета вещей для промышленности.  4. Иннопром. Москва – крупнейшая выставка промышленных технологий.  5. Российская неделя высоких технологий – мероприятия по ИИ, Big Data, IoT.  6. Сколково AI & Industrial Tech Summit – про искусственный интеллект в промышленности.  7. CNews Forum: Цифровизация промышленности – цифровые технологии в реальном секторе.  8. Smart Manufacturing Forum – про индустрию 4.0 и умные производства.  9. ПМЭФ Технологии – технологический блок Петербургского экономического форума.  10. Big Data Conference – конференция про анализ данных и автоматизацию в промышленности.  Для точных дат и регистрации рекомендую зайти на официальные сайты этих мероприятий или агрегаторы событий. Если нужно подготовить письмо или обоснование для участия – тоже помогу! 😊Этому ответу можно было бы поставить твердую пятерку, если бы в нем были ссылки на сайты мероприятий, а так придется гуглить самостоятельно. Мы такое не уважаем, поэтому идем интересоваться у китайских коллег GPT, что они могут противопоставить. DeepSeek:Вот список актуальных мероприятий по ИТ-решениям для промышленных предприятий, которые пройдут в Москве в ближайшие 3 месяца. Эти события будут полезны ИТ-специалистам, руководителям предприятий и техническим специалистам промышленных отраслей:Далее нейросеть пошла по такому фрейму:1. Цифровая индустрия промышленной России (ЦИПР) 2023Дата: 23–25 ноября 2023Описание: Крупнейшая конференция по цифровизации промышленности, включая ИТ-решения для промышленных предприятий.Сайт: https://cipr.ruИ прислала 10 мероприятий, половину с битыми ссылками:2. Industrial IT Forum 20233. Цифровая трансформация в промышленности4. Иннопром. Москва 20235. Цифровое производство: ИТ-решения для промышленности6. Smart Manufacturing Expo 20237. Форум «Цифровая фабрика»8. Конференция «ИТ в промышленности: вызовы и решения»9. Выставка «Автоматизация. Производство. Бизнес»10. Форум «Цифровая энергетика и промышленность»Эти мероприятия охватывают ключевые аспекты цифровизации и внедрения ИТ-решений в промышленности. Рекомендую уточнять даты и регистрацию на сайтах мероприятий, так как информация может обновляться.На этом примере можно увидеть, что нейросети в целом не работают в реальном времени и просто не знают, какой сейчас год и месяц. Зато они могут найти список мероприятий и даже дать ссылки, по которым можно найти конференции для этого года. Сама подборка не вся релевантна, но с этим точно можно дальше работать. Qwen:Вот список из 10 мероприятий, связанных с ИТ-решениями для промышленных предприятий, которые пройдут в Москве или онлайн в ближайшие 3 месяца. Эти мероприятия охватывают темы цифровизации, автоматизации, промышленного интернета вещей (IIoT), искусственного интеллекта и других технологий, актуальных для промышленности.Далее следует похожий на DeepSeek фрейм, но чуть больше информации о мероприятиях:1. Цифровая Индустрия Промышленной России (ЦИПР-2024)Дата: Октябрь 2024Описание: Крупнейшая конференция по цифровизации промышленности в России. Обсуждаются вопросы внедрения цифровых технологий, автоматизации и управления производственными процессами.Сайт: ципр.рфДалее Qwen: прислал следующие мероприятия по такому же фрейму:2. Industry 4.0 Forum3. Международная выставка ""Металлообработка-2024""4. Smart Manufacturing Summit5. IoT&AI Tech Conference6. Digital Enterprise Show Russia7. Выставка ""ЭкспоЭлектроника""8. Форум ""Цифровые технологии в энергетике""9. Конференция ""Big Data in Manufacturing""10. Выставка ""Инновационные Технологии для Промышленности""И добавил примечание:Даты и формат мероприятий могут меняться, поэтому рекомендуется проверить актуальность информации на официальных сайтах.Для участия в некоторых мероприятиях может потребоваться регистрация или покупка билета.Если вас интересуют онлайн-мероприятия, уточните наличие гибридного формата на сайтах.Здесь подбор мероприятий отработан, что называется, «с первой полки». С одной стороны, у нас есть больше описаний по мероприятиям, с другой —  они гораздо узконаправленнее,  я же просила список тем расширить. Думаю, что этот батл не так однозначен, как первый. Тут уже детали сильно зависят от того, как составлен промт и какую первичную задачу нужно было решить. ИМХО: Chat GPT эту гонку тоже проиграл, я бы склонялась к ответу DeepSeek. Объяснение технологий простым языкомЧасто в работе маркетолога требуется быстро разобраться в технине, чтобы быть в контексте задачи. Такой случай идеально подходит для работы с нейросетью и промт выглядит примерно так:Привет. Ты — технический специалист. Ты должен объяснить студенту, как работает серверная виртуализация. Объясни, что это за технология и как она работает так, чтобы даже не технический специалист понял, что это такое. Chat GPT:Выдал самый короткий ответ, разложил плюсы. В общем, погрузил в контекст, но не от всего сердца:Привет! Представь, что у тебя есть один мощный компьютер (сервер), но тебе нужно запустить на нём сразу несколько операционных систем (например, Windows, Linux и macOS), как если бы это были отдельные компьютеры. Серверная виртуализация — это технология, которая позволяет создать виртуальные машины (VM), каждая из которых работает независимо, но на одном физическом сервере.Как это работает?Гипервизор – это специальная программа, которая управляет виртуальными машинами. Он распределяет ресурсы (процессор, память, диск) между ними.Каждая виртуальная машина получает свою операционную систему и работает так, как если бы она была отдельным устройством.Это позволяет одному серверу заменить сразу несколько физических машин, экономя деньги, электроэнергию и упрощая управление.Зачем это нужно?Экономия ресурсов – не нужно покупать много серверов, можно использовать один, но эффективно.Безопасность и изоляция – если одна виртуальная машина сломается, другие продолжат работать.Гибкость – можно быстро создать или удалить сервер, не покупая новое оборудование.Если совсем просто, то серверная виртуализация — это как многоквартирный дом: один большой дом (сервер), в котором живёт много квартир (виртуальные машины), и каждая из них независима от соседей. 🚀Решил ли Chat GPT задачу? Да, оптимизировав нагрузку для себя и для читателя. Можно было бы сделать это интереснее и понятнее? Спросим у его конкурентов. DeepSeek:Точно пошел в задачу более ответственно, разложил не только на образном примере, но и на реальном с расчетами.Привет! Давай разберемся, что такое серверная виртуализация, и я постараюсь объяснить это максимально просто.Что такое серверная виртуализация?Представь, что у тебя есть один мощный компьютер (сервер). Этот сервер может выполнять много задач одновременно, но обычно он используется для одной конкретной задачи, например, для хранения данных или работы сайта. Это не очень эффективно, потому что большая часть его мощности простаивает.Серверная виртуализация — это технология, которая позволяет ""разделить"" один физический сервер на несколько виртуальных серверов. Каждый из этих виртуальных серверов может работать как самостоятельный компьютер со своей операционной системой и приложениями. Это похоже на то, как если бы ты взял один большой торт и разрезал его на несколько кусочков, чтобы каждый мог насладиться своей порцией.Как это работает?Физический сервер: Это реальный, ""железный"" компьютер, который находится в дата-центре.Гипервизор: Это специальная программа, которая управляет виртуализацией. Она ""разделяет"" ресурсы физического сервера (процессор, память, дисковое пространство) между виртуальными серверами.Виртуальные машины (ВМ): Это те самые ""кусочки торта"". Каждая виртуальная машина — это изолированная среда, которая работает как отдельный сервер. На ней можно установить свою операционную систему и приложения.ПримерПредставь, что у тебя есть один мощный сервер с 16 ядрами процессора и 64 ГБ оперативной памяти. Без виртуализации ты мог бы использовать его только для одной задачи, например, для работы сайта. Но с помощью виртуализации ты можешь создать на этом сервере несколько виртуальных машин:Одна виртуальная машина с 4 ядрами и 16 ГБ памяти для сайта.Вторая виртуальная машина с 4 ядрами и 16 ГБ памяти для базы данных.Третья виртуальная машина с 8 ядрами и 32 ГБ памяти для тестирования нового приложения.Все эти виртуальные машины работают на одном физическом сервере, но каждая из них думает, что она единственная и имеет доступ ко всем ресурсам.Зачем это нужно?Экономия ресурсов: Вместо того чтобы покупать несколько физических серверов, можно использовать один мощный сервер с виртуализацией.Гибкость: Виртуальные машины можно легко создавать, удалять или изменять их конфигурацию в зависимости от потребностей.Надежность: Если одна виртуальная машина выйдет из строя, это не повлияет на другие виртуальные машины на том же сервере.ИтогСерверная виртуализация — это технология, которая позволяет эффективно использовать ресурсы одного физического сервера, создавая на нем несколько виртуальных серверов. Это как если бы ты мог использовать один мощный компьютер для множества задач одновременно, не покупая дополнительные устройства.Надеюсь, теперь стало понятнее! Если есть еще вопросы, задавай. 😊Ответ, на мой взгляд, больше отвечает поставленной задаче. Давайте посмотрим на последний вариант. Qwen:Мне в этом кейсе больше всего нравится ответ этой сети. Образная аналогия, разъяснение уже на серверной составляющей, пример подсчетов и внизу сокращенная суммаризация вышеописанного для экономии времени. Привет! Конечно, я постараюсь объяснить серверную виртуализацию максимально просто и понятно. Давай представим ситуацию из реальной жизни, чтобы было легче понять.Начнем с аналогииПредставь себе большой дом, который можно использовать как офисное здание. В этом доме много комнат, но раньше его использовали только для одной компании — она занимала всё здание целиком, даже если ей не нужно было столько места. Это было неэффективно: часть помещений пустовала, а содержать весь дом было дорого.Теперь представь, что кто-то придумал способ разделить этот дом на несколько небольших офисов, каждый из которых работает независимо от других. Каждый офис может быть оборудован под конкретные нужды разных компаний: одна использует его для бухгалтерии, другая для дизайнеров, третья для разработчиков программного обеспечения. При этом все они находятся в одном здании, но у каждой компании есть свой ""кусочек"" пространства, который она использует так, будто он полностью принадлежит ей.Теперь перенесем это на компьютерыСерверная виртуализация — это как раз тот ""способ разделить дом"". Вместо одного большого физического сервера (это мощный компьютер, который работает на предприятии или в дата-центре), мы можем создать несколько ""виртуальных серверов"" на его базе. Эти виртуальные серверы работают независимо друг от друга, как будто они — отдельные компьютеры, хотя на самом деле они используют ресурсы одного физического сервера.Как это работает?Гипервизор — это специальная программа, которая управляет процессом виртуализации. Она как администратор дома, который следит за тем, чтобы каждому ""офису"" (виртуальному серверу) досталось достаточно места, электроэнергии и других ресурсов.Физический сервер — это наш ""дом"". Он предоставляет вычислительные ресурсы: процессор, память, жесткий диск и сеть.Виртуальные машины (ВМ) — это наши ""офисы"". Каждая ВМ работает как отдельный компьютер со своей операционной системой (например, Windows, Linux) и своими программами. Они ничего не знают о том, что делят ресурсы с другими ВМ.Разделение ресурсов — гипервизор распределяет ресурсы физического сервера между всеми виртуальными машинами. Например, если у сервера 32 ГБ оперативной памяти, то одну ВМ можно выделить 8 ГБ, другую — 16 ГБ, а остаток оставить в запасе.Зачем это нужно?Экономия денег : Вместо того, чтобы покупать много физических серверов, можно использовать один мощный сервер и разделить его на несколько виртуальных. Это дешевле и проще в обслуживании.Гибкость : Если одной компании (или одному приложению) нужно больше ресурсов, гипервизор может быстро перераспределить их. Это как передвинуть стену в офисе, чтобы освободить больше места.Надежность : Если что-то случится с одной виртуальной машиной (например, она зависнет или сломается), другие продолжат работать. Это как если бы в одном офисе произошла авария — остальные офисы в здании не пострадают.Изоляция : Каждая виртуальная машина работает независимо. Это важно, например, если ты хочешь протестировать новое программное обеспечение — ты можешь сделать это в отдельной ВМ, не рискуя основной системой.Пример из жизниПредставь, что ты работаешь в компании, которая предоставляет облачные сервисы (например, хранение файлов или веб-хостинг). Вместо того, чтобы покупать отдельный сервер для каждого клиента, ты можешь создать для каждого клиента свою виртуальную машину на одном физическом сервере. Это позволит тебе обслуживать больше клиентов, экономить на оборудовании и легко масштабироваться (добавлять новые серверы или ресурсы).ИтогСерверная виртуализация — это технология, которая позволяет разделить один физический сервер на несколько виртуальных машин. Каждая виртуальная машина работает как отдельный компьютер, но все они используют ресурсы одного сервера. Это делает работу более эффективной, гибкой и экономичной.Если коротко:Один физический сервер = много виртуальных машин.Управляет этим процессом гипервизор.Это удобно, дешево и надежно.Надеюсь, теперь стало понятнее! Если остались вопросы, спрашивай. 😊Ответ:Серверная виртуализация — это технология, которая позволяет разделить один физический сервер на несколько виртуальных машин, каждая из которых работает независимо, как отдельный компьютер.На мой взгляд, этот ответ лучше всего закрывает задачу: примеры, пояснения, краткое содержание, тут есть всё, чтобы разобраться в теме. Отдаю свое предпочтение Qwen в этом задании. Что в итоге лучше?На наглядных примерах вам будет легче определиться с инструментом. Конечно, всегда лучше проверять на практике: так вы быстрее поймете, какая генеративная нейросеть хороша именно для ваших задач, а может для разных задач вы будете использовать разные нейронки. В любом случае буду рада прочесть ваши комментарии по теме: для каких задач и что подходит именно вам. "
30,30,"Переводим пользовательский интерфейс на RTL: быстро, качественно, недорого",iSpring,Платформа для онлайн-обучения,68.37,"Веб-разработка, Программное обеспечение, Мобильные технологии",2025-03-13,"Всем привет! Меня зовут Арина, я - frontend-разработчик в отделе разработки конструкторов iSpring. Наша команда занимается разработкой и развитием продукта iSpring Page - облачной платформы для создания курсов.С помощью наших инструментов многие компании обучают своих сотрудников по всему миру. Наши продукты уже локализованы на 33 языка. Большинство языков, которые мы внедряли в проект, были на латинице и кириллице, из интересных есть, например, тайский, каталонский и узбекский языки. В 2023 году наша компания начала активно развиваться на рынке Ближнего Востока, и поэтому возник вопрос поддержки арабского языка! А это означало, что нам нужно не только внедрить переводы, но и развернуть или “отзеркалить” пользовательский интерфейс всех наших продуктов.Сразу обозначу пару аббревиатур, которые буду использовать в статье. Они описывают, как текст и элементы интерфейса должны быть организованы на веб-странице:LTR (Left to Right) - это стандарт для языков, где чтение и написание текста происходят слева направо. Например, русский и английский.RTL (Right to Left) - используется для языков, где текст читается и пишется справа налево. Например, арабский и иврит.Ниже представлен пример того, как одно из наших приложений выглядит в LTR:Плеер лонгрида в LTRВ RTL тот же самый продукт выглядит следующим образом:Плеер лонгрида в RTLДля нас задача перевода всех наших продуктов на RTL была настоящим вызовом: сделать быстро, качественно и недорого автоматизированно 😅 Изначально мы ставили себе задачу спроектировать фичу так, чтобы в будущем не заниматься ее дополнительной поддержкой.Когда мы приступили к изучению предметной области и проектированию своего решения, то нашли очень мало материалов на эту тему. Собственно, поэтому решили поделиться своим рецептом, как можно реализовать поддержку RTL.Поэтому в данной статье я расскажу вам:какими могут быть варианты реализации поддержки RTL языковкак реализовали мыс какими трудностями столкнулись и как их решиликак поддержка RTL повлияла на работу смежных командКакими могут быть варианты реализации поддержки RTL языков?Как уже упоминалось выше, еще на старте мы исключили мануальную доработку продукта. Поэтому для своей реализации мы не использовали ни один из нижеперечисленных вариантов, т.к. их поддержка отнимала бы у нас много сил и времени. Но они вполне применимы для небольших проектов и одноязычных сайтов.Отдельные CSS для LTR и RTLНаверное, это первое решение, которое приходит в голову, когда мы слышим, что нужно реализовать несколько вариантов отображения одной и той же веб-страницы.Вся суть подхода заключается в разделении CSS на 2 файла: для LTR и RTL версии. При этом на самой странице подключается только 1 вариант (в зависимости от языка сайта). Такой подход будет хорошо работать, если язык страницы статичный, т.е. у пользователя нет возможности его изменить.Из хорошего:быстро реализуетсяна странице будут грузиться только нужные стилиИз плохого:все стили нужно дублировать в оба файла, что значительно увеличивает сложность и длительность разработки такого продуктаЕстественно, можно найти много ресурсов, которые могут помочь, упростить и ускорить разработку при таком подходе, например, генераторы RTL-стилей на основе LTR.Нам такой вариант не подошел, потому что в наших продуктах пользователь самостоятельно может изменять язык интерфейса, но такого подхода вполне можно придерживаться, если вы пилите сайт на одном языке.Дополнительный CSS для RTLМожно пойти дальше и немного усложнить первый вариант - избавимся от дублирования стилей.При таком подходе мы все еще делим CSS на 2 файла: файл с основными стилями страницы и файл для RTL-версии. В файле с основными стилями мы собираем стили для LTR-версии (позиционирование, отступы и прочие красивости), а во втором файле переопределяем исключительно позиционирование и отступы под RTL-вариант отображения.Такой вариант по-прежнему быстрый и дешевый + мы избавляемся от дублирования стилей в оба файла, но также есть ряд минусов, которые хочется улучшить:все еще приходится поддерживать 2 файла стилей вместо одногочасть стилей (связанная с позиционированием и отступами) в RTL-версии будет постоянно переопределятьсяRTL и LTR стили в одном CSSТо есть поддерживать только один файл стилей и подключать на обе версии сайта только его:Такой вариант реализации тоже имеет место быть, но:он достаточно неоптимальный (т.к. мы постоянно грузим лишние стили)его сложно поддерживать (т.к. в одном файле будет находиться большой объем кода)Как реализовали мы?Атрибут dirАтрибут dir устанавливает, в каком направлении будет выводиться текст внутри элемента: слева направо или справа налево. Суть в том, что на основании выбранной локали пользователя мы задаем базовое направление текста для всего документа. Проще говоря, применяем атрибут dir для тега html.Наши пользователи могут самостоятельно изменять локаль продукта, поэтому значение атрибута не зашито в шаблоне страницы, а меняется динамически в зависимости от выбранного языка.После того, как мы меняем значение атрибута dir, браузер автоматически изменяет не только направление текста, но и его выравнивание: для LTR версии текст выравнивается по левому краю, для RTL - по правому.При изучении направленности символов, управляющих символов и атрибута dir в частности мы вдохновлялись статьей ребят из 2ГИС, которую можно найти тут. Ребята, если вы это читаете, мы шлем вам плюсик в карму за крутую и полезную статью!Изменения в блочной моделиНаше следующее изменение коснулось восприятия блочной модели. При упоминании словосочетания “блочная модель” все мы привыкли представлять что-то подобное:Блочная модель, построенная на физических свойствах CSSПри написании CSS-свойств для элементов страницы мы всегда оперировали физическими свойствами. Например, направлениями “left” и “right” - это прекрасно работало и работает до сих пор. Но такие свойства не дадут вам желаемого эффекта при смене направления текста, т.е. при отзеркаливании интерфейса пользователя.Дело в том, что физические свойства CSS будут добавлять отступы и бордеры с фиксированной стороны. Например, если мы добавляем padding-left, то мы явно задаем отступ слева и для LTR, и для RTL версий страницы. Конечно, мы можем переопределить стили, и задать для RTL версии свойство padding-right, но это не то, что нам нужно и поддерживать такой вариант не сильно приятно, т.к. подобных изменений будет большое количество.Поэтому мы пришли к использованию логических свойств CSS вместо физических. Что это значит? Вообще блочная модель, построенная на логических свойствах, выглядит следующим образом:Блочная модель, построенная на логических свойствах CSSЕсли кратко, то:Мы перестали думать терминами право-лево и верх-низЗаменили их на inline-start, inline-end и block-start, block-endПрименение логических свойств в проекте коснулись не только блочной модели, но и еще ряда свойств, таких как:позиционированиевыравнивание текстаобтеканиеЕсть и хорошая новость: гриды и флексбоксы уже построены на новых логических свойствах. А значит пользоваться ими мы можем привычным нам способом 🙂Подробно про логические свойства написали ребята из Веб-стандартов, ознакомиться можно тут. А тут ребята из Skillfactory подробно написали про свой опыт использования логических свойств с примерами и картинками.АвтоматизацияОбъем кодовой базы большой, поддержать RTL нам нужно во всех продуктах и переводить физические свойства CSS на логические вручную - казалось бы вечностью. Поэтому мы сразу задумались об автоматизации.Для ускорения нашей работы и повышения ее надежности мы пришли к доработке конфига линтера и использованию плагина stylelint-use-logical:{   ""plugins"": [     ""stylelint-use-logical""   ],   ""rules"": {     ""csstools/use-logical"": [       ""always"",       {         ""except"": [           ""float""         ]       }     ]   } }После прогона линтера наш код существенно изменился и большинство физических свойств CSS были заменены на логические. Ниже представлен пример физических свойств:.block {   margin-top: 20px;   padding-left: 10px; }А это то, что получилось после прогона линтера:.block {   margin-block-start: 20px;   padding-inline-start: 15px; }Казалось бы, фича готова и львиная часть работы позади! Давайте посмотрим на поддержку логических свойств.Поддержка браузерамиА теперь самое интересное 🙂 В пользу быстрой и качественной поддержки RTL мы отказались от:IESafari 14Потому что доля пользователей продукта с браузера IE и Safari 14 крайне низкая (мы проверяли), а поддержка логических свойств выглядит следующим образом:Скриншот для ленивыхКак же мы были счастливы, наконец-то отказавшись от поддержки IE!Какими правилами руководствовались?Чтобы снизить количество споров и багов, мы договорились на ряд правил, которых теперь придерживаемся в своих продуктах.Мы используем Григорианский календарьРабочая неделя все так же начинается с понедельника, несмотря на то, что ряд стран начинает ее с воскресенья.Знак % всегда справаЗдесь, думаю, понятнее будет с примером:Телефон, email, дата, время всегда пишутся слева направоИконки воспринимаем как  “имя собственное”, а по правилам мы их не зеркалимБольшинство наших иконок остались без изменений при переводе приложений на RTL, но есть исключения. Если иконка по смыслу указывает на направление - мы ее меняем на противоположную. Например, иконки, которые содержат стрелки, направление выравнивания текста, макеты и тд.С какими трудностями столкнулись и как их решили?Как отображать компонент?Если с направлением текста браузер справляется самостоятельно, то иконки за нас уже никто не отзеркалит.Поэтому если есть необходимость развернуть иконки или при разработке нам не удалось обойтись только логическими свойствами, то мы решили прокидывать в компоненты пропс direction:type Direction = ‘ltr’ | ‘rtl’  type ComponentProps = {   ...   direction: Direction,   ... }А внутри компонента уже в зависимости от значения пропса условно рендерим LTR или RTL иконку.Двунаправленный текстВсе понятно, когда весь текст написан на одном языке: LTR или RTL. Начинаются танцы с бубном, когда текст смешанный:Тут вариантов решения 2:Доработать разметку с использованием dir=”<direction>”Добавить управляющие символы: &lrm; и &rlm;При работе с переводами мы придерживались второго варианта.Шорткаты в CSSЕсть нюанс при работе плагина stylelint-use-logical с шорткатами padding и margin. Дело в том, что padding и margin - это тоже физические свойства, и плагин не конвертирует их в логические (внезапно).На ответственность разработчиков этот момент мы оставлять не стали и добавили еще одно правило линтера, явно запретив использование этих свойств:{   ...   ""property-disallowed-list"": [     [       ""padding"",       ""margin""     ],     {       ""message"": ""Use logical properties: margin-block, margin-inline, padding-block, padding-inline""     }   ],   ... }А для уже существующих в кодовой базе шорткатов мы подготовили скрипт, который заменил текущие шорткаты на соответствующие логические свойства CSS. Ну не красота ли?CSS-переменные не работаютМы отказались от IE и старых версий Safari, но уже в процессе тестирования обнаружили, что CSS-переменные в шорткатах не работают в Safari 14.1. А значит - нужно править! Для начала мы запретили использовать переменные в шорткатах:{   ...   ""declaration-property-value-disallowed-list"": {     ""/^(margin|padding)-(block|inline)$/"": [""/var/""]   },   ... }А если есть необходимость использования CSS-переменной при определении отступов - мы используем полные версии свойств, например, margin-block-start или padding-inline-end. С этими свойствами проблем с поддержкой нет.Прозрачность текстаДа, на это мы теперь тоже обращаем внимание. Если с кириллицей и латиницей проблем с полупрозрачным текстом не возникает, то с арабским все интереснее!Если приглядеться, то можно заметить, что межсимвольный интервал на арабском языке отличается и символы накладываются друг на друга. Смотрится неопрятно, согласны? Тут может быть два варианта решения:Скорректировать letter-spacing и изменить расстояние между символамиИспользовать цвет без прозрачностиДолго не думая, мы выбрали второй вариант 🙂Как сделать еще лучше?К сожалению или к счастью, не все наши проекты реализованы с помощью известных фреймворков. Часть наших продуктов написана на самописном реактоподобном движке (и тем интереснее, а если вам тоже интересно - дайте знать и мы вам расскажем). И нам приходилось прокидывать в некоторые компоненты настройку направления текста в качестве пропса. Но если вы используете, например, React, то для реализации подобного функционала логично было бы использовать React Context.Ведь в таком случае:значение контекста наследуется дочерними компонентамиконтекст поддерживает вложенностьвыглядит это элегантнее, чем пропсы компонентовИспользование контекста могло бы значительно упростить поддержку RTL для мультиязычных аккаунтов. Это когда язык приложения LTR, а пользователь просматривает или редактирует RTL-документы и наоборот.Например так один из наших редакторов выглядит, если язык приложения русский, а контент создают на арабском:И наоборот, если в арабском аккаунте создают контент, например, на русском:Как поддержка RTL повлияла на работу других команд?Что мы имеем в результате:3 месяца разработки140 закрытых задач300+ страниц, переведенных на RTL2500+ измененных файловочень много сэкономленного времени мануальной работы 💪полная готовность к внедрению иврита и персидского языка 😎Как мы и планировали, реализовали фичу так, что для ее поддержки не нужно предпринимать каких-либо дополнительных усилий. Ниже расскажу, как изменилась работа в командах:При разработке мы теперь всегда используем логические свойства (спасибо линтеру)Дизайны по-прежнему собираются в LTR (отдельно RTL версию нам не готовят)При подготовке дизайна в случае необходимости сразу добавляются иконки для LTR и RTL версий (тут придерживаемся правил, описанных выше)При подготовке переводов появился нюанс с добавлением управляющих символовПри тестировании теперь проверяем каждую фичу и в LTR, и в RTL версииПолезные ссылкиНиже оставила ряд полезных ссылок, которые помогли нам при поиске своего варианта реализации:https://rtl.wtf/https://rtlstyling.com/https://web-standards.ru/articles/logical-css-props/https://habr.com/ru/companies/2gis/articles/358148/https://habr.com/ru/companies/skillfactory/articles/546108/С удовольствием отвечу на ваши вопросы! Если вы тоже реализовывали поддержку RTL в своих проектах, поделитесь, какие у вас были трудности и как их решили? :)"
31,31,Кибер Бэкап и защита СУБД,Киберпротект,Разработчик систем резервного копирования,49.37,"Программное обеспечение, Информационная безопасность",2025-03-13,"Продолжаем рассказывать про то, как быстро и просто начать использовать нашу систему резервного копирования Кибер Бэкап.В первой части мы обсудили, как развернуть Кибер Бэкап на ОС Linux и создать план резервного копирования. Вторую часть посвятили защите платформ виртуализации. Сегодня расскажем о резервном копировании и восстановлении баз данных.Коротко напомним о нашем продукте для тех, кто с нами в первый раз. Система резервного копирования Кибер Бэкап, помимо собственно резервного копирования и восстановления, включает ряд дополнительных функциональных возможностей, таких как обеспечение возможности восстановления и управление нагрузкой. К обеспечению возможности восстановления относятся механизмы репликации резервных копий, позволяющие реализовать правило 3–2–1, а также функции проверки резервных копий на возможность восстановления из них.Кибер Бэкап состоит из трех основных компонентов: сервера управления, агентов защиты и узлов хранения. Агенты выполняют основную работу по защите данных — это не только резервное копирование, но и другие задачи:очистка хранилищпроверка возможности восстановлениярепликация и пр.АгентыАгенты устанавливаются на объекты защиты. Агенты выполняют резервное копирование, восстановление, задачи обеспечения жизненного цикла резервных копий и связанные задачи.Сервер управленияРаздает агентам задачи по защите, собирает информацию о статусе выполнения этих задач.Имеет достаточно низкие системные требования, может быть размещен в виртуальной машине.Поддерживает кластерные конфигурации для обеспечения высокой доступности.Поддерживает размещение своих служебных данных в СУБД SQLite или MS SQL Server, а начиная с версии 17 — в PostgreSQL. В релизе 17.1 появилась поддержка кластера Patroni. Поддерживаются версии Patroni 3.0–3.2.1.Узлы храненияПомимо хранения резервных копий, узел хранения осуществляет проксирование, дедупликацию, парольную защиту централизованных хранилищ и работает с лентами.Объекты защитыСамый важный вопрос о системе резервного копирования: «А что она умеет защищать?» Мы разделяем защищаемое ПО на три класса:операционные системыплатформы виртуализацииприложения (СУБД, коммуникационное ПО и службы каталогов)Мы стараемся поддерживать отечественные, зарубежные и свободно‑распространяемые системы. Это позволяет нам помогать компаниям на всех этапах импортозамещения защищать данные в существующих системах и в новых, на которые они переходят.Самый важный вопрос о системе резервного копирования: «А что она умеет защищать?» Мы разделяем защищаемое ПО на три класса:операционные системыплатформы виртуализацииприложения (СУБД, коммуникационное ПО и службы каталогов)Мы стараемся поддерживать отечественные, зарубежные и свободно‑распространяемые системы. Это позволяет нам помогать компаниям на всех этапах импортозамещения защищать данные в существующих системах и в новых, на которые они переходят.Поддержка отечественных систем идет в тесном сотрудничестве с нашими технологическими партнерами, которых у нас более 60. С частью из них у нас подписаны стратегические соглашения. Мы активно работаем как с производителями ПО, так и с производителями оборудования.В области СУБД на базе PostgreSQL нашими технологическими партнерами являются компании Postgres Professional, Газинформсервис, Orion soft, Тантор Лабс и СберТех.Способы защиты СУБД в Кибер БэкапеЗащита СУБД в Кибер Бэкапе может быть реализована различными способами:В первую очередь это агенты, которые позволяют нам обеспечить максимальное взаимодействие с сервером СУБД, использовать API и библиотеки вендора.Резервное копирование с поддержкой приложений служит для того, чтобы обеспечить возможность восстановления данных без восстановления всей машины.В ряде случаев мы используем команды до и после для интеграции внешних инструментов — вендора и сторонних. Установка отдельного агента не требуется: команды выполняются агентом для Linux или Windows. Про команды до и после недавно писали в нашем блоге.Теперь давайте посмотрим, как Кибер Бэкап обеспечивает резервное копирование и восстановление поддерживаемых СУБД.Защита Microsoft SQL ServerКибер Бэкап поддерживает резервное копирование и восстановление баз данных Microsoft SQL Server самых востребованных нашими заказчиками версий. Резервное копирование и восстановление данных выполняется агентом для SQL, который необходимо установить на машину с сервером Microsoft SQL Server.Резервное копированиеРезервная копия базы данных Microsoft SQL содержит файлы базы (.mdf,.ndf), журналы (.ldf) и другие связанные файлы. Их резервные копии создаются с помощью службы теневого копирования томов (VSS) и SQL Writer.После каждого успешного резервного копирования выполняется сокращение журналов транзакций SQL, если в плане резервного копирования включена соответствующая опция.Резервное копирование с поддержкой приложенийПри резервном копировании с поддержкой приложений обеспечивается следующее:Резервные копии приложений в согласованном состоянии, поэтому доступны немедленно после восстановления машины.Можно восстановить базы данных SQL без восстановления всей машины.После каждого успешного резервного копирования выполняется сокращение журналов транзакций SQL.ВосстановлениеПоддерживается восстановление базы данных в запущенный экземпляр SQL Server, а также присоединение файлов базы данных SQL к экземпляру SQL Server. Базы данных также можно восстанавливать в виде файлов. Это может быть полезным при необходимости извлечь данные для интеллектуального анализа данных, аудита или дальнейшей обработки с использованием инструментов сторонних поставщиков.Все системные базы данных экземпляра, включая базу master, восстанавливаются одновременно. При восстановлении системных баз программа автоматически перезапускает целевой экземпляр в однопользовательском режиме. После завершения восстановления программа перезапускает экземпляр и восстанавливает другие базы данных (если есть).Также Кибер Бэкап поддерживает кластерные конфигурации Always On Availability Groups (AAG).Защита Oracle DatabaseКибер Бэкап поддерживает резервное копирование и восстановление баз данных Oracle Database 11g и 12c. Резервное копирование и восстановление данных выполняется агентом для Oracle, который необходимо установить на машину с запущенной Oracle Database.Резервное копированиеРезервное копирование сервераМы можем создать резервную копию всего сервера Oracle, используя резервное копирование с поддержкой приложений для согласованности состояния базы данных Oracle с приложениями.Преимущества:Меньшее директивное время восстановления при сбое всего сервера.Резервное копирование полностью настраивается в графическом интерфейсе пользователя. Не обязательно иметь навыки работы с Oracle Recovery Manager (RMAN).Имея резервную копию всего сервера, можно легко создать виртуальную машину, которая развертывается за считанные секунды и заменяет исходный сервер.Резервное копирование базы данныхСоздается резервная копия базы данных Oracle с помощью Oracle Recovery Manager (RMAN) в локальной папке, затем создается резервная копия полученных файлов в другом хранилище, при этом в локальной папке сохраняются только последние резервные копии RMAN. Решение предоставляет сценарии RMAN, которые могут автоматически запускаться до резервного копирования файлов. Поэтому всю процедуру можно настроить и выполнить в одном рабочем процессе. При этом не требуется навыков создания сценариев RMAN.Преимущества:Поддержка баз данных, которые хранятся на необработанных разделах или томах Oracle Automatic Storage Management (ASM).ВосстановлениеВосстановление сервераПоддерживается восстановление всего сервера на момент времени создания резервной копии. При восстановлении сервера на «голое железо» Oracle Database восстанавливается среди других данных и будет в согласованном состоянии.Восстановление с использованием Oracle ExplorerСуществует возможность восстановления файлов данных по состоянию на определенный момент времени с использованием инструмента Oracle Explorer, который входит в состав решения. Oracle Explorer использует RMAN и включает в себя целый ряд полезных параметров RMAN в удобном пользовательском интерфейсе. Oracle Explorer позволяет выполнить восстановление как из резервных копий с поддержкой приложений, так и из резервных копий баз данных.Восстановление с использованием сценариевМожно восстановить базу данных по состоянию на время создания последней резервной копии из локально сохраненных резервных копий RMAN. Для такого восстановления решение предоставляет сценарий RMAN. Чтобы восстановить базу данных по состоянию на время создания предыдущей резервной копии RMAN, сначала следует восстановить эту резервную копию RMAN из резервной копии на уровне файлов, а затем использовать сценарии для восстановления базы данных.Защита MySQL и MariaDB, MongoDB и РЕД База ДанныхMySQL и MariaDBРезервное копированиеДанные MySQL или MariaDB можно защитить с помощью резервного копирования на уровне дисков с поддержкой приложений. При этом собираются метаданные приложения и обеспечивается детальное восстановление на уровне экземпляра, базы данных или таблицы.ВосстановлениеИз резервной копии с поддержкой приложений можно восстановить экземпляры, базы данных и таблицы MySQL или MariaDB. Также можно восстановить весь сервер, на котором запущены экземпляры, или восстановить файлы и папки с этого сервера.Для резервного копирования баз данных необходимо установить агент для MySQL/MariaDB на физическую или виртуальную машину, на которой запущены экземпляры MySQL или MariaDB. Агент для MySQL/MariaDB устанавливается вместе с агентом для Linux (64-разрядная версия).MongoDBДанные MongoDB можно защитить с помощью резервного копирования с использованием встроенных инструментов MongoDB (например, утилит mongodump и mongorestore). При этом обеспечивается восстановление на уровне серверов СУБД, баз данных и коллекций MongoDB.Чтобы защитить физическую или виртуальную машину, на которой запущен экземпляр MongoDB, необходимо установить агент для Windows или агент для Linux (в зависимости от операционной системы сервера). Агент защиты может быть установлен как на хост, на котором находятся базы данных MongoDB, так и на другой хост. РЕД База ДанныхДля резервного копирования РЕД База Данных рекомендуется заблокировать базы данных на время резервного копирования. При такой блокировке основной файл базы данных замораживается, изменения при этом фиксируются во временном файле дельты. После окончания резервного копирования основной файл базы данных разблокируется, временный файл дельты и основной файл базы данных объединяются. Операции блокирования и разблокирования базы выполняются скриптами, доступными для скачивания и вызываемыми командами до и после резервного копирования. Пакет скриптов входит в комплект поставки Кибер Бэкапа. Защита PostgreSQL и СУБД на ее основеСвободно распространяемая СУБД PostgreSQL и российская коммерческая СУБД Postgres Pro, разработанная на базе PostgreSQL, являются одними из самых популярных систем управления базами данных в России.СУБД Postgres Pro была значительно переработана для соответствия требованиям корпоративных заказчиков. На PostgreSQL/Postgres Pro работают многие отечественные системы автоматизации бизнес‑процессов и офисной деятельности.В настоящее время PostgreSQL — это платформа, на которую переносят базы данных Microsoft SQL и Oracle, и эта задача более чем актуальна с учетом ухода с рынка зарубежных вендоров.История поддержки резервного копирования СУБД PostgreSQL началась еще в Кибер Бэкапе 15: в этой версии СРК можно было создавать только полную резервную копию всего сервера на локальный диск.В версии 16 мы добавили инкрементное резервное копирование и возможность хранения резервных копий в сетевых папках, NFS, управляемых хранилищах узлов хранения и на лентах.Развитие поддержки резервного копирования СУБД PostgreSQL мы продолжили в Кибер Бэкапе 16.5: добавили возможность использовать SDS Кибер Инфраструктура как хранилище резервных копий PostgreSQL, а также сохранять резервные копии во внешние S3-хранилища.В версии 17 мы добавили гранулярное восстановление — одной базы или ее части, восстановление набора данных на определенный момент времени в прошлом (PITR) и поддержку резервного копирования и восстановления кластера Patroni.И наконец, в версии 17.1 была реализована поддержка команд до и после резервного копирования.Релиз 17.2 в плане работы с PostgreSQL был больше стабилизационным, но в версии 17.3, которая появится в скором времени, вас ждет ряд новинок.Более подробно о том, как делали поддержку СУБД PostgreSQL в Кибер Бэкапе, см. здесь. Резервное копирование и восстановление данных выполняется агентом для PostgreSQL, который необходимо установить на сервер баз данных. Поддерживаемые версии СУБДДля версии 17.2 Кибер Бэкапа актуален следующий список:Резервное копирование и восстановление СУБД PostgreSQL:PostgreSQL 11, 12, 13, 14, 15, 16Patroni 3.0–3.2.1Резервное копирование и восстановление СУБД на базе PostgreSQL:Postgres Pro Standard 11, 12, 13, 14, 15, 16Postgres Pro Enterprise 11, 12, 13, 14, 15, 16Proxima DB 2.0, 3.0Jatoba 5TantorPlatform V Pangolin SEСУБД для сервера управленияВ 17-й версии Кибер Бэкапа мы сделали значительный шаг в этом направлении, внедрив поддержку СУБД PostgreSQL для хранения служебных данных сервера управления.Возможность использовать СУБД PostgreSQL повышает производительность сервера управления и позволяет управлять резервным копированием большего числа устройств в единой инсталляции системы. PostgreSQL можно установить либо на машину, предназначенную для сервера управления, либо на отдельную машину. Для установки можно использовать PostgreSQL или Postgres Pro версии 14, 15 или 16.В настоящее время можно выбрать PostgreSQL в качестве СУБД для сервера управления при инсталляции на ОС Linux. Эта возможность поддерживается только для новых развертываний СРК. При установке сервера управления необходимо явно изменить тип используемой СУБД. Инструменты миграции данных сервера управления появятся в следующих версиях продукта.Доработки на сервере управления позволили поддерживать до 6000 объектов защиты при использовании встроенной СУБД SQLite (в предыдущих версиях — до 500 объектов) и до 8000 объектов защиты при использовании внешней СУБД Microsoft SQL Server или PostgreSQL.В версии 17.1 мы пошли еще дальше и реализовали работу с кластером Patroni, чтобы обеспечить высокую доступность и отказоустойчивость СУБД сервера управления и быстрый возврат системы в рабочее состояние.Попробуйте самиЕсли вас заинтересовало решение для резервного копирования Кибер Бэкап, вы можете развернуть систему у себя и оценить все ее возможности в течение бесплатного 30-дневного пробного периода.Посмотрите видеоВ рамках серии онлайн‑мероприятий о быстром старте работы с системой резервного копирования Кибер Бэкап в феврале 2025 г. мы провели мероприятие «Кибер Бэкап: руководство по быстрому старту. СУБД», на котором наглядно продемонстрировали резервное копирование и восстановление баз данных. Основные темы вебинара:Ключевые возможности СРК Кибер Бэкап и поддерживаемые источники данных для резервного копированияВарианты резервного копирования СУБД: при помощи агентов, с поддержкой приложений, с использованием команд до и послеРазвитие поддержки PostgreSQL в Кибер Бэкапе и текущие возможности по защите СУБДЭкосистема компании Киберпротект и ее технологических партнеров — разработчиков СУБДИспользование СУБД PostgreSQL для хранения служебных данных сервера управления Кибер БэкапаПосмотреть запись мероприятия можно здесь."
32,32,Как SonarQube помогает автоматизировать код-ревью: гид для начинающих,Skillfactory,Учим работать в IT на курсах и в магистратурах,59.23,Консалтинг и поддержка,2025-03-13,"SonarQube — это инструмент анализа качества кода, написанный на Java. Он выявляет баги, уязвимости, дублирование кода и «запахи кода» (code smells).SonarQube используют в CI/CD (Continuous Integration, Continuous Delivery — непрерывная интеграция и доставка) для автоматической проверки и улучшения качества кода.Для чего нужен SonarQube:— улучшает качество кода;— повышает его поддерживаемость и безопасность;— упрощает работу над командным проектом;— повышает продуктивность разработчиков;— обеспечивает соответствие стандартам.Какие языки программирования поддерживаетВ бесплатной версии SonarQube поддерживает Java, C#, Python, JavaScript (TypeScript), Ruby, Kotlin, Go, Scala и другие языки. Платные версии расширяют этот список.Версии SonarQubeУ SonarQube есть четыре версии: Community Editionдля начинающих разработчиков, Developer Edition для специалистов, Enterprise Edition для крупных проектов и Data Center Edition для глобальных инфраструктур.Community EditionБазовая бесплатная версия с поддержкой 22 языков программирования, которая подходит для тех, кто только начинает работать с CI/CD-пайплайном. Этого инструментария будет достаточно для начинающих DevOps.Developer Edition Версия с поддержкой более 30 языков, расширенной безопасностью (Static Application Security Testing, SAST), анализом веток и Pull Requests. В версии разработчика можно предотвратить слияние веток с уязвимым кодом.Enterprise Edition Подходит для больших проектов. Поддерживает несколько DevOps-окружений, управление портфелем проектов и экспорт отчетов в PDF, Monorepo(монорепозиторий) для Pull Requests.Data Center EditionВключает все возможности Enterprise Edition для глобальных инфраструктур. Есть горизонтальное масштабирование для работы с большими нагрузками, восстановление данных при сбоях.Бесплатна только Community Edition, а купить лицензию для остальных версий на официальном сайтеиз России нельзя. Однако анализаторы кода от SonarSource доступны для покупки у некоторых российских поставщиков ПО.В примерах будем работать только с бесплатным Community Edition.Архитектура SonarQubeSonarQube состоит из сервера для обработки результатов анализа кода, базы данных для хранения результатов обработки и сканера, который эту обработку проводит.Сервер SonarQubeЭтот компонент отвечает за взаимодействие пользователей с системой и обработку результатов анализа.Основные части сервераВеб-серверИнтерфейс для просмотра метрик качества кода, отчетов и конфигурации. Позволяет разработчикам, менеджерам и администраторам управлять проектами. В нем можно настраивать разрешения, плагины и интеграции.Сервер поискаОснован на Elasticsearch и отвечает за быструю индексацию и поиск данных. Нужен для поиска ошибок, уязвимостей и других метрик по проектам.Вычислительный механизмОбрабатывает отчеты, которые приходят от сканеров. Вычисляет ключевые метрики: покрытие кода, дублирование, сложность, безопасность. Сохраняет обработанные данные в базе данных.База данных SonarQubeХранит всю информацию о проектах и результатах их анализа.Что хранится в БДКонфигурация экземпляра SonarQubeДанные о настройках сервера, пользователях, разрешениях и подключенных интеграциях.Снимки проектов (Snapshots)История анализов, позволяющая сравнивать разные версии проекта для отслеживания прогресса.Плагины и расширенияПоддержка новых языков программирования. Интеграция с DevOps-инструментами (Jenkins, GitHub, GitLab и др.).SonarQube поддерживает PostgreSQL, MySQL, Oracle и Microsoft SQL Server, но PostgreSQL — рекомендованный вариант.СканерыЭти инструменты анализируют код перед отправкой данных в SonarQube.Как работают сканерыЗапускаются в среде CI/CD (Jenkins, GitHub Actions, GitLab CI или локально с SonarScanner CLI).Анализируют код проекта (исходные файлы, тесты, зависимости).Генерируют отчет и отправляют его на сервер SonarQube.Типы сканеровSonarScanner CLI запускают вручную из командной строки.SonarScanner для Maven — интеграция с Maven-проектами.SonarScanner для Gradle — для Gradle-проектов.SonarScanner для Jenkins/GitHub/GitLab —автоматизированные сканеры для CI/CD.Установка SonarQubeС помощью ZIP-архива Скачайте Community EditionЗагрузите ZIP-файл со страницы загрузкиSonarQube.Распакуйте файл Если у вас нет прав администратора (root), распакуйте ZIP-файл в любое место, где у вас есть право записи, например C:/sonarqube или /home/user/sonarqube.Запустите серверВ Windows (для 64-битной версии)Перейдите в C:\sonarqube\bin\windows-x86-64\ и запустите StartSonar.bat.В Linux/macOSВыполните команду/home/user/sonarqube/bin/[ОС]/sonar.sh consoleЗапуск через console покажет логи в терминале.Чтобы получить доступ к панели управления, откройте веб-браузер и перейдите по адресу http://localhost:9000. Войдите в систему, используя стандартные учетные данные: admin/admin. Вы сможете сменить пароль после первого входа.Вход через интерфейс SonarQubeДеплой через DockerВ Docker есть специальный образ для развертывания SonarQube. О том, как пользоваться докером, рассказывали тут. Найдите версию Community Перейдите на страницу SonarQube в Docker Hub.Найдите в тегах контейнер с версией Community.Перед запуском контейнера образ нужно загрузить. Его можно скачать в локальное хранилище заранее, скопировав и запустив команду (docker pull <имя образа>).Пример докер-образа с версией CommunityЛибо, если образа нет в локальном хранилище, он автоматически скачается после запуска docker run на следующем шаге.Запустите серверВыполните команду в терминале:    docker run -d —name sonarqube -p 9000:9000 <имя образа>Убедитесь, что контейнер запущен:  docker psВойдите в системуКак и при установке ZIP-архива, войдите в систему по адресу http://localhost:9000 с учетными данными admin/admin.Как начать работу с проектом в SonarQubeПосле входа в SonarQube откроется начальная страница создания проекта. Тут можно выбрать, с каким сервисом связать проект: GitHub, GitLab, Azure DevOps и т. д. Либо можно создать локальный проект.Страница создания проектаИмпортировать проекты из удаленных репозиториев удобно, но в качестве примера создадим локальный проект.Переходим к созданию локального проекта. Нажимаем Create a local project (создать локальный проект), указываем имя, ключ проекта и название главной ветки.Create a local projectКлюч проекта (project key) в SonarQube — это уникальный идентификатор проекта внутри SonarQube.Название ветки (branch name) аналогично именованию веток на GitHub и также нужно для контроля версий проекта. Далее выбираем способ определения нового кода. Это нужно, чтобы SonarQube правильно идентифицировал код как новый. Оптимальный вариант — Use the global setting. Тогда новым будет считаться тот код, который появился после обновления версии проекта.Use the global settingТеперь нужно выбрать, как мы хотим анализировать репозиторий.Способы анализа репозиторияВнешние ресурсы по типу GitHub Actions или Jenkins дают инструменты для анализа кода, но в примере воспользуемся SonarScanner CLI, поэтому выбираем Locally.На следующей странице нужно создать токен. Можно оставить поля без изменений либо изменить имя токена и срок его действия. Токен нужен для доступа к API и анализа кода без ввода пароля.Создание токенаПосле нажатия Generate вы получите токен. Скопируйте его в удобное место и нажмите Continue.Теперь нужно выбрать сборку вашего проекта: Maven, Gradle и т. д. Например, для работы с проектами на Python выбираем Other.Выбор сборки проектовНа этой же странице нужно выбрать ОС для работы с SonarScanner. Скачайте SonarScanner для вашей системы по ссылке на официальную документацию. Распакуйте ZIP-архив со сканером. Если зайти в папку конфигурации conf и открыть файл properties, то можно увидеть, что URL-адрес сервера SonarQube по умолчанию http://localhost:9000. Архив со сканеромТак что если сервер на самом деле развернут на другом порту, то нужно заменить строку.Работа со сканеромРабота с SonarScanner CLIВ той же папке сканера есть папка bin, где находится приложение SonarScanner.Для работы нам нужен сам код. Возьмите какой-нибудь локальный тестовый образец или скачайте его из удаленного репозитория. Перед запуском сканера у вас должна быть локальная папка с проектом.Перейдем в папку с проектом.cd /путь/к/проектуЗапустим анализ проекта с ключом sample и токеном,который мы ранее получили в SonarQube.путь/к/папке/bin/sonar-scanner \    -Dsonar.projectKey= sample \    -Dsonar.sources=. \    -Dsonar.host.url=http://localhost:9000 \    -Dsonar.login=ВАШ_ТОКЕНЗдесь:sonar.projectKey — ключ проекта (должен совпадать с указанным в SonarQube);sonar.sources — путь к исходному коду (по умолчанию — текущая папка);sonar.host.url — URL сервера SonarQube (по умолчанию http://localhost:9000);sonar.login — ваш токен аутентификации.Если процесс execution прошел успешно, то на сервере http://localhost:9000 отобразятся метрики. В случае, если код прошел проверку, вы увидите Passed. Какие метрики рассчитывает SonarScannerCode Smells («запахи кода»)Подозрительные конструкции, ухудшающие поддержку кода.Пример: большие методы, дублирование кода, сложные условия.Bugs (ошибки)Логические ошибки, приводящие к некорректной работе программы.Пример: потенциальные NullPointerException, неправильные сравнения.Vulnerabilities (уязвимости)Потенциальные проблемы безопасности.Пример: SQL-инъекции, утечки данных, слабые пароли.Code Coverage (покрытие кода тестами)Доля кода, проверенного Unit-тестами (%).Duplications (дублирование кода)Повторяющиеся участки кода (процент дублирования).Cyclomatic Complexity (цикломатическая сложность)Число возможных путей выполнения программы. Высокая сложность приводит к сложному в понимании коду.Maintainability Rating (рейтинг поддерживаемости)Измеряется от A (лучший) до E (худший) и зависит от количества проблем, влияющих на работу приложения.Reliability Rating (надежность)Также измеряется от A до E. Отражает количество багов в коде.Security Rating (безопасность)Оценка защищенности кода, учитывает уязвимости.Интеграции SonarQube, упрощающие работу с кодомВ примере выше мы работали с кодом локально. Но чтобы сделать процесс проверки кода быстрее и удобнее, SonarQube можно интегрировать с системами контроля версий (SCM), хостингами репозиториев, CI/CD-системами и даже средами разработки (IDE).Настройка Quality GatesQuality Gates (контрольные точки качества) —это набор правил, которые определяют, можно ли считать код качественным. Если код не проходит QualityGates, он считается неудовлетворительным и его можно заблокировать на этапе CI/CD.Где настроить Quality GatesЗайдите в интерфейс SonarQube.Перейдите в Quality Gates.Создайте новый Quality Gate или используйте стандартный.Добавьте правила (например, «не более 3% дублированного кода», «тестовое покрытие ≥ 80%»).Сохраните и назначьте его для проекта.Подключение к CI/CD-системам контроля версийSonarQube поддерживает работу с GitHub, GitLab, Bitbucket, Azure DevOps и Mercurial.Как это работаетРазработчик делает коммит в репозиторий.CI/CD-система (например, Jenkins, GitHub Actions или GitLab CI) запускает SonarScanner.Код отправляется на сервер SonarQube. Там происходит анализ наличия ошибок, уязвимостей, дублирования кода.Результаты отображаются в SonarQube UI и их можно встроить в Pull Request.Если есть критические проблемы, коммит можно заблокировать (Quality Gate).Пример интеграции SonarQube с CI/CDЧтобы SonarQube автоматически анализировал код после каждого коммита, его нужно настроить в CI/CD.Пример в Jenkins (Pipeline)pipeline {      agent any      stages {          // Запускаем анализ кода  stage('SonarQube Analysis') {              steps {                  // Используем конфигурацию SonarQube  withSonarQubeEnv('SonarQube') {                      // Анализ кода через Maven                      sh 'mvn clean verify sonar:sonar -Dsonar.login=$SONAR_TOKEN'  }  }  }          // Проверка Quality Gate  stage('Quality Gate') {              steps {                  // Устанавливаем тайм-аут на 5 минут  timeout(time: 5, unit: 'MINUTES') {                      script {                          // Ожидаем результата Quality Gate  def qualityGate = waitForQualityGate()                          // Если код не прошел проверку  if (qualityGate.status != 'OK') {                              // Прерываем сборку                              error ""Quality Gate failed: ${qualityGate.status}""  }  }  }  }  }  }  } Здесь:В stage(‘SonarQube Analysis’) запускает анализ кода через Maven.В stage(‘Quality Gate’) SonarQube проверяет код, и, если он не проходит, Jenkins остановит сборку.Пример в GitHub Actionsjobs:    sonar:      runs-on: ubuntu-latest      steps:  # Загружаем код из репозитория        - name: Checkout code           uses: actions/checkout@v2  # Запускаем анализ SonarQube        - name: SonarQube Scan           run: sonar-scanner -Dsonar.projectKey=my_project -Dsonar.host.url=https://sonarqube.company.com -Dsonar.login=${{ secrets.SONAR_TOKEN }}  # Проверяем, прошел ли код Quality Gate        - name: Check Quality Gate           run: |            curl -u ${{ secrets.SONAR_TOKEN }} ""http://sonarqube.company.com/api/qualitygates/project_status?projectKey=my_project""Здесь:actions/checkout@v2 загружает код.sonar-scanner отправляет код в SonarQube для анализа.curl запрашивает статус Quality Gate: если он не пройден, можно запретить мердж (недоступно в Community Edition).Как интегрировать Git/GitHub/GitLab с SonarQubeВключить Pull Request анализ в SonarQubeДобавьте в sonar-project.properties параметры для анализа Pull Request:# Уникальный ключ PR  sonar.pullrequest.key=${CI_COMMIT_REF_NAME}  # Текущая ветка PR  sonar.pullrequest.branch=${CI_COMMIT_BRANCH}  # Базовая ветка (например, main)  sonar.pullrequest.base=main  # SCM-платформа  sonar.pullrequest.provider=GitHubДобавить в CI/CD скрипт для анализа, например в GitHub Actions:- name: SonarQube Scan    run: sonar-scanner -Dsonar.login=${{ secrets.SONAR_TOKEN }}Здесь:${{ secrets.SONAR_TOKEN }} — секретная переменная, которая хранится в GitHub Secrets (ее нужно создать заранее).Анализ в IDE через SonarLintДля быстрой проверки качества кода на этапе его написания используют SonarLint. Это плагин, который интегрируется с популярными IDE.Как работает SonarLint?Запускает локальный анализ кода прямо в IDE.Показывает ошибки, уязвимости, «запахи кода» в реальном времени.Дает рекомендации по исправлению проблем.Его можно синхронизировать с SonarQube илиSonarCloud (облачной версией SonarQube).Поддерживаемые IDE: IntelliJ IDEA, WebStorm, PyCharm, PhpStorm (JetBrains IDEs).Коротко о SonarQubeSonarQube — мощный инструмент DevOps для улучшения качества кода и предотвращения ошибок на ранних стадиях разработки. Его ключевые возможности:Автоматический анализ кода. Выявляет баги, уязвимости, дублирование.Поддержка множества языков программирования. Работает с различными языками (Java, Python, JavaScript и др.).Quality Gates. Позволяет создавать контрольные точки, блокирующие плохой код на этапе CI/CD.Интеграция с CI/CD. Поддержка Jenkins, GitHub Actions, GitLab CI и других инструментов.Вариативность развертки. Работает в локальной среде и в облаке. Устанавливается через ZIP-архив или запускается из Docker.Поддержка Pull Request анализов (но только в платных версиях SonarQube). Проверка кода перед мерджем в основную ветку.Обучиться с нуля на программиста и растить навыки до техлида или архитектора ПО можно на магистратуре МИФИ и Skillfactory по направлению «Программная инженерия». Оттачивайте навыки на реальных задачах от бизнеса и набирайтесь опыта."
33,33,Realtime тестирование UserGate DCFW,UserGate,Компания,72.18,"Программное обеспечение, Аппаратное обеспечение, Информационная безопасность",2025-03-13,"Привет, Хабр! Меня зовут Кирилл Прямов, я менеджер по развитию NGFW в UserGate. Не так давно мы анонсировали новый NGFW для крупных компаний и операторов дата-центров, который мы назвали UserGate Data Center Firewall (UserGate DCFW).Сегодня я хочу рассказать про то, как мы тестировали этот продукт на новой аппаратной платформе F8010, официальные продажи которой начнутся к концу марта 2025. А в следующий раз я расскажу про тестирование нового гибридного устройства G9300 из новой серии G, которую мы выведем на рынок к лету 2025. Изображение: Patrick Turner (Unsplash License)Что вообще такое DCFWUserGate DCFW — это специализированный NGFW, заточенный на работу в дата-центрах и масштабных корпоративных сетях. Одна из его главных отличительных черт по сравнению с классическим UserGate NGFW — применение нашей новой технологии векторного файрвола. Это позволяет UserGate DCFW обрабатывать до 130 тыс. правил межсетевого экрана, что является рекордом для российских разработок класса NGFW (как это выглядит на практике, мы посмотрим далее во время тестов).Если говорить про «железную» составляющую, то UserGate DCFW может использовать как наши традиционные старшие аппаратные платформы F8000 форм-фактора 2U, которые обычно используются для защиты ЦОД и штаб-квартир, так и платформы нового поколения — E1010, E3010, F8010 и FG. Все перечисленные платформы находятся в 2-х реестрах Минпромторга: ПП 719 (реестр российской продукции) и ПП 878 (реестр российской радиоэлектронной продукции).На аппаратной платформе UserGate FG стоит остановиться отдельно, так как в ней впервые для российских NGFW используется аппаратный ускоритель на базе FPGA (он же ПЛИС, он же вентильная матрица). Традиционный процессор отвечает за идентификацию потоков и генерацию флоу-правил, а ускоритель реализует функции безопасности. Такой подход обеспечивает высокую производительность и позволяет обходить ограничения, присущие решениям на базе универсальных процессоров, например, невозможность обрабатывать elephant flows (например, сессия для репликации базы данных или большого бэкапа в ЦОД).В настоящее время на FPGA доступная работа функций FW L3/L4 (150 Гбит/с на UDP 1518 байт и 90 Гбит/с EMIX. Про состав EMIX расскажу чуть позже. К началу апреля платформа FG также начнет официально поддерживать обработку функций IPS. До конца 2025 года мы также надеемся перенести на сопроцессор обработку функций контроля приложений (FW L7), а также планируем в 2 раза ускорить работу FW L3/L4. В более отдаленной перспективе — реализуем на FPGA обработку функций VPN и контентной фильтрации.Еще одна особенность платформы FG — для нее возможны два варианта внедрения. В первом случае платформа работает автономно (мы назвали его режим standalone). На сегодня доступен кластер Active-Passive, работаем над реализацией кластера Active-Active.Схемы кластера платформ UserGate FG в режиме standalone (слева) и в режиме гибридного устройства (серия G, справа)Второй режим является уникальным для российского рынка и предназначен для задач, требующих обработки большого числа транзакций и еще более высоких скоростей. Так как UserGate DCFW поддерживает явное разделение уровня управления (control plane) и уровня обработки данных (data plane), мы можем объединять несколько платформ в гибридное устройство. В данном случае платформа E1010, E3010 или F8010 берет на себя функции control plane, а более производительная FG — функции data plane. Для удобства мы решили выделить такие гибридные устройства в отдельную серию G. Соединение между плоскостью данных и плоскостью управления осуществляется через стандартные 10-гигабитные интерфейсы. При этом для администратора гибридное устройство выглядит и ведет себя как единое целое — веб-интерфейс для двух платформ один.Сейчас можно объединять в гибридное устройство серии G две платформы, но в перспективе мы хотим объединять 4-5 платформ для получения рекордных для российского рынка скоростей обработки трафика.Обзор тестового стендаДля начала хочу сказать большое спасибо своему коллеге Павлу Никулину — главному инженеру службы внедрения UserGate. Именно он проводил тесты, которые я опишу далее.Сперва в двух словах расскажу, как мы проводим тестирование для заказчиков. Когда к нам обращаются с просьбой провести нагрузочное тестирование, первым делом мы согласуем параметры теста: оборудование, конфигурацию и дополнительные условия (если такие имеются). В этом плане мы опираемся на базовые настройки конфигурации, которые предоставляет заказчик, и на профиль трафика.Количество соединений в секунду и правил межсетевого экрана, а также набор сетевых протоколов в трафике зависят от профиля заказчика. Например, у банков обычно выше требования к количеству одновременных соединений, новых соединений в секунду и правил межсетевого экрана. В целом выбор тестов зависит только от желания заказчика и возможности их технической реализации: в каждом конкретном случае заказчик знает, что ему необходимо, и тестирует решения в соответствии с требуемыми параметрами.По нашей статистике, 90% всех тестирований происходят в режиме онлайн, 10% — в офлайн-формате в одном из наших офисов. По времени согласование всех параметров занимает от одного дня до нескольких недель. Само тестирование – обычно 1,5-2 часа. В редких случаях, когда нужно реализовать серьезное ПМИ от крупного заказчика, — до четырех-пяти часов. Бывают случаи, когда заказчики просят проводить тестирование в течение суток, чтобы убедиться в способности NGFW стабильно работать длительное временя.Теперь про сам тестовый стенд. Мы применяем два генератора трафика: Spirent Cyberflood и Keysight BreakingPoint (бывшая Ixia), которые используют ведущие мировые вендоры сетевого оборудования. Это выгодно отличает нас от многих российских разработчиков NGFW, вынужденных использовать бесплатные генераторы трафика Trex или Яндекс.Танк, имеющие ряд ограничений. Клиентская часть генератора использует свой набор подсетей для генерации трафика и передает его серверной части. Оба компонента имеют интерфейсы, подключенные к коммутатору. Он, в свою очередь, соединен с испытуемым устройством.Для генератора Spirent Cyberflood мы применяем встроенный профиль EMIX:Параметры EMIX в настройках интерфейса генератора трафика Spirent CyberfloodА вот пример одного из профилей трафика EMIX в Keysight BreakingPoint. Можно увидеть набор протоколов и веса, при помощи которых мы можем регулировать тестовый трафик. Байты в конце — это полезная нагрузка для каждого протокола в отдельности.Параметры EMIX в настройках интерфейса генератора трафика Keysight BreakingPointТесты платформы UserGate F8010UserGate F8010 — наша новая флагманская платформа в форм-факторе 2U. В ней используются два процессора AMD EPYC с 48 ядрами каждый и 256 ГБ RAM DDR4. Для сравнения, ее предшественница F8000 оперирует двумя Intel Xeon E5-2697V4 по 18 ядер и 128 ГБ RAM DDR4. По умолчанию включена технология виртуального разделения физических ядер процессора на два, то есть используется 192 виртуальных потока.В стандартной поставке платформы F8010 доступно 8 интерфейсов RJ45 1 GbE и 4 оптических интерфейса SPF+ 10 GbE, а также 6 слотов для дополнительных сетевых карт (NIC), вплоть до QSFP28 на 100 GbE. Проведем четыре теста, используя 131 тыс. правил файрвола (позднее мы решили зарезервировать 1 000 правил под служебные задачи, но тест провели до этого события, поэтому теперь говорим про 130 тысяч правил).CPS-тест, HTTP, 1 Байт (BreakingPoint)Этот тест позволяет оценить максимально возможное количество соединений в секунду. Под соединением будем понимать процесс трехэтапного согласования TCP (SYN, SYN-ACK и ACK), за которым следуют запросы HTTP GET и HTTP Response с последующим закрытием TCP-подключения.Для начала откроем интерфейс BreakingPoint и отредактируем профиль трафика. Для этого нажмем на кнопку MULTI EDIT.Далее, кликнем на иконку увеличительного стекла, чтобы открыть окно с настройками:Установим значение параметров Random response max (min) length в 1 байт (как в условиях теста):Возвращаемся на главный экран и запускам тест кнопкой Save and Run:Трафик пошел — задача установить, сколько новых подключений за секунду мы «перевариваем» на устройстве. Нас интересует вторая графа — TCP Client Connection Rate. Сейчас она показывает, что почти все соединения (их 78 000) закрываются без запаздывания. На пропускную способность мы не смотрим, в этом тесте она нам не интересна. Главное, чтобы не было большого числа Closed (RST) в окне Cumulative Client TCP Connections, которое отражает накопленные подключения.Параллельно можем видеть на графике в дашборде UserGate DCFW, как возрастает нагрузка на процессор.Длительность CPS-теста — 4 минуты, этого достаточно в демонстрационных целях. Для CPS-тестов нет строгих стандартов по времени, их задача — обнаружить возможные утечки памяти. Хотя иногда их проводят дольше — некоторые клиенты заказывают суточные проверки.Тест успешно завершился: при 131 000 правил у нас получилось 78 000 новых соединений в секунду.По умолчанию CPS-тест выполняется без логирования. Однако заказчики часто просят писать логи. Такой подход немного снижает производительность, но эффект, как правило, несущественен и зависит от объема трафика и количества событий. Если заглянуть в журнал трафика, то можно увидеть, что у нас были множественные срабатывания по правилу pass_131K.EMIX-тест (Cyberflood)Второй нагрузочный тест с использованием разнообразных типов трафика. Используем генератор Cyberflood, который будет самостоятельно их подбирать. Все, что от нас требуется, — указать пропускную способность.Генерация трафика будет происходить в соответствии с тем самым стандартным профилем EMIX, который я упоминал ранее.Можно запускать тест — нажимаем кнопку Start.Система загрузит конфигурацию на блейды тестового стенда, которых у нас двенадцать штук. Потом потребуется минута на «разгон», в это время Cyberflood будет плавно наращивать объем генерации микс-трафика.На дашборде UserGate DCFW видим, что степень использования CPU снова начала расти.Во время теста красная полоска в окне Test Criteria Charts обозначает, что некоторые сессии были закрыты с опозданием. Но тест будет провален только в том случае, если количество таких ошибок превысит один процент.По окончании теста количество «дропов» можно просмотреть для каждого блейда по отдельности в столбце Unsuccessful Transactions. Текущие цифры с ошибками — это уровень погрешности.HTTP-тест, 524К (BreakingPoint)В этом тесте вновь перейдем в интерфейс BreakingPoint. Будем использовать 12 блейдов с парой интерфейсов (клиент и сервер).Перед запуском генерации заглянем в профиль трафика. Значение поля Random response min length в настройках равно 524 288 байт — это объем полезной нагрузки.Запустим тест — у него также есть разгонная часть длительностью чуть менее двух минут. Сразу после генерация выровняется.Здесь нас интересует пропускная способность устройства и сколько транзакций оно будет передавать. Так как у нас один протокол, то TCP Client Connection Rate будет равен количеству транзакций.Через две минуты вышли на стабильный уровень генерации трафика и смотрим на графики в дашборде. Загрузка устройства не превышает 60% — это нормальный рекомендуемый уровень. Нагружать устройство до 90% не имеет смысла, так как это нерабочий режим.Серый график, обозначенный как vCPU, отражает количество очередей на ядре процессора. Синий график — использование процессора — это усредненная утилизация процессорного времени. Если график vCPU находится выше графика использования процессора, значит, CPU накапливает очередь перед их обработкой.Генерация прекратилась, можем посмотреть отчет. Для HTTP с полезной нагрузкой в 524К пропускная способность UserGate DCFW на платформе F8010 при включении 131 000 правил межсетевого экрана составила стабильные 60 Гбит/с.EMIX-тест + СОВ (Cyberflood)Проведем тест, аналогичный второму, но поменяем профиль, подключив систему обнаружения вторжений. У подавляющего большинства наших заказчиков обрабатываемый трафик меньше или равен 20 Гбит/с — установим параметр объема трафика с учетом производительности тестируемой платформы. В открывшемся окне устанавливаем стандартный профиль СОВ: Default IDPS profile.Этот профиль содержит около 4 300 сигнатур 3–5 уровня угроз и подразумевает разрешение всех подключений. Что не критично, поскольку тест будет проводиться в условиях отсутствия атак на систему.Мы не будем использовать полный профиль, который содержит сигнатуры с высоким уровнем влияния на производительность, поскольку подавляющему большинству заказчиков не нужно защищаться от атак такого рода.Хотя замечу, что мы проводим тесты с симуляцией атак под нагрузкой с основным профилем по запросу. В основном при помощи Keysight BreakingPoint, который имеет свою большую базу сетевых атак. В редких случаях симуляция проводится путем «проигрывания» PCAP-файлов (c заранее записанными сессиями атак) в Trex или Keysight BreakingPoint.Запускаем тест и видим, что у нас сильно возросла утилизация процессора, и она выше, чем vCPU. Это означает, что очереди не накапливаются, как на предыдущем тесте.Можно сказать, что UserGate DCFW без труда обрабатывает трафик на скорости 20 Гбит/с на том же количестве правил (131 000) с системой обнаружения вторжений c 4 300 сигнатурами.Спасибо за внимание! Я и наша команда тестирования будем рады ответить на ваши вопросы в комментариях. В следующий раз я расскажу про тесты гибридного устройства G9300 с аппаратным ускорением (платформа FG + платформа E3010) на аналогичных тестах."
34,34,Как я создавал Telegram-бота с помощью ChatGPT,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2558.03,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-13," Не так давно мне пришла мысль попробовать создать собственного Телеграм-бота (просто из любопытства). Никаких знаний в программировании у меня нет, поэтому первого бота я создал с помощью специального конструктора для Телеграм-ботов. Довольно удобная штука, но зачастую обладает ограниченным функционалом или требуют оформления подписки. И тут мне в голову пришла идея воспользоваться популярным ChatGPT и попробовать создать бота с нуля, во всём следуя инструкциям нейронки. Устанавливать дополнительный софт на своём основном ПК мне не очень хотелось, поэтому разместить бота я решил на VDS-сервере.  ▍ Подготовка Нет принципиальной разницы, какую операционную систему выбирать для размещения бота на сервере, но из-за личных предпочтений и удобства настройки я остановился на Windows Server. Благодаря тому, что в этой ОС есть графическая оболочка, а расположение сервера я выбрал за рубежом (в моём случае в Казахстане), я смог прямо с сервера, без какого-либо дополнительного софта пользоваться ChatGPT и напрямую копировать полученный код в файл бота. Конфигурации VDS сервера для небольшого бота хватило бы самой минимальной, но для возможности использовать версию Windows Server поновее, я взял конфиг: CPU 2, RAM 2, SSD 20.   В первую очередь необходимо было подготовить сервер к работе Телеграм-бота. Как изначально и планировалось, в этом вопросе я полностью положился на нейронку и отправил запрос:   Хочу написать Телеграм бота и разместить его на своем VDS сервере (ОС Windows). Я полный 0 в этом и не знаю, с чего начать. Опиши пошагово.  В ответ получил довольно подробную инструкцию, включающую себя следующие шаги:   Регистрация бота в Telegram и получение токена с помощью BotFather. Настройка VDS сервера (установка Python и нужных библиотек). Создание папки и файла bot.py, в котором будет храниться код бота.   ▍ Написание кода  Теперь начинается самое интересное — написание кода. Недолго думая, начать решил с кода для показа курса популярных валют (доллар, евро и юань). Отправил запрос:  Напиши мне код бота для Телеграм, который будет по запросу присылать курс валют: доллара, евро и юаня  Скопировал полученный код, вставил в файл bot.py и при запуске бота командой python3 bot.py ожидаемо получил ошибку:  «SyntaxError: unterminated f-string literal (detected at line 26) (, line 26)»  Так как написанием кода занимается ChatGPT, то и исправлением ошибок пусть занимается он. Копируем текст ошибки, отправляем его нейронке и получаем описание ошибки и исправление в коде.   Вновь пробуем запустить код и снова получаю ошибку. На этот раз текст ошибки следующий:  «Traceback (most recent call last): File „C:\telegram_bot\bot.py“, line 5, in  from aiogram.utils import executor ImportError: cannot import name 'executor' from 'aiogram.utils' (C:\telegram_bot\venv\Lib\site-packages\aiogram\utils\__init__.py)»  Снова получаем описание ошибки и исправленный код. Сделав ещё несколько попыток (во время которых GPT указал на необходимость добавления токена из Telegram в код бота) наконец получилось запустить бота без ошибок. Теперь отправляемся в Телеграм, находим бота, запускаем его командой /start и, следуя инструкциям, пробуем получить курс валют командой /rate:   Отлично, всё работает. Однако постоянно пользоваться командой /rate не очень удобно, а в случае когда команда не одна, можно просто в них запутаться. Намного удобнее, если бы вместо ввода команд можно было бы просто кликнуть на кнопку. Отправляем новый запрос:  Бот работает, но постоянно отправлять команды не очень удобно. Хочу, чтобы у бота была кнопка, при нажатии на которую он присылал курс валют.  Как и в прошлый раз, полученный код не удалось запустить с первого раза и возникли новые ошибки, но после нескольких исправлений бот корректно запустился. Теперь при его запуске в Telegram появилось дополнительное меню, в котором отображается кнопка, при нажатии на которую получается тот же курс валют, что выводился при вводе команды /rate.   Другой разговор, с кнопкой намного удобнее. В целом, не без ошибок, но вполне ожидаемо, что ChatGPT справился с такой простой задачей. Но мне стало интересно, как он себя покажет при добавлении нового функционала (как схожего, так и отличающегося) и не сломает ли уже работающий код. Начать решил аккуратно и отправил следующий запрос:  Хочу, чтобы была ещё одна кнопка для вывода последних новостей. Пусть новости будут браться с ria.ru  Бот успешно запустился с обновлённым кодом, и в Telegram появилась новая кнопка «Последние новости», вот только при нажатии по ней бот выдаёт ошибку: «Ошибка при получении новостей. Попробуйте позже».   В консоли на сервере также появилась ошибка:  «ERROR:root: Ошибка получения новостей: No module named 'bs4' INFO:aiogram.event:Update id=644057290 is handled. Duration 316 ms by bot id=7290802386»  Как в случае с курсом валют, потребовалось несколько раз вносить изменения в код бота, а также установить дополнительные библиотеки на сервер, но в итоге удалось получить рабочий результат.   По этому же примеру, без особого труда удалось добавить ещё одну кнопку с выводом прогноза погоды в нужном городе. В отличии от новостей и курса валют, для вывода погоды боту потребовался API-ключ сервиса OpenWeatherMap, и GPT любезно предоставил инструкцию, как этот ключ получить:   Перейди на сайт: home.openweathermap.org/users/sign_up Зарегистрируйся или войди в аккаунт. Перейди в раздел API keys (или API ключи). Там будет твой стандартный API-ключ (или создай новый, если нужно). Скопируй этот ключ и замени your_openweathermap_api_key в коде на свой API-ключ.  При нажатии на кнопку «Погода» бот запрашивает город. При вводе города выводится погода   Весь вышеописанный функционал бота работает по схожему принципу и при запросе без проблем выводит нужную информацию из определённых источников. Проблемы начались при добавлении нового функционала. В голову мне пришло добавить возможность создавать напоминания. Идея заключалась в том, чтобы добавить в уже существующее меню новую кнопку, при нажатии на которую будет открываться новое меню с новыми кнопками, отвечающими за управление напоминаниями. Для начала я отправил следующий запрос:  Хочу добавить ещё одну кнопку, при нажатии на которую будет открываться новое меню. В этом меню нужно сделать 2 кнопки. Одна — для создания напоминания: Сначала отправляем текст, а затем дату. Вторая — для показа всех напоминаний.  К моему удивлению, бот запустился с первого раза. Как я и хотел, в меню появилась новая кнопка «Напоминания».   При нажатии на кнопку успешно открылось новое меню с двумя новыми кнопками. При нажатии на кнопку «добавить напоминание» бот сначала запрашивает текст для напоминания, затем дату. И если текст напоминания бот принял без проблем, то как бы я ни пытался указать дату, он постоянно запрашивал её повторно:   Эту ошибку нейронке удалось исправить с первого раза, слегка изменив логику обработки сообщения с датой, и при повторном запуске напоминание успешно сохранилось.   При нажатии на кнопку «Показать напоминания» также успешно выводятся сохранённые напоминания.   В таком виде напоминания хранятся в памяти бота и автоматически удаляются при его выключении / перезагрузке. Чтобы этого не происходило, я попросил переписать код, чтобы все созданные напоминания хранились в отдельном файле.  Помимо создания напоминаний, необходима была возможность и удалять их. Также не очень удобно постоянно указывать дату напоминания вручную (особенно когда необходимо создать напоминание на сегодня / завтра). Поэтому я отправил следующий запрос:  Нужно добавить возможность удалять напоминания. Также нужно добавить возможность быстро оставить напоминание на сегодня / завтра с помощью специальных кнопок, появляющихся после ввода текста напоминания.  Потребовалось несколько раз исправлять код, но в итоге удалось получить тот результат, на который я рассчитывал. При вводе текста напоминания теперь появляется 4 кнопки, для добавления напоминания на сегодня / завтра, напоминания «без даты» и напоминания на любую другую дату.   ▍ Возникшие проблемы Казалось бы, всё, бот работает, напоминания создаются, что может пойти не так? А то, что пока я увлечённо занимался напоминаниями, исправляя возникающие с ними ошибки, я совсем забыл про тестирование уже ранее добавленного функционала (Курс валют, новости и прогноз погоды). Вернувшись к главному меню бота, я выяснил, что кроме кнопки «Напоминания», остальные кнопки просто не работают. Кроме того, в консоли на сервере также не было никаких ошибок. Я написал про это нейронке, и она начала вносить изменения в код, не сильно помогающие в исправлении проблем с нерабочими кнопками, но при этом ломающие оставшийся рабочий функционал. Сначала бот перестал показывать созданные напоминания, затем перестал создавать новые. В итоге перестали работать все кнопки, и бот превратился в тыкву. Устав биться с нейронкой, я решил вернуться к последней версии кода, где работали напоминания, и посмотреть, в чём же дело. Как выяснилось, ChatGPT, пока писал код для напоминаний, беспощадно удалил весь код с остальных кнопок, оставив только сами кнопки.  В итоге я решил немного изменить подход. Я запустил новый чат, чтобы сломанный код не влиял на ответ нейронки, взял две рабочие версии кода (версию до создания кнопки “напоминания” и версию с “напоминаниями”) и отправил следующий запрос:  У меня есть два кода. Первый код с рабочими кнопками «Курс валют», «Последние новости» и «Погода» (сам код целиком) и код с рабочими «Напоминаниями» (сам код целиком). Нужно объединить их в один рабочий код.  К моему приятному удивлению, после объединения бот запустился с первого раза и без ошибок, а кнопки работали как и планировалось. В итоге я получил следующий код:  import asyncio import requests import logging import json from aiogram import Bot, Dispatcher, types from aiogram.types import Message, ReplyKeyboardMarkup, KeyboardButton from bs4 import BeautifulSoup from datetime import datetime, timedelta   # Токен бота TOKEN = ""Ваш Telegram токен"" WEATHER_API_KEY = ""Ваш API ключ"" REMINDERS_FILE = ""reminders.json""   logging.basicConfig(level=logging.INFO)   bot = Bot(token=TOKEN) dp = Dispatcher()   reminders = {} pending_reminders = {} waiting_for_city = {}   def load_reminders():     global reminders     try:         with open(REMINDERS_FILE, ""r"", encoding=""utf-8"") as file:             reminders = json.load(file)     except (FileNotFoundError, json.JSONDecodeError):         reminders = {}   def save_reminders():     with open(REMINDERS_FILE, ""w"", encoding=""utf-8"") as file:         json.dump(reminders, file, ensure_ascii=False, indent=4)   async def set_reminder(user_id, text, date=None):     if user_id not in reminders:         reminders[user_id] = []     reminders[user_id].append((text, date))     save_reminders()     return ""Напоминание сохранено!""   async def get_reminders(user_id):     if user_id in reminders and reminders[user_id]:         return ""\n"".join([f""{idx + 1}. 📅 {r[1] if r[1] else 'Без даты'} - {r[0]}"" for idx, r in enumerate(reminders[user_id])]).encode(""utf-8"", ""ignore"").decode(""utf-8"")     return ""У вас нет напоминаний.""   async def delete_reminder(user_id, index):     if user_id in reminders and 0 <= index < len(reminders[user_id]):         del reminders[user_id][index]         save_reminders()         return ""✅ Напоминание удалено!""     return ""❌ Неверный номер напоминания.""   def get_currency_rates():     url = ""https://www.cbr-xml-daily.ru/daily_json.js""     try:         response = requests.get(url)         data = response.json()         usd, eur, cny = data[""Valute""][""USD""][""Value""], data[""Valute""][""EUR""][""Value""], data[""Valute""][""CNY""][""Value""]         return f""Курс валют:\n💵 Доллар: {usd:.2f} ₽\n💶 Евро: {eur:.2f} ₽\n🇨🇳 Юань: {cny:.2f} ₽""     except Exception as e:         logging.error(f""Ошибка получения курса валют: {e}"")         return ""Ошибка при получении курса валют. Попробуйте позже.""   def get_latest_news():     url = ""https://ria.ru/""     try:         response = requests.get(url, headers={""User-Agent"": ""Mozilla/5.0""})         soup = BeautifulSoup(response.text, ""html.parser"")         headlines = soup.find_all(""a"", class_=""list-item__title"") or soup.find_all(""a"", class_=""cell-list__item-link"")         news = ""\n"".join([f""🔹 {h.text.strip()}"" for h in headlines[:5]])         return f""Последние новости:\n{news}"" if news else ""Не удалось получить новости.""     except Exception as e:         logging.error(f""Ошибка получения новостей: {e}"")         return ""Ошибка при получении новостей. Попробуйте позже.""   def get_weather(city):     url = f""http://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric&lang=ru""     try:         response = requests.get(url)         data = response.json()         if ""main"" in data:             temp, description = data[""main""][""temp""], data[""weather""][0][""description""].capitalize()             return f""🌤 Погода в {city}: {temp}°C, {description}""         return ""Не удалось получить погоду. Проверьте название города.""     except Exception as e:         logging.error(f""Ошибка получения погоды: {e}"")         return ""Ошибка при получении погоды. Попробуйте позже.""   keyboard = ReplyKeyboardMarkup(     keyboard=[         [KeyboardButton(text=""💰 Курс валют""), KeyboardButton(text=""📰 Последние новости"")],         [KeyboardButton(text=""🌤 Погода""), KeyboardButton(text=""⏰ Напоминания"")]     ],     resize_keyboard=True )     reminder_keyboard = ReplyKeyboardMarkup(     keyboard=[         [KeyboardButton(text=""➕ Добавить напоминание""), KeyboardButton(text=""📋 Показать напоминания"")],         [KeyboardButton(text=""🗑 Удалить напоминание""), KeyboardButton(text=""🔙 Назад"")]     ],     resize_keyboard=True )     quick_date_keyboard = ReplyKeyboardMarkup(     keyboard=[         [KeyboardButton(text=""📅 Сегодня""), KeyboardButton(text=""📅 Завтра"")],         [KeyboardButton(text=""📌 Без даты""), KeyboardButton(text=""✏ Ввести дату вручную"")]     ],     resize_keyboard=True )     @dp.message() async def handle_message(message: Message):     user_id = str(message.from_user.id)     text = message.text       if user_id in waiting_for_city:         weather_info = get_weather(text)         del waiting_for_city[user_id]         await message.answer(weather_info)         return       if text == ""/start"":         await message.answer(""Привет! Выбери действие ниже:"", reply_markup=keyboard)     elif text == ""💰 Курс валют"":         await message.answer(get_currency_rates())     elif text == ""📰 Последние новости"":         await message.answer(get_latest_news())     elif text == ""🌤 Погода"":         waiting_for_city[user_id] = True         await message.answer(""Введите название города:"")     elif text == ""⏰ Напоминания"":         await message.answer(""Выберите действие:"", reply_markup=reminder_keyboard)     elif text == ""➕ Добавить напоминание"":         pending_reminders[user_id] = {""stage"": ""text""}         await message.answer(""Введите текст напоминания:"")     elif text == ""📋 Показать напоминания"":         reminders_text = await get_reminders(user_id)         await message.answer(reminders_text)     elif text == ""🗑 Удалить напоминание"":         reminders_text = await get_reminders(user_id)         if ""нет напоминаний"" in reminders_text.lower():             await message.answer(reminders_text)         else:             await message.answer(f""Ваши напоминания:\n{reminders_text}\n\nВведите номер напоминания для удаления:"")             pending_reminders[user_id] = {""stage"": ""delete""}     elif text == ""🔙 Назад"":         await message.answer(""Главное меню:"", reply_markup=keyboard)     elif user_id in pending_reminders:         stage = pending_reminders[user_id].get(""stage"")           if stage == ""text"":             pending_reminders[user_id][""text""] = text             pending_reminders[user_id][""stage""] = ""date_choice""             await message.answer(""Выберите дату:"", reply_markup=quick_date_keyboard)         elif stage == ""date_choice"":             date = None             if text == ""📅 Сегодня"":                 date = datetime.now().strftime(""%d.%m.%Y"")             elif text == ""📅 Завтра"":                 date = (datetime.now() + timedelta(days=1)).strftime(""%d.%m.%Y"")             elif text == ""✏ Ввести дату вручную"":                 pending_reminders[user_id][""stage""] = ""date""                 await message.answer(""Введите дату в формате ДД.ММ.ГГГГ ЧЧ:ММ:"")                 return             await set_reminder(user_id, pending_reminders[user_id][""text""], date)             del pending_reminders[user_id]             await message.answer(""✅ Напоминание сохранено!"", reply_markup=reminder_keyboard)   async def main():     load_reminders()     await dp.start_polling(bot)   if __name__ == ""__main__"":     asyncio.run(main())   ▍ Вывод В целом, я остался доволен получившимся результатом. Да, при написании кода не один раз возникали ошибки, но ChatGPT по большей части с ними справлялся и давал корректный код. Увы, он не осилил добавление функционала напоминаний к уже имеющемуся коду и заставил изрядно попотеть, чтобы разобраться в причине, но при написании кода с использованием нескольких чатов (один чат для одного функционала, второй чат для другого) подобных проблем возникать не должно.  © 2025 ООО «МТ ФИНАНС»  Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
35,35,Badge From Scratch – Problem Driven Development,Positive Technologies,Лидер результативной кибербезопасности,4.21,"Веб-разработка, Программное обеспечение, Информационная безопасность",2025-03-13,"Всем привет! На связи Positive Labs, R&D-подразделение Positive Technologies, занимающееся исследованиями и разработкой hardware и не только. В своих прошлых статьях [этой и этой] мы грузили читателей техническими подробностями наших исследований. Сегодня же мы предлагаем вернуться в 2024 год и послушать историю создания интерактивного бейджа для фестиваля PHDays Fest 2. Устраивайтесь поудобнее и приготовьте ваши фейспалмы: пользоваться ими придется часто :)О чем вообще речьДля тех, кто в танке: компания Positive Technologies ежегодно в мае проводит киберфестиваль PHDays, который в прошлом году носил название PHDays Fest 2. Специально для этого мероприятия мы выпустили интерактивный бейдж (собственно, он и представлен на превью). История интерактивных бейджей берет свое начало с хакерской конференции DEF CON. На официальном сайте конференции есть отдельная статья, посвященная созданию и развитию концепции бейджей (DEF CON Hacking Conference — The Badge). В нашем Telegram-канале можно посмотреть, что умеет PHDays Badge. Но если вкратце: он умеет отображать картинки на пиксельном экране. Есть набор стандартных, но можно подключиться к нему по Wi-Fi и нарисовать собственную картинку.Ну что же, раз все в контексте, начнем, пожалуй.Проблема 0 — ИдеяИдея создания бейджа родилась задолго до создания Positive Labs. Можно сказать, что это была мечта одного из сотрудников, на которую не хватало времени и средств. Но вот на дворе начало 2024 года, и нашему подразделению приходит запрос: «А не хотите поучаствовать в фестивале PHDays Fest 2?» Такой шанс выпадает нечасто, и мы поняли, нужно воспользоваться возможностью по максимуму: кроме организации собственной зоны на фестивале мы решили создать простое, но при этом крутое и веселое устройство. Спустя несколько часов за рулем (да-да, концепт бейджа был разработан в лучших традициях IT — во время созвона в пробке) был сформирован список качеств, которыми должен обладать будущий бейдж (спойлер: не все мечты сбываются):Яркий — бейдж должен быть заметен издалека и цеплять взгляды всех проходящих мимо детейГромкий — приближение обладателя бейджа должно быть не только видно, но и слышноУникальный — бейдж должен быть кастомизируемым, чтобы каждый обладатель имел возможность подчеркнуть свою индивидуальностьКомпактный — размеры и вес бейджа должны позволять носить его на шее и убрать в карман при необходимостиАвтономный — сиять надо как минимум с 9 до 19 (график диктует фестиваль)Беспроводной — все становится лучше с Wi-FiС ИК-портом — ну потому что ретро рулитВ центре всей задумки был уникальный матричный дисплей в лучших традициях пиксель-арта, который должен был стать главной фишкой бейджа. На основе всех сформированных требований и пожеланий был разработан бесподобный концепт-арт будущего устройства.Да-да, так выглядел бейдж в тот вечерНесмотря на всю проработанность и детальность концепт-арта, на следующий день он был доработан, благодаря чему его уже можно было использовать.Концепт-арт бейджа на следующий деньИтак, мы получили идею, которую можно развивать упрощать. Наш план-график был следующим:две-три недели на разработку схемотехники и трассировку первой версии платыдве недели на производство первого макетапараллельно с разработкой схемотехники и производством первого макета выполняется закупка компонентов и разработка прошивкимесяц на производство и проверку всей партиипара запасных недель на случай непредвиденных обстоятельств Итого в худшем случае у нас должно было уйти на все не более трех месяцев, а до начала PHDays Fest 2 на тот момент оставалось четыре. Целый месяц в запасе, звучит как «вроде изян», верно? Что же может пойти не так?Проблема 1 — СхемотаНа следующий день после презентации нашей идеи (да-да, вот такие вот мы менеджеры) руководителю Алексею (@Benonline), выяснилось две новости:Хорошая: идея топ, надо делатьПлохая: все заняты работой, делать некомуНа выручку пришел наш хороший знакомый. С учетом того, что он тоже смог помогать нам только в неурочное время, первая версия бейджа была готова только через месяц:Рендер платы перед отправкой в производствоНа этом этапе мы еще думали, что все наши хотелки будут реализованы. Однако, получив первую версию бейджа, мы выяснили, что немного ошиблись в выборе пинов на ESP32 для ИК-светодиода: пин использовался ESP как bootstrap, а мы подали на него внешний pullup. Это препятствовало загрузке ESP, и в итоге бейдж даже не включался.Знакомый с разработкой устройств читатель может спросить: «Почему бы просто не выпустить вторую ревизию перед отправкой на производство, в чем проблема?» А проблема была в том, что при разработке нашего чудесного план-графика мы не учли подобное развитие событий и договорились о старте производства. Завод срочно требовал Gerber-файлы, а иначе «никаких гарантий», что бейджи будут изготовлены в срок. Так мы потеряли нашу первую фичу — ИК-порт.Проблема 2 — ЭкранПроблемы с макетом на этом не закончились. Изображение на экране выглядело пересвеченным на любом уровне яркости, и картинка не читалась. Поэтому сиять не выходило, максимум — отсвечивать.Конечно же, мы ожидали, что так будет, и немного заранее занялись конструктивным реверс-инжинирингом колонки Divoom Ditoo, экран которой был одним из референсов для нашей идеи.Неповторимый оригинал Жалкая пародияНа жертвенный алтарь реверс-инжиниринга лег личный экземпляр одного из сотрудников: все во имя науки PHDays Fest 2!Разобрав колонку, мы поняли, чего нам не хватало:Световоды — изолируют светодиоды друг от друга и не позволяют цветам соседних «пикселей» смешиватьсяРассеивающая свет пленка — в сочетании со световодами она придает «пикселям» квадратную формуЗатемняющий экран — позволяет получить насыщенный черный цвет и повышает контрастность итогового изображенияПосле недели поиска подходящих материалов и активного тестирования их на макете мы смогли подобрать сочетание, которое бы выглядело максимально «пиксельно».Тут мы допустили стратегическую ошибку и расслабились, думая, что бейдж готов к производству и мы можем попивать смузи в лучших традициях индустрии. Но как обычно, судьба распорядилась иначе.Проблема 3 — ПроизводствоПримерно в этот момент к команде разработки бейджа присоединяется широко известный в узких кругах любитель 3D-печати Николай (nlef · GitHub) (вы вполне могли пользоваться его Telegram-ботом для Klipper). Вообще, немного забегая вперед, можно смело утверждать, что именно благодаря Коле бейдж получился таким, какой он есть.Итак, наш спец по 3D-печати поспешил нас разочаровать: задуманный нами световод изготовить методом FDM-печати будет сложно. На то было несколько причин:Световод имел «висящие в воздухе» элементы, поэтому при печати необходимо было использовать поддержки, что приводило к порче внешнего видаГеометрия стенок световода не учитывала возможности технологии, из-за чего они были слишком тонкими и хрупкимиПроект трассировки устройства не учитывал необходимость установки световода, и даже малая погрешность установки компонентов на плату приводила к образованию зазора между платой и световодом, из-за чего появлялись засветы между соседними «пикселями»После нескольких итераций изменения конструкции и ее согласования печатные детали все-таки отправились в производство.Наше предвкушение неминуемого успеха снова прерывается звонком от партнеров, занимающихся производством платы. Оказывается, что при планировании производства большого количества устройств (а бейджей, на минуточку, было выпущено почти 1000 штук) необходимо предварительно убедиться в наличии всех необходимых компонентов на складе. И мы убедились, но есть нюанс.Минутка математики.Дано: ● для производства одного бейджа нужно 100 светодиодов● всего нужно произвести 1000 бейджей● у поставщика А на складе есть 50 000 светодиодов● у поставщика Б на складе есть 50 000 светодиодовВопрос: хватит ли светодиодов на складах, чтобы произвести нужное количество бейджей?Ответ: нет, потому что у разных поставщиков один и тот же склад.Как вы поняли, первоначальные расчеты нас подвели. Скажем коротко: спасибо ребятам с производства, что смогли быстро привезти из Поднебесной недостающие компоненты.Оставалось только нарезать световую пленку, «затемнитель» и корпус — и купить 8000 винтиков. Но даже тут мы не смогли учесть все детали. ПЭТ-пленка, которую мы использовали для корпуса устройства, поставляется в рулоне. Резка идет с помощью лазерного станка. Если развернуть пленку в плоскость станка, она не принимает форму плоскости. Вместо этого по всей поверхности пленки расходятся «волны», что приводит к геометрическим искажениям корпуса и большому проценту брака.Довольный Коля после подгонки всех компонентовПроблема 4 — FirmwareВремя шло, бейдж уже отправился в релизное производство, а воз код был и ныне там: экран мог загораться для проверки, но не более того. До гордого названия «интерактивный» нам не хватало интерактивности… Тут второй раз всплывает имя Николая. Вспомнив, что, помимо хобби, Коля является квалифицированным highload-разработчиком, и решив, что писать под embedded — это то же самое, мы поставили перед ним задачу вдохнуть в бейджик жизнь.Тут стоит еще упомянуть, что на дворе конец апреля и до PHDays Fest 2 уже меньше месяца. Сделав все возможное и невозможное, Коля смог реализовать:Веб-интерфейс, чтобы пользователи могли рисовать свои картинкиСупермегаоригинальный способ подключения к бейджу через Wi-Fi, который он сам же и раздаетНу и все необходимое, чтобы это вообще работало :)К сожалению, из-за острой нехватки времени в процессе разработки нам пришлось «потерять» звук, и бейдж остался без крутых мелодий.Пока Коля оживлял интерактивность бейджа, контент-директор Positive Labs Егор приступил к наполнению галереи. Запал закончился ко второму десятку: оказалось, что качественная адаптация картинок под разрешение 10 на 10 пикселей требует значительного количества времени, которого не было, и нас опять пришлось выручать знакомой-дизайнеру.Проблема 6 — Burn, baby, burn!Когда пришло время отдавать релизную прошивку на завод для установки, мы поняли, что не успеваем. Поэтому вся партия в 1000 бейджей вышла с производства с тестовой прошивкой. В результате нам пришлось самим «шить» свое же устройство.Вдобавок ко всему после того, как было прошито уже более 300 бейджей, обнаружилась бага: бейдж уходил в спящий режим не через 15 минут, а через 15 часов. Оставлять такую багу было нельзя, поэтому процесс прошивки пришлось начинать заново.В итоге для прошивки всех бейджей понадобилось 8 рабочих рук и каких-то 6 часов ручной работы, совмещенной с просмотром классики отечественного кинематографа.Всем причастным посвящаетсяВместо заключенияБейдж стал результатом работы большого числа людей, без которых все наши задумки остались бы нереализованными. За бейдж разразилась нешуточная борьба на конкурсах в шатре Positive Labs на PHDays Fest 2. И хотя это просто игрушка, мы искренне надеемся, что кого-то она смогла вдохновить на создание чего-то своего.Кроме того, по многочисленным просьбам мы публикуем в open source исходные файлы PHDays Badge 2024.P. S. Конечно, на грядущем PHDays Fest 2025, который пройдет 22–24 мая в Лужниках, будет представлена вторая версия бейджа, которая получит масштабное обновление, включающее новые функций, переработанный дизайн и расширенные возможности по кастомизации. Следите за новостями в канале Positive Labs — вас ждет много интересного!Positive Labs"
36,36,Scala/Chisel против SystemVerilog: генерируем сложные цифровые схемы,YADRO,Тут про железо и инженерную культуру,716.94,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-03-13,"Привет, Хабр! Меня зовут Денис Муратов, я ведущий инженер по разработке систем на кристалле в компании YADRO. Специализируюсь в основном на разработке RTL для ASIC и FPGA.В наши дни общепризнанный стандарт для RTL-описаний — это язык SystemVerilog, но популярность сейчас набирает его альтернатива, Chisel. Далее я расскажу подробней об этом языке, его преимуществах, недостатках и рисках, связанных с переходом на Chisel со стандартного стека. Отдельно остановлюсь на функциональном программировании — возможности Chisel, которой нет в SystemVerilog, — и на дополнительных возможностях Chisel, улучшающих механизм переиспользования модулей. А также о том, почему код на Chisel менее подвержен ошибкам и всегда работает. Ну, почти всегда.Проблемы SystemVerilogДля полноты картины начну с краткого обзора SystemVerilog. Хоть он и считается стандартом в разработке RTL, но если у вас есть опыт работы с современными языками программирования, то к SystemVerilog у вас наверняка имеются претензии. Ведь это, по сути, расширение Verilog, а Verilog уже более 40 лет. И хотя новые стандарты SystemVerilog выходят по сей день, многие проблемы в нем до сих пор не решены. Перечислю основные.Синтезируемое подмножество ограничено. Если вы создаете на SystemVerilog RTL-описания, то не используете даже 10% его языковых конструкций. В работе с ASIC часто приходится привлекать инструменты и САПР различных производителей, и каждый из них понимает синтезируемое подмножество языка по-своему.Низкая портируемость кода. Например, вы привыкли работать с Vivado, но потребовалось портировать код на ASIC. И оказывается, что часть ваших конструкций уже не синтезируется, не поддерживается, так что код приходится переписывать.Нет ООП и других техник современных языков программирования. Точнее, в SystemVerilog ООП в каком-то виде есть, но не все средства синтеза это понимают.Нет единой точки входа в IP. При разработке ASIC вы пишете не только RTL, но и много других поддерживающих файлов типа SDC, UPF. Постепенно дизайн может стать настолько сложным, что при изменении RTL-параметров придется сразу менять и SDC, и еще кучу других файлов — из-за отсутствия единой точки входа в IP.Избыточность языковых конструкций. Здесь SystemVerilog далеко до VHDL. Тем не менее, когда код пестрит всякими endmodule, module, begin, end, function endfunction, он теряет в читабельности.Что такое ChiselVerilog прежде всего разрабатывался для моделирования аппаратуры. Лишь со временем инженеры осознали, что некоторые описания на Verilog можно синтезировать в логическую схему. В отличие от Verilog, Chisel изначально позиционировали как HDL, Hardware Description Language, то есть язык именно для описания RTL.В Verilog для описания желаемой схемы может не хватать языковых конструкций, и на вход САПР для синтеза приходится подавать поведенческое описание схемы. Синтезаторы понимают его, как умеют, и синтезируют, но фактически мы имеем дело не со схемой, а с поведением схемы и многое отдаем на откуп САПР. Из описания на Chisel генерируются описания на Verilog, VHDL, SystemVerilog, но в любом случае это не HLS (High Level Synthesis). DARPA, управление перспективных исследовательских проектов Минобороны США, описывает Chisel как технологию, позволяющую маленьким командам создавать большие цифровые проекты. И я вполне могу с этим согласиться, но есть нюансы.Chisel — это, по сути, библиотека Scala, а точнее, Domain Specific Language. Языку Scala уже больше 20 лет, он постоянно развивается, сочетает функциональное и императивное программирование. При написании кода на Scala вам доступны все библиотеки Java. Scala — это масштабируемый язык, который позволяет добавлять свои языковые конструкции. На основе Scala можно создать язык под свои задачи. Так 12 лет назад и поступили инженеры в Беркли: выкинули из Verilog 90%, оставив только нужное, и обернули все это в Scala. Получился Chisel. Как я уже писал, Chisel используют прежде всего для создания RTL-описаний. Также он позволяет проводить симуляцию несложных модулей. Это удобно для создания юнит-тестов и моделирования работы различных алгоритмов. В плане симуляции не стоит возлагать на Chisel такие же надежды, как на System C или что-то подобное. Симулировать вы сможете лишь очень маленькие схемки, а генерировать — хоть целые кластеры из тысяч процессоров, вообще все, что захотите.На основе Chisel/Scala можно написать свой HLS-инструмент (High Level Synthesis), где одним росчерком пера вы будете создавать очень большие схемы, что с использованием одного Verilog невозможно. Наконец, в отличие от дорогостоящих САПР на рынке, Chisel — это open source-технология, распространяемая по лицензии Apache 2.0. При этом Chisel выделяется из open source-конкурентов тем, что на его основе есть целый ряд коммерческих решений. Например, чипы на основе процессорных ядер SiFive и Andes. И технология развивается быстрыми темпами: в год выходит по несколько новых версий.Пример проекта на ChiselРассмотрим пример простой цифровой системы с процессором, системной шиной и памятью, которые нужно соединить вместе, чтобы все заработало и можно было оценить площадь и производительность. Описание на SystemVerilog получится весьма объемным: объявить модули, задать их параметры, разобраться, как и что соединить, провести тесты.Теперь посмотрим, как это будет выглядеть на Chisel/Scala:(f << c(cp).* << b(bp).* << m(mp).*).!Мало что понятно, но здесь есть процессор, шина и память, которым переданы параметры. После нажатия на кнопку генерации все сгенерируется и код можно будет передать на вход САПР для требуемых оценок.Конечно, писать именно так не стоит, если только вы не хотите зашифровать свою систему. Я немного переписал программу и показал, что в ней происходит:(fabric <<   CpuGen(cpuParam).getAll <<   SysBusGen(sysBusParam).getAll <<   SysMemGen(sysMemParam).getAll ).generateВо второй строке объявлен генератор процессора, которому мы передаем параметры. getAll — это вызов метода, который возвращает все генераторы внутри нашего процессора. Их может быть сколь угодно много внутри одного IP ядра, но нам о них знать не нужно. А нужно просто их все подтянуть, используя этот вызов. В третьей и четвертой строке то же самое сделано для системной шины и памяти. В первой строке передаем это на фабрику, а в пятой вызываем метод фабрики generate, чтобы получить все необходимое: RTL, SDC, RDL, UPF, технологическую память в нужных конфигурациях и остальное.Подобный фреймворк уже реализовали в Berkeley под именем ChipYard. Описание, что я привел, может быть не совсем точным, но концепция такая же. Если вы разбираетесь в этом фреймворке, то минут за 20 вполне сможете сгенерировать довольно сложную систему.Кто-то может сказать, что это можно написать на C++ или Python. Да, можно. Но в этом случае вам придется изобретать очень много велосипедов, которые Chisel поставляет в готовом виде. Разработку на Chisel можно представить так: вы пишете на Verilog и можете в любой момент прямо в коде Verilog вызвать функцию C++. Или наоборот: писать программу на C++ и где-нибудь посередине метода С++ добавить кусочек на Verilog. Очень удобно.Различия описаний Chisel и SystemVerilogЕсли смотреть на простые вещи, то в принципе отличий немного. Вот объявление одной простой шины шириной WIDTH на SystemVerilog:logic [WIDTH-1:0] data;А вот на Chisel:val data = Wire(UInt(width.W))Описание на Chisel кажется более громоздким, но с описанием на SystemVerilog проще допустить ошибки в коде. Например, почти всегда в коде пишут –1, но иногда не пишут. Это по ошибке? Или автор так и хотел? Не всегда очевидно. В Chisel все тоже записывается в одну строчку, здесь мы без дополнительных арифметических операций создаем Wire с типом unsigned integer и шириной width. Scala — строго типизированный язык, и это дополнительно ограждает нас от ошибок. На Verilog вполне можно соединить что-нибудь несовместимое, но на Chisel из-за таких сложных UInt(width.W) ошибиться очень сложно. Например, сигнал типа SInt (signed integer) или Clock нельзя просто взять и подключить к нашему проводу.Посмотрим теперь, как создать и подключить две шины. В SystemVerilog требуется три строчки:logic [WIDTH-1:0]     data; logic [(WIDTH+1)-1:0] data_inc;  assign data_inc = data + 1’b1;В Chisel меньше, две:val data     = Wire(UInt(Width.W)) val data_inc = WireInit(data +& 1.U)Во второй строке на SystemVerilog мы создаем еще одну шину, в третьей подсоединяем к ней результат суммы data с единицей. Шина data_inc будет шире на один бит, что учтено во второй строке: +1 и –1 компенсируют друг друга.На Chisel мы создаем вторую шину, и, что интересно, нам не нужно объявлять ее тип — это будет тип результата арифметической операции внутри WireInit(). Амперсанд после плюса означает, что нужно расширить результат на один бит.Я привел для примера очень простой код. Но если у вас очень сложный дизайн, datapass-логика и при этом в SystemVerilog вы явно задаете типы каждого провода и регистра, то изменение какого-нибудь арифметического оператора в середине может привести к изменению ширины других проводов и регистров. Придется спешно править оставшиеся провода вручную.В Chisel можно заменить операцию в середине, и через WireInit инициализация типа автоматически пройдет по всему коду до выхода модуля. На маленьких дизайнах это незаметно, но на сотне тысяч гейтов вы оцените разницу. Подобный рефакторинг кода на SystemVerilog обеспечит вам выходные на работе, а на Chisel все будет исправлено за несколько минут.А вот так на Chisel могут выглядеть вычисления с матрицами:res := (a * b) + сКогда рядом добавлено очень много всевозможной арифметики, это описание выглядит очень наглядно и читабельно. Причем, если вы захотите, в сгенерированном RTL эта логика будет выражена в виде отдельных модулей, что упростит вам анализ STA-отчетов.Сравним теперь описание D-триггера с решением на SystemVerilog:logic my_ff;  always_ff @(negedge rst_n or posedge clk)   if (!rst_n)     my_ff <= 1’b0;   else     my_ff <= my_ff_next;И в Chisel:val my_ff = RegNext(my_ff_next, false.B)Наверняка каждый, кто много писал на Verilog, задумывался, зачем он пишет этот negedge, posedge, тянет clock по всей иерархии и допускает из-за этого ошибки. Инженеры в Berkeley тоже подумали об этом и сократили описание триггера до объявления RegNext. В этом примере на входе D-триггера — сигнал my_ff_next, а false.B означает, что после сброса на его выходе будет значение 0.А вот так будет выглядеть наше описание, если мы захотим поставить регистр на выходе умножителя матриц выше:val res = RegNext(a * b)SystemVerilog — это язык моделирования аппаратуры, предлагающий гибкое описание аппаратных элементов. Например, здесь можно описать логику работы RS-триггера. Но для разработчиков цифровых систем это не нужно. Помимо краткости записи, преимущество Chisel в том, что в дизайне не будет никаких «интересных» элементов. В записи на SystemVerilog можно допустить кучу ошибок: сделать latch, неожиданные мультиплексоры на входе, подать не тот сигнал на reset. В итоге все может прекрасно симулироваться, но когда вы дойдете до synthesis и будете изучать отчеты static timing analysis, рано или поздно всплывет ошибка. Еще хуже, когда в симуляции ваше устройство будет работать так, а после синтеза описания — иначе. Chisel вас полностью от этого оградит, ведь здесь вы занимаетесь исключительно цифровым дизайном.Еще одна особенность Chisel: в нем нет точек с запятой. Если вы долго пишете на Chisel, а потом переходите на SystemVerilog, это может стать большой проблемой.А где же clock?Все-таки в дизайне нужно где-то определить clock и назначить его триггерам. Предположим, у нас есть модуль:Объявим его на SystemVerilog:module my_module #(   parameter WIDTH = 8 ) (   input              clock,   input              reset,   input              io_data_i,   output [WIDTH-1:0] io_data_o );    ... endmoduleИ на Chisel:class my_module (dataWidth: Int = 8) extends Module {    val io = IO(new Bundle {     val data_i  = Input(Bool())        val data_o  = Output(UInt(dataWidth.W))      })      ... }Каждый модуль, каждая шина и проводок Chisel — это класс. Объявляя сигналы и провода, мы используем существующие классы Chisel, а в случае с модулем создаем собственный класс my_module (первая строка) и наследуем его из класса Module в Chisel. Clock и reset мы автоматически наследуем из базового класса, поэтому дополнительно объявлять их не требуется.Более того, если ваш модуль составной и в нем есть другие блоки, унаследованные от Module, то clock и reset к ним подтянутся автоматически. Перетаскивать их каждый раз не потребуется. Это очень удобно, это уменьшает число глупых ошибок в коде, ускоряя процесс разработки. В строках 3–5 выше я объявляю интерфейс этого блока: data_i как input, data_o как output. Если нужны свои clock и reset, это тоже решаемо: их можно объявить в этом интерфейсе. Потом обрамить нужные регистры и подмодули конструкцией withClockAndReset, и Chisel сам подтянет к ним ваши clock и reset. В простейшем же случае ваш модуль будет выглядеть так, будто clock и reset у него вообще нигде нет. Это существенно уменьшает количество ошибок в коде, но будет заметно только на крупных дизайнах.Мы рассмотрели важные различия в описании RTL на SystemVerilog и Chisel. Перейдем к тому, чего в SystemVerilog нет вообще. Функциональное программирование в ChiselС математической точки зрения, вы пишете в функциональном стиле, если в вашей программе функции последовательно вызывают друг друга: аргументами одной функции является результат другой, аргументы которой — результат третьей и т. п. Как правило, в такой программе нет изменяемых переменных, циклов и каких-то состояний, что снижает вероятность очень нехороших ошибок.Программисты определяют функциональное программирование чуть иначе: функция написана в функциональном стиле, если она не имеет никаких побочных эффектов. Побочные эффекты — это изменение чего-либо вне этой функции. В функциональном программировании функция должна работать только со своими аргументами и возвращать результат. Ее код не должен менять что-либо во внешнем мире.Поясню на примере. Представим, что ваша задача — пнуть мяч и попасть в ворота. Если программа написана функционально, то вы пинаете мяч, он летит, потребляет некоторое количество ресурсов, и результата у такой программы возможно два: попал в ворота или не попал. Если программа использует императивное программирование — как в C++, C, SystemVerilog — то вы пинаете мячик, а сзади вас может рухнуть дом. Потому что программа зависит от внешнего контекста и может его менять.Нам, разработчикам аппаратного обеспечения, ближе математическая трактовка функционального программирования: это парадигма, в которой процесс вычисления трактуется как вычисление значений функций в математическом понимании последних. То есть каждую логическую схему в дизайне мы можем выразить в виде функции с сигналами на входе и выходе.Допустим, у нас есть функция с четырьмя аргументами:Scala позволяет это красиво описать и на выходе возвращает кортеж:val (y0, y1) = func(x0, x1, x2, x3)Кортеж — это набор данных, причем они могут быть любого типа: y0, допустим, проводок, а y1 — вообще какой-нибудь сложный интерфейс. Простота записи очень удобна.Для примера рассмотрим поиск минимального значения у вектора данных, x0, x1, x2, x3.Вот как это будет выглядеть на SystemVerilog в самом простом варианте:assign y0 = (x0 < x1) ? x0 : x1; assign y1 = (x2 < x3) ? x2 : x3; assign y  = (y0 < y1) ? y0 : y1;Описание функциональное, но если нам нужно переменное число входных сигналов, оно не будет работать. Каждый раз придется руками добавлять новые assign в код.Попытаемся параметризовать наш код на SystemVerilog:always_comb begin   logic [WIDTH-1:0] tmp;   int i;   tmp = 2 ** WIDTH-1;   for (i = 0; i < LEN; i = i + 1) begin     if (tmp > x[i]) begin       tmp = x[i];     end   end   y = tmp; endЭтот код не зависит от количества входных сигналов: их может быть хоть несколько сотен. Но это поведенческое описание, хотя оно и успешно синтезируется. Лет 20–30 назад оно бы не синтезировалось, но со временем синтезаторы научились определять это в коде и сейчас легко выдают неплохую схему на выходе.Строго говоря, это описание выполнено в императивном стиле и в нем целый ряд проблем: временные переменные, арифметические операции, которые не связаны с задачей, ну и переменная tmp, которая непонятно что собой представляет и непонятно во что синтезируется. Ей много раз присваиваются какие-то значения. Все это — места типичных ошибок в RTL-описаниях.Посмотрим, как это будет выглядеть на Chisel:val y = x.reduce((n, k) => Mux(n < k, n, k))В отличие от SystemVerilog, у нас всего одна довольно простая строчка кода. Но что здесь происходит? На входе у нас есть некий вектор x. Для этого вектора целиком мы вызываем reduce, стандартный метод Scala. Он представляет собой функцию высшего порядка, которая принимает на вход другую функцию. Мы не хотим объявлять ее отдельно, придумывать ей имя, да и вообще не стоит создавать миллион функций, которые мы используем в коде лишь один раз. Для таких сценариев Scala предлагает так называемые анонимные функции — мы их объявляем и сразу же используем. В нашем случае у функции два аргумента, n и k. Тело функции — это мультиплексор Chisel: он вернет n, если n меньше k, а иначе вернет k. Через вызов нашей функции для всех элементов вектора x метод reduce преобразует этот вектор в скалярное значение, которое передается переменной y. Запись простая, и перепутать здесь можно только n и k местами, в отличие от описания на SystemVerilog, где поле для ошибок очень широкое.Помимо очевидных плюшек, преимущество кода на Chisel и в том, что он не зависит от типа данных. Эта функция может быть использована где угодно. На входе может быть не просто вектор unassigned шин, а какие-нибудь сложные интерфейсы. Имплементация не зависит от того, какой тип данных мы передаем, что очень удобно для больших дизайнов.Такие блоки работают на высоком уровне абстракции, и мы работаем здесь как архитекторы, не беспокоясь о том, что происходит ниже. Другие пользователи нашей имплементации могут просто передать с ней другой тип и получить иную логику. Достигается это за счет полиморфизма в ООП: оператор «<» может иметь разную имплементацию, и вызываться будет та, которая относится к типу переменной x. И эта имплементация может быть определена в совершенно ином месте.Но это же не дерево!Reduce вызывает функцию для всех элементов массива. Рассмотрим подробнее. Reduce вначале берет первые два элемента массива и вызывает для них функцию. Потом берет третий элемент и вызывает эту функцию с результатом. Для четвертого — опять функция с результатом и так далее. Мы получаем ту же самую логику, что и на SystemVerilog. Но и в Chisel, и в SystemVerilog у нас описано не дерево.Современные инструменты синтеза это понимают и способны построить хорошую схему и для Chisel, и для System Verilog. Если же вы принципиально хотите сделать дерево, то вот оно:val y = x.reduceTree((n, k) =>   Mux(n < k, n, k) )Мы заменили reduce на reduceTree, и наша анонимная функция стала, по сути, узлом дерева. Можно заглянуть в имплементацию reduceTree — это рекурсивная функция из 20 строк, которая синтезируется в Chisel. То есть в Chisel можно писать код рекурсивно и не беспокоиться, что какая-то САПР вас не поймет.Усложняем схемуТеперь допустим, что после каждого минимума нам нужно поставить регистр и запустить логику на 5 ГГц.В Chisel для решения понадобится четыре строки:val y = x.reduceTree((x, y) => {   val reg = RegNext(Mux(x < y, x, y))   reg })По сути, у нас изменилась только анонимная функция внутри reduceTree. Во второй строчке мы добавили регистр, у него на входе значение мультиплексора, которое выдает минимум. Каждая функция в Scala должна возвращать значение, и в третьей строке мы возвращаем reg. Узел нашего дерева лишь немного усложнился. Аналогичное описание на SystemVerilog будет гораздо больше. Тратить время на его изобретение не вижу смысла. Зачем, если я за полминуты могу описать эту схему на Chisel и она сразу заработает?Посмотрим на примеры результатов генерации кода. Получаем своеобразный netlist:module my_min(   input         clock,   input  [15:0] io_x_0,                 io_x_1,                 io_x_2,                 ...   output [15:0] io_y );   reg [15:0] io_y_reg;   reg [15:0] io_y_reg_1;   reg [15:0] io_y_reg_2;   reg [15:0] io_y_reg_3;   reg [15:0] io_y_reg_4;   reg [15:0] io_y_reg_5;   reg [15:0] io_y_reg_6;   always @(posedge clock) begin     io_y_reg <= io_x_0 < io_x_1 ? io_x_0 : io_x_1;     io_y_reg_1 <= io_x_2 < io_x_3 ? io_x_2 : io_x_3;     io_y_reg_2 <= io_x_4 < io_x_5 ? io_x_4 : io_x_5;     io_y_reg_3 <= io_x_6 < io_x_7 ? io_x_6 : io_x_7;     io_y_reg_4 <= io_y_reg < io_y_reg_1 ? io_y_reg : io_y_reg_1;     io_y_reg_5 <= io_y_reg_2 < io_y_reg_3 ? io_y_reg_2 : io_y_reg_3;     io_y_reg_6 <= io_y_reg_4 < io_y_reg_5 ? io_y_reg_4 : io_y_reg_5;   end   assign io_y = io_y_reg_6; endmoduleИмена получились читаемыми, код — легко понятным, описание — довольно простым. Это большое преимущество Chisel. Такое описание можно портировать в любую САПР, и она его обработает без всяких предупреждений и тем более ошибок. Все проблемы в коде сведутся к модулям проекта, написанным на SystemVerilog.Но здесь же кроется и самый большой недостаток Chisel: сгенерированный код получается очень большим. Проект выше совсем маленький, буквально игрушечный, в реальных проектах Chisel за несколько десятков секунд генерирует сотни тысяч строк кода. Если делать мощную процессорную систему, строки можно считать миллионами. К этому придется привыкнуть. Зато итоговый RTL будет очень простым: без interface, for, generate и т. п.Повторное использование модулейПредставим, что нам нужно найти какой-либо модуль в общей библиотеке, чтобы переиспользовать его и не изобретать велосипед. Какие возможности переиспользования дает SystemVerilog?Параметры модулей — у каждого модуля есть параметры, которые мы можем задать в своем проекте при использовании этого модуля. Есть глобальные макросы — ими лучше не пользоваться для параметризации модулей, но они тоже бывают удобны, например, для отделения FPGA-кода от ASIC-кода или передачи параметров из компилятора.Часто возникает проблема: «нужен такой же модуль, как в библиотеке, но другой». Путей решения здесь много. Допустим, нам нужно переписать 10% логики работы модуля.Что мы делаем в SystemVerilog? Берем нужный модуль из библиотеки, но боимся что-то менять. Ведь модуль уже используется в других проектах, и после изменений обязательно прибегут и скажут, что мы все испортили. У меня был случай, когда в модуле поменяли всего лишь комментарий и все равно нашлись недовольные. Чтобы такого не произошло, мы копируем модуль себе в проект, меняем его, делаем tape-out, и все должно отлично работать.Но представим, что в модуле из библиотеки был баг, который исправили, но только после того, как мы этот модуль себе скопировали. Исправление мы не заметили, ведь таких модулей у нас много и других дел тоже. В итоге в чипе оказался модуль с багом.Исправить ситуацию можно через изменения в общем коде. Но представьте: что-то добавили вы, что-то добавил ваш коллега из соседнего отдела, а что-то еще какой-нибудь студент-стажер. В итоге может получиться динозавр, которым вообще непонятно как пользоваться.Чем в этом плане лучше Chisel, а точнее, Scala? Здесь есть наследование.Каждый модуль в Chisel — это класс. Допустим, в библиотеке есть класс A. Мы наследуем из класса A свой класс B, вносим в класс B свои правки и делаем tape-out. Если в классе A поправили баг, это исправление автоматически подтянется к нам. Другие могут сделать на основе класса A модуль C или модуль D — и никто из нас не будет мешать друг другу, а исходный класс А останется «чистым» от правок соседей..Когда на SystemVerilog мы хотим написать универсальный библиотечный модуль, то задаемся вопросом: как сделать его таким, чтобы он решал все возможные задачи? Мы добавляем кучу сигналов, лишнего функционала, который надо долго тестировать. В Chisel вопрос другой: как написать модуль, чтобы он легко расширялся и любой мог его добавить к себе, унаследовать и использовать? Первый вопрос в принципе не имеет ответа, а второй вполне решаем.Итоги сравненияКакие преимущества есть у Chisel по сравнению с SystemVerilog?Простое описание сложных вещей. Код на Chisel где-то в 5–10 раз меньше (а бывает и в 100 раз меньше), чем на SystemVerilog. Это не касается конечных автоматов — здесь вам Chisel вряд ли поможет. Но если вы создаете сложные алгоритмы, коммутацию, процессорные элементы, работаете с n-мерными массивами, сложными структурами данных, Chisel раскроется в полной мере. Код менее подвержен случайным ошибкам. Это достигается благодаря строгой типизации Scala, уникальным подходам к описанию RTL, а также автоматическому заданию типов. Вообще, Chisel будет постоянно вас подталкивать к тому, чтобы вы делали меньше ошибок в коде. Бывает, что вы просто несколько часов исправляете ошибки, указанные компилятором Scala, и потом в симуляции сразу все работает.Очень простой рефакторинг. Иногда нужно поменять в коде что-нибудь солидное, много модулей, затрагивающих иерархию проекта. На SystemVerilog это очень долго. В Chisel вы меняете что-то в одном месте, запускаете компилятор и просто правите, что компилятор вам говорит. А потом все работает.Высокая портируемость кода. Можно не заморачиваться, будете ли вы работать на ASIC или на FPGA, какие инструменты использовать. Ваш код очень прост, любая современная САПР гарантированно его поймет. Плюс в Chisel вы можете легко играть со сгенерированным RTL, крутить код, как захочется, менять имена, иерархии модулей. Возможностей очень много.Перейду к недостаткам Chisel.Размер сгенерированного кода. Часто достигает сотен тысяч строк. С этим можно бороться, и со временем вы освоите нужные техники.Это тяжело и непонятно. В определенный момент вам придется пересмотреть свой подход к написанию RTL. Поначалу вы точно будете часами сидеть с Chisel над тем, что смогли бы за пять минут написать на SystemVerilog. Но если вы пересилите себя и не вернетесь в привычное русло, со временем снова переходить на SystemVerilog уже не захочется. Кривая обучения на Chisel очень крутая, и, возможно, в том числе по этой причине технология не так распространена в индустрии, как SystemVerilog.Статья написана по мотивам выступления Дениса Муратова на конференции «FPGA-Systems x YADRO 2024.02: Scala, RISC-V, open source и производительность». Посмотреть запись этого и других выступлений вы сможете в блоге YADRO."
37,37,"ETL-потоки «VACUUM FULL», или Как учесть особенности жизненного цикла данных в условиях высоконагруженных хранилищ",Сбер,"Технологии, меняющие мир",444.24,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-03-13,"Привет, Хабр! Наша команда работает в Сбере и отвечает за развитие аналитического хранилища данных (АХД), а также обеспечение финансово‑аналитической информацией ключевых подразделений. В нашем технологическом стеке используются Greenplum на основе PostgreSQL и Hadoop, что позволяет эффективно обрабатывать большие объёмы данных, гарантируя их надёжное хранение и своевременную доступность для потребителей.Основными потребителями данных являются бизнес‑подразделения банка, которые используют информацию для принятия стратегических решений и управления операционной деятельностью. Главная цель нашей работы — поддержка процессов построения различных видов отчётности:управленческой (для внутреннего анализа и планирования);обязательной (в соответствии с внутренними требованиями банка);регуляторной (для предоставления данных контролирующим органам).То есть наша деятельность напрямую способствует качественному принятию решений и соблюдению нормативных требований, что является важным звеном успешной работы компании.Перейдём к технологической стороне работы с данными. Стоит уделить внимание особенностям системы управления базами данных, а именно PostgreSQL. Эта СУБД использует механизм MVCC (Multi‑Version Concurrency Control) для управления параллельными транзакциями. Когда строки обновляются или удаляются, их старые версии, называемые «мёртвыми кортежами», не удаляются немедленно из физического хранилища. Вместо этого они помечаются как невидимые для новых транзакций, а занимаемое ими место остаётся незадействованным. Это критично в контексте работы с большими данными. Любой ETL‑процесс, включающий операции удаления (delete) и обновления (update), приводит к образованию «мёртвых кортежей». Эти кортежи занимают пространство и могут серьёзно повлиять на производительность системы.В PostgreSQL и аналогичных базах данных для освобождения места, занимаемого «мёртвыми кортежами», используются команды VACUUM (FULL) и её автоматизированный вариант — AUTOVACUUM.VACUUM — это критически важный инструмент обслуживания в PostgreSQL и Greenplum, который помогает поддерживать оптимальную производительность и эффективное использование хранилища. В свою очередь, AUTOVACUUM автоматизирует запуск VACUUM. Однако универсальность AUTOVACUUM становится недостатком в системах с высокими требованиями к производительности, ограниченными ресурсами или специфическими условиями работы. В частности, критичность данных для процессов подготовки отчётности. Проблемы избыточного потребления ресурсов, сложности с мониторингом, неоптимальная работа с большими таблицами и риски конфликтов с критическими задачами делают использование стандартного AUTOVACUUM не всегда приемлемым.Особенности VACUUMОсвобождение места. VACUUM сканирует таблицу, выявляя «мёртвые кортежи», которые можно освободить. Однако он не уменьшает физический размер файла таблицы, вместо этого помечая место как доступное для повторного использования.Предотвращение переполнения ID транзакций. В PostgreSQL каждой строке присваивается идентификатор транзакции (XID). Если этот счётчик достигнет предела, то могут возникнуть проблемы с целостностью данных, если старые строки не будут «заморожены». VACUUM помогает предотвратить это, замораживая старые строки и превращая их в постоянные.Обновление статистики. VACUUM ANALYZE объединяет функциональность VACUUM и ANALYZE. Помимо освобождения места, он обновляет статистику для планировщика запросов, что способствует улучшению производительности запросов.Неблокирующая операция. В отличие от VACUUM FULL, стандартная команда VACUUM не блокирует таблицу. Она позволяет читать и записывать во время её выполнения.Что такое VACUUM FULL?VACUUM FULL — это более ресурсоемкая версия VACUUM, которая перезаписывает всю таблицу в новый файл, исключая «мёртвые кортежи», и компактирует её на диске. Этот процесс физически уменьшает размер таблицы, постоянно освобождая место на диске. Основные особенности VACUUM FULL:Перезапись таблицы. VACUUM FULL создаёт новую копию таблицы без «мёртвых кортежей», эффективно уменьшая её размер.Блокировка таблицы. Во время выполнения VACUUM FULL таблица блокируется для записи (и иногда для чтения), что означает, что другие транзакции не смогут модифицировать таблицу до завершения операции.Ресурсоёмкость. VACUUM FULL требует значительных ресурсов ввода/вывода (I/O) и CPU, так как включает в себя копирование всей таблицы и её индексов.Временное место на диске. Процесс требует дополнительного места на диске, так как старая и новая версии таблицы существуют одновременно в процессе выполнения операции.Зачем комбинировать VACUUM и VACUUM FULL с ANALYZE?После выполнения команд VACUUM или VACUUM FULL полезно обновить статистику таблицы с помощью ANALYZE. Это гарантирует, что планировщик запросов имеет актуальную информацию о структуре таблицы и распределении данных, что улучшает производительность запросов. Комбинированное использование VACUUM и ANALYZE особенно важно в следующих случаях:Частые обновления и удаления. Таблицы, которые подвергаются частым изменениям (обновлениям и удалениям), могут иметь устаревшие статистические данные, что может привести к неоптимальным планам запросов.Большие таблицы. Для крупных таблиц актуальные статистические данные критичны для эффективного планирования запросовПосле VACUUM FULL, когда структура таблицы меняется, обновление статистики позволяет планировщику запросов адаптироваться к новой структуре.Что такое AUTOVACUUM и на что обратить внимание?AUTOVACUUM — это автоматическая система обслуживания в PostgreSQL, которая работает в фоновом режиме и выполняет задачи по очистке и оптимизации таблиц базы данных. Она была введена в PostgreSQL 8.1, чтобы минимизировать необходимость ручного вмешательства в обслуживание базы данных. Основная цель AUTOVACUUM — автоматическое выполнение команд VACUUM и ANALYZE.Однако есть свои особенности использования AUTOVACUUM:Избыточное потребление ресурсов. В системах с ограниченными ресурсами или высокой конкуренцией выполнение AUTOVACUUM может вызвать чрезмерную нагрузку на CPU, I/O и память, что снижает производительность. Невидимость для администратора. Так как AUTOVACUUM работает в фоновом режиме, его использование ресурсов и прогресс не всегда видны администраторам, что затрудняет диагностику проблем с производительностью.Проблемы с очень большими таблицами. Для больших таблиц AUTOVACUUM может не запускаться достаточно часто или работать слишком долго, что приводит к накоплению «мёртвых кортежей» и ухудшению производительности, о которой писали ранее.Конфликты с задачами. AUTOVACUUM может конфликтовать с другими задачами, такими как резервное копирование или массовая загрузка данных, если они требуют блокировки таблиц или интенсивного использования I/O, что приводит к задержкам и конфликтам.AUTOVACUUM имеет множество параметров конфигурации, такие как, autovacuum_vacuum_scale_factor, autovacuum_analyze_threshold, которые требуют тщательной настройки. Неправильные настройки могут привести либо к избыточному использованию ресурсов, либо к недостаточному обслуживанию.Несмотря на то, что AUTOVACUUM автоматически обрабатывает задачи обслуживания, он не всегда соответствует специфическим требованиям, таким как необходимость запуска VACUUM в часы минимальной нагрузки или сосредотачиваться на критических таблицах. Это требует дополнительного ручного вмешательства.Кастомный ETL‑поток «VACUUM FULL»В нашем аналитическом хранилище, где гарантированная и точная доставка данных потребителю имеет первостепенное значение, мы пришли к необходимости создать и внедрить кастомизированный подход к выполнению процессов VACUUM. Это позволило не только оптимизировать производительность, но и минимизировать риски, связанные с накоплением объёма «мёртвых кортежей», что могло бы повлиять на стабильность обеспечения бизнес‑подразделений данными. Мы выделили ряд важных факторов, которые требуют особого внимания для поддержания эффективной работы хранилища данных:Определённые временные окна для запуска VACUUM. Процессы VACUUM должны запускаться исключительно в заранее определённые временные окна, чтобы минимизировать влияние операций VACUUM на основные рабочие процессы системы и избежать конфликтов с задачами, связанными с регуляторной отчётностью.Гибкость в управлении процессами. Команда сопровождения должна иметь возможность управлять запуском процесса VACUUM без привлечения администраторов базы данных. Это подразумевает возможность временно отключить процесс или перенести время его запуска, что значительно повышает гибкость и автономность управления хранилищем.Ограничение нагрузки на систему. Для минимизации нагрузки на систему процессы VACUUM должны потреблять ограниченное количество ресурсов. Это достигается использованием специальной учётной записи, которая входит в определённую ресурсную группу, что позволяет эффективно контролировать использование вычислительных мощностей. Исключение ненужных таблиц из процесса VACUUM. Некоторые таблицы не требуют участия в процессе VACUUM, так как они полностью пересоздаются в ходе регулярных загрузок данных. Включение таких таблиц в процесс VACUUM было бы избыточным и привело бы к ненужной трате времени и ресурсов. Кастомизированная реализация позволяет исключать эти таблицы из обработки.Учитывая эти факторы, кастомизированный процесс VACUUM значительно повысит гибкость, контроль над операциями и оптимальное использование системных ресурсов. Эти факторы играют ключевую роль для обеспечения стабильной работы хранилища данных, особенно с учётом важности процессов, связанных с подготовкой отчётности.Реализация кастомизированного процесса «VACUUM FULLМы разработали и успешно внедрили ETL‑потоки «VACUUM FULL», которые автоматически выполняют операции VACUUM над таблицами, когда соотношение между количеством «мёртвых кортежей» (n_dead_tup) и «живых» строк (n_live_tup) достигает заранее установленного порогового значения. Данные о состоянии таблиц берутся из метаданных pg_stat_all_tables, что позволяет точно определять таблицы, требующие очистки.Для обеспечения эффективной работы этих потоков крайне важно поддерживать актуальность статистики по таблицам в кластере Greenplum. Поэтому мы уделили особое внимание регулярному сбору статистики, что гарантировало точность входных данных для принятия решений о выполнении операций VACUUM. Такой подход не только повысил производительность системы, но и минимизировал риски, связанные с неэффективным использованием дискового пространства и замедлением работы хранилища.Используемый ETL‑фреймворкЧтобы детальнее описать реализацию, стоит сделать небольшое отступление для знакомства с ETL‑фреймворком SberETL, используемым в нашей системе для создания и управления ETL‑процессами.SberETL поддерживает взаимодействие с различными СУБД и технологиями, обеспечивая работу с базами данных Greenplum, PostgreSQL, SDP Hadoop, ClickHouse, а также интеграцию с внутренними сервисами. Он поддерживает мультистрочные операции в одном сценарии с помощью оператора multi и условные операторы IF, что позволяет реализовывать сложную логику обработки данных. SberETL:работает с промежуточными переменными через плейсхолдеры и оператор var;поддерживает итеративную обработку данных с помощью циклов;использует структурные переходы для управления потоком выполнения сценариев;позволяет проверку условий и прерывание сценариев при выполнении заданных условий;взаимодействует с файловой системой для чтения и записи данных.На уровне SberETL реализована возможность использования шаблонов для создания сценариев или их отдельных шагов, что значительно ускоряет разработку и снижает вероятность ошибок.… Вернёмся к реализации кастомизированного процесса «VACUUM FULL»Применение SberETL в рамках этой задачи продемонстрировало свою полную обоснованность и высокую эффективность. Процессы «VACUUM FULL» были успешно реализованы в виде сценариев фреймворка SberETL, размещённых в сервисной схеме ***_db_dmetlfw. Структура сценария для потока «VACUUM FULL» включает в себя три последовательных шага), каждый из которых выполняет строго определённую функцию.Шаг 1. Получение переменной, содержащей перечень схем для последующей обработки. Данная операция реализована через SQL‑запрос к таблице метаданных ETL_TASK_PARAM, в которой лежат параметры для ETL‑процессов:/*variable column=2*/ select 'schemas' as key, param_val as schemas   from ***_db_dmetlfw.etl_task_param  where param_name = 'VACUUM_SCHEMAS200';Шаг 2. Формирование команд логирования и выполнения операции vacuum. На втором этапе для каждой таблицы из списка обрабатываемых схем формируются команды двух типов:команда выполнения операции VACUUM FULL ANALYZE;команда записи логов о начале и завершении обработки в таблицу debug_log.Для повышения производительности процесс реализован с использованием параллельного выполнения в пяти одновременно работающих потоках. Таблицы обрабатываемых схем делятся на пять групп с использованием формулы mod(relid::bigint, 5). Все группы обрабатываются параллельно.Важный момент: набор команд формируется динамически при запуске потока на основе данных из словаря базы данных (pg_tables). Это позволяет автоматически включать новые таблицы в процесс обработки без дополнительных настроек./*variable column=2*/ with a as (   select case when mod(relid::bigint, 5) = 0         then string_agg('/*sql*/ select count(*) from prc_debug(''Start ' || schemaname || '.' || relname ||''', ''[WORKFLOW_RUN_ID]'', ''sp_vacuum_full'');'||chr(10)||                 '/*sql*/ vacuum full analyze ' || schemaname || '.'|| relname || ';'||chr(10)||                 '/*sql*/ select count(*) from prc_debug(''Finish ' || schemaname || '.' || relname ||''', ''[WORKFLOW_RUN_ID]'', ''sp_vacuum_full'');',                 chr(10)) end as vacuum     from pg_catalog.pg_stat_all_tables    where ${schemas}        and n_dead_tup > 10000        and n_dead_tup / (case when n_live_tup = 0 then 1 else n_live_tup end)::numeric > 0.05    group by mod(relid::bigint, 5) ) select 'vacuum' as key,      coalesce(max(vacuum), 'select 1'::text) as vacuum   from aШаг 3. Выполнение сформированных команд. На заключительном этапе выполняется непосредственное исполнение команд, сформированных на предыдущем шаге. Для этого используется следующая конструкция:/*multi*/ {vacuum}Таким образом, каждый поток «VACUUM FULL» обрабатывает свой набор схем данных с заданной степенью распараллеливания. Потоки запускаются в соответствии с установленным расписанием, которое учитывает время выполнения для каждой конкретной схемы. Такая архитектура обеспечивает высокую производительность и гибкость системы, позволяя эффективно управлять ресурсами и минимизировать время обработки данных.ЗаключениеРазработанное решение для ETL‑потоков «VACUUM FULL» демонстрирует, что гибкость и контроль могут быть достигнуты через кастомизированную реализацию. Это решение позволило:запускать VACUUM в строго заданные временные окна, избегая конфликтов с основными процессами;точно управлять ресурсами и исключать из обработки таблицы, не требующие очистки;автоматизировать принятие решений на основе актуальной статистики (pg_stat_all_tables), обеспечивая реактивность системы;минимизировать зависимость от ручного вмешательства, передав управление второй линии сопровождения.Это решение позволило нам поддерживать необходимый баланс между автоматизацией и ручным управлением в высоконагруженной системе. AUTOVACUUM остаётся ценным инструментом, но его использование должно быть осознанным, дополненным оптимизациями, особенно там, где стандартные механизмы не справляются с уникальными вызовами производственного и бизнес‑процессов.  Авторы:Николай Абрамов, участник профессионального сообщества Сбера DWH/BigData. Профессиональное сообщество отвечает за развитие компетенций в таких направлениях как экосистема Hadoop, PostgreSQL, GreenPlum, а также BI‑инструментах Qlik, Apache SuperSet и др.Евгений Видман, участник профессионального сообщества Сбера DWH/BigData. Профессиональное сообщество отвечает за развитие компетенций в таких направлениях как экосистема Hadoop, PostgreSQL, GreenPlum, а также BI‑инструментах Qlik, Apache SuperSet и др."
38,38,"«Подписка на пончики», или Как получить облачные ресурсы со скидкой",Инферит,Компания,63.34,"Программное обеспечение, Аппаратное обеспечение",2025-03-13,"Когда речь заходит о стоимости облачных ресурсов, каждый лишний рубль может стать решающим для бюджета компании. В условиях растущей неопределенности, вызванной резкими изменениями в рыночной конъюнктуре, любая организация стремится сократить издержки.В этой статье мы разберем преимущества и риски резервного потребления (CVoS) от Yandex Cloud и подскажем, как FinOps-практики могут помочь максимально эффективно использовать этот инструмент.Облачные ресурсы под давлением спроса  Сегодня облачные технологии используются практически во всех аспектах цифрового бизнеса. Вне зависимости от того, предоставляете ли вы услуги на базе облачных приложений или разрабатываете инфраструктуру для анализа больших данных, использование облака становится стандартом для современных компаний. Данные по потреблению приводит Yandex Cloud:Результаты Yandex Cloud за 2023 год    Количество пользователей облачного провайдера выросло в 1,2 раза. Основной сегмент — это Enterprise (48%), а самым популярным сервисом остается инфраструктура и сеть (58%).Спрос на облачные ресурсы стремительно растет, и для бизнеса облачные технологии становятся неотъемлемой частью цифровой экосистемы. Компании все активнее переходят в облака, масштабируя инфраструктуру и оптимизируя процессы. Но этот рост — как взрывной пузырь, который требует продуманного подхода. Да, облако стало стандартом, но его потребление — это не просто новая норма, а новая зона ответственности. Именно здесь выходят инструменты для эффективного потребления ресурсов, одним из которых является CVoS.Да кто такой этот ваш CVoS?Теперь давайте углубимся в то, что представляет собой «подписка на пончики» от Yandex Cloud, и как она помогает компаниям экономить на инфраструктуре. Резервируемое потребление (или Committed Volume of Services) — это специальное соглашение, предоставляемое провайдером, которое позволяет клиентам резервировать определенный объем ресурсов на срок от 6 месяцев до 1 года с гарантированной скидкой.Представьте, что вы обожаете пончики и знаете, что будете покупать их каждое утро в пекарне возле дома. Вместо того, чтобы платить за каждый пончик по отдельности, вы договариваетесь с пекарем: заранее оплачиваете определенное количество пончиков на полгода или год вперед, а в ответ получаете щедрую скидку. И неважно, насколько подорожает мука или другие ингредиенты — для вас цена остается фиксированной, ведь вы заключили «сладкую» сделку заранее.По такому же принципу работает CVoS: если вы уверены в стабильном использовании ресурсов Yandex Cloud в течение конкретного периода, вы можете зарезервировать их заранее и получить скидку. Чем больше срок резервирования и объем потребляемых ресурсов, тем выше экономия. Это соглашение распространяется на все облака, подключенные к вашему платежному аккаунту, что делает его особенно удобным для компаний, использующих несколько проектов в одном аккаунте.Что еще может дать CVoS помимо скидки:Простота подключения: выбрать срок резервирования (6 месяцев или 1 год) и дату начала действия соглашения можно в личном кабинете без дополнительной бумажной волокиты.Автоматическое применение скидок: как только CVoS подключен, скидка на использование ресурсов применяется автоматически, что исключает необходимость следить за дополнительными настройками.Удобство бизнес-планирования: предсказуемость расходов — ключевой аспект для компаний, которые планируют свои бюджеты на квартал и год вперед. CVoS позволяет зафиксировать траты на заранее известный объем ресурсов.Важно помнить: договор дороже денег. В случае изменения объемов потребления в меньшую сторону вы обязаны оплатить зарезервированные ресурсы, даже если фактически их не использовали. Как и объяснить пекарю, что вы не голодны или пончики больше не нравятся, не получится. Поэтому подключение CVoS должно основываться на точных прогнозах и разумном управлении инфраструктурой. Но как понять, какие ресурсы можно зарезервировать без риска просчитаться?У вас есть два путиРазумный сценарий выглядит так: компания нанимает FinOps-специалиста, который анализирует данные по расходам на облако и предлагает шаги для покупки CVoS.Альтернативный сценарий: компания подключает свое облако к FinOps-платформе, которая автоматически анализирует расходы и предлагает рекомендации по резервированию потребления.Сравним два подхода с точки зрения выгод и затрат для компании:FinOps-специалистFinOps-платформаАвтоматизация и масштабируемостьОграничена объемом данных, которые специалист может обработать вручнуюПолностью автоматизированный процесс, способный обрабатывать тысячи виртуальных машин и сервисовЧеловеческий факторВозможны ошибки из-за усталости, субъективных решений или нехватки данныхОтсутствие ошибок, связанных с человеческим фактором. Используются точные алгоритмы и данныеЭкономическая эффективностьТребует значительных затрат на зарплату, обучение и поддержку специалистаБолее выгодна в долгосрочной перспективе, фиксированные расходы на подписку, меньшие затраты на обслуживаниеТочность данных и аналитика  Зависит от квалификации специалиста. Данные могут быть неполными или недетализированнымиПлатформа предоставляет детализированные данные и аналитические отчеты в реальном времениМониторинг и доступностьРаботает в ограниченные рабочие часы, нет круглосуточного мониторингаОбеспечивает 24/7 мониторинг и немедленные рекомендации по оптимизацииГарантия непрерывностиСпециалист может отсутствовать (болезнь, отпуск), что создает пробелы в мониторинге и анализеПлатформа работает непрерывно, обеспечивая постоянную поддержкуЕсли вкратце, то робот сумел превратить пустой холст в произведение искусства. Осталось разобраться, по каким критериям объекты облачной инфраструктуры попадают в рекомендации по резервированию потребления.Рекомендации по резервированию ресурсов от FinOps-платформы  Мы знаем, что пользователи Хабра не высоко ценят контент про продажи, поэтому здесь мы останавливаемся только на функциональных особенностях FinOps-платформы «Клаудмастер» (входит в экосистему «Инферит») и рассказываем о том, как именно предлагаются рекомендации по резервированию потребления и какие факторы учитываются при составлении рекомендаций.Процесс подключения и настройки рекомендаций по CVoS укладывается в 3 шага:Подключение облачного аккаунта Yandex Cloud через API и предоставление доступа к метрикам ресурсов, журналам событий и данным мониторинга. Это включает интеграцию с Yandex Cloud Billing для получения полной картины по затратам.Анализ инфраструктуры начинается автоматически. Примерно через 10 минут после подключения платформа «выносит приговор» и предлагает рекомендации по оптимизации.С помощью настроек можно детализировать мониторинг под конкретные потребности компании. Например, настроить интервалы анализа активности ВМ и другие параметры, чтобы рекомендации были максимально точными и соответствовали бизнес-целям компании.Для формирования рекомендации по CVoS «Клаудмастер» использует данные мониторинга загрузки CPU и RAM, журналы событий виртуальных машин и историю биллинга. Эти данные обрабатываются в реальном времени, чтобы понять, какие ресурсы используются стабильно и могут быть зарезервированы.Вот несколько ключевых факторов, которые учитываются при формировании рекомендаций:ВМ должны быть активны 24/7 на протяжении 14 или более дней. Этот параметр можно настраивать — например, если вам необходим больший или меньший интервал для анализа нагрузки.ВМ должны быть «непрерываемыми». Это означает, что они настроены на постоянное использование без возможности прерывания при перегрузке системы, что автоматически делает их кандидатами для долгосрочного резервирования.Рекомендации касаются только тех виртуальных машин, у которых гарантированная доля CPU составляет 100%. Это важно, потому что такие машины обладают предсказуемой нагрузкой. Это позволяет прогнозировать использование ресурсов и экономить на них без риска снижения производительности.Рекомендации по резервированию ресурсов поддерживаются только для сервиса Compute Cloud от Yandex Cloud. Это связано с тем, что сервис — основной потребитель инфраструктурных ресурсов (CPU, RAM) и имеет наиболее стабильные паттерны потребления, что делает его идеальным для анализа и резервирования через CVoS.При выполнении условий «Клаудмастер» формирует отчет, в котором указывается:Количество vCPU и объем оперативной памяти (RAM), которые могут быть зарезервированы для каждой виртуальной машины, работающей на поддерживаемых платформах (например, Intel Ice Lake или Intel Cascade Lake).Потенциальная сумма экономии для каждой группы ресурсов, исходя из возможного объема бронирования CVoS. Обычно это экономия в диапазоне от 10% до 15% от стоимости каждой группы зарезервированных ресурсов.Этот расчет основан на текущем использовании ВМ, а также на применении любых скидок, которые вы могли получить от Yandex Cloud через программы лояльности или другие соглашения.В Yandex Cloud существует собственный инструмент для формирования рекомендаций по резервированию ресурсов, однако сам провайдер не указывает, какие данные используются для формирования этих рекомендаций. Это может затруднить оценку эффективности рекомендаций и снизить предсказуемость экономии, что подчеркивает важность использования специализированных FinOps-решений для более точного просчета оптимизации расходов.Что имеем в итоге?CVoS от Yandex Cloud кажется отличным решением для экономии: вы резервируете ресурсы, получаете скидки и оптимизируете затраты. Но, как и в любом «бесплатном сырье», тут есть свои подводные камни. Если объемы или сроки были рассчитаны неправильно, вы рискуете платить за ресурсы, которые не используются. А это уже не экономия, а лишние расходы, спрятанные за красивыми обещаниями скидок.И вот тут на помощь приходит FinOps-платформа. Она не только анализирует, как вы используете облачные ресурсы, но и предлагает точные рекомендации, когда и на какие сервисы выгоднее подключать CVoS. «Клаудмастер» делает резервирование осознанным процессом: вы экономите там, где действительно нужно, и не попадаете в ловушки излишних обязательств.Так что CVoS — это полезный инструмент, но без грамотного анализа его использование может оказаться не таким выгодным, как кажется на первый взгляд. А с FinOps-платформой ваши скидки станут настоящей экономией, а не мнимой выгодой.И да, не забывайте иногда радовать себя пончиками.СсылкиМетодика попадания объектов в рекомендации и расчета экономии от «Клаудмастер»Документация по резервируемому потреблению от Yandex CloudРезультаты Yandex Cloud за 2023 годТелеграм-канал «Клаудмастер»"
39,39,Offset Explorer: разбираемся в базе,ИдаПроджект,Proptech разработчик №1,113.31,"Веб-разработка, Дизайн и юзабилити, Веб-сервисы",2025-03-13,"Всем привет! Меня зовут Макс. Я Lead Backend в компании ИдаПроджект и автор YouTube-канала PyLounge.  Любой крупный проект так или иначе используют брокеры сообщений — и чаще всего это Apache Kafka. Так что уметь взаимодействовать с Kafka (хотя бы на базовом уровне) будет полезно как разработчику, так и тестировщику или аналитику. Я в своей работе практически ежедневно взаимодействую с кафкой, поэтому решил поделиться опытом. В команде мы используем графический интерфейс Offset Explorer (далее OE), о котором сегодня и пойдет речь.Погнали!Что же тут будет (и не будет)В этом материале мы:узнаем, что такое Offset Explorer, зачем он нужен и как его установитьнаучимся подключаться как Kafka-кластеру через OEпознакомимся с интерфейсом OEузнаем, как создавать и просматривать топикиувидим, как просматривать и отправлять сообщение в топикинаучимся перечитывать топики кафки (редкий навык по моим наблюдениям)познакомимся с инструментами импорта и экспортаа также посмотрим на альтернативные варианты OE — AHKQ и Kafka-UIА вот чего здесь НЕ будет:информации про то, что такое Kafka, зачем нужна и как она работаетинформации о том, что такое топики, партишены, zookeeper, кластеры и т.д.Этот материал посвящен преимущественно Offset Explorer и его применению при работе с Kafka. Поэтому подразумевается, что читатель имеет хотя бы минимальное базовое понимание концепций Kafka :)Я продемонстрирую на практике все, что сам использую в повседневной работе. Материалов в Интернете про это немного (особенно на русском языке), поэтому начинающим специалистам пригодится в работе. Есть, конечно, официальная документация на английском, которую я тоже использовал для подготовки этого материала. Ссылки на все источники я обязательно предоставлю. P.S. Материал рассчитан на новичков и тех, кто хочет научиться базовой работе с Kafka со стороны НЕ ее администратора. Поэтому помните — не всегда то, что очевидно мне или вам, понятно кому-то другому :)P.S.S. Offset Explorer, как и любой инструмент, можно разбирать довольно долго. В этом материале рассмотрены базовые вещи, которые, тем не менее, покрывают 90% пользовательских кейсов, которые я вижу по своему опыту. Но держите в голове, что «темные пятна» все равно останутся и, возможно, в чем-то придется разбираться самому :)УстановкаДля начала необходимо установить Offset Explorer. Переходим на официальный сайт и скачиваем подходящий под вашу ОС вариант установщика (OE доступен для Windows, Linux и MacOS).WindowsВ случае с Windows скачивается обычный .exe-файл. Запускаем установщик и последовательно проходимся по формам установочного виззарда.MacOSВ случае с MacOS скачивается .dmg-файл. После загрузки открываем Finder и переходим в раздел «Загрузки». Дважды щелкаем на загруженный файл *.dmg, перетаскиваем значок приложения в папку «Приложения», чтобы установить приложение. Закрываем окно и нажимаем кнопку извлечения в Finder.LinuxВ случае с Linux открывается .sh-файл (скрипт для установки), который необходимо сохранить — например, комбинацией клавиш Ctrl + S. После этого переходим в каталог, куда сохранили файл, и запускаем его командой sh offsetexplorer.shЕсли файл не удалось запустить из-за нехватки прав — выставляем права на выполнение и запускаем снова.sudo chmod +x offsetexplorer.sh sh offsetexplorer.shЗапуск Kafka (опционально)Если у нас нет уже существующего сервера Kafka, к которому можно подключиться, то развернем его локально с помощью Docker. Для этого воспользуемся готовым образом kafka-kraft из DockerHub.Загрузим образ и запустим экземпляр контейнера на порту 9092 с помощью этой команды:docker run -p 9092:9092 -d bashj79/kafka-kraftДля управления Kafka из коробки доступен Kafka CLI. Мы можем использовать его, подключившись к контейнеру.Сначала узнаем имя (NAMES) или Container id контейнера с помощью команды docker psТеперь подключимся к контейнеру по его id:docker exec -it d9bcb5ec2669 shМожно, например, создать топик, посмотреть уже существующие и отправить в только что созданный сообщение:$ cd /opt/kafka/bin  # создаём топик pylounge.videos $ sh kafka-topics.sh --bootstrap-server localhost:9092 --create --topic pylounge.videos --partitions 1 --replication-factor 1  Created topic pylounge.videos.  # список топиков $ sh kafka-topics.sh --bootstrap-server localhost:9092 --list pylounge.videos  # отправить сообщение в топик $ sh kafka-console-producer.sh --bootstrap-server localhost:9092 --topic pylounge.videos  >django >fastapi >litestarЧто такое и зачем нужен Offset ExplorerOffset Explorer (ранее Kafka Tool) — это приложение с графическим интерфейсом для работы с Kafka.ОЕ позволяет быстро просматривать объекты в кластере Kafka, а также работать с сообщениями в топиках кластера.Что позволяет делать OE:просматривать содержимое кластеров Kafka, включая брокеров, топики и consumer’овпросматривать и добавлять сообщения в партициипросматривать и выставлять offset у consumerвыводить сообщения в JSON, XML и Avro формате с возможностью их локального сохраненияуправлять топиками, их конфигурациямиобеспечить поддержку протоколов и механизмов аутентификации и шифрования для Apache Kafkaписать свои собственные плагины для вывода кастомных форматов данных и т.д.Итак, мы установили OE и локально подняли Kafka. Переходим к подключению.Подключение к Kafka через Offset ExplorerЧтобы работать с данными в кластере Kafka, необходимо создать подключение. Это можно сделать с помощью пункта 'Add New Connection' в меню File или через контекстное меню каталога Cluster. В диалоговом окне при создании нового подключения необходимо указать следующие значения в разделе General:Cluster Name — имя, которое мы хотим дать кластеру (чисто для вашего удобства)Bootstrap Servers — серверы (с портами), настроенные в кластереВерсия — версия кластераZookeeper Host — имя хоста или IP-адрес хоста zookeeper в кластереПорт Zookeeper — порт хоста zookeeperchroot path — путь, по которому данные кластера Kafka отображаются в Zookeeper.Создадим новое подключение к запущенной в Docker Kafk’e.Вводим данные для подключения:Cluster Name — pylounge (самопиар)Bootstrap Servers — localhost:9092 (поскольку Kafka у нас запущена локально в Docker, и мы пробросили порты на 9092)Версия — 3.7 (просто оставляем по умолчанию)Enable Zookeeper access — в нашем случае галочку можно не ставить (у нас очень простой «кластер», zookeeper для управления настраивать нет смысла).Если кластер использует механизм безопасности PlaineText (такое, как правило, бывает в локальных или тестовых средах — как в нашем случае), то настраивать дополнительные атрибуты безопасности не нужно. Мы остановимся именно на этом варианте.Если же кластер настроен на SASL, то следует либо указать специальную конфигурацию JAAS в пользовательском интерфейсе, либо передать файл конфигурации JAAS в Offset Explorer при его запуске.В Windows нужно запустить Offset Explorer:offsetexplorer.exe -J-Djava.security.auth.login.config=c:/мой_jaas_файл.confДля Linux запуск другой:offsetexplorer -J-Djava.security.auth.login.config=/мой_jaas_файл.confНа MacOS потребуется помощь файла offsetexplorer.sh, расположенного в следующей папке:/Applications/Offset Explorer.app/Contents/java/appПеред этим необходимо настроить предоставленный файл мой_jaas_файл.conf в соответствии с конфигурацией кластера Kafka.Если вы не передаете файл конфигурации JAAS при запуске Offset Explorer, то его можно указать на вкладке JAAS Config для каждого соединения. Пример из документации:При использовании SASL Plaintext необходимо изменить свойство клиента sasl.mechanism на PLAIN. Это можно сделать в текстовом поле 'SASL Mechanism' в разделе 'Advanced'.Если кластер Kafka настроен на использование SSL, то необходимо задать параметры конфигурации SSL. В случае, когда брокеры Kafka не используют сертификат сервера, выпущенный публичным центром сертификации (СА), потребуется указать локальный сертификат с собственной подписью, которым подписан сертификат брокеров.Если SAN(ы) в сертификате вашего сервера не совпадают с реальным именем хоста брокеров, к которым мы подключаемся, при попытке подключения получим SSL-ошибку (No subject alternative DNS name matching xxx found). Можно избежать этого, сняв флажок 'Validate SSL endpoint hostname' в разделе 'Broker security'. При этом свойство клиента ssl.endpoint.identification.algorithm будет установлено в null.Если кластер требует наличия клиентского сертификата (двусторонняя аутентификация), то необходимо настроить атрибуты хранилища ключей. В keystore содержится закрытый ключ, который мы используем для аутентификации в брокерах Kafka. Также нужно настроить пароль для хранилища ключей и пароль для закрытого ключа в этом хранилище.Также можно подключиться к кластерам из облачных сервисов (например, Amazon), но об этом рекомендую почитать в документации OE (в разделе с подключениями).Чтобы убедится, что мы все верно сконфигурировали, нажмем кнопку Test для проверки подключения. Если соединение проходит — клацаем кнопку Add для добавления свежесозданного подключения.У нас добавился кластер pylounge. Чтобы подключиться к нему, вызовем контекстное меню и выберем пункт Connect. В этом же контекстном меню можно: отключиться, переподключиться, удалить подключение, клонировать его или экспортировать.Как только индикатор рядом с именем кластера загорелся зелёным, значит, мы успешно подключились и можем приступать к работе.Основные элементыВ левой панели есть три основных раздела: Брокеры (Brokers), Топики (Topics) и Потребители (Consumers).Кластер Kafka состоит из брокеров. По сути это набор узлов с запущенной Kafka и сконфигурированных определенным образом. Все брокеры соединены друг с другом сетью и действуют сообща, образуя единый кластер.Кликнув на каталог Brokers, можно посмотреть список брокеров. Видим, что в нашем кластере всего один брокер с id=1 с хостом localhost и работающий на порту 9092.Также можно выбрать брокер отдельно и посмотреть информацию о нем (увидим примерно то же самое).Выбрав каталог Topics, увидим список топиков нашего кластера (доступен поиск). Здесь можно создавать и удалять топики.Можно выбрать конкретный топик и посмотреть более детальную информацию о нем. Так, во вкладке Properties видно имя топика (Topic Name) и есть выбор формата отображения сообщений (для привычного вида выбираем тип String для свойств Key и Value). Также мы видим количество сообщений в топике Total number of messages — в нашем случае в pylounge.videos есть четыре сообщения.На вкладке Config можно редактировать настройки (свойства) топика: политику хранения сообщений, максимальный размер сообщений и т.д.На вкладке Data видно содержимое топика. Чтобы получить список сообщений, необходимо нажать на кнопку в виде проигрывателя с зелёной стрелочкой ().После извлечения записей можно просмотреть следующую информацию о каждом сообщении:Смещение (offset) — информация о смещенииКлюч (Key) — ключ сообщенияЗначение (Value) — значение сообщенияВременная метка — когда сообщение было добавленоЗаголовки — данные заголовкаКроме того, данные ключей и значений можно просматривать в формате JSON и XML. Эти данные можно увидеть, получив сообщения и щелкнув по одному из них. За выбор формата отвечает параметр View Data AS — Text, Hex, JSON, XML.Мы можем отфильтровать сообщения (поле Filter), которые отображаются в данный момент (на уровне топика или партиции), что позволяет найти конкретное сообщение с помощью следующих параметров фильтрации:Оффсет (Смещение) — если щелкнуть этот параметр, то фильтр покажет все сообщения, у которых в смещении указан фильтрKey (Ключ) — если щелкнуть этот параметр, фильтр покажет все сообщения, в которых фильтр введен в ключевые данныеValue (Значение) — при щелчке на этой опции фильтр покажет все сообщения, в которых фильтр введен в данные значенияHeader Key (Ключ заголовка) — если щелкнуть эту опцию, фильтр покажет все сообщения, в которых фильтр введен в данные ключа заголовкаHeader Value (Значение заголовка) — при нажатии на эту опцию фильтр покажет все сообщения, в которых фильтр введен в данные значения заголовкаRegex — поиск с использованием регулярных выраженийТакже можно задать порядок сортировки сообщений — сначала новые (Newest) или сначала старые (Oldest).Кроме того, сортировку (по возрастанию или убыванию значения) можно задать по любому из столбцов — необходимо лишь кликнуть на заголовок столбцаТакже мы можем указать, сколько сообщений хотим отобразить — поле Max Messages в правом нижнем углу. По умолчанию отображаем 100 сообщений.Вкладка Partitions отвечает за информацию о партициях топика. Партиции задаются при создании топика. У нашего топика всего одна партиция на localhost:9092.В самой партиции можно узнать ее номер, на каком брокере расположен лидер, а также посмотреть оффсеты — начальный (Start), конечный (End) и сколько сообщений в этой партиции (Size). Там мы видим, что начальный оффсет 0, конечный 4, и у нас в этой партиции всего 4 сообщения.Вкладка Replicas показывает реплики каждой партиции. У нас она всего одна на localhost:9092 в режиме In-Sync.Во вкладке Data можно просматривать сообщения из этой партиции и добавлять конкретно в нее новые (по аналогии со вкладкой Data у топика).Вкладка потребители (Consumers) отображает группы потребителей — приложений, которые читают сообщения из топиков Kafka.На данный момент список потребителей пуст, из OE такую группу потребителей создать нельзя. Consumer group создастся автоматически, когда приложение начнет читать данные из Kafka. Поэтому, чтобы добавить новую группу, нужно создать небольшое приложение (consumer) для чтения топика и настроить consumer так, чтобы он использовал некоторый group.idДля этого установим библиотеку kafka-pythonpython pip install kafka-pythonЗатем создадим файл consumer.py и добавим туда следующий код:from kafka import KafkaConsumer  # Настройка консумера consumer = KafkaConsumer(     'pylounge.videos',                      # Имя топика     bootstrap_servers=['localhost:9092'],  # Адрес Kafka-брокера     auto_offset_reset='earliest',    # Считывать сообщения с самого начала, если оффсет не найден     enable_auto_commit=True,         # Автоматически коммитить оффсет     group_id='consumer-from-python-app', # Идентификатор группы консумеров     value_deserializer=lambda x: x.decode('utf-8')  # Десериализация значений сообщений )  print(""Starting the consumer..."")  # Чтение сообщений из топика try:     for message in consumer:         print(f""Received message: {message.value}"") except KeyboardInterrupt:     print(""Stopping the consumer..."") finally:     consumer.close()Теперь запустим своего consumer с заданным group.id, и он появится в списке consumer groups в Offset Explorerpython manage.py consume.pyПосле запуска увидим следующие сообщения:Starting the consumer...   Received message: django Received message: fastapi Received message: litestarЕсли переключиться на каталог с потребителями, то увидим список всех consumer group с указанием их активностиЕсли выбрать определённого консумера, то на вкладке Properties увидим общую информацию — id, активность, тип консумера и где хранится его offsetНа вкладке Offsets видно, какой топик читает этот консумер и докуда он его прочитал. Сейчас стоит Offset 5 — значит, консумер прочитал все сообщения (потому что End = 4).Lag — это разница между офсетом, до которого дочитал консумер и последним офсетом в топике. То есть это дает нам понять, сколько сообщений потребитель НЕ дочитал до конца топика. В нашем случаем lag = 0, значит, консумер прочитал все сообщения из топика.Создание нового топикаЧтобы создать новый топик, можно либо перейти в папку Topics и воспользоваться кнопкой Add New Topic (иконка зеленый плюс на панели инструментов), либо воспользоваться пунктом меню Create topic (щелкните правой кнопкой мыши по каталогу Topics). В диалоговом окне Add topic необходимо указать следующие значения в разделе General:Имя — имя, которое вы хотите дать топикуPartition Count — количество партиций в топикеReplica Count — количество реплик для каждой партиции в топике.Давайте создадим топик flats, в который будут приходить сообщения о квартирах. Сразу укажем свойства cleanup.policy (политику хранения старых сообщений, delete — будет удалять старые сегменты, когда будет достигнуто их время хранения или предельный размер) и retention.ms (максимальное время хранения, прежде чем мы будем удалять старые сегменты, чтобы освободить место). Свойства добавляются в раздел Configs с помощью кнопки Add.После создания топика она добавится в каталог Topics.Давайте сразу перейдем в этот топик и заменим key и value на тип строки String, чтобы мы могли отправлять и смотреть сообщения в формате обычных строк (в конце не забываем нажимать кнопку Update).Добавление сообщения в топик через OEМы будем добавлять несколько сообщений похожего вида в формате JSON. Каждое сообщение описывает одну квартиру:{   ""id"": ""d4e4a1f0-fd3e-4e3c-bd0a-f6d0f97c7e01"",   ""description"": ""Уютная двухкомнатная квартира в центре города. Просторная гостиная, полностью оборудованная кухня, балкон с видом на парк."",   ""location"": ""Москва, ул. Арбат, д. 12"",   ""price"": 75000,   ""rooms"": 2,   ""area"": 65,   ""floor"": 5,   ""amenities"": [""Wi-Fi"", ""Кондиционер"", ""Стиральная машина""] }  {   ""id"": ""f3e7117b-7b5b-4c7b-9398-73f03f9a6d4c"",   ""description"": ""Однокомнатная квартира с современным ремонтом в новостройке. Панорамные окна, хорошая инфраструктура, близость к метро."",   ""location"": ""Санкт-Петербург, пр. Энгельса, д. 45"",   ""price"": 55000,   ""rooms"": 1,   ""area"": 40,   ""floor"": 10,   ""amenities"": [""Парковка"", ""Домофон"", ""Холодильник""] }  {   ""id"": ""a9b8e8b2-3d56-4b3c-bf8b-8e4c8b6f9cfa"",   ""description"": ""Трехкомнатная квартира с отличным ремонтом в тихом районе. Две спальни, просторная гостиная, две ванные комнаты."",   ""location"": ""Новосибирск, ул. Достоевского, д. 18"",   ""price"": 85000,   ""rooms"": 3,   ""area"": 90,   ""floor"": 3,   ""amenities"": [""Лифт"", ""Балкон"", ""Подогрев пола""] }Чтобы добавить сообщение в топик, необходимо выбрать нужную партицию (в нашем случае Partition 0) и перейти на вкладку Data. Там через элемент управление с иконкой «зеленого знака плюс» мы можем добавить одно или несколько сообщений. Выберем одно сообщение — Add Single MessageВ окне добавления для Key и Value выставляем загрузку сообщения в ручном режиме (не из файла) — Enter Manually [Text].Само тело сообщения копируем в блок Value, в качестве ключа сообщения будем использовать id квартиры — его и добавим в поле Key. Теперь нажмем Add. По аналогии добавим еще два сообщения.Просмотр сообщений в топике и фильтрацияЧтобы увидеть переданные сообщения, кликаем на сам топик (чтобы мы видели сообщения из всех партиций, а не только из нулевой), переходим на вкладку Data, выставляем Max Messages (можно оставить по умолчанию 100) и нажимаем на иконку «зеленого проигрывателя» — Retrieve Messages. После того, как сообщения отобразились, можно дополнительно кликнуть на заголовок Timestamp, чтобы отсортировать сообщения по времени отправки.Кликнем на любую ячейку в столбце Value, чтобы посмотреть само сообщение. Не забудем в выпадающем списке View Data As выбрать формат отображения JSON.Мы можем сохранить это сообщение в файл, нажав на иконку с сохранения (дискета).Также можно отфильтровать сообщения по ключу. Например, я хочу увидеть все сообщения у которых id = d4e4a1f0-fd3e-4e3c-bd0a-f6d0f97c7e01. Тогда нужно в поле Filter вставить этот id:Если мне нужны квартиры в описании которых есть слово «Новосибирск», то я тоже укажу это в фильтрах.Как перечитывать топики KafkaДопустим, производитель (producer) отправил нам в топик тысячу сообщений. Наше приложение их прочитало, но не смогло сохранить в базу данных поскольку в коде приложение была ошибка. Через какое-то время мы фиксим баг, и нам снова нужно прочитать эту тысячу сообщений из топика, чтобы записать их в БД.У нас есть два варианта, как это сделать:Заставить производителя ещё раз отправить эти же сообщения (что не всегда возможно).Перечитать сообщения из топика.Когда консумер читает топик, у него выставляется определенный оффсет, который указывает, до какого сообщения в топике он дочитал. Чтобы перечитать сообщения из топика, можно искусственно (ручками) сдвинуть этот офсет назад. Тогда консумер решит, что он дочитал только до офсета, который выставили мы, и будет заново пытаться прочитать все сообщения, которые идут после установленного нами офсета.Чтобы обновить офсеты через offset explorer, нужно выбрать раздел consumers, найти нашего консумера consumer-from-pylounge-app (или другой нужный). Затем выбрать нужные топики и задать офсеты (нажать Edit Consumer Offset, установить поле Offset), нажать Update.Перед тем как делать перечитку, надо отключить все сервисы-консумеры. А после того как офсет сдвинут, включить их обратно.Сейчас в топике flats всего три сообщения. Офсет начала 0, последний офсет 3:Наш консумер прочитал все эти сообщения — офсет 3, лаг — 0Допустим, мы хотим, чтобы консумер заново прочитал сообщения из топика flats, но не все, а пропустил первое сообщение.Для этого нам нужно:Выключить на этом время консумера.Для нужного топика нужного консумера нажимаем Edit consumer offset.3. Выставляем офсет на 1 (так как самое первое сообщение имеет офсет 0, его пропускаем). Затем нажимаем Update.4. Проверяем, что текущий офсет для этого топика стал 1. Заметим, что lag стал равен 2.5. Запускаем консумера python consumer.pyВидим, что наше приложение ещё раз прочитало второе и третье сообщение. Значит, мы успешно перечитали топик кафки с офсета 1!Импорт экспортИнструмент экспорта позволяет экспортировать данные из топиков в различные файлы. Когда мы открыли инструмент экспорта и выбрали топик для экспорта, необходимо выполнить несколько действий, чтобы экспорт прошел успешно:Выбрать директории, куда сохраним файлыПоставить формат шаблона имен ключевых файловЗатем — формат имени файла значенийНе забудем о количестве сообщений в каждом разделе, которые мы хотим экспортировать Партиции — выбираем, какие партиции в топике мы хотим включить в экспортСначала нам предложат выбрать топик, из которого мы хотим экспортировать данные:Затем укажем директорию для экспорта, что конкретно экспортировать из топика (в данном случае мне нужны и ключи, и значения сообщений), сколько сообщений и из каких партиций. Нажмем Next.За 102 мс экспорт завершился.Инструмент импорта позволяет импортировать данные в топик. После выбора топика для импорта данных необходимо выполнить несколько действий, чтобы импорт прошел успешно.Исходный каталог — выбираем папку с импортируемыми даннымиФормат шаблона имени ключевого файлаФормат имени файла значенийТопики — выбираем, в какую партицию (партиции) топика необходимо импортировать данныеПорядок действий аналогичен экспорту:Также можно экспортировать конфиг для подключения к кластеру. Для этого кликнем по название кластера и в контекстном меню выбираем Export Connection.Выбираем файл, куда сохранить конфигурацию (не забудем вписать название файла, а не только выбрать каталог).В итоге получим xml-файл с конфигом для подключения.Импортировать подключение из конфига можно с помощью пункта Import Connection директории с кластерами:ПлагиныOffset Explorer поддерживает пользовательские плагины на языке Java. Они позволяют просматривать сообщения, которые не понимает Offset Explorer, в формате, который мы сочтем нужным.Пример написания плагина находится в папке plugins/example в каталоге установки Offset Explorer.За подробностями — в официальную документацию.Альтернативы OEВ качестве альтернативы Offset Explorer выступают и пользуются популярностью еще два инструмента:Kafka-UI — https://github.com/provectus/kafka-uiAKHQ — https://akhq.io/Kafka-UI выглядит стильно, модно, молодежно, но лично я ей не пользовался, поэтому ничего не могу особо сказать. Но, возможно, вам она понравится больше.Чуть чаще в компании мы используем AKHQ.В качестве бонуса поделюсь инструкцией по запуску:Создаем директорию akhqВ этой директории создаем файл docker-compose.yml со следующим содержимым:version: '3.6'   services:   akhq:     image: tchiotludo/akhq     restart: always     environment:       AKHQ_CONFIGURATION: |         akhq:           connections:             docker-kafka-server:               properties:                 bootstrap.servers: ""localhost:9092""     ports:       - 9000:8080(bootstrap.servers — адрес и порт на котором живет кластер Kafka)3. Запускаем с помощью команды docker compose up4. Переходим в строке браузера по адресу localhost:90005. Наслаждаемся результатом.ЗаключениеOffest Explorer — удобный графический инструмент для работы с Kafka, у которого все же есть альтернативы. Выбирайте то, что вам удобнее, я же показал то, чем сам пользуюсь в 99% случаев на работе. Надеюсь, вам было полезно и интересно. Если есть что добавить или я где-то ошибся, милости прошу в комментарии :) Давайте сделаем этот материал лучше, чтобы он точно стал хорошим подспорьем для всех начинающих.А каким инструментом для работы с Kafka пользуетесь вы? Может Kafka CLI? :) Источникиhttps://habr.com/ru/companies/piter/articles/352978/https://offsetexplorer.com/documentation/docindex.htmlhttps://www.baeldung.com/apache-kafkahttps://habr.com/ru/companies/nsd/articles/661007/https://www.baeldung.com/java-kafka-consumer-lag"
40,40,Производные в реактивности,FirstVDS,Виртуальные серверы в ДЦ в Москве и Амстердаме,233.12,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-13,"С этой статьи я начну цикл материалов, посвященных базовым концепциям реактивности, основанных на идеях и примерах, которые подробно изложил автор SolidJS, Райан Карниато (Ryan Carniato), в своем блоге. Наша цель — разобрать фундаментальные принципы, лежащие в основе реактивных систем, и показать их применимость в различных контекстах.Обычно, когда вы впервые сталкиваетесь с реактивной системой, вводный пример выглядит примерно так:let name = state(""John"");  effect(() => {   console.log(""Hi"" + name); }) Мы будем использовать псевдокод, не ориентируясь на синтаксис конкретной библиотеки или фреймворка.Не нужно много усилий, чтобы понять, что при обновлении этого состояния происходит событие, которое вызывает побочный эффект. Также несложно реализовать это поведение самостоятельно. Но это далеко не вся история.Независимо от того, пытаетесь ли вы забыть React, строите представления на Vue или склоняетесь к Angular, эта тема актуальна. Она выходит за рамки виртуального DOM или сигналов. Прежде чем винить useEffect во всех бедах, давайте разберем ключевую часть реактивности: производные.Деривация и синхронизацияВы, наверное, уже сталкивались с производными значениями в своем любимом JS-фреймворке или реактивной системе. Возможно, они выглядели как useMemo, вычислялись автоматически или просто обозначались символом $. Но во всех этих случаях суть оставалась неизменной — они использовались для создания реактивных зависимостей. Но неизменным во всех этих случаях было то, что вам говорили в разных документациях, что они предназначены для создания реактивных зависимостей. A - это сумма B + C, даже если B или C меняются:let a = state(1); let b = state(1);  const c = memo(() => a + b);  effect(() => console.log(c));Возможно, вы уже знаете, что производное значение должно быть чистым, то есть не изменять никакое другое состояние. Но важно понимать, что это правило относится и к самому производному значению.На первых порах можно использовать следующую ментальную модель для представления производного состояния:function memo(fn) {   let internal = state();   effect(() => internal = fn());   return internal; }Однако такой подход не сработает должным образом.Большинство UI-фреймворков ориентированы на интерактивность: они принимают данные от пользователя, обновляют состояние и перерисовывают интерфейс. В упрощенной форме это можно выразить как UI = fn(state), но на самом деле это циклический процесс, повторяющийся снова и снова.С точки зрения реактивности это выглядит так:Причина в том, что конечному пользователю, взаимодействующему с пользовательским интерфейсом, необходима конситстентность. Все, что он видит (и не видит), должно быть синхронизировано. Вы не можете взаимодействовать с частью страницы, которая устарела, поскольку это создает ложные ожидания. Библиотеки пользовательского интерфейса планируют синхронное выполнение побочных эффектов, чтобы конечный пользователь мог доверять своим ощущениям.Это означает, что существуют определенные этапы выполнения кода. Библиотеки должны гарантировать, что все зависимые вычисления будут решены до рендеринга.Это позволяет понять, в чем именно заключается разница между подходами.Первый пример — деривация, когда производное состояние является функцией состояния, от которого оно зависит. let name = state(""John""); const upperName = memo(() => name.toUpperCase());  updateUI(name, upperName);Второй пример — синхронизация, когда изменение состояния приводит к обновлению другого состояния. Это важное различие.  let name = state(""John""); let upperName = state(); effect(() => upperName = name.toUpperCase());  updateUI(name, upperName);Во втором примере, в зависимости от вашей модели реактивности, может произойти не просто немедленное отображение всего, как ожидалось. Эффект может быть запланирован до или после рендеринга пользовательского интерфейса.При обновлении имени вы можете ненадолго увидеть обновленное имя вместе с предыдущим значением upperName. В таких фреймворках, как React, ваш компонент может быть запущен дважды. Потому что обновление состояния в эффекте запускает цикл заново.Консистентность без глюковДаже если вам удается синхронизировать состояние перед рендерингом, выражения все равно могут выполняться несколько раз с разными промежуточными значениями — возможно, неожиданными — до тех пор, пока весь граф зависимостей не стабилизируется.Производные устраняют эту неопределённость, обеспечивая предсказуемое и устойчивое поведение.Многие реактивные системы сейчас гарантируют, что при любом обновлении состояния каждый узел запускается только один раз, и когда он запускается, то не происходит сбоев. Под безглючным понимается то, что код, который разработчик предоставляет библиотеке, никогда не сможет показывать промежуточное или устаревшее состояние.Рассмотрим на простом примере работу кода:let a = state(1); const b = memo(() => a + 1); const c = memo(() => a + 1); const d = memo(() => c + 1); const e = memo(() => b + d);  effect(() => console.log(e));А теперь представим его в виде графа:  Разные системы функционируют по-своему, но в каждом случае мы ожидаем, что при первом запуске значение e окажется равным 5, хотя не факт, что это произойдет сразу.Кроме того, очевидно, что одни состояния могут зависеть от других. Например, если мы обновляем a = 2, то, независимо от механизма, понятно, что c должно вычисляться раньше d, а d — раньше e. Это важно, если мы хотим, чтобы каждый узел при новых значениях переменной обновлялся только один раз.Push vs PullКак же мы подходим к тому, чтобы узлы обновлялись только один раз? Обычно это начинается с одной из двух идей. Планирование (pull) и события (push). Давайте рассмотрим каждую из них на примере предыдущего раздела.Pull (Планирование)Идея состоит в том, чтобы начинать с побочного эффекта (целевого результата) и затем запрашивать значения переменных по мере их возникновения. Solid обычно воспринимается как система, построенная на «pull»-модели реактивности. В этой модели при обновлении состояния система запускает проверку, чтобы понять, нужно ли выполнять какую-то работу.Но что именно она проверяет? Наивно предположить, что проверяется всё подряд, ведь система не знает, что конкретно изменилось. Однако многие UI-библиотеки, включая Solid, построены на компонентах. Если состояние связано с конкретным компонентом, то при обновлении этого состояния система планирует повторный рендеринг компонента . «Pull»-системы, как правило, являются крупнозернистыми (coarse-grained). Они опираются на полное обновление сверху вниз, поскольку не обладают информацией о том, что именно изменилось. Если же представить более гранулярную «тянущую» систему, она не сможет определить, что произошло изменение, пока не проследит цепочку зависимостей до источника этого изменения — который, к слову, может и не существовать в её графе зависимостей. Такой дополнительный обход становится лишней работой, особенно если в итоге всё равно приходится выполнять обновление сверху вниз.В нашем примере на картинке мы запускаем эффект, который сначала запрашивает значение e. Однако мы не можем определить, изменилось ли e, без предварительного выполнения узлов b или d. Использование явных зависимостей (например, массивов зависимостей в React) позволяет нам выстроить путь вверх по графу зависимостей без необходимости выполнять промежуточные узлы. Таким образом, мы можем отследить зависимости e → b → a, выполнить b, затем отследить d → c → a, выполнить c, d, e, и только после этого запустить наш эффект.Если подумать, раз уж мы всё равно проверяем всю область (скажем, весь компонент) при изменении корневого элемента, хотя не все его части реально поменялись, получается, что многие из этих проверок просто лишние.Мемоизация (часто в форме производных), позволяющая ранние выходы, помогает оптимизировать этот сценарий, но подход, хотя и последовательный, в корне неэффективен.Push (События)Идея заключается в том, что обновление распространяется наружу от исходного состояния, которое было изменено. RxJS служит популярным примером реактивности, основанной на «push»-модели. В такой системе каждый узел подписывается на события изменения своих зависимостей. Получив уведомление об изменении, узел выполняется и, если его собственное значение изменилось, уведомляет своих “наблюдателей”.При этом стоит рассмотреть распространение обновлений в глубину, поскольку это соответствует тому, как выполнение происходит при первоначальном создании системы.Когда обновляется a, оно уведомляет b и c. Затем b выполняется и уведомляет e. После этого e запускается и видит обновленное значение b, но сталкивается с проблемой: d еще не выполнился и содержит устаревшее значение.У подхода с обходом в ширину (Breadth-first) есть схожая проблема. Поскольку d и e находятся на одинаковом расстоянии от источника a, это снова приводит к ситуации с устаревшими значениями. Система попытается выполнить b, затем c, после чего перейдет к e и d, но обнаружит, что d не был оценен раньше e.Системам на ""push""-модели сложно гарантировать нужное поведение без лишних вычислений. Один из подходов — топологическая сортировка, которая упорядочивает зависимости и помогает распространять изменения, но даже она не спасает от ненужной работы, если эффектов для обновления в итоге нет. Система знает, что изменится, но не может заранее предугадать итоговое влияние.Push-PullТретий подход заключается в объединении обеих техник — «push» и «pull». Сигналы (Signals) представляют собой де-факто гибридную реактивную систему «push-pull». Подписки и уведомления работают по принципу «push», а планирование выполнения задач осуществляется по принципу «pull». Благодаря этому планируются только те действия, которые действительно могут привести к изменениям, а всё остальное остаётся нетронутым.Рассмотрим наш пример: при обновлении a узлы b и c получают уведомления о возможных изменениях, а затем передают эти уведомления дальше — к e и d. В итоге эффект, который зависит от e, ставится в очередь на выполнение. Когда эффект запускается, он начинает подтягивать значения по мере необходимости, как в нашей гипотетической системе «pull», описанной ранее. Однако в данном случае в очередь добавляются только те эффекты, которые действительно могут быть затронуты изменениями.Такой подход позволяет точно знать, что именно изменилось и как эти изменения влияют на систему, гарантируя выполнение только необходимой работы.Если вас интересует более подробная информация о том, как работают алгоритмы «push-pull» в различных реализациях сигналов (Signals), вы можете ознакомиться с дополнительными материалами.Итог«Что можно вывести, то и нужно выводить».Эту фразу сказал Мишеля Вестрейта, создатель MobX, и это действительно принцип, который стоит взять на вооружение.Само собой разумеется, что последовательность, которую мы привыкли ожидать от наших библиотек и фреймворков, была бы недостижима, если бы мы опирались исключительно на состояние и эффекты. Когда эффект записывает данные в состояние, пройтись по графу зависимостей, чтобы понять, что нужно вычислить, становится невозможно. Зависимость попросту исчезает. Это не только неэффективно — это чревато ошибками.Тема эта невероятно глубокая, и я лишь слегка затронул её основы. Существуют и другие нюансы в противопоставлении «push» и «pull», и даже внутри одной категории систем можно найти множество тонкостей. В следующий раз мы углубимся в различия между ленивыми и нетерпеливыми вычислениями (lazy vs eager derivations), а также рассмотрим возможности работы с асинхронными операциями.НЛО прилетело и оставило здесь промокод для читателей нашего блога:-15% на заказ любого VDS (кроме тарифа Прогрев) — HABRFIRSTVDS."
41,41,Need for Speed: Most Wanted. Как легендарная игра уничтожила всю серию,Selectel,IT-инфраструктура для бизнеса,4.45,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-13," 15 ноября исполнится 20 лет с момента выхода самой успешной для Electronic Arts игры из серии Need for Speed. Most Wanted впечатлила и игроков, и прессу, и критиков. По всему миру на сегодняшний день продано около 17 млн копий. И ни одна из последующих игр серии не смогла побить этот рекорд, хотя геймдев в целом за это время прошел огромный путь. Более того, NFS MW стала настолько популярной и любимой игроками, что фактически похоронила все последующие части Need for Speed еще до их релиза. Как вышло, что Electronic Arts создали монстра, который их погубил? Давайте посмотрим.  Мы в Selectel готовим новый сервис. Если арендуете серверы в рабочих или личных проектах, нам очень поможет ваш опыт — записывайтесь на короткое онлайн-интервью. За участие подарим плюшевого Тирекса и бонусы на услуги Selectel.   Используйте навигацию, если не хотите читать текст полностью: → За что мы так любим Need for Speed: Most Wanted → Культ BMW M3 GTR → Неудачные попытки повторить успех → Нежелание внедрять best practices конкурентов → Недостаточная смелость в экспериментах → Выводы  За что мы так любим Need for Speed: Most Wanted Полицейские погони Предыдущая NFS — Underground 2 — была, безусловно, крайне хороша. По сравнению с первой частью мы получили открытый мир, расширенные возможности тюнинга, больше кастомизации, больше режимов и так далее.  Создавая Most Wanted, разработчики из EA Canada пошли проверенным путем, но при этом вернули игру к истокам. Да-да, я говорю о том, что уличные гонки снова стали по-настоящему нелегальными. Собственно, весь сюжет NFS MW крутится теперь не только вокруг стрит-рейсинга, но и вокруг противостояния с полицией.   Погоня в Need for Speed: Most Wanted. Источник.  Если раньше мы просто соревновались с ботами-гонщиками, то теперь в игру вмешивались боты-полицейские. Да и в режиме свободной езды появился риск нарваться на копов. Добавилась механика с уровнями розыска, в зависимости от которых полицейские использовали разные тачки вплоть до Chevrolet Corvette, а также шипы и даже вертолеты. Кстати, получалось у вас сбить своей машиной вертолет во время погони?  Потрясающий саундтрек Говорите что хотите, но лучшая музыка в серии NFS была именно в Most Wanted. Конечно, олдскулы все еще сводит и от Get Low из первого Underground (тот самый эрон-дон-дон), и от Riders on the Storm из второго. Но Blinded In Chains от Avenged Sevenfold или Decadance от Disturbed во время гонки в Most Wanted создавали просто непередаваемые ощущения.    Графика Тут надо, пожалуй, сделать оговорку. По современным меркам графика NFS MW вообще не такая, какую ждешь от культовой игры. Но для 2005 и нескольких последующих лет она была очень даже ничего. Кстати, движок Most Wanted достался в наследство от Underground 2, хоть и с некоторыми доработками. Но в целом это был тот же EA Graphics Library (EAGL).  В Underground 2 одной из его главных задач была быстрая прогрузка на лету открытых локаций, по которым игрок мог перемещаться, как ему вздумается. В Most Wanted на максимальных настройках в кадре помещалось обычно около 150 000 полигонов. Не так уж много по современным меркам. Да и если бы вы попытались разглядеть детали окружающего мира, то наверняка заметили бы их угловатость, плоскость и так далее. Но обычно по миру Most Wanted игрок перемещался на огромной скорости, так что во время активной игры все выглядело весьма недурно. Уж точно не хуже большинства других игр того времени.   Как выглядела Need for Speed: Most Wanted. Источник.  К тому же, EAGL предлагал и другие фичи. Среди них поддержка динамических теней, шейдоу-маппинг, HDR (хоть технически он еще не был полноценным), размытие по краям экрана, изменение глубины резкости, преломление света в каплях дождя на лобовом стекле, имитация приспособления зрения к изменяющимся условиям освещения (например, при выезде из туннеля на освещенную улицу) и так далее.  Сегодня никакого вау-эффекта все это не производит. Но то ли было 20 лет назад.  Кинематографичность Еще одна особенность NFS MW — кат-сцены, снятые специально для большего погружения игрока в сюжет. И мы не наблюдаем за действием со стороны. Персонажи, обращаясь к главному герою, разговаривают именно с игроком. Опять же — сейчас кажется странным записывать это в преимущество. Но прежде все гоночные аркады концентрировались исключительно на гонках, а Most Wanted сделал что-то новое для жанра — поместил самого игрока, а не только персонажа, в эпицентр событий. Кроме того, сцены были сняты, а не просто сгенерированы. Хотя их и знатно обработали, чтобы они не слишком выбивались из общего визуального ряда игры.   Сержант Кросс в одной из начальных кат-сцен. Разговаривая с главным героем, он фактически обращается к самому игроку. Источник.  Кажется, я увлекся ностальгией. Давайте уже поговорим о том, почему Most Wanted не оставил шанса всем остальным играм Need for Speed.   Культ BMW M3 GTR Всего в игре доступно 33 автомобиля, если не считать тех, на которых можно прокатиться в отдельных несюжетных режимах. 32 из них можно открыть по мере прохождения сюжета и купить в автосалоне. Ну или забрать до 14 машин у побежденных соперников, если повезет. Еще одна — это BMW M3 GTR, на которой мы начинаем игру, теряем в самом начале из-за жульничества антагониста Рейзора, а потом возвращаем после финальной гонки. Получить ее до завершения сюжетной линии нельзя, если только вы не читер.   Загрузочный экран Need for Speed: Most Wanted. Источник.  Именно эта машина стала буквально синонимом NFS MW. Electronic Arts даже пытались эксплуатировать ее образ уже в следующей части игры — Need for Speed: Carbon. Вообще-то это сюжетное продолжение Most Wanted, но давайте честно: это жалкая пародия на неповторимый оригинал. Возможность меньше минуты в самом начале прокатиться на той самой тачке не принесла успеха Карбону. Более того, разработчики хоть и добавили несколько новшеств в игру, все же не смогли воспроизвести ту самую непередаваемую атмосферу. Впрочем, об этом мы еще поговорим ниже.  А культовая BMW появлялась в играх серии NFS еще не раз. В том же обвесе и той же расцветке ее добавляли и в Carbon, и в ProStreet, и во многие другие релизы. Чего уж, реплику этой машины можно встретить даже в куда более поздних играх от других компаний. Попробуйте, например, зайти в автосалон в Forza Horizon 5, найти там BMW M3 GTR и посмотреть на пользовательские винилы и дизайны, созданные самими игроками.   Удивительное разнообразие винилов и дизайна для BMW M3 GTR в Forza Horizon 5.  Здесь можно возразить, мол, это творчество игроков. Разработчик Forza Horizon — Playground Games — не копировал машину из другой игры. Да, это так, но что вы скажете на проделки Rockstar? В GTA Online, как и во всех GTA, разработчики не добавляют точные копии автомобилей, чтобы сэкономить на лицензировании. Но они делают игровые модели максимально похожими на реальные прототипы. В игре есть марка Übermacht — гташный аналог BMW. И есть модель Cypher, собранная, судя по всему, из частей M2, M5 и 8. Но главное — в автомастерской ее можно покрасить почти так же, как была покрашена та самая BMW в NFS MW. Копия снова не точная, чтобы избежать юридических рисков, но неужели кто-то примет этот дизайн за совпадение? И это не пользовательский мод, а самый что ни на есть официальный контент от разработчика.   Уникальная раскраска «Демон скорости» для Übermacht Cypher в GTA Online. Обычно она заблокирована в игре, но иногда Rockstar открывают к ней доступ за выполнение внутриигровых испытаний. Источник.  Хотели ребята из Electronic Arts того или нет, но они создали икону. Для фанатов это была не просто машина, а настоящая легенда. Символ справедливости, если хотите, который в начале сюжета нечестно отобрали. Впервые запустив Carbon, я был крайне рад снова прокатиться на этой машине. Мою радость превзошло лишь мое разочарование от того, что все это оказалось только мишурой. Если бы в начале игроку дали любую другую машину, ничего бы в дальнейшем не изменилось. А сюжет и геймплей Carbon только усилили разочарование.  M3 GTR из NFS MW получилась настолько легендарной, что в BMW создали ее реальную копию к 20-летию игры. Неудачные попытки повторить успех Можете со мной не согласиться, но я считаю каждую игру серии Need for Speed после 2005 года всего лишь попыткой повторить успех Most Wanted. Взять к примеру тот же Carbon, вышедший в 2006. Как я уже сказал, это сюжетное продолжение NFS MW. Но с сюжетной связью мы разобрались — она длится около минуты, после чего обрывается. Герой попадает в аварию, BMW конфискуют и больше мы ее уже не увидим. Дальше игра не имеет никакой связи с предыдущей частью.  Да, в Carbon сохранилась примерно та же система полицейских погонь, но ничего нового мы не получили. Да и в более поздней Undercover, по большому счету, тоже. Она вообще, на мой взгляд (можете бросить тапком), была провальной.  Визуально Carbon вернулся куда-то к Underground. Конечно, тут появилась куда более продвинутая система теней, но ее еще нужно было рассмотреть, ведь все действие снова происходило исключительно по ночам. Впрочем, появились намеки на новые технологии для улучшения графики. И чисто формально Carbon в плане визуала был чуть более продвинутым, чем Most Wanted. Но тот факт, что действие теперь происходило ночью, свел практически на нет все усилия «отдела красивости» разработчиков.  Механика с использованием навыков напарника, разделение автомобилей по типам и классам, некоторые элементы симулятора (детали обвеса теперь влияли на аэродинамику и поведение машины) — все это само по себе, конечно, здорово. И это даже вызывало некоторый дополнительный интерес к игре. Но в целом все равно оставалось ощущение, что Carbon пытаются выдать за наследие Most Wanted, которым он не был. Разработчики словно экспериментировали, но при этом боялись сделать что-то совершенно новое, цепляясь за успех NFS MW.  В целом примерно все то же самое я могу сказать о любой следующей части Need for Speed. Ну и как тут обойти вниманием NFS MW 2012 года? Технически игра куда более совершенна, чем та, что вышла в 2005. На этом из плюсов все. Если Carbon был неудачной попыткой выехать на успехе Most Wanted 2005, то Most Wanted 2012 — это, скорее, работа над ошибками. Словно школьник получил двойку за сочинение, попытался все переписать, но получилось в лучшем случае на троечку. В игру вернули день, тени, HDR и вот это все. Но, на мой взгляд, это не помогло.   Отзыв о Need for Speed: Most Wanted (2012) в Steam. Хоть и положительный, но без сравнения с игрой 2005 года не обошлось.  Нежелание внедрять best practices конкурентов Давайте взглянем на другую гоночную аркаду, которую я уже упомянул в этой статье — Forza Horizon 5. Знаете, сколько там автомобилей доступно игроку? 894. Или возьмем GTA Online. Хоть это и не гоночная аркада, но гонки против других игроков там есть. Геймерам доступно более 700 видов транспортных средств, включая автомобили, мотоциклы, самолеты, вертолеты, лодки, спецтехнику и прочее.  А сколько машин было в играх Need for Speed? Пользователи Reddit посчитали за меня:   Количество автомобилей в разных играх Need for Speed. Чуть выше я говорил, что в Most Wanted доступно 33 автомобиля, а здесь автор утверждает, что их было 36. Возможно, дело в разных изданиях. Точно знаю, что в некоторых были доступны Chevrolet Camaro SS и Nissan 350Z, которых в моей копии не было. Источник.  Не стану утверждать, что заваливание игроков огромным количеством контента — это прямо вот best practice. Тем более, в той же Forza из почти 900 доступных автомобилей многие активно используют, ну, может, десяток-другой, не более. Тем не менее, конечный пользователь хочет иметь выбор. Предлагать ему полсотни автомобилей, когда конкуренты за ту же цену дают почти тысячу, несколько странно.  33 (или 36) автомобилей в 2005 году — это хороший показатель. Но это было 20 лет назад. А почему позже их почти ни в одной игре не было существенно больше?  Я неоднократно встречал утверждения, что во всех Need for Speed чуть ли не каждый полигон на модели автомобиля выполнен вручную ради безупречной детализации и точности, а в той же Forza машины — это эдакий фастфуд, жертва быстрого рендеринга 3D-модели, в которой не все выверено до миллиметра. Но это не совсем так.  Если упростить, то в нулевые годы каждая машина была 3D-моделью на основе полигональной сетки, обтянутой 2D-изображением. За детализацию отвечали несколько десятков тысяч полигонов, а за некоторые визуальные особенности — карты нормалей и кубические карты. Знаете что изменилось за 20 лет? Конечно, знаете — технологии.  Количество полигонов сейчас исчисляется миллионами, для визуальных эффектов подвезли полноценный HDR и PBR (Physically Based Rendering), для оптимизации — DLSS (Deep Learning Super Sampling) и так далее. А еще сами модели сейчас создаются с помощью лазерного сканирования, тогда как раньше их действительно рисовали вручную на основе чертежей и фотографий. И если все обобщить и упростить, то процесс создания игровой модели автомобиля для Need for Speed и для той же Forza Horizon плюс-минус одинаков. Выходит, создавать машины для NFS не сложнее, чем для любой другой гоночной аркады.   Кто упрекнет Forza Horizon 5 в недостаточной детализации или отсутствии визуального реализма?  Дело тут, скорее, в том, что Electronic Arts больше концентрируются на визуальном тюнинге, который доступен игроку. Еще со времен Underground мы привыкли к тому, что у каждого автомобиля должно быть хотя бы 4-5 вариантов обвеса, пара десятков спойлеров и т. д. Но вспомните первый Underground: модели бамперов, порогов и спойлеров там были одинаковыми для всех автомобилей, их просто чуть подгоняли под каждую модель. В Most Wanted у нас не было возможности установить спереди один бампер, сзади — другой. Мы могли выбрать для каждой машины один из нескольких готовых вариантов широкого кузова. Зато каждый из них был действительно уникальным. Не было двух одинаковых обвесов для двух разных машин. Да, это было круто, но сыграло с разработчиками злую шутку.  Я бы сказал, что они сами себя загнали в ловушку, установив в Most Wanted весьма высокую планку в части кастомизации транспорта. Игроки теперь ждали, что каждую машину можно будет модернизировать по-своему, и создателям Need for Speed пришлось оправдывать эти ожидания.  Но стала ли та же Forza Horizon хуже от того, что для большинства автомобилей есть всего один-два бампера (а для некоторых их вообще нет)? Вряд ли. Возможно, если бы в Electronic Arts решили пожертвовать детализированной кастомизацией ради количества, это пошло бы на пользу. Но увы, смелые эксперименты — это не про Need for Speed.  Я понимаю, что сравнивать Need for Speed и ту же Forza не вполне корректно. В конце концов, издатель Forza — Xbox Games Studios. Forza Horizon 2 вообще была эксклюзивом для Xbox и получала почти бесконечное финансирование от Microsoft (не то чтобы последняя Horizon 5 его не получала). Отчасти это и позволило добавлять в игры так много машин, покупая у автопроизводителей лицензии на каждую из них. Rockstar Games вообще не заморачиваются вопросами лицензирования, чуть меняя внешность реальных автомобилей и добавляя их в GTA под выдуманными брендами. Electronic Arts тут приходится сложнее всех: у каждой машины должна быть лицензия, которая стоит денег и делает разработку игры дороже. Недостаточная смелость в экспериментах Нулевые — время, когда игры Need for Speed были на пике популярности. Но потом наступили 2010-е, на арену вышли новые аркады и новые симуляторы. У аудитории появился выбор. Следовательно, игровое сообщество начало разделяться. Фанаты симуляторов и реализма ушли в Gran Turismo или Forza Motorsport, а ценители аркад стали пробовать что-то новое типа The Crew или Forza Horizon.  Electronic Arts оказались перед непростым выбором. Продолжать ли историю аркадных гонок с полицейскими погонями? Попробовать силы в симуляторах с акцентом на тонкостях управления автомобилем? Или дать игрокам простор для исследования открытого мира со множеством деталей и большим количеством машин? Кажется, в EA не смогли определиться.  Разработчики все еще цеплялись за успех Most Wanted, поэтому их попытки ступить на неисследованную территорию выглядели крайне робкими. Они пробовали сделать шаг в сторону симуляторов еще в 2009, когда выпустили Shift. Но игра не достигла успеха. Многие игроки тогда заявили, что они хотят аркады в духе NFS MW.  Продолжать развивать аркадный стиль? Да, попытки были, но разработчики словно искали что-то новое, боясь отпустить старое. Поэтому и новая версия Most Wanted в 2012, и Rivals в 2013, и Need for SpeedTM в 2015, да и многие другие игры получились какими-то безвкусными.  В поисках нового волшебного рецепта издатель Electronic Arts передавал разработку игр разным компаниям. В ней поучаствовали EA Canada, EA Black Box, Criterion Games, Ghost Games и другие. Вообще, если разобраться, то все это дочки Electronic Arts. Но как бы то ни было, у каждой студии было свое понимание прекрасного. А вот вся уникальность в результате была утеряна. Игры получались слишком разными, что само по себе даже хорошо. Но это разнообразие накладывалось на то, что от игры к игре фанатам предлагали все ту же историю про BMW M3 GTR и полицейских, только под разными соусами. Издатель словно каждый раз говорил: «Ребята, у меня для вас кое-что совершенно новое», но при этом пытался продать одну и ту же идею.  Выводы Скорее всего, лучшей игрой в серии Need for Speed навсегда останется оригинальный Most Wanted 2005 года. Это не значит, что Electronic Arts уже не сделают ничего хорошего в легендарной серии гоночных аркад. Возможно, если они будут достаточно смелыми, чтобы оставить Most Wanted в прошлом, они создадут новый шедевр. Но последние 20 лет говорят о том, что это вряд ли случится.  А каково ваше мнение о NFS MW? Делитесь в комментариях."
42,42,"Кто сообщает картам, где едет ваш транспорт",Timeweb Cloud,То самое облако,1677.09,Связь и телекоммуникации,2025-03-13,"Приветствую всех! Когда-то давно здесь уже был пост про то, как работает отображение движения транспорта на Яндекс-Картах. С тех пор мне не раз предлагали рассказать про то, что происходит на «транспортной» стороне: что вообще за устройства установлены на подвижном составе, в каком формате они передают положение и что с этими данными делают дальше. Самое время восполнить это упущение.    Итак, в сегодняшней статье поговорим про то, что за девайсы используются на транспорте, позволяя нам знать, когда приедет наш маршрут. Заодно попробуем их запустить, посмотрим на них в работе и узнаем, как работают информаторы в новых автобусах и трамваях.  ❯ Суть такова   Помню, когда-то давно я писал про транспортный автоинформатор. Тогда в комментариях справедливо заметили, что в новых моделях уже нет необходимости каждый раз жмякать кнопку для проигрывания объявления: всё происходит автоматически, едва транспорт подъезжает к точке с заданными координатами. Я всё ждал удобного случая, чтобы как-нибудь поинтереснее рассказать и о более новых экземплярах, и вот такой момент наконец пришёл.    Так вот. Очень часто информатор, управление маршрутными табло, контроль положения и других параметров, связь с диспетчером и ещё некоторые реализованы в одном-единственном устройстве, специально предназначенном для мониторинга городского транспорта. Именно о таких штуках мы и поговорим. И, конечно, посмотрим на них в работе.  ❯ Что это вообще за железки?   Обычно такие девайсы называются бортовыми навигационно-связными терминалами. Некоторые экземпляры (например, Гранит-навигатор) в инструкции именуются возимыми радиостанциями.     Наибольшее распространение они получили после 2018 года, когда всех пассажирских перевозчиков обязали установить практически на весь свой транспорт оборудование ГЛОНАСС. Впрочем, в крупных городах подобные устройства использовались и ранее: их установка обеспечивала улучшение контроля за движением и за соблюдением расписания.  ❯ Автоинформаторы Помимо упомянутых БНСТ существуют и «традиционные» информаторы, оснащённые приёмником GPS/ГЛОНАСС исключительно для автоматического объявления остановок. Как нетрудно догадаться, принцип их работы довольно схожий.    К примеру, вот такие вот девайсы модели ИР-0651 стоят практически во всех питерских трамваях.    А вот информатор «Орбита» с таким же принципом работы.  ❯ Обзор оборудования Ну а пока что взглянем на девайсы, которые мы сегодня будем пробовать оживлять.    Первым на очереди идёт вот такой вот Гранит-навигатор. Девайс, хоть и испытал немало жизненных ударов, всё ещё не рассыпается и выглядит рабочим.    С обратной стороны кронштейн и разъёмы MicroFit для подключения. С нижнего торца кнопка Reset, разъёмы для SIM-карты и microSD, а также гнездо miniUSB для подключения к компьютеру и 3,5 мм для тангенты. Также видны прорези контрольного динамика, однако в моей модификации его нет.    А вот это Орбита. Пожалуй, самый распространённый девайс такого назначения на транспорте в нашей стране.    С обратной стороны мало интересного.    Снизу два разъёма. По бокам ещё три — под карту памяти (полноразмерную SD), miniUSB и 8P8C для подключения тангенты. Сверху красная кнопка SOS. По назначению она несколько отличается от той, что можно увидеть в системах вроде «Эра-ГЛОНАСС»: действие при её нажатии настраивается в ПО для конфигурации.    Тангента для объявлений в салон или для связи с диспетчером.  ❯ Гранит Самое время попробовать запустить. Начнём с «Гранита». Согласно распиновке из инструкции подаём питание — ничего не происходит.    Разбираем. В глаза сразу бросается отлетевший конденсатор справа. Один из них всё ещё припаян, а вот второй просто валялся в корпусе. Девайс построен на микроконтроллере LPC2468FET208 на ядре ARM7TDMI. Также на плате расположены GSM-модем, ГЛОНАСС-приёмник Geostar, GPS-модуль Fastrax и керамическая антенна. В белом прямоугольнике некогда был приклеен аккумулятор, который, видимо, когда-то давно вздулся и был выкинут. Разъёмы по периметру промазаны некоей гадостью, в которой легко угадывается обычный поксипол. На нераспаянном разъёме видна нумерация контактов.     Обратная сторона платы.    На дисплее маркировка UDLP-GT240128B, даташит сходу не гуглится.    Припаял отвалившийся конденсатор на место. За кадром выяснил и причину неисправности: отвалилась пайка одного из контактов разъёма. Расковырял эту заливку и пропаял всё нормальным свинцовым припоем.    Подал питание, и девайс успешно ожил. Можно собирать.  ❯ Запускаем Гранит   При подаче питания девайс отображает вот такую заставку. К слову говоря, некоторые модификации и вправду использовались для контроля легкового транспорта.    На главном экране отображаются координаты, скорость движения, расстояние до цели, число спутников, объём трафика, загрузка процессора, аптайм, напряжение аккумулятора и ещё ряд параметров.    Первая кнопка — вызов диспетчера. Здесь ничего особенного: при её нажатии девайс совершает звонок на прошитый в него номер.    Вторая кнопка передаёт сообщения диспетчеру. Таким образом, если это настроено на сервере у перевозчика, водитель может вызвать техпомощь, сообщить о причине задержки рейса или о том, что тот пройден без проблем.    Наконец, две последних — информатор и громкая связь.  ❯ BlazeMaster Больше всего в этом устройстве меня интересовал, конечно же, информатор. Ведь интересно же не просто запустить эту штуку, а попробовать загрузить в неё записи и посмотреть, как всё это работает.    Для настройки этой штуки используется такой софт как BlazeMaster. Хотя он не гуглится, найти его не составляет проблем: он лежит на сайте производителя (Сантэл-Навигация).    У меня нет реально использовавшихся в каких-то городах записей (ни для Гранита, ни для Орбиты), так что пришлось создать какой-то тестовый маршрут.    Для каждой остановки выбираем координаты, в пределах которых она будет активироваться.     Теперь, если загрузить эти данные в память девайса, при его попадании в указанные координаты он будет выполнять требуемые действия (например, проигрывание записи, вывод текста на табло или отправку на сервер данных о том, что остановка пройдена). Водителю требуется только выбрать маршрут, дальше всё будет происходить само.    Но, к сожалению, заставить его видеть файлы маршрута у меня так и не вышло. Я пробовал разные карты памяти, форматировал их в FAT и FAT32, однако аппарат упорно прикидывался кирпичом и файлов не видел. Пообщавшись с теми, кто занимался этими приборами, узнал, что под него было несколько прошивок и, вероятно, у меня стоит сборка, где информатор отключен.  ❯ Орбита Теперь очередь второго экземпляра. Этот навигатор — один из самых массовых в нашей стране, устанавливается на автобусы, троллейбусы, трамваи и маршрутки. Возможно, вы сами даже видели данный аппарат в кабине у водителя, стоя на танцплощадке у передней двери.    Вот он в кабине трамвая 71-931М «Витязь-М».    Он же в электробусе БКМ Е321.    Орбита-Информатор в модернизированном трамвае 71-605 (маленькая штучка с тремя кнопками над пультом).    Автобус ЛиАЗ-6213 с весьма характерного вида табло из комплекта «Орбиты».    Что мы делаем, едва заполучив интересный девайс? Естественно, мы его разбираем. Тут для этого надо открутить шесть саморезов с обратной стороны. Никаких пломб нет, так как снятие крышки необходимо в том числе для установки симки и карты памяти.    Плата управления. GSM-модуль SIM900R, урезанная версия некогда очень популярных в среде AVRщиков модулей SIM900 и SIM900D. Довольно специфический МК dsPIC33EP256MU810, выпускаемый Microchip специально для автомобильной электроники.    Обратная сторона платы. Дисплей на этот раз текстовый от МЭЛТ, HD44780-совместимый. Маленький аккумулятор поддерживает работу устройства, когда двигатель автобуса заглушен или низковольтные цепи трамвая или троллейбуса отключены. Реле коммутирует салонный динамик, благодаря чему при отсутствии трансляции он не издаёт посторонних звуков.    Этот экземпляр оказался полностью рабочим. При подаче питания он сходу загрузился и автоматически подтянул дату и время по GPS.      Интерфейс здесь на этот раз текстовый. В плане пунктов меню он практически полностью идентичен предыдущему экземпляру, так что демонстрировать его подробно я не буду.    Разумеется, я не упустил возможность вставить симку и позвонить прямо на этот девайс.    Ну и, конечно же, можно сказать что-то по громкой связи.  ❯ Передаём данные У меня нет софта, который используется организаторами перевозок, поэтому для демонстрации работы железки воспользуемся чем-то более простым. Большинство таких транспортных навигационных терминалов могут отправлять данные в формате NDTP (Navigation Data Transfer Protocol) и EGTS (протокол Эра-ГЛОНАСС). «Орбита» не исключение. Второй протокол нас и интересует: его могут принимать некоторые открытые сервисы для мониторинга транспорта. Для нашего девайса воспользовался сайтом free-gps.ru, основанным на софте Traccar. Выбран он был благодаря упоминанию данного прибора на сайте.     Так что регистрируемся и добавляем наш девайс. В качестве идентификатора указываем IMEI.     Чтобы аппарат мог передавать данные, его необходимо настроить. Для этого ожидаемо есть программа-конфигуратор. Подключаем прибор к компу, выбираем в настройках COM-порт и при его выборе прожмякиваем галочку о перезагрузке после записи. Далее пробуем установить связь и прочитать данные. И, если всё было успешно, в полях покажутся актуальные значения.    Теперь настраиваем параметры для связи с сервером.    Записываем конфигурацию. Девайс подвиснет, перезагрузится и напишет «USB Bootloader». Ещё через полминуты он будет готов к работе.    Втыкаем симку, включаем аппарат и ждём чуда.     И, если всё было настроено правильно, прибор зарегистрируется в сотовой сети, соединение установится, а на сайте появится местоположение. Если не работает — скачиваем обновление прошивки и устанавливаем его.   ❯ Выход в город Увидеть на карте метку с этим девайсом, конечно, круто, но как насчёт прогуляться с ним по городу и посмотреть, каких данных он там накидает?     Больше всего думал над питанием: от USB ни один экземпляр не работает, а брать с собой свинцовый аккумулятор от ИБП не хотелось. В итоге по-быстрому был собран вот такой девайс, состоящий из сгоревшего энергобанка (обычно в них дохнут аккумуляторы, а тут, на удивление, вылетела плата зарядки, которую у меня всё не доходят руки попробовать оживить), некоторого количества проводов с разъёмами, платы защиты и модуля повышающего преобразователя на чипе MT3608. Поскольку это всё разовый опыт (хотя после этого я всерьёз задумался над тем, что надо бы собрать компактный автономный источник питания на 12 или 24 В), просто замотал детали в пупырку.    Собранная конструкция подозрительно напоминала бомбу работала так, как и было задумано.    Убеждаемся, что связь установилась, и едем по своим делам.    А вот так в нём работает GPS-спидометр.    Вернувшись домой, смотрим, что же передал прибор.    Здесь всё так же, как с обычными GPS-трекерами (потому что по сути это и есть навороченный GPS-трекер): маршрут, пробег, скорость и тому подобные параметры.    Видно, что девайс передаёт данные не постоянно, а с довольно большими интервалами (тридцать секунд). Это создаёт необходимость дополнительных расчётов для того, чтобы транспорт плавно ехал по карте, а не двигался рывками.  ❯ Что дальше делают с этими данными? Во время движения ТС девайс передаёт данные на сервер депо, парка или же сразу организатору перевозок. Принятые от обслуживаемого предприятием транспорта данные обрабатываются для оценки работы этого маршрута, а также преобразуются в какой-то воспроизводимый формат (например, небезызвестный в узких кругах GTFS) и передаются или продаются другим сервисам (тем самым 2GIS, Яндекс и другим). В разных городах дела с форматом, передачей и прочими нюансами обстоят по-разному: где-то «сырые» данные лежат в открытом виде (например, в Питере они есть тут), где-то с их получением сложнее (в Челябинске некогда можно было забрать тут, но сейчас такой возможности нет). Помню, как в Челябинске несколько месяцев не работал Bustime, так как тот источник данных, откуда он их брал, перестал быть доступен.  ❯ Про маршруты   Помню, в посте от Яндекса меня удивило, что номер маршрута не «подтягивается» из передаваемых бортовым устройством данных (ведь, казалось бы, он должен их передавать), а рассчитывается исходя из трассы. Как оказалось, EGTS и вправду их не передаёт, при выборе такого протокола эти параметры передаются на сервер автопарка отдельно.    NDTP же оказался более специализированным для городского транспорта: вся необходимая информация там есть. Почему для определения маршрута используются алгоритмы привязки, мне неведомо: возможно, это сделано для совместимости с навигаторами, которые ничего подобного не передают (и которых в каких-то городах подавляющее большинство), возможно, из-за того, что организатор перевозок не выдаёт эти данные, а, возможно, для борьбы с глюками из-за того, что водитель забыл переключить автоинформатор или вовсе выставил на нём, что едет в парк.  ❯ Что же в итоге? В отличие от некогда показанного информатора, эти девайсы — не древность из начала нулевых, а вполне себе актуальное оборудование, использующееся на транспорте и сейчас. Впрочем, заполучить их в коллекцию и увидеть в работе было от этого не менее интересно.    Отдельно стоит сказать про GPS-приёмник «Орбиты»: этому экземпляру удавалось найти спутники даже в глубине квартиры. Единственным имеющимся у меня девайсом, который тоже так может, оказался КПК Asus MyPal A639 (тот самый КПК с откидной GPS-антенной). Такие дела.  ❯ Ссылки  Тот самый пост Яндекса Аналогичный пост от 2ГИС Гранит-Навигатор 02 Софт Гранита Софт Орбиты Настройка трекера ОРБИТА Навигатор Использовавшийся сервис для бесплатного мониторинга Тот самый Traccar   Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩   Перед оплатой в разделе «Бонусы и промокоды» в панели управления активируйте промокод и получите кэшбэк на баланс."
43,43,Особенности национальной верстки: PWA,SimbirSoft,Лидер в разработке современных ИТ-решений на заказ,72.82,"Веб-разработка, Программное обеспечение, Мобильные технологии",2025-03-13,"Всем привет! В этой статье мы не будем в очередной раз размышлять о том, почему бизнес и пользователи все чаще начинают отдавать предпочтение PWA вместо мобильных приложений, какие у них есть плюсы, минусы и так далее.Сегодня мы сосредоточимся на проблемах (а точнее на одной конкретной), с которой вы можете столкнуться, решив сделать свое веб-приложение прогрессивным.Забегая вперед, сразу скажу, кому статья может быть полезна. Не столько важно, являетесь вы владельцем продукта или разработчиком. Если вы создаете PWA-приложение, в котором планируется реализовать открытие ссылок на сторонние ресурсы в браузере, вы можете столкнуться с неочевидной проблемой в верстке. Об этом я, frontend-разработчик IT-компании SimbirSoft Эльвина, расскажу в статье. Дисклеймер:И да, сразу оговорюсь, что это скорее сторителлинг, никакого rocket science тут не будет. Рассказываю о том, как баг нашли, локализовали и исправили. Статью написала, чтобы опытом поделиться. Ну и мемчиками вас позабавить.Одной из важнейших, на мой взгляд, особенностей PWA является то, что оно выглядит как стандартное мобильное приложение: имеет отдельную иконку запуска на рабочем столе / главном экране, занимает всю его площадь, позволяя нам не захламлять экранное пространство браузерными элементами управления вроде адресной строки или панели вкладок, а сосредоточиться на приложении. В конце концов, мы можем практически полностью перенести функциональности мобильного приложения в PWA. Безусловно, ограничения (а точнее — невозможность реализации определенные фичи) все еще существуют. Но на данный момент технология PWA продолжает совершенствоваться и дополняться новыми фичами, что со временем приведет к тому, что PWA будут неотличимы от мобильного приложения.А теперь перейдем к сути.Для пользователя PWA очень удобен – функциональность мобильного приложения, а весит в 100500 раз меньше.Для владельца продукта есть тоже несомненный плюс – это дешево, да еще и устанавливается на разные ОС без каких-либо проблем, не нужно искать разработчиков отдельно под каждую платформу.Для разработчика… А вот для разработчика это «веб со своими приколами», над которыми, зачастую, придется поломать голову.Приведу пример того, над чем поломали голову мы.Мы разрабатывали приложение bnpl-сервиса Подели, в котором пользователи могут добавлять карты для оплаты. Привязка осуществляется через эквайринг, который открывается по ссылке в браузере. После успешной (или неуспешной) привязки карты, нас должно перебросить обратно в приложение.Это должно работать так↓А вот что увидели мы ↓Причем баг пропадал после того, как пользователь сворачивал и снова разворачивал приложение. Ключевым моментом во всей этой истории было то, что баг воспроизводился ТОЛЬКО в PWA, в вебе все было ок.Первая мысль была примерно такая: у нас есть элемент «подложка», скорее всего что-то вылезло, нужно фиксануть расположение элементов после возврата, и все будет четко.Ну как звучит, так и оценили: «Вроде изян».Поскольку мы знали, в каком случае приложение «восстанавливает свою высоту», решение должно казалось простым – написать обертку для «условной» перезагрузки страницы. Выглядела она примерно так:export const WithWindowPersistedCacheCheck = ({   children, }: {   children: JSX.Element; }) => {   useEffect(() => {     const handleWindowPersistCheck = (event: PageTransitionEvent) => {       if (event.persisted) {         location.reload();       }     };      window.addEventListener(""pageshow"", handleWindowPersistCheck);     return () => {       window.removeEventListener(""pageshow"", handleWindowPersistCheck);     };   }, []);    return children; };Кажется, все просто: перезагружаем страницу в зависимости от условия, в результате высота должна «сброситься к исходному значению». Однако, это решение не сработало, высота осталась прежней. Тем не менее в будущем нам пригодится этот ХОК, так что запомним его.Поскольку баг воспроизводился только в PWA, отладка происходила немного сложнее, чем обычно.В этот момент мы и обнаружили самое интересное: элемент, который нам мешал, – вовсе не элемент. В этом месте вообще нет никаких элементов! Проблему обнаружили в процессе игры «Найди 10 отличий». Обратите внимание на 2 скриншота:Да-да, высота, которую у нас якобы занимала подложка, оказалась высотой элементов управления браузером!Собственно, мы подошли к моменту кульминации понимания нашей проблемы: при открытии внешней ссылки в браузере из PWA, при возвращении PWA «думает», что оно обычное веб-приложение и перестает занимать всю высоту экрана. Иными словами, оно резервирует место на экране под браузерные элементы управления.Первое, с чего мы начали, – попытались вычислить высоту элементов управления браузером.Высота панели навигации:window.outerHeight - window.innerHeight;Высота верхней панели:window.screen.availHeight - window.outerHeight;На основе этих данных мы надеялись получить недостающую высоту приложения и увеличить на это значение высоту окна браузера, получив таким образом искомую высоту PWA.Проблема этого решения заключалась в том, что:- во-первых, не у всех браузеров есть верхняя панель (зависит от ОС и оболочки), - во-вторых, применив это решение, в вебе мы получили бы выпадающие элементы, которые скрывались за навигационной панелью.Нужно было конкретно определять, находимся ли мы в режиме PWA (полноэкранный режим).Для этого существует 2 способа:• первый – из CSSdisplay-mode: standalone• второй – из JSnavigator.standaloneДля первого варианта мы просто попытались установить высоту нашего приложения 100vh. Однако этого оказалось недостаточно, т.к. нужно было реагировать на событие перехода из приложения в эквайринг и обратно.Для второго варианта мы попытались создать resize-хук:export const usePWAResize = () => {   const [minAppHeight, setMinAppHeight] = useState(0);    const calculateTargetHeight = () => {     let windowHeight = window.innerHeight;     if (window.visualViewport) {       windowHeight += window.visualViewport.offsetTop;     }     if (navigator.standalone) {       windowHeight += window.outerHeight - window.innerHeight;     } else {       windowHeight +=         window.screen.height -         window.innerHeight -         window.visualViewport?.offsetTop;     }     setMinAppHeight(windowHeight);   };    useEffect(() => {     calculateTargetHeight();     window.addEventListener(""resize"", calculateTargetHeight);     return () => {       window.removeEventListener(""resize"", calculateTargetHeight);     };   }, []);    return minAppHeight; };Из него мы получили искомую высоту приложения и подставили её инлайном к нашему элементу-обёртке.Это даже как-то заработало, но ключевое слово тут «как-то». Как я сказала ранее, высота панелей навигации у всех браузеров разные +, а у каких-то её нет совсем. Из-за этой особенности на некоторых устройствах верстка вставала как надо, а на других что-то да выпадало. Но самой главной проблемой было то, что это не работало на устройствах Android – в браузере этой ОС не существует свойства standalone у интерфейса navigator. Он есть только в safari на iOS (яблочники ликуют :) )Нужно было решение более простое и универсальное.Для начала мы решили вывести все известные нам высоты в режимах PWA и веб:screen.availHeightscreen.heightwindow.outerHeightdocument.getElementById(“wrapper”)?.offsetHeightПолучается, что document.getElementById(“wrapper”)?.offsetHeight – высота «обертки» нашего приложения. Именно это значение и отличалось в зависимости от режима приложения, но всегда оставалось корректным.И тут нам в голову пришла идея: если при входе в приложение мы видим корректную высоту, которая изменяется только при возвращении из браузера, почему бы нам не запомнить это значение, сохранив его где-нибудь в сессии. И потом, в случае, если высота будет отличаться от исходной, «прибивать её назад гвоздями».Решение не элегантное, зато выглядит как то, что должно работать) Пробуем:useEffect(() => {     const pageWrapper = document.getElementById(""wrapper"");     const currentHeight = pageWrapper?.offsetHeight;     let initialHeight = sessionStorage.getItem(""initialHeight"") || ""0"";     let parsedInitialHeight = initialHeight || 0;      if (initialHeight === ""undefined"" || initialHeight === ""0"") {       sessionStorage.setItem(""initialWrapperHeight"", JSON.stringify(currentHeight));       initialHeight = JSON.parse(         sessionStorage.getItem(""initialWrapperHeight"") as string       );     }     if (initialHeight !== ""undefined"" && initialHeight !== ""0"") {       parsedInitialHeight = initialHeight;     }     if (       Number(parsedInitialHeight) &&       currentHeight &&       Number(parsedInitialHeight) > currentHeight     ) {       pageWrapper?.setAttribute(""style"", `min-height: ${initialHeight}px`);     }   }, []);Добавив этот эффект и обернув наш компонент в ХОК WithWindowPersistedCacheCheck, мы добились нужного нам поведения. Причем как в вебе (его эти изменения не затронули, т.к. он работал изначально корректно), так и в PWA.Некоторые скажут, что можно было бы и забить, ведь баг не критичный. Но мы в команде Подели и SimbirSoft привыкли делать удобный и качественный продукт, чтобы пользователь видел качественную картинку  без дополнительных перезагрузок приложения.Возможно, это решение не самое элегантное. Так что если среди наших читателей найдутся те, кто сталкивался с такой проблемой и действительно решил её более легким способом, велком в комментарии) Спасибо за внимание!"
44,44,История Ultimate Play the Game — легендарного разработчика игр для ZX Spectrum,Яндекс,Как мы делаем Яндекс,662.68,"Поисковые технологии, Мобильные технологии, Веб-сервисы",2025-03-13,"Мало какая компания пользуется среди поклонников компьютера ZX Spectrum такой любовью и уважением, как Ultimate Play the Game. За свою недолгую историю она выпустила два с лишним десятка игр, бóльшая часть которых моментально становились бестселлерами. Многие из них мы ставим на «Спектрумы» в Яндекс Музее. Особенной любовью публики пользуются Jetpac и Knight Lore, причём последняя зачастую заставляет посетителей удивлённо переспрашивать: «А этой игре точно недавно стукнуло 40 лет?»В шедевры Ultimate Play the Game я начал играть с того момента, как у меня появился ZX Spectrum — то есть с 1991 года. Тогда мало кто знал, откуда появилась эта компания и как она умудрилась наделать такое количество прекрасных игр за столь короткое время.И вот, спустя много‑много лет, я решил найти ответы на эти вопросы. Для этого я достал с полки все фирменные игры Ultimate, купленные в Великобритании, для удобства скачал их образы из интернета и потратил несколько дней, чтобы как следует в них наиграться. А затем обложился журналами Crash, Your Sinclair и Sinclair User, нашёл в интернете несколько десятков статей про Ultimate Play the Game и её создателей… И погрузился в расследование.Кто же эти гении, буквально за полтора года прошедшие путь от Jetpac до Knight Lore? И почему информации о создании всех игр Ultimate так мало? Давайте разбираться вместе.Игровая индустрия. НачалоДля многих спектрумистов это может стать открытием, но на самом деле Ultimate Play the Game — это всего лишь торговая марка компании с более скучным названием: Ashby Computers and Graphics Limited. Братья Крис и Тим Стэмперы основали её в 1982 году в небольшом городке Эшби‑де‑ла‑Зуш, что находится в графстве Лестершир.Автором идеи податься в игровую индустрию был Крис. Он с детства интересовался электроникой, собирал осциллографы и компьютеры, учился программировать и писал всякие программы. Крис хотел получить степень по электронике и физике, для чего поступил в Технологический университет Лафборо. Но в 1981 году бросил учёбу, чтобы на полную ставку заниматься программированием.Поначалу Крис возился с электроникой аркадных автоматов типа Space Invaders в одиночку, но потом уговорил Тима к нему присоединиться. Братья вместе с их другом Джоном Лэтбери какое‑то время поработали геймдизайнерами в компании Associated Leisure, а затем последовали за её директором в новую фирму Zilec Electronics. И уже там как следует набили руку на создании аркадных игр.Крис и Тим Стэмперы (фото увеличено с помощью нейросетей)Точный список проектов, над которым трудились Стэмперы, неизвестен. Но мы знаем их количество — двенадцать штук. Некоторые источники утверждают, что услугами Zilec Electronics пользовались такие уважаемые компании, как Sega и Konami, а среди игр числилась довольно известная космическая стрелялка Gyruss. В её официальных «кредитсах», правда, ни Стэмперы, ни Лэтбери не значатся, но гордые японцы могли просто не упомянуть каких‑то никому не известных англичан.Работа в Zilec Electronics предполагала командировки по всему миру, в том числе и в Японию, где братья познакомились с японской игровой индустрией и начали ещё лучше разбираться в хороших аркадных играх. Где‑то в 1980 году Крис купил дешёвый британский компьютер ZX80 и в свободное от основной работы время учился на нём программировать. По счастью, этот компьютер имел кое‑что общее с аркадными автоматами, для которых Zilec делала игры — популярный процессор Z80.В 1982 году случилось знаковое событие: Стэмперы и Лэтбери поняли, что пора уходить на вольные хлеба. Именно тогда и родилась Ashby Computers and Graphics Limited, а в команде появилась Кэрол Уорд — девушка Тима, которая через три года станет его женой.Свежеоткрывшаяся компания принялась по привычке делать игры для аркадных автоматов. Основным заказчиком была Jaleco — для неё A.C.G. произвела по меньшей мере три игры: Dingo, Grasspin и Saturn. Иногда в этот список включают одноэкранную аркаду Blue Print, но её наша троица, скорее всего, разработала всё же во времена Zilec Electronics.Аркадная игра Dingo — флаер и скриншот. ИсточникВ первые годы Ultimate Play the Game (давайте всё же называть компанию именно так) ютилась в небольшом четырёхкомнатном доме — по соседству с семейным магазином братьев. Над играми работали вчетвером: Крис и Джон программировали, Тим занимался графикой, Кэрол ему в этом помогала и по совместительству исполняла обязанности секретаря.Вход в Ashby Computers and Graphics Ltd. Источник: Журнал Commodore UserЭпоха ZX SpectrumВ этом же 1982 году на британский рынок вышел тогда ещё не легендарный, но уже подающий большие надежды компьютер ZX Spectrum. Он был простым в использовании, достаточно универсальным и недорогим: базовая модель с 16 Кбайт ОЗУ стоила 125 £, а версия с 48 Кбайт — всего 175 £. Как правило, конкуренты либо уступали «Спектруму» по характеристикам, либо стоили заметно дороже.Именно эту машину решила выбрать Ultimate как свою основную платформу. И не прогадала — по приблизительным подсчётам, только в Великобритании было продано не менее 5 миллионов разных моделей «Спектрума». Великолепные познания Криса в процессоре Z80 пришлись как нельзя кстати — в компьютере стоял именно этот чип.Компьютер ZX Spectrum из Яндекс МузеяЧтобы охватить как можно больше пользователей, Ultimate решила делать игры для базовой модели с 16 килобайтами оперативной памяти. Это, с одной стороны, не давало разработчикам как следует развернуться, а с другой — даже упрощало задачу. В конце концов, первыми играми студии должны были стать чистые аркады, а для них важен в первую очередь геймплей, а не количество уровней или разнообразных спрайтов.Первой игрой Ultimate Play the Game для ZX Spectrum стала одноэкранная стрелялка Jetpac. В ней вы управляете космонавтом по прозвищу Джетман, который работает лётчиком‑испытателем в Acme Interstellar Transport Company. Ваша задача — собирать доставленные на разные планеты космические корабли, а затем заправлять их, летая на реактивном ранце и отстреливаясь от всякой инопланетной жизни. А после завершения миссии — отправляться на другую планету.Jetpac появился в продаже в мае 1983 года, когда ZX Spectrum только‑только набирал популярность. Многие игры того времени не просто плохо выглядели: зачастую они были написаны не на ассемблере, а на Бейсике — со всеми вытекающими из этого последствиями. На их фоне Jetpac казался пришельцем из будущего: детализированная и красочная графика, отличное звуковое оформление, высокая и стабильная частота кадров в секунду. А главное — ураганный и дико затягивающий геймплей.Два издания Jetpac на аудиокассете. Обычное (5,5 £) и более позднее бюджетное (1,99 £)16 Кбайт оперативки игре хватало за глаза — каким-то диким разнообразием врагов или уровней она похвастаться не могла. Но благодаря этому Jetpac стала одной из немногих игр, выпущенных на картриджах для ZX Spectrum. В то время практически весь софт для 8-битных компьютерах продавался на самых обычных аудиокассетах, потому что они были дешёвыми, а загрузить их в память компьютера можно было при помощи самого обычного магнитофона.Геймплей JetpacНекоторые компьютеры сразу обладали дополнительным разъёмом для картриджей (например, Commodore 64 или TI-99/4A), но у «Спектрума» ничего подобного не имелось. Чтобы запускать игры на картриджах, надо было сначала купить специальное устройство под названием ZX Interface 2 или его аналог. Картриджи при этом не могли содержать больше 16 Кбайт данных, и всё это сильно ограничивало как круг покупателей, так и выбор игр.Ничего удивительного, что идея не взлетела. Но это, разумеется, не помешало кассетам с Jetpac разлетаться с прилавков как горячие пирожки.Картридж с JetpacВыход Jetpac моментально вознёс Ultimate Play the Game на вершину спектрумовского Олимпа. Игру обожали и журналисты, и простые пользователи. Журнал Your Sinclair поставил Jetpac на 73-е место в топ-100 самых лучших игр по версии читателей, а в более современном журнале Retro Gamer эта аркада удостоилась 14-го места.Не подкачали и продажи — большинство источников сходится на цифре 300 000 копий только для ZX Spectrum (а игру чуть позже портировали на VIC-20 и BBC Micro). Самой Untimate этот релиз принёс порядка 1 миллиона фунтов стерлингов дохода. Забавный факт: к 1983 году было продано около 1 миллиона «Спектрумов»; выходит, что Jetpac купил примерно каждый третий спектрумист!Закрепление успехаДолго почивать на лаврах братья Стамперы не стали. Уже в июне 1983-го выпустили ещё одну игру — Pssst. В этой одноэкранной аркаде вы управляете роботом по имени Робби и защищаете своё любимое растение Thyrgodian Megga Chrisanthodil (в реальности его, конечно, не существует) от насекомых‑вредителей.Сложность заключается в том, что каждое насекомое нужно опрыскивать баллончиком соответствующего цвета, а если замешкаться — вредители просто сожрут ваше ненаглядное растение. Оглушительного успеха своей предшественницы Pssst не добилась, но продавалась всё равно очень здорово, да и игроки с журналистами не жаловались. Эта игра тоже вышла на картриджах для ZX Interface 2, поскольку прекрасно на них помещалась.Геймплей PssstСтавка на 16-килобайтные игры полностью себя оправдала. Времени на них Стамперы тратили не очень много (по их собственным словам — в два раза меньше, чем на 48-килобайтные релизы), а отдача была такой же. Поэтому уже в июле того же года Ultimate выпустила ещё две игры для базовых «Спектрумов»: Cookie и Tranz AM.Cookie прессе показалась слишком похожей на Pssst — это снова была одноэкранная аркада, в которой нужно по кому‑то стрелять. На этот раз игрок управляет поваром Чарли, который должен направлять взбесившиеся ингредиенты для готовящегося блюда в миску для смешивания.Графически Cookie не вызывала никаких претензий — «фирменный стиль Ultimate» проявился и тут. А вот зашкаливающий уровень сложности не помянул тихим незлым словом разве что ленивый. Впрочем, это не помешало игре снова получить высокие оценки и разойтись приличным тиражом.Геймплей CookieTranz Am на первый взгляд выглядела не так красочно, как предыдущие релизы Ultimate, но брала своё техническим исполнением. Плавный скроллинг во всех восьми направлениях на «Спектруме» в середине 1983 года? В то время, напомним, когда многие коммерческие релизы других компаний не могли справиться с парой спрайтов на одном экране?Логично, что Tranz Am оказалась очередной успешной игрой Ultimate. И это несмотря на простенький геймплей в стиле «катаемся по постапокалиптическим Соединённым Штатам, собираем какие‑то кубки и улепётываем от врагов».Геймплей Tranz Am1983 год ещё и не думал заканчиваться, а Ultimate умудрилась выпустить для ZX Spectrum уже четыре хитовые игры! А ведь с первого релиза миновало всего три месяца. Секрет такой производительности заключался в большом опыте команды и в грамотном распределении обязанностей: пока Крис и Джон корпели над кодом, Тим и Кэрол занимались графикой. Если задачи по текущей игре заканчивалась, сотрудники переключались на следующий проект.К концу 1983-го пользователей ZX Spectrum с 48 Кбайт стало заметно больше. А многие из тех, кто купил 16-килобайную модель, просто проапгрейдили её до большего объёма ОЗУ. Братья Стэмперы решили, что пора взрослеть, и окончательно забросили базовую модель «Спектрума», полностью переключившись на более продвинутую. Поэтому Tranz Am стала последней игрой, которой хватало 16 Кбайт памяти — всем остальным требовалось в три раза больше.Возвращение ДжетманаПосле Tranz Am компания взяла крошечную паузу и вернулась уже в ноябре 1983 года — с новой игрой про Джетмана.Lunar Jetman — это прямой сиквел Jetpac, который при этом радикально отличается от предшественника геймплеем. На этот раз нелёгкая заносит Джетмана на какую‑то неизвестную планету (название намекает на Луну, но инструкция к игре об этом помалкивает), чьи обитатели зачем‑то собираются уничтожить Землю, запустив в её сторону кучу ракет.Нашему отважному космонавту приходится взять на себя обязанности спасителя всего человечества. А для этого нужно летать по поверхности планеты, отстреливаться от сотен злобных врагов и сбрасывать бомбы на ракетные станции. Путешествовать по планете можно и на луноходе, который защищает Джетмана от любых неприятностей. Есть одна проблема — этот агрегат умеет кататься только по ровной поверхности и не может преодолеть даже маленький кратер, возникший из‑за падения бомбы или подбитого Джетмана.Геймплей Lunar JetmanLunar Jetman — дико сложная игра, в которой поначалу очень часто умираешь, но потом потихоньку осваиваешься и начинаешь получать гору удовольствия. Грамотно маневрировать здесь не менее важно, чем метко стрелять, а ещё стоит запастись большим терпением.При этом Lunar Jetman ещё и очень технологически продвинутая игра. В статике она выглядит ничуть не хуже, а то и лучше Jetpac, вот только здесь присутствует плавный горизонтальный скроллинг, да ещё и с цветными спрайтами. А это для скромного «Спектрума» — та ещё задачка, тем более в начале 80-х.В общем, пресса и обычные игроки хвалили эту игру на все лады не только за геймплей, но и за внешний вид — красочная графика, плавная анимация и вышеупомянутый скроллинг сделали своё дело. Некоторые журналы даже сравнивали Lunar Jetman с играми на аркадных автоматах — и часто не в пользу последних.Восторженная рецензия на Lunar Jetman в первом номере журнала СrashЗабегая вперёд, стоит заметить, что Джетман был очень важен для Стэмперов. Так, в июле 1984 года Ultimate предпримет попытку рекламировать через этого персонажа свой бренд. В журнале Сrash с 6-го номера начнут выходить юмористические комиксы про Джетмана, которые будет рисовать штатный художник Джон Ричардсон.Комиксы окажутся настолько популярными, что переживут саму Ultimate и будут выходить вплоть до сентября 1991 года! А отсутствие приключений Джетмана в одном‑единственном 57-м номере будет встречено дружным читательским негодованием (если верить редакции журнала, конечно).Первый и последний комиксы про Джетмана, напечатанные в журнале Crash (6-й и 93-й выпуски)А в ещё более далёком 1990 году Стэмперы закажут студии Zippo Games финальную часть трилогии про этого персонажа — Solar Jetman. Правда, к тому моменту компания братьев будет называться совсем иначе, а игра выйдет на приставке NES. Впоследствии её захотят портировать на 8-битные компьютеры, включая ZX Spectrum, но из‑за слабых продаж оригинала проект прикроют.Джетман в интро Solar Jetman нарисован в стиле комиксов из журнала Crash. ИсточникПутешествие по замкуНо вернёмся в конец 1983-го. Любая другая компания выпустила бы Lunar Jetman на Рождество и пожинала бы лавры. Но только не Ultimate. Игра и так взлетела на первое место в чартах продаж по всей Великобритании, а у братьев Стэмперов в рукаве был ещё один козырь — их первый релиз, отошедший от чисто аркадного жанра.Atic Atac вышла в конце ноября 1983-го (хотя некоторые источники настаивают на декабре) и являлась приключенческой игрой, в которой нужно было бегать по огромному замку и искать три части Золотого ключа ACG (это, разумеется, очевидная отсылка к названию компании).Замок действительно огромный — он состоит из 140 комнат, не считая лестничных проёмов. Каждая комната кишит врагами, от которых можно отстреливаться — да, это приключенческая, но всё‑таки в изрядной степени аркадная игра. В самом начале можно выбрать персонажа — Волшебника, Воина или Крестьянина. От выбора зависит скорость передвижения главного героя, его оружие и даже возможность заходить в определённые двери.Карта Atic Atac. ИсточникТакже по замку разбросаны разные предметы: это и три части золотого ключа, и цветные ключи для открытия соответствующих дверей, и даже вещи, которые воздействуют на определённых врагов. Благодаря всему вышеописанному Atic Atac можно проходить несколько раз, и каждое прохождение будет немного отличаться от предыдущего.Atic Atac быстро взлетела на первое место по продажам и произвела на публику очень сильное впечатление. Читатели Your Sinclair поставили игру на 79-е место среди всех игр для ZX Spectrum, а много лет спустя читатели журнала Retro Gamer вознесли Atic Atac на 8-ю строчку.Геймплей Atic AtacИменно благодаря этой игре появилась культовая британская телепередача Knightmare, продержавшаяся целых 8 сезонов и 112 эпизодов. Её продюсер Тим Чайлд, увидев в 1985 году Atic Atac и Dragontorc (другую игру того же жанра), понял, что пора совершать революцию на телевидении.Результатом стало увлекательнейшее шоу, в котором команда из четырёх детей отправлялась в путешествие по замку — разумеется, полному загадок и опасностей. Один путешественник надевал на голову рыцарский шлем, который закрывал бóльшую часть поля зрения, и исполнял роль главного героя. А остальные трое давали ему команды в стиле «Повернись налево, пройти три шага вперёд, остановись», «Возьми со стола лампу и рубин» или «Стой, не двигайся!».Типичный кадр из Knightmare. Ведущий даёт советы игрокам. ИсточникГлавный герой ходил по синему залу — своего рода большому хромакею. А внутреннее убранство всех комнат замка частично создавалось на мощной графической системе Spaceward Supernova, частично — рисовалось от руки.Выглядело всё это очень эффектно и действительно слегка похоже на Atic Atac. Загадки встречались более чем интересные, и детей (да чего там — и взрослых!) было сложно оторвать от телевизора. Не зря Knightmare продержалась на экранах целых семь лет — с сентября 1987-го по ноябрь 1994-го.Кадр со съёмок Knightmare. ИсточникПервый год позади1983 год закончился, но самая влиятельная игра Ultimate Play the Game была ещё впереди. А пока команда Стэмперов могла себя искренне похвалить — всего за 7 месяцев эти товарищи выпустили шесть популярных игр, многие из которых впоследствии станут культовыми. Суммарный тираж этих игр наверняка перевалил за 1 миллион копий, хотя точных цифр мы, увы, не знаем.Стэмперы вообще не очень любили хвастаться продажами и очень редко общались с прессой. Поначалу это объяснялось дикой занятостью команды — о каких интервью идёт речь, когда работаешь чуть ли не круглые сутки семь дней в неделю? Впрочем, подобная нелюдимость играла Ultimate только на руку, придавая компании своего рода ореол загадочности.Почти вся команда Ultimate, но уже в 1988 году. Слева стоят Тим Стэмпер, Кэрол Стэмпер и Крис Стэмпер. ИсточникВсем хотелось узнать, чем же занимаются эти гении сейчас и как они умудрились добиться таких впечатляющих результатов за столь короткий срок. Но ни Стэмперы, ни их коллеги не спешили ничего рассказывать. Такой стиль общения с прессой — без какой‑либо неприязни, но и без постоянных пресс‑релизов, светских вечеринок, ведения колонок в игровых журналах и других активностей, стал визитной карточкой Ultimate.Первые несколько месяцев 1984 года прошли спокойно — особенно для конкурентов, которым Ultimate несколько портила жизнь, одну за другой выпуская хитовые игры. Но уже в апреле‑мае в компьютерных журналах начала появляться первая реклама новой игры — Sabre Wulf.Выглядела она красиво и загадочно — всё в лучших традициях Ultimate. Фиолетовый логотип игры на чёрно‑зелёном фоне, лого Ultimate Play the Game, платформа (разумеется, ZX Spectrum 48K) и неожиданно высокая цена — 9,95 £.Реклама Sabre Wulf в журнале Crash. ИсточникДо этого все игры Ultimate продавались почти в два раза дешевле — за 5,5 £. Правда, и выпускались они, как правило, в самых обычных кассетных кейсах с вкладышем, который одновременно выполнял роль инструкции. Увеличить цену Ultimate решила по двум причинам. Во‑первых, проверить, будут ли игры продаваться так же хорошо в премиальной упаковке, но вдвое дороже. Во‑вторых, Стэмперы сочли, что покупатели дорогой игры с меньшей охотой будут её копировать и отдавать друзьям. Проще говоря — пиратить.Обычно борьба с пиратством игр на аудиокассетах выглядела иначе. Одни издатели вкладывали в комплект с игрой набор кодов, которые надо ввести при старте. Другие — специальную пластиковую линзу, которую нужно было приложить к экрану, чтобы прочитать появившееся там сообщение, а затем ввести его в компьютер. Просто увеличить отпускную цену на игру в два раза не пытался, кажется, ещё никто. Но репутация Ultimate была столь безупречна, что это могло бы и сработать.Пример использования пластиковой линзы Lenslok. ИсточникSabre Wulf поступила в продажу в мае 1984 года в стильной чёрной коробке, обе стороны которой мало отличались от постера — два логотипа, платформа и всё. Никакой дополнительной информации об игре или хотя бы скриншотов не предполагалось. Ultimate решила поддерживать атмосферу загадочности во что бы то ни стало.Сравнение размеров коробок Sabre Wulf и предыдущих игр Ultimate Play the GameПринеся коробку с Sabre Wulf домой и запустив игру на своём компьютере, покупатели обнаруживали, что Ultimate снова сделала игру‑лабиринт — в ней нужно было бегать по джунглям и собирать кусочки амулета. Экранов стало ещё больше (целых 256), а выглядели они сногсшибательно: если в предыдущих играх Ultimate, за исключением Tranz Am, доминировал чёрный цвет, то на этот раз всё утопало в ярких красках. Многие критики и игроки называли Sabre Wulf самой красивой игрой для ZX Spectrum и были недалеки от правды.Разумеется, игра снова стала хитом, причём самым крупным за всю историю компании. Продажи перевалили за 350 000 копий — эксперимент Ultimate с повышением цены полностью оправдался. Не всем критикам и обычным игрокам такой ход пришёлся по нраву, но к визуалу и геймплею претензий было очень мало. Sabre Wulf не только принесла своим разработчикам кучу денег, но и стала первой игрой про главного героя по имени Сэйбермен. Главный герой носил пробковую шляпу и сражал врагов верным мечом. А джунгли в Sabre Wulf прямо‑таки кишели врагами — в этом смысле игра мало отличалась от Atic Atac, разве что была куда сложнее.Геймплей Sabre WulfПосле выхода своей седьмой игры (всего за год!) Ultimate снова затаилась на несколько месяцев и вернулась лишь в ноябре. Зато сразу с двумя релизами, которые вышли, судя по всему, с разницей ровно в одну неделю.Первой объявилась приключенческая аркада со странным названием Underwurlde. В ней продолжились приключения Сэйбермена, но на этот раз в формате платформера, а не бродилки по лабиринту. Экранов стало ещё больше — целых 600 штук! Такой большой игры Ultimate не делала ещё ни разу, и геймеры это оценили. У Underwurlde были проблемы — например, странные прыжки Сэйбермера и его манера далеко улетать от малейшего соприкосновения с врагами. Тем не менее общее впечатление от игры было очень положительным, и она без проблема начала занимать первые места во всевозможных чартах продаж.Геймплей UnderwurldeНо уже через неделю Underwurlde пришлось уступить лидерство ещё одной игре Ultimate. Той, что навсегда изменит игровой ландшафт на ZX Spectrum, породит огромное количество подражателей и станет самым знаковым релизом компании на этом компьютере. Разумеется, речь о Knight Lore.Эра Knight LoreОбычно сдержанная Ultimate на этот раз позволила себе немного похвастаться и выпустила пресс‑релиз, в котором называла Knight Lore «первым шагом в новом поколении компьютерных приключенческих симуляторов, которые представляют собой вершину разработки программного обеспечения на Spectrum 48K». Не больше и не меньше!Официальный постер Knight Lore. ИсточникНи пресса, ни обычные игроки вплоть до релиза не представляли, что их ждёт. Тем сильнее оказался шок, когда выяснилось, что новая игра Ultimate — это очередное приключения Сэйбермена, но исполненное на совершенно новом технологическом уровне, в изометрической проекции.Привыкшие к полностью двухмерным играм владельцы ZX Spectrum не могли поверить своим глазам, когда их взгляду представал детальный трёхмерный мир игры. По нему можно было перемещаться не только на плоскости, но и по вертикали, двигать каменные блоки и предметы, а также ставить одни объекты на другие. Разумеется, игры с изометрией выходили и до Knight Lore — достаточно вспомнить широко известную в то время Ant Attack. Но только Стамперам удалось реализовать концепцию по‑настоящему трёхмерного мира столь изящно и почти безошибочно.Сюжетно Knight Lore продолжал события Sabre Wulf и Underwurlde. Сэйбермена укусил оборотень, и наш герой обратился за помощью к волшебнику Мелькиору. Чтобы избавиться от проклятия, нужно собрать шесть разных объектов, бросить их в котёл и сварить зелье. На всё про всё отводится 40 игровых дней, причём по ночам Сэйбермер превращается в оборотня. На геймплей это влияет не сильно, но анимация превращения уж больно хороша, да и постоянная смена спрайта главного героя придаёт игре разнообразия.Геймплей Knight LoreЧто под капотом?Движок, на котором была сделана Knight Lore, даже получил собственное название — Filmation. Им Ultimate очень гордилась, и не зря — ничего подобного на скромном 8-битном «Спектруме» просто не существовало. Каждый экран Knigt Lore содержал кучу объектов, которые нужно было вывести на экран в правильной последовательности — чтобы они не накладывались друг на друга абы как. Для этого объекты приходилось сначала сортировать по расстоянию от камеры, а потом выводить на экран один за другим — от самого дальнего до самого близкого.При выводе объектов использовалась так называемая маска, визуально похожая на контур спрайта. Она позволяла очистить фон под спрайтом объекта, сохранив при этом соседние пиксели. Без этой технологии, которая в то время использовалась далеко не во всех играх, не получилось бы так филигранно накладывать объекты друг на друга.Пример работы маски спрайта. ИсточникКонечно же, за изометрию и высокую детализацию графики пришлось платить. Экранов в игре стало всего 128 — сильно меньше, чем в предыдущих играх про Сэйбермена. Красочность Sabre Wulf и других игр Ultimate тоже канула в Лету — основное игровое поле Knight Lore было полностью монохромным. Это объяснялось особенностью графической системы ZX Spectrum — одно знакоместо 8×8 пикселей не могло содержать больше двух цветов. Если бы Ultimate попыталась изобразить другим цветом хотя бы главного героя, он бы «окрашивал» собой фон и соседние объекты. Выглядело бы это примерно так:Так выглядел бы Sabre Wulf c «клэшингом цветов»Впрочем, монохромные игры на «Спектруме» — не редкость, так что непривычная для игр Ultimate цветовая палитра мало кого смущала. Как и заметные «подтормаживания» игры, когда на экране присутствовало слишком много объектов. Всё это с лихвой перекрывалось роскошным визуалом и необычным геймплеем, который на этот раз был сдвинут в сторону приключений и головоломок, а не традиционной для Ultimate аркады.В восторге остались не только игроки и журналисты, но и коллеги Стэмперов по цеху. Knight Lore вдохновил огромное количество разработчиков на создание своих изометрических игр. Активно выходить они начали с 1985–1986 годов, причём среди них встречались как откровенные клоны типа Sweevo's World или Molecule Man, так и игры со своей изюминкой. Например, Fairlight или Head Over Heels, которые сами по себе заслужили статус нестареющей классики.Cкриншоты Fairlight и Head Over Heels. ИсточникВ 1987 году компания CRL даже выпустила собственный движок и редактор под названием 3D Game Maker. Стоила эта программа те же 9,95 £, что и релизы Ultimate, но позволяла делать свои игры, как две капли воды смахивающие на Knight Lore. Только в 80-е при помощи 3D Game Maker было сделано минимум три десятка игр, хотя в продажу поступили далеко не все.Вкладыш 3D Game Maker. ИсточникНаследники и подражателиЧтобы опередить конкурентов, уже в феврале 1985 года Ultimate выпустила ещё одну игру на движке Filmation — Alien 8. В ней Стэмперы вернулись к научно‑фантастическому сеттингу — на этот раз вы играли за робота, который перемещается по огромному космическому кораблю и должен поддерживать в порядке криогенные системы жизнеобеспечения. Для этого нужно собирать схемы в виде геометрических фигур и вставлять их в соответствующие гнёзда.Публика приняла Alien 8 очень тепло, журналы ставили игре высочайшие оценки — иногда даже выше, чем Knight Lore. При этом со стороны могло показаться, что Ultimate сделала Alien 8 впопыхах, буквально за два‑три месяца. Но это было не так.Геймплей Alien 8В интервью журналу Crash от апреля 1988 года Стэмперы раскрыли страшную тайну, достойную фильмов М. Найта Шьямалана. Оказывается, Knight Lore была полностью готова раньше, чем Sabre Wulf и Underwurlde, а Alien 8 — готова наполовину! Правда, анализ исходников всех этих игр показывает, что у Knight Lore код более оптимизирован, но это не противоречит утверждению разработчиков. Они вполне могли сделать сначала Knight Lore, затем Sabre Wulf и Underwurlde, а после этого вернуться к Knight Lore и немного позаниматься оптимизацией.Да, хитрюги Стэмперы, сделав первой самую технологически продвинутую игру про Сэйбермена, предпочли задержать её почти на год, чтобы не мешать продажам более скромных Sabre Wulf и Underwurlde. План сработал — все три игры удостоились похвалы и отличных продаж.Вот и получается, что Alien 8 находилась в разработке отнюдь не три месяца, а куда дольше. Скорее всего, её Стэмперы могли выпустить одновременно с Knight Lore, но тоже немного придержали. А освободившееся время потратили на создание новой версии своего знаменитого движка — Filmation 2.Реклама первой игры на движке Filmation 2 — Nightshade. ИсточникСерьёзный апгрейдСерьёзно улучшить Filmation можно было, пожалуй, лишь в трёх направлениях: оптимизировать производительность, реализовать скроллинг и добавить цвет. Именно это и сделали Стэмперы в своём новом релизе — игре Nightshade, которая поступила в продажу в августе 1985 года. Конкуренты к тому времени только‑только начали осваивать выпуск изометрических игр со статичными экранами, поэтому Ultimate снова оказалась впереди всех.Nightshade была похожа на смесь Knight Lore и Atic Atac — это уже не головоломка, а аркада со стрельбой, просто в изометрической перспективе. Сэйбермена было решено отправить на заслуженный отдых, а главным героем стал некий рыцарь, который ходит по чумной деревне, сражается с полчищами врагов (бывшими жителями деревни) и намеревается сразить четырёх демонов.Геймплей NightshadeUltimate всё ещё была любимицей прессы, а Nightshade получилась хорошей игрой, поэтому и оценки ей ставили неплохие — как правило, в районе 80–90%.Та же судьба ждала и Gunfright — ещё одну игру на движке Filmation 2, которая вышла несколько месяцев спустя. По геймплею она напоминала Nightshade, но события перенеслись на Дикий Запад, главным героем оказался шериф, а геймплей стал разнообразнее: например, здесь можно было оседлать лошадь или пострелять по мишеням в перспективе от первого лица: он включался, когда шериф находил одного из особо опасных бандитов и вступал с ним в перестрелку. В самой игре служителю закона предлагалось пострелять по падающим мешкам с деньгами, чтобы немного подзаработать. С лошадью парни из Ultimate решили пошутить — она, судя по анимации, не настоящая, а игрушечная. Поэтому путешествие по городу на лошади из стремительного галопа превращается какой‑то, извините, хоббихорсинг.Геймплей GunfrightНесмотря на отличные продажи и хорошую прессу, было заметно, что публика начала немного уставать от изометрических релизов компании. Братья Стэмперы прекрасно это видели. А ещё — понимали, что выжали из «Спектрума» все соки. Всего за два года они выпустили огромное количество хитовых игр, практически каждая из которых награждалась эпитетами вроде «великолепная», «революционная», «культовая» и иже с ними.Да, существовали и другие компьютеры: например, Commodore 64, BBC Micro, Amstrad CPC и MSX. На них тоже выходили игры от Ultimate, хотя портированием обычно занимались совсем другие люди.Интереснее всего получилось с версией Nightshade для Commodore 64. Её по личной просьбе Ultimate сделал Шахид Ахмад — разработчик, создавший откровенный клон Alien 8 под названием Chimera сразу для нескольких компьютеров, включая ZX Spectrum и C64. Можно сказать, что этого клонодела братья Стэмперы одобрительно похлопали по плечу. Шахид планировал портировать на Commodore 64 ещё две игры Ultimate: Knight Lore и Alien 8. Но сделка, увы, по какой‑то причине сорвалась.Слева — версия Nightshade для Commodore 64. Справа — Chimera для ZX Spectrum. ИсточникА ещё на Commodore 64 существовало шесть игр от Ultimate, эксклюзивных для этого компьютера: The Staff of Karnath (1984), Entombed (1985), Imhotep (1985), Blackwyche (1985), Outlaws (1985) и Dragon Skulle (1986). О них почтенная спектрумовская публика, как правило, и не слышала. Немудрено — к этим релизам братья Стэмперы не имели ну вообще никакого отношения: Ultimate просто исполняла роль издателя.Обложки всех эксклюзивных игр для Commodore 64 в фирменном стиле Ultimate Play the Game. ИсточникПревращаться в людей, которые просто‑напросто штампуют релизы, паразитируя на былой славе, Стэмперы не хотели. Пришло время уходить на другие платформы, которые сулили куда большие перспективы, чем ZX Spectrum и прочие 8-битные компьютеры. Бренд Ultimate Play the Game при этом всё ещё был очень известен публике, поэтому Тим и Крис просто продали часть компании крупному издателю U.S. Gold, сохранив при этом контрольный пакет акций.Эпоха U.S. GoldИменно при участии U.S. Gold вышло несколько релизов, которые фанаты не всегда относят к «настоящим играм Ultimate»: Cyberun, Pentagram, Martianoids и Bubbler.Авторство первых двух до сих пор находится под вопросом. В одном из поздних интервью Стэмперы сообщили, что последней игрой, которую они сделали старой командой, стала Gunfright. Однако ситуация с первой трилогией про любимца публики Сэйбермена доказывает, что порядок выхода игр не всегда соответствует графику их разработки.Pentagram вышла в 1986 году и оказалась уже четвёртой игрой про Сэйбермена. Это было ожидаемо, ведь в трёх разных концовках Underwurdle главный герой попадал в одну из трёх будущих игр. Первой была Knight Lore, а второй — как раз Pentagram.Удивительным оказалось другое — игру сделали на первой версии движка Filmation, что многим показалось шагом назад. Правда, главный герой стал волшебником и наконец‑то научился стрелять во врагов заклинаниями, что роднит Pentagram с более технически продвинутыми Nightshade и Gunfright. Врагов тоже стало больше, но игра не превратилась в аркаду, оставшись в первую очередь приключением‑головоломкой.Геймплей PentagramНельзя исключать, что Pentagram разрабатывалась ещё во времена Knight Lore и Alien 8 силами братьев Стэмперов, просто вышла заметно позже этих двух игр. А может быть, Тим и Крис участвовали в разработке лишь в роли сценаристов и геймдизайнеров, оставив программирование и графику сотрудникам U.S. Gold.Космическая стрелялка Cyberun объявилась чуть раньше — в конце 1985 года. По геймплею она смахивала на Lunar Jetman, но популярностью предшественницы, разумеется, похвастаться уже не могла. Код для этой игры писал Майк Беттеридж, который чуть позже прославится на весь спектрумовский мир чудесным «арканоидом» под названием Batty.А вот к Martianoids и Bubbler братья Стэмперы совершенно точно не имели никакого отношения. Их разрабатывала внутренняя студия U.S. Gold, причём над Bubbler опять‑таки работал Беттеридж.Скриншоты Cyberun, Bubbler и Martianoids. ИсточникПресса это как будто почувствовала и оценки ставила соответствующие. Даже очень лояльно настроенный к Ultimate журнал Сrash, редко ставивший играм этой компании оценки ниже 90%, наградил Bubble рейтингом 78%, а Martianoids — 58%.Король умер, да здравствует король!На этом поток релизов от Ultimate Play the Game окончательно иссяк. Так и не вышла ожидаемая многими фанатами игра Mire Mare, которая тизерилась в третьей концовке Underwurlde. Про неё очень долго ходили всевозможные легенды: от «игра была полностью закончена, а потом утеряна» до «в основе лежал движок Filmation или даже Filmation 2».И лишь значительно позже выяснилось, что игру действительно начали делать, ориентируясь на геймплей и внешний вид Sabre Wulf. Вот только работа застопорилась на этапе базового дизайна: не было написано ни строчки кода и не нарисовано ни пикселя графики. Всё, что осталось нам в наследие от этой загадочной игры — красивый арт.Арт Mire Mare. ИсточникРазумеется, такое положение дел не устроило фанатов игр Ultimate Play the Game, поэтому они взяли дело в свои руки. Впрочем, множественное число здесь не очень подходит, поскольку отдуваться за всех пришлось итальянцу по имени Лука Бордони. Он умудрился выпустить целых две «игры по мотивам»: Land of Mire Mare в 2014-м и просто Mire Mare в 2020-м. Обе, конечно, не дотягивают до уровня работ братьев Стэмперов, но в остальном это довольно неплохие игры.Скриншоты из Land of Mire Mare и Mire Mare. ИсточникВ 1988 году U.S. Gold выпустила сборник Ultimate Play The Game: The Collected Works по очень привлекательной цене — 12,99 £ за издание на кассетах и 14,99 £ за дисковое издание. Этот сборник содержал почти все релизы легендарной компании вплоть до Gunfright. За бортом осталась только Underwurlde — якобы по причине того, что её не получилось адаптировать для нормальной работы на компьютере ZX Spectrum 128K. Отсутствие Cyberun, Pentagram, Martianoids и Bubbler лишь убедило поклонников Ultimate в том, что братья Стэмперы к разработке этих игр имели либо нулевое, либо очень опосредованное отношение. Обложка сборника Ultimate Play The Game: The Collected Works. ИсточникСами же Стэмперы всё это время хранили гордое молчание. Позже выяснилось, что они не сидели сложа руки, а упорно трудились. Но на благо не «Спектрума», а совсем другой платформы — Famicom, она же NES, она же «Денди».В 1985 году, незадолго до сделки с U.S. Gold, Стэмперы основали ещё одну компанию — Rare Limited. Офис переехал в деревеньку Твикросс — неподалёку от Эшби‑де‑ла‑Зоуч. К этому моменту братья Стэмперы так хорошо освоили Famicom, что быстро добились расположения самой Nintendo.Логотип Rare, появившийся в 90-х. ИсточникВ итоге Rare получила возможность делать игры для этой и других консолей Nintendo, за всю свою историю выпустив около сотни таких релизов. Многие из них на слуху до сих пор — например, Battletoads, Donkey Kong Country, Banjo‑Kazooie и Golden Eye 007. Но это, как принято говорить в таких случаях, уже совсем другая история.Что до бренда Ultimate Play the Game, то его Стэмперы в 1988 году обратно выкупили у U.S. Gold, но уже ничего под ним не выпускали. И правильно — некоторым вещам, особенно таким легендарным, лучше оставаться в прошлом.«Земля Ультиматума» — карта мира, состоящего из игр Ultimate Play the Game. Источник.Но игры этой легендарной компании по‑прежнему пользуются неувядающим успехом среди спектрумистов. Если вы никогда не играли в Jetpac или Knight Lore — сейчас самое время открыть для себя эти шедевры. А если играли — почему бы не тряхнуть стариной, достав с полки свой «Спектрум», запустив эмулятор или просто зайдя в гости в Яндекс Музей?"
45,45,Разработчики 1С – как с ними дружить?,OTUS,Цифровые навыки от ведущих экспертов,721.07,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-03-13,"Сегодня во многих коммерческих проектах для целей учета платформа 1С выходит на первое место, но конечно же, есть исключения: непрерывные или сложные данные, большой объем операций, несколько сотен тысяч документов в день (хотя есть интересные «пробы пера») и многие другие. В связи с этим коллеги из настоящего ИТ начали включать в компетенции технологию 1С, но несмотря на это для многих «true-программистов» язык 1С все еще считается чем‑то не из мира разработки, хотя задач интеграции и адаптаций 1С с каждым днем становится все больше. В статье поговорим о практическом применении навыков разработки с точки зрения бизнес‑пользователя, а также поделимся некоторыми кейсами из опыта наших бизнес‑коллег, попробовавших себя в разработке.Когда применяется 1С? В случаях потребности решить типичную дискретную прикладную бизнес‑задачу вариант создания MVP приложения на 1С будет самым быстрым решением, даже если потом будет разработка решения на ином стеке технологий.Также для очень многих бизнес‑пользователей 1С может стать точкой отсчета и первым знакомством с внутренней стороной системы — создание простейших приложений для «домашнего учета». И, кроме этого, навыки разработчика помогут бизнес‑аналитику 1С или пользователю‑эксперту получить навыки системного аналитика, так как в 1С ценность «fullstack» аналитика (понимающего не только бизнес‑процессы, но и возможности 1С платформы) постоянно растет.Как применить начальные навыки разработчика?Примеры кейсов коллег «не из ИТ», применивших навыки разработки:Кто/системаПричина/ЗадачаРезультатКоммерческий директор небольшой компании/1С Управление торговлейНеобходимо было включение в печатные формы документов дополнительной информации для упрощения работы склада. Интегратор выставил счет более 2 млн рублей. Прошел базовое обучение, проконсультировался по архитектуре своего решения у ментора. Самостоятельно реализовал без изменения конфигурации.Юрист/1С Документооборот Компания готовила множество типовых документов, поля которых можно было заполнить в автоматическом режиме, но необходимо было понимать правила установки в зависимости от особенностей видов документов и сделки.Приобретена библиотека скриптов, изучен синтаксис заполнения и подготовки документов. Теперь коллеги готовят документы сразу с полями автозаполнения, которые сами загружают в систему. Скорость автоматизации возросла в несколько раз.Маркетолог/1С ERPНеобходимо было как в самой системе ERP, так и в BI системе постоянно делать новые выборки‑исследования и искать зависимости между клиентами/продажами/продукцией и т. п. с целью адаптации отчетности для топ менеджмента и подготовки акций.Изучен базовый функционал для работы с СКД, запросами и необходимыми для работы с BI системой инструментами, созданы собственные библиотеки запросов, которые стали основой проекта автоматизации ценообразования.С чего же можно начать получение опыта разработчика? Для имеющих опыт работы пользователя:Регистрируемся как разработчик на портале https://developer.1c.ru, устанавливаем и регистрируем платформу по инструкциям с портала.Скачиваем полнофункциональные демо‑конфигурации https://online.1c.ru/catalog/free/learning.phpВ некоторых архивах с учебными версиями есть учебная платформа 1С — ее не устанавливаем. В учебных целях начинающим рекомендуется 2 конфигурации (готовые демо базы 1С):Демонстрационное приложениеКонфигурация «Управление нашей фирмой учебная» (3.0.9.150) + учебная платформа 1С:Предприятие 8.3.24.1548Даже на этом шаге вам потребуется умение разобраться с тем, как отличить базу от платформы, как развернуть конфигурацию и запустить ее. Проще будет уже знакомым с 1С пользователям, но даже для них некоторые шаги могут быть непривычными. Советую на первых порах не пытаться охватить все инструменты, про которые можете прочитать: EDT и т. п.Основы программирования и конфигурирования 1С можно изучить на онлайн-курсе «Разработчик 1С. Basic».Если будут сложности с установкой, первое задание с которым вам можно самостоятельно поработать — книга «1С:Программирование для начинающих. Детям и родителям, менеджерам и руководителям. Разработка в системе „1С:Предприятие 8.3“».Рисунок 1. 1С: Программирование для начинающихКаких ошибок стоит избегать?Итак, вы получили первые навыки и уже представили как добавите все 10–20 полей. После этого первое чему вам нужно научиться — не модифицировать типовую систему. Любые эксперименты вы можете провести с демо‑конфигурацией, с демонстрационными типовыми базами, но на реальной работающей системе лучше:Делать модификации системы с подготовкой технического задания даже для себя в упрощенном виде.Научится делать резервные копии перед любыми изменениями, в том числе перед обработкой данных внешними инструментами.Не снимать с конфигурации «замок», избежать модификации кода или реквизитов, пока пока не поймете и не получите подтверждения от опытных коллег, что без этого не обойтись, и ваше решение корректно.Исключать изменения системы или массового изменения данных во время работы ваших коллег.Избежать изменений цветов или шрифтов типовой конфигурации — здесь система достаточно консервативна.Какие эксперименты можно поставить?Но вы спросите — а как тогда учиться, если не делать изменения в реквизитах и коде? Могу дать следующий путь (это только один из вариантов от простого к сложному):Модификации внешних печатных форм и доработка для вывода своей информации, как использовать программно добавляемые поля объектов.Разработка собственных внешних отчетов (но помните, что неоптимальный отчет также может «загрузить» вашу рабочую базу и остановить работу, если ваша 1С не настроена на предотвращение избыточной нагрузки), поэтому — делаем опыты на копии.Механизм расширений, доработка интерфейса под «себя» — после этого можно модифицировать встроенные печатные формы/отчеты.Библиотека БСП — как она может облегчить решение ваших задач. На базе этой библиотеки придумать и сделать свой собственный домашний проект.Внешние обработки для интеграций, например с тем же Телеграм для уведомлений — обращаемся к infostart.ru.Для более сложных задач желательно начать работать в команде, так как самостоятельная модификация типовых без помощи в изучении их внутреннего устройства может отнять немало вашего времени и привести к реализации сложных/неоптимальных решений — «изобретению велосипеда».Как взаимодействовать в команде?Итак, вы получили навыки разработки. Понимаете, что 1С можно адаптировать под вас и ваш бизнес, стремитесь теперь эффективнее управлять вашими коллегами, и пишете задачу разработчику в виде:Добавить реквизит «Ответственный за сборку» в документ «Сборка товаров» в 1С ERP, вывести на форму списка документов «Внутренние документы (все)» и добавить запись в регистр «Заказы на сборку».Что же тут может пойти не так:Разработчик не задает никаких вопросов и делает все как вы описали — с такими разработчиками лучше дел не иметь, так как не всегда решение даже опытного аналитика технически будет лучшим.Ваш разработчик спрашивает — а зачем вам это необходимо и вместо уточнения вы сами реализуете задачу, например с использованием расширений.До того, как ваши решения получат подтверждения их корректности (с вашим опытом или с опытом коллег) на этом этапе лучше сконцентрироваться на подготовке качественных технических заданий с формулировкой бизнес‑задачи и предоставляя коллегам пространство для применения альтернативных решений, если они есть. Шаблон технического задания в общем виде:ТЕХНИЧЕСКОЕ ЗАДАНИЕ<наименование>1. Назначение и цель работДля чего нам нужно изменение, что оно даст2. Технические условия реализации /система реализацииГде будут произведены работы, в какой системе3. Задание на разработкуПодробное описание выполняемых работ, что должно получиться, если есть — расписать этапы работ4. Сценарий проверки успешной реализации работКритерий успеха или процесс, который должен получиться по итогу, условия проверки5. Сроки сдачи работ / этаповКонтрольные промежуточные даты/Даты окончанияПример заполненного технического задания по шаблону:ТЕХНИЧЕСКОЕ ЗАДАНИЕВнесение изменений в документСборка товаров в системе 1С ERP1. Назначение и цель работВнесение изменений в документ позволит фиксировать ФИО ответственного за сборку товара (вписывается вручную в акте сборки) и в случае претензий от клиентов найти сотрудника. Также планируется использовать показатели для расчета премиальной части сотрудникам без отрицательных отзывов.2. Технические условия реализации /система реализации1С ERP. Изменение не должно модифицировать типовую конфигурацию.3. Задание на разработкуНеобходимо отображать в документе «Сборка товаров» ФИО ответственного за сборку, предоставить возможность вывода реквизита в форме списка «Сборка товаров» и запретить проведение документа без заполнения ответственного. При проведении проверять что выбранный ФИО является действующим сотрудником или работающим по ГПХ.4. Сценарий проверки успешной реализации работНа закладке дополнительно выведено поле «Сборщик». Без заполнения документ записывается, но не проводится. В том числе не проводится если на дату действия документа выбранный сотрудник не действующий.При выборе действующего сотрудника документ проводится.В форме списка «Сборка товаров» можно вывести реквизит «Сборщик» и начальники складов могут вывести данное поле самостоятельно / или им установиться данная колонка автоматически.5. Сроки сдачи работ / этапов21/06/2025/22.07.2025Ваши технические задания параллельно реализуете сами и смотрите за способами решения опытных коллег.Что это даст вам — вы можете параллельно с реализацией сложных задач изучать, как они создавались исполнителями, при договоренностях задать им вопросы и сверять свой опыт/гипотезы с реализацией коллег. Если же вы реализуете все самостоятельно — найдите на этапе профессионального роста доверенного консультанта и покажите техническое задание и вашу реализацию ему.Как расти и куда двигаться дальше?Для тех, кто планирует развиваться в карьере как 1С разработчик — можно следовать «стандартным» рекомендациям 1С: после базовой книги пройти обучение по автоматизации отдельных видов учета — оперативному, регламентированному, МСФО и т. д., но предупреждаю, что на практике задачи будут на порядок сложнее — для вас не будут снижать «сложность» конфигураций. Типовые конфигурации содержат множество механизмов, требующих как понимания архитектуры решения, так и бизнес‑логики, и кроме этого, постоянно модернизируются и встав на путь разработчика 1С вам придется освоить принципы непрерывного самообучения или обращаться за помощью к опытным коллегам и менторам. Что вам может потребоваться при переходах на следующие уровни:изучить стандарты разработки 1С;разобраться с интеграциями и обменами данных (распределенные базы, типовые обмены, шины данных);изучить способы групповой разработки, использования хранилища и даже перейти к использования GIT — полезно даже если вы работаете один;освоить инструмент EDT;познакомиться с пакетом утилит OneScript.Для бизнес‑пользователей — рекомендую пойти по пути освоения прикладных особенностей и инструменты 1С:Создание отчетов и применение СКД (есть отдельные хорошие книги);Проектирование интерфейса;Инструменты импорта в 1С и особенности экспорта из 1С.Часто на консультациях задается вопрос — а нужно ли фундаментальное образование как программиста, чтобы стать разработчиком 1С? Ответ — оно может быть вспомогательным, но его отсутствие не является стоп‑фактором. Гораздо полезнее собрать свой собственный образовательный трек исходя из своих интересов/сильных сторон и конечно же планов в вашей карьере. Но только если вы мечтаете самостоятельно разрабатывать плагины, познакомиться с конвейером непрерывной разработки и поставки CI/CD даже дорабатывать саму платформу — то тут без фундаментальных знаний не обойтись, вам потребуется С++, java а для них уже понимания прикладной области будет недостаточно.Если ваша цель — стартовать максимально эффективно, с поддержкой группы и возможностями задать практические вопросы и получить ответы — приходите на курс для желающих быстро погрузиться в разработку: Разработчик 1С. Basic."
46,46,"Как провести демо, после которого бизнес-заказчик останется доволен",КОРУС Консалтинг,Заряжаем бизнес инновациями,4.68,"Консалтинг и поддержка, Электронная коммерция, Оптимизация",2025-03-13,"Привет, Хабр! Меня зовут Маргарита, я аналитик в департаменте e-commerce КОРУС Консалтинг. Я провела множество демонстраций разработанной функциональности на разных проектах, а также присутствовала на них как слушатель и специально наблюдала за реакциями участников со стороны. В этой статье я поделюсь опытом о том, как эффективно провести демо для бизнес-заказчиков на примере нового раздела «Обращения» на b2b-портале. Многие приемы применимы и к другим форматам — обучению пользователей, пресейлам и т.д.В конце статьи я сформулировала 9 советов, которые помогут повысить эффективность демо для бизнес-заказчика.Этап 1. Подготовка к проведению демонстрацииДемо проводится в конце этапа разработки (спринта, MVP, блока задач) как этап сдачи функциональности заказчику. Я рассматриваю его не как простую формальность, а как публичное выступление с определенными целями:Продемонстрировать выполненную командой работу так, чтобы задачи быстрее перешли на следующий этап (были приняты бизнесом к тестированию, получили согласование релиза на прод и т.д.)Рассказать об изменениях в продукте так, чтобы бизнес точно понимал их.Получить обратную связь и вместе сформулировать идеи по развитию, которые пополнят бэклог.Поддержать и развить партнерство с заказчиком.Достижение этих целей требует выделения достаточного времени на подготовку, которая состоит из следующих шагов. Определяю аудиториюПеред любым выступлением краткий анализ аудитории помогает определить, на каких моментах нужно сделать акцент, а какие затрагивать нет смысла. Так можно эффективнее распределить время, уменьшить число вопросов, а главное — удерживать внимание слушателей. Так, например, на демо для руководителей бизнес-подразделений я сделаю упор на пользе для компании и конечного пользователя, а не на деталях технической реализации. Я узнаю, кто будет приглашен, и формулирую:Какова роль каждого из слушателей? Какие у них цели?     Насколько они погружены в продукт (проект)?     Какие термины привыкли использовать?Иногда на этом этапе происходит обратное — содержание выполненных задач приводит к пересмотру состава участников со стороны бизнеса.На последнее демо моей команды по новому разделу «Обращения» на b2b-портале были приглашены текущие и новые участники проекта из разных подразделений. Все они — представители бизнес-направлений, которые погружены во множество задач и потоков информации помимо развития нашего сайта. ИТ-терминология для них специфична и не всегда понятна. Иногда они используют разные термины для одних и тех же объектов. И моя задача — сделать демо понятным для каждого слушателя.Готовлю общую информацию о системе Если на демо будут слушатели, которые увидят систему впервые, я уделяю несколько минут общей информации о навигации и ключевых функциях, даже если они не связаны с демонстрирующими задачами напрямую. Такая вводная поможет слушателям сосредоточиться на основной части демо и меньше отвлекаться на изучение меню или других элементов.Так, для демо по обращениям на b2b-портале я выделила следующие вопросы:Какие имеются навигационные элементы? Где найти основные разделы?Какие имеются роли пользователей? В чем их ключевые отличия?Как получить доступ и авторизоваться в системе? Определяю список сценариев, которые важны для бизнесаЯ рассматриваю демо как имитацию работы будущего пользователя в системе. Как правило, бизнес-заказчика интересуют основные сценарии, по которым будет работать пользователь. В моем примере это создание обращения и просмотр информации о нем в каждом из статусов.Если времени достаточно, я дополняю их альтернативными сценариями. Например, сохранение данных в Local storage, чтобы продолжить заполнение, если вкладка браузера была закрыта до отправки формы. Еще это может быть обработка ожидаемых ошибок, вызываемых пользовательскими действиями.А вот от негативных сценариев, связанных с обработкой технических ошибок, я в большинстве случаев отказываюсь. Они вызывают много вопросов, ведь бизнес ожидает, что система будет работать без сбоев. Хотя все зависит от задачи, и иногда эти сценарии могут быть не менее важны.Также на демо для бизнеса я опускаю технические аспекты реализации, если они не отражаются на работе пользователей. В моем примере это особенности реализации интеграции с S3 для хранения прикрепленных к обращениям файлов — передача файлов во временный бакет, инициация запроса на их перенос в постоянный и т.д.Хочу отметить, что такие вопросы могут включаться в план демо, если на нем будут присутствовать ИТ-специалисты со стороны заказчика (актуально для подрядчиков). Учитываю интеграции со смежными системамиЕсли в задаче участвует интеграция со смежной внутренней системой, я стараюсь организовать демо сквозного бизнес-процесса от его начала и до завершения. В зависимости от состава участников и их погруженности в продукты, действия в смежной системе может выполнять команда разработки или сам заказчик. Важно договориться о зонах ответственности заранее, чтобы все были готовы.Этот подход может не подойти для сложных бизнес-процессов, затрагивающих множество сервисов. Но он выглядит более понятным для бизнеса и может быть реализован, если в процессе задействовано 2-3 системы.В моем примере мы подготовили демо вместе с командой CRM. Мы показали весь бизнес-процесс обработки обращения: его создание клиентом на b2b-портале, действия менеджеров в CRM и отображение каждого из них для клиента.Прописываю шаги и важные акцентыДетализация может казаться излишней, но она упрощает формирование структурного рассказа, в котором не будут упущены важные для слушателей акценты. Можно заранее сформулировать ценность для бизнеса или пользователя, которую несут функции, проверки и подсказки, встречающиеся в сценариях. Сделать это помогает проведенный ранее анализ аудитории.Обсудите расстановку акцентов с аналитиком, который собирал требования к задаче. Он поможет учесть то, что было особенно важным для заказчика. В моем примере при обсуждении обращений заказчик несколько раз говорил о своей боли: из-за отсутствия раздела на b2b-портале клиенты отправляют обращения на почту, но забывают указать данные заказа. Без них рассмотреть обращение невозможно, поэтому менеджеры должны позвонить клиенту и уточнить их. На демо я уделила особое внимание этому блоку полей на форме и особенностям его заполнения.Приведу начало сценария создания обращения на b2b-портале:Перейти к форме создания обращения, проговорить значение по умолчанию в поле «Тема» и соответствующие ему блоки полей.Изменить значение в поле «Тема», проговорить изменение следующих полей. Проговорить логику формирования списка заказов (только отгруженные за последние 3 месяца) и работы поиска (по полям A, B, C) в поле «Номер заказа», заполнить поле.Далее в этом сценарии описаны все шаги заполнения формы, отправка обращения, изменения на стороне CRM в ходе его обработки и отображение этих изменений на b2b-портале.Уделяю внимание администрированию системыДля упрощения дальнейшей коммуникации я проговариваю, какие изменения можно проводить без участия разработчиков: указывать настройки, редактировать справочники, загружать новости и т.д. Если у участников демо есть нужные права, я показываю им, как это сделать. В противном случае я проговариваю, к кому они могут обратиться. Готовлю данные, которые не будут сбивать слушателейПо моим наблюдениям, на демо важно показывать данные, приближенные к реальным. Заказчик часто воспринимает данные (содержимое базы данных и других хранилищ) и логику их обработки (работающий код) как единую систему. А абстрактные данные (тест 1, 1234567890, abcde), неактуальные данные (закрытые склады, снятые с производства товары), некорректные тестовые данные (сумма заказа 200 руб. при первоначальной стоимости 100 000 руб. и скидкой 50%) перетягивают внимание на себя. Они заставляют слушателей представлять корректные данные на нужных местах и отвлекают от демо, побуждают задавать вопросы об этих несоответствиях.Также важно согласовать данные с командой, чтобы в них не вносились изменения.Для демо раздела «Обращения» я выбрала компанию с подходящими заказами и удалила обращения темами вида «Тест 1». Также я заранее подготовила содержание обращения, чтобы быстро скопировать его, когда потребуется.Предварительно прохожу по сценариям При подготовке я прохожу по составленным сценариям на той среде разработки, на которой буду проводить демо. Так можно заметить проблемы, если некоторые части кода не были доставлены или были удалены при решении конфликтов.Я как аналитик использую этот шаг не только для подготовки к демо, но и как возможность стать пользователем собственной системы и оценить ее удобство. Это влияет на мои решения при выполнении следующих задач. А иногда так возникают идеи для наполнения бэклога, которые не были очевидны при проектировании. Этап 2. Проведение демонстрацииВначале я называю задачи, которые буду демонстрировать, и их ценность для бизнеса и пользователя, а затем перехожу к подготовленному плану.Прохожу по сценариямЯ провожу демо по памяти, но имею под рукой написанные сценарии. Хорошая подготовка позволяет чувствовать себя уверенно, что сразу слышно в речи и влияет на восприятие информации, которую я передаю.Я стараюсь озвучивать выполняемые шаги по структуре: действие пользователя — отклик системы. Например: Для перехода к форме я нажимаю «Создать обращение». Отобразилась сокращенная форма, по умолчанию поле «Тема» заполнено значением Х.Если я выбираю вариант Y в поле «Тема», форма изменяется. В конце появляются дополнительные поля А, B и C, а также подсказка D.В поле «Номер заказа» доступен список заказов моей компании, отгруженные в течение последних 3 месяцев. Это ограничение ускоряет загрузку страницы. Если я хочу отправить обращение по более раннему заказу, необходимо… и т.д.За счет того, что я проговариваю не только свои действия, но и отклик системы, а также ценность функции, длительность комментариев к каждому шагу увеличивается. У слушателей появляется больше времени для изучения интерфейса и подготовки комментариев, а у экрана — для загрузки при разной скорости интернета. При этом в речи не возникает пауз, она воспринимается естественно.Использую термины, понятные для аудиторииПо моим наблюдениям, важно следить за использованием терминов и давать их определения, когда без них не обойтись. Я всегда стараюсь заменять ИТ-термины, англицизмы и просторечия на их аналоги:Деплой → процесс доставки кодаШарить экран → демонстрировать экранЗадизайблено → поле недоступно для редактирования, кнопка неактивнаПоп-ап → всплывающее окно или сообщениеПлашка → отметка или информационное сообщение и т.д.Хотя термины могут казаться понятными из контекста и сопровождаются демо, при их использовании я часто вижу нахмуренные брови и другие проявления напряжения на лицах бизнес-заказчиков :)Поэтому я стараюсь их избегать, чтобы облегчить восприятие информации. Это проявление уважения к клиенту, который часто перегружен большим количеством информации по другим процессам. На демо по обращениям участники разных бизнес-подразделений иногда использовали разные термины для одних и тех же объектов, поэтому я старалась проговаривать оба термина.Обрабатываю обратную связь и ищу в ней ценные идеиВо время демо возникают вопросы, замечания и даже споры. Это нормальный процесс, который может стать источником идей для развития продукта. Он указывает на то, что участники вовлечены в демонстрацию и для них важен результат. Поэтому на встрече должен присутствовать аналитик, который занимался задачей. Он прокомментирует, по какой причине и в каком контексте было принято решение в пользу конкретной реализации. Иногда заказчик может задавать вопросы не по теме встречи или по уже показанной функциональности. Тогда я кратко отвечаю на них, но продолжаю модерировать встречу и удерживать участников в рамках подготовленного плана. Это повышает вероятность успеть показать все задачи за отведенное время и запустить дальнейшие процессы — тестирование заказчиком, выпуск релиза и т.д. Вместо ответов в моменте я предлагаю запланировать отдельную встречу и обсудить вопросы на ней, если это необходимо. Эта встреча может иметь более узкий состав участников.Проговариваю дальнейшие шагиДоговориться о дальнейших шагах, сроках и ответственных за их выполнение — важный этап любой встречи. Это могут быть вопросы о тестировании со стороны бизнеса, сроках поставки релиза, устранения замечаний и обсуждения открытых вопросов. После встречи у всех участников должно быть единое понимание следующих действий. Этап 3. После демонстрацииКонечно, после проведения демо мы с командой обрабатываем обратную связь и устраняем замечания. Но есть еще два важных шага, которые я выделяю.Отправляю протокол встречиПосле демо, как и после любой встречи с заказчиком, я отправляю всем участникам протокол. Я включаю в него краткое описание задач (просто копирую подготовленные Release Notes), а также все замечания, идеи, вопросы и договоренности.Рефлексирую и оцениваю полученный опытВажно выделить время, чтобы проанализировать полученный опыт и сделать выводы на будущее. Какую обратную связь озвучил заказчик? Что ему было не понятно в ходе демо? На что он реагировал негативно, а на что позитивно? Ответы на эти вопросы и применение их в дальнейшем позволяют развиваться профессионально и улучшать отношения с клиентом, ведь он будет видеть изменения и внимательное отношение. Хотя обычно мы говорим, что наша компания работает с компаний-заказчиком, на самом деле работают друг с другом конкретные люди, отношения между которыми влияют на общий результат. Даже небольшие, но регулярные улучшения со временем приводят к большим переменам.9 советов, как сделать демо для бизнес-заказчика эффективнее Перед демо проанализируйте состав участников и адаптируйте рассказ под их позицию и опыт.Если на демо приглашены слушатели, которые увидят систему впервые, начните встречу с вводной информации о ней, и только потом переходите к частным сценариям.Сосредоточьтесь на основных сценариях использования системы. Опустите ошибки и технические аспекты реализации, если они не влияют на работу пользователей.Если есть возможность, лучше показывайте сквозной бизнес-процесс от и до во всех задействованных системах.На демо показывайте данные, приближенные к реальным. Тестовые данные будут сбивать слушателей с толку.Сократите использование специфической ИТ-терминологии и англицизмов. Речь должна быть понятной и привычной для слушателей.Обратная связь, даже если она негативная — это источник идей и показатель заинтересованности в результате. Не игнорируйте предложения заказчиков. После завершения демо выясните, почему у них возникли конкретные предложения и оцените их вместе.Все участники должны одинаково понимать дальнейшие шаги, их сроки и ответственных. Зафиксируйте их и направьте участникам по почте.После демо уделите несколько минут рефлексии и проанализируйте полученный опыт. Маленькие шаги постепенно приведут к большим результатам.А какие интересные демо для бизнеса проводили вы? Буду очень рада узнать о вашем опыте в комментариях."
47,47,Как мы автоматизировали обжарку кофе и доставляем 100 тысяч заказов в месяц почти без ошибок,Tasty Coffee,Компания,169.48,Электронная коммерция,2025-03-13,"Привет! Мы — Tasty Coffee. Каждый месяц обжариваем, упаковываем и доставляем кофе тысячам покупателей и кафе по всей России. Формально кофейные зерна хранятся до двух лет, но в реальности они остаются свежими только один-два месяца после обжарки. Дальше с каждым днем во вкусе появляется больше неприятной горечи, а аромат кофе улетучивается. Поэтому мы стремимся, чтобы зерна приезжали быстро и максимально свежими, но при этом без путаницы в заказах.В первые годы было попроще: заказов мало, поэтому один сотрудник быстро все упаковывал и отправлял, ошибок почти не было. Мы постепенно росли, и вместе с объемом заказов прибавилось работы и, соответственно, ошибок. Уже к 2018–2019 годам заказов стало так много, что люди почти каждый день жаловались, что им привезли не тот кофе или пачка не того объема. Так мы пришли к утопической мечте избавиться от ошибок и в итоге сократили их до трех в месяц. Дальше расскажем, как мы это сделали.Снижение с 0,7% до 0,0039% может казаться незначительным. Но если бы такой процент ошибок сохранился, каждый день пришлось разбираться с десятками перепутанных заказов и недовольных покупателейПринимали пару заказов в день и не ошибались, но выросли, и всё сломалосьИнтернет-магазин стал отдельным стартапом внутри компании. Офис был в обычной квартире в Ижевске. Сайт создали на бесплатном OpenCart и проработали на нем 5 лет, прежде чем написали свой с нуля. В первой команде было три человека: руководитель, менеджер и водитель. Всё делали сами: принимали заказы, собирали, возили на почту. Успевали всё и работали почти без ошибок, потому что принимали пару заказов в день.Через пару лет объем продаж вырос: один сотрудник стал принимать и собирать больше тридцати заказов в день. Было радостно, но рост открыл сложности на следующих этапах:Сборка. Тогда у нас было несколько сотен видов товара (SKU): кофе в зернах, пачки с мелким, средним и крупным помолом, упаковки по 250 г и 1 кг. При этом они выглядят почти одинаково — черные с белыми буквами, да и названия некоторых лотов очень похожи. Если не вчитываться, легко перепутать сорта кофе и отправить покупателю не то.Слева — сорт Бразилия Серрадо, справа — Эфиопия Иргачефф. Чтобы отличить пачки, нужно вчитыватьсяУпаковка. Чтобы стоимость доставки была оптимальной, мы старались подбирать коробки подходящего размера для заказов. Но это не всегда получалось. Например, в заказе большая кофемолка и одна пачка кофе. Сборщик берет большую коробку, заказ в ней болтается, кофе мнется, а аксессуар может разбиться.Если взять неподходяшую коробку, пачки не помещаются, упаковка мнется и выглядит неопрятноВсё это сказывалось на качестве работы и себестоимости доставки. Кроме того, покупатели получали мятые заказы, оставались недовольны и жаловались. Поэтому мы проанализировали проблемы, которые у нас возникали, и сейчас расскажем, как их решали.Проблема №1. Записывали заказы вручнуюСначала все заказы на доставку мы переносили вручную в Excel, сами составляли заявки для нашего производства и тратили на это кучу времени. Поэтому процесс автоматизировали первым — так у нас появилась выгрузка на производство в формате XLS по одному клику. Для сборщиков мы сделали таблицу, куда выгружались заказы и товары, которые в них входят.Так выглядела наша первая табличка. Жирным выделяли товар, на который сборщику нужно обратить внимание. Например, молотый кофе, который заказывают реже, чем зерновой, пачки весом 1 кг и количество, если пачек больше однойТакже мы сделали столбец с группировкой товаров по заказу, где видно, сколько пачек каждого веса нужно отправить. По этому столбцу сборщик мог быстрее сориентироваться, какую коробку выбрать для упаковки.Проблема №2. Доставляли заказы Почтой России и все вечера проводили в ее офисахДесять лет назад мемы о том, какая Почта России медленная, были очень актуальны. С тех пор многое поменялось, и теперь доставка работает так же быстро, как и у других транспортных компаний. Но раньше у почты не было личного кабинета для юрлиц, поэтому все посылки оператор регистрировал вручную. Если нужно отправить пару штук — ничего страшного. Но если заказов больше, оператор тратил очень много времени. Посылки нужно было принести в отделение Почты России, отстоять очередь, оплатить отправку наличкой, дождаться, пока система выдаст трек-номер, сохранить чек. А потом вернуться в офис и вручную занести каждый в нашу систему.Найти в длинном чеке от Почты России крошечный трек-номер и переписать его без ошибок — сложная задача, которая отнимает много времениЧтобы избежать очередей, мы выбирали почтовое отделение, которое работает до 22:00 и приезжали за час до закрытия. Ужас на лицах сотрудников почты помним до сих пор 😅А еще мы очень боялись отправлять заказы без предоплаты, наложенным платежом. Мы обжариваем кофе для каждого клиента и в случае возврата не сможем его потом перепродать: продукт перестанет быть свежим и вкусным. Если покупатель не забирал кофе, нам приходилось его выбрасывать, пить самим или раздавать друзьям.Через несколько месяцев после начала работы мы нашли филиал курьерской компании Shop-Logistics в Ижевске. Еще через год узнали, что в нашем городе есть филиал СДЭКа. У них были адекватные цены и удобный личный кабинет, чтобы регистрировать посылки онлайн, поэтому мы решили с ними работать.Первое время мы заносили заказы вручную, а потом начали выгружать напрямую из нашей системы. Скорость менеджера увеличилась в несколько раз: без автоматизации на обработку одного заказа уходило 3–4 минуты, а с ней всё стало почти моментально.Автоматизация — это удобно, но есть и обратная сторона. Например, однажды утром нам позвонили из сервиса доставки PickPoint и сказали, что компания работает последние 2 часа и закрывается навсегда. А у нас заказы ехали на их склад в Москву, при этом следующую машину мы уже загружали. Пришлось останавливать погрузку, разворачивать фуру и целую неделю в огне согласовывать детали и перенаправлять заказы. Даже позвали на помощь коллег из соседних отделов, чтобы распределить ручную работу. Увы, часть заказов так и застряла у транспортной компании — пришлось отправить новые за свой счет. С ситуацией справились, но осадочек остался.Проблема №3. Сборщики путали пачки и выбирали неподходящие коробкиМы фиксируем все ошибки, даже небольшие, чтобы не пропустить системную проблему. Очевидные косяки исправляем сразу же, а если не понимаем причины — ждем, когда повторится. Все идеи обсуждаем и вносим в список на исправление — там всегда есть над чем работать. Мы успели найти и исправить много проблем благодаря собственной программе для сборки заказов. Вот с чем больше не сталкиваемся:Путаница с заказами. Чтобы сборщики клали в заказы правильные пачки кофе, мы стали печатать на них QR-коды с информацией о каждом сорте. Первые коды мы ввели на производстве, а потом эту же систему внедрили для интернет-магазина.Когда сборщик сканирует код, система распознает это и автоматически вычеркивает пачку из заказа. Это помогает избежать ошибок при сборке:Видно, если отсканировали не ту пачку.Нельзя отсканировать одну пачку несколько раз, потому что уникальный QR-код генерируется на пачку, а не на сорт.Не получится закрыть заказ, пока все пачки не отсканированы.Так сборщики сканируют пачки в заказеА вот как это выглядит на странице сборки заказаСложности с размером коробки. Мы проанализировали миллионы заказов, чтобы помочь сборщикам. Благодаря большому массиву данных мы почти всегда знаем размер коробки, который подошел для похожего заказа в прошлый раз. В результате сборщикам не нужно решать, какую коробку взять, и они собирают заказы быстрее.Сейчас система подсказывает подходящую коробку в 95% случаев. При этом, если сборщик не согласен с рекомендацией, он может взять другую коробку. В дальнейшем планируем подключить машинное обучение, которое сможет подсказывать верный размер даже для заказов, которых у нас еще не было.Так рекомендация выглядит на странице сборки заказамЗдесь сборщик взял правильную коробку, всё идеально помещаетсяНеправильная комплектация заказа. Чтобы сборщикам было сложнее перепутать состав заказов, мы начали проверять вес после сборки. Например, мы точно знаем, сколько весит пачка кофе, шоколадка, аксессуар и другие товары. Когда сборщик ставит заказ на весы, система суммирует вес всех товаров, которые в него входят, добавляет коробку, а потом сравнивает с эталонным. Если всё верно, можно работать дальше. Если нет, нужно разобрать заказ, заново отсканировать товары и найти проблему.Так выглядит предупреждение в личном кабинете сборщикаДолгие размышления, где найти товар. Чтобы сборщикам не приходилось тратить много времени на поиск нужного товара из сотен наименований, мы разработали специальную систему адресного хранения.У популярных товаров есть свой уникальный адрес на полке. Если товар заказывают редко — несколько раз в день, мы кладем его в микс-контейнеры. Товары, которые не входят в один контейнер, мы разбиваем на две категории:На полку с определенным адресом. Ставим контейнеры с товарами на полки у станции сборки. Самый популярный товар в категории находится под адресом №1 и так далее по убыванию.В кеш. Используем для временного хранения. Сборщики берут товар из кеша, когда он заканчивается на основной полке.Получается, что товары, которые покупают чаще всего, находятся на уровне рук и поближе к станциям. Это экономит время и силы сборщиков, поэтому они успевают сделать больше.Так выглядит рабочее место сборщикаБывает, что производство выдает некоторые лоты позже, чем начинается смена в интернет-магазине. Тогда система сборки автоматически пропускает те заказы, товары для которых еще не готовы.Беготня по цеху в поисках товара. Чтобы сотрудникам не приходилось бегать по всему цеху за пачками кофе, мы посчитали количество шагов от каждой станции сборки до каждой ячейки и написали специальный алгоритм. Система выдает заказы так, чтобы люди на каждой станции проходили минимальное количество шагов. Например, сборщик работает на станции у ячеек №1 и 2. Ему будут чаще попадать заказы с товарами из них или ближайших.А чтобы сотрудникам было легче запомнить состав заказов, мы стали сортировать товары внутри них по номеру адресной ячейки. Раньше товары в заказе были в том порядке, в котором клиент клал их в корзину, — например, 6, 4, 3, 8, 1. Это сложно запомнить. Куда проще, если номера идут по возрастанию.Еще мы добавили в программу фото товаров в упаковке. Так находить их получается гораздо быстрееДо того как мы ввели все эти правила, за смену каждый сотрудник собирал от 100 до 200 заказов. Сейчас объемы выросли до 400 заказов на каждого сборщика.Опоздания с отправкой заказов в транспортные компании. Если заказ собран правильно, система отправляет на принтер ярлык транспортной компании. Он заранее сохраняется в кеше системы, поэтому печать занимает меньше секунды. Сборщик приклеивает ярлык на коробку, и заказ всегда уезжает по верному адресу.Заказы для каждой транспортной компании мы собираем к определенному времени. Мы определяем его так, чтобы перевозчик успел забрать у нас заказы, привезти на свой склад, обработать и сразу же отправить по маршруту. Главное правило — чтобы заказы уехали из Ижевска в тот же день, а покупатель получил максимально свежий кофе.Если клиент из Москвы, Казани или Питера, мы можем доставить заказ за 1–2 дня, потому что отправляем своими машинами. Время прибытия в город мы планируем на раннее утро, пока на дорогах свободно. В этом случае заказы можно успеть доставить клиенту в этот же день.Проблема №4. Не все сотрудники могут быть одинаково мотивированы на работуЛюдей на сборке становилось все больше, поэтому мы задумались: вдруг возникнет ситуация, когда один старается, а другой работает медленно или вообще сидит в телефоне. Зарплата у всех одинаковая, несправедливо.Мы ввели несколько систем, которые помогли сделать процессы и результат открытыми для всей команды. Если кажется, что сосед спит, можно просто проверить общий рейтинг. А еще мы добавили немного геймификации, чтобы собирать заказы было не скучно. Вот самые интересные способы:Определение среднего времени сборки каждого заказа. Мы стали анализировать данные: кто с какой скоростью собирает заказы, сколько успевает за смену. Теперь у нас есть таблица, в которой все сборщики видят результаты друг друга. Если кто-то собирает заказ существенно дольше среднего времени, система ненавязчиво напоминает: хорошо бы ускориться, чтобы успеть к дедлайну.Это главный экран сборщика в программеЗащита от пропуска неудобных заказов. Мы добавили сборщикам возможность пропускать заказы, которые нельзя собрать прямо сейчас. Это нужно, чтобы станции не простаивали, если товара по какой-то причине не оказалось в цехе. Но возможностью можно злоупотреблять: например, когда лень собирать сложный заказ. Чтобы такого не происходило, система выделяет пропущенный заказ. Если кто-то из команды не будет браться за сложные или неудобные заказы, это заметят все.Так выглядит сообщение о пропуске заказа — видно, кто это делал и когдаКонтроль количества пауз. Сборщик может поставить смену на паузу, чтобы отлучиться от станции на время. Но иногда паузами злоупотребляют, например, чтобы повысить свою скорость. Бывало, что сборщики открывали заказ, запоминали адреса ячеек, ставили паузу и бежали за кофе. А система засчитывала меньшее время сборки. Мы заметили это, но решили не запрещать паузы, а, наоборот, показывать всем, кто сколько взял.А вот общий рейтинг, где видно паузы. Например, здесь Владислав не взял ни одной, а Рустам — целых 22, потому что у него есть дополнительные обязанностиПонимаем, что элементы соревнований могут нравиться не всем, поэтому у нас нет штрафов или премий за скорость сборки. Система просто помогает оценить свою производительность по сравнению с остальными. Это дает результаты. Например, после того как мы внедрили все новые правила, скорость увеличилась еще на 20%.За 10 лет команда сборщиков выросла всего в 13 раз, а объем заказов — более чем в тысячу разЗа первый год работы мы отправили всего 734 заказа, а за 2023 год — 636 736, а в 2024 году — почти 1 млн заказов. Штат сборщиков при этом остался небольшим — сейчас у нас работает 13 человек, и они справляются почти без переработок.Вот как это выглядит на графиках.В 2014 году один сотрудник собирал 3 заказа в день, а в 2024-м — уже больше 300До 2022 года в штате не было и десяти сборщиков. Несмотря на объем заказов, команда остается небольшойБлагодаря автоматизации сборка одного заказа обходится нам примерно в 15 ₽ — в 8,5 раза меньше, чем 10 лет назадСейчас мы отправляем около 100 000 заказов в месяц, и ошибки, конечно же, случаются, но всего 2–3 раза в месяц. Помог наш подход: анализировать процессы, находить недочеты и исправлять по возможности. Мы не наказываем сотрудников за ошибки, а, наоборот, стараемся сделать так, чтобы косяки не повторялись.Все эти годы мы активно растем: не только по объему заказов, но и по количеству персонала в штате. А количество ошибок, наоборот, снижается — в 2024-м их было только 38Автоматизация склада влияет не только на качество и скорость работы, но и на удовлетворенность покупателей. Люди получают именно то, что заказали, остаются довольны и возвращаются к нам.Не все идеи оказывались удачными. Например, как-то попробовали сборку по категориям, но она получилась не такой удобной, как система с шагами. В другой раз купили весы без дисплея и кнопки «Тара», а они задевали бортики и плохо работали. Или заменили поставщика чая, но из-за новых артикулов вся система сборки встала. Мы нормально относимся к таким провалам — это тоже опыт, без него нет роста.Поэтому не планируем останавливаться на фишках, которые сейчас хорошо работают, а постоянно придумываем что-то новое. Например, вот пара свежих идей:Мотивирующие сообщения. Ребята, которые работают быстрее других, получают вдохновляющие комментарии от системы. Например: «Ты собираешь заказы лучше, чем сын маминой подруги!», «У тебя что, 8 рук?!»,«А ты точно человек?» или «Псс… не хочешь поучаствовать в программе клонирования?». При этом мы понимаем, что кому-то такие сообщения могут не понравиться, поэтому их можно отключить.Вот пример такого сообщения в программе. Когда сборщик ее открывает, он видит еще и россыпь конфеттиУчет расходных материалов. Это коробки, шоколадки, стикеры и брю-гиды. Сейчас руководитель цеха сборки каждый день составляет заявку, чтобы заказать расходники. Мы написали систему, которая сама считает их остатки на складе, прогнозирует расход на следующий день и отправляет заявку, если нужно.Нам важно, чтобы люди вокруг нас были довольны. Команда быстро собирала и отправляла заказы, а покупатели пили свежий и вкусный кофе. Будем рады, если наши идеи помогут кому-то еще. А если у вас есть советы, что улучшить в работе, расскажите — мы ценим все идеи ❤️Заходите на наш YouTube-канал, и мы проведем вам экскурсию по цеху сборки. А чтобы попробовать свежий кофе, заглядывайте в наш интернет-магазин 🤗"
48,48,Как умный дом помогает воспитывать трех детей,Wiren Board,Оборудование для автоматизации и мониторинга,215.61,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-03-13,"Мы продолжаем рассказывать об умных квартирах и домах в разных уголках России. На этот раз посетили Красноярск, встречу организовал интегратор AVA Electro. Хозяин квартиры Антон — инженер по промышленной автоматизации, поэтому после покупки квартиры она обречена была стать умной. Интегратор даже пожаловался: на этапе пусконаладки, когда не все устройства еще были прописаны в системе, вдруг полезли ошибки. Оказывается, Антон уже начал экспериментировать со сценариями.Нам удалось познакомиться с умной квартирой и пообщаться с владельцем. Конечно, мы поинтересовались тем, какие преимущества Антон видит в умном доме. Ответы оказались весьма неожиданны: помимо обеспечения комфорта, умный дом взял на себя некоторые воспитательные функции. Кстати, детей трое: сын и дочь детсадовского возраста и одна дочь — ученица младших классов. О воспитании мы подробнее расскажем в разделе сценариев.А пока позвольте познакомить с умной квартирой площадью 205 м² с пятью комнатами: три детских, хозяйская спальня и кухня, совмещенная с гостиной. Плюс два санузла, постирочная и гардеробная. И три балкона, два из которых теплые.Дизайн-проект и реальностьСначала покажем дизайн-проект квартиры, чтобы оценить задумку дизайнера и реализацию. Под спойлером добавили больше иллюстраций из проекта, а в статье использовали уже фотографии интерьера. Кстати, дизайнер — Евгения Гарбенко из Красноярска. По ссылке вы можете посмотреть больше фотографий интерьера квартиры.ПрихожаяДополнительные фотоПлан квартиры с расстановкой мебелиСлева — прихожая, справа — кухня-гостинаяКухня-гостинаяКухня-гостинаяКухня-гостинаяКухня-гостиная (слева), коридор (справа)Кухня-гостинаяКухня-гостинаяМастер-спальняМастер-спальняМастер-спальняМастер-спальняБалкон при мастер-спальнеКомната сынаКомната сынаКомната сынаКомната младшей дочериКомната младшей дочериКомната младшей дочериКомната старшей дочериКомната старшей дочериБалкон при комнате старшей дочериВанная комнатаВанная комнатаГостевой санузелГостевой санузелПостирочнаяПостирочнаяПостирочнаяГардеробнаяКак все начиналосьАнтон уже имел опыт с умными устройствами на старой квартире. «Там я игрался с беспроводными решениями на Wi-Fi и Zigbee — Алисой, лампочками, датчиками, но часто сталкивался с их ненадежной работой», — вспоминает он во время нашей беседы. «Изначально для новой квартиры идей не было, но было интересно попробовать что-то новое. Это был небольшой эксперимент», — делится он.В офисе Антона уже работает автоматизация на KNX для управления светом, вентиляцией и температурой. Однако оборудование KNX оказалось слишком дорогим, а отсутствие доступа к исходным кодам пугало: «Если интегратор исчезнет, проект может встать», — объясняет он. Тогда Александр из AVA Electro предложил Wiren Board — открытую платформу на базе Linux, поддерживающую много протоколов (Modbus, Zigbee, RS485 и др.). «Это решение оказалось доступнее, гибче и с ним можно получить полный контроль», — отмечает хозяин.Кухня-гостинаяДополнительные фотоКоридорЛоток для кота установили в гардеробной, снизу оставили зазор, чтобы питомец мог попадать внутрь даже при закрытых дверях. Там же расположили парковку пылесосаА вот и сам питомецМастер-спальня. Найдите котаМастер-спальняВ постирочной обитают крысы. Кстати, кота они гоняют, хотя казалось быПрихожая. Слева — вход в гардеробнуюТурник в комнате сынаВид из коридора на гостиную. Слева — дверь в постирочнуюМозг системы: Wiren Board и Sprut.HubКонтроллер Wiren Board стал «мозгом» умного дома, интегрируя все умные устройства. А всего в квартире Антона установили:21 светильник без регулировки яркости, 16 групп диммируемых LED-лент (включая одноцветные и MIX) и 6 треков. В общей сложности получилось 42 группы управления светом и 69 клавиш. Управление через реле WB-MR6C v.2 и диммеры WB-LED с усилителями WB-AMPLED.7 раздвижных штор в проекте, но пока установили 4, моторы A-OK AM68 с управлением через RS-485.8 электрических теплых полов с термостатами Lytko 101Plus Zigbee и Wi-Fi.5 кондиционеров — в умный дом их будут интегрировать ближе к лету через модули ONOKOM HR-1-MB-B.5 бризеров TION 4s, сейчас подключен один через шлюз ONOKOM TN-1-MB-B (остальные четыре установят позже).11 датчиков протечки Neptun SW005 и 6 шаровых кранов Neptun Pro 12V для защиты от протечек, их подключили к двум модулям WB-MWAC v.2.11 универсальных датчиков WB-MSW v.4 для контроля температуры и влажности, шесть из них еще определяют уровень CO₂ и качество воздуха. Корпуса датчиков покрасили, чтобы они вписывались в дизайн квартиры.4 тихих двухскоростных вытяжных вентилятора, подключены через реле WBIO-DO-R10R-4. 9 отключаемых групп розеток, работают через релейные модули WB-MRWL3.Видеодомофон Aqara Doorbell G4 и Mesh-сеть из 3 роутеров для стабильной связи.Входная дверь с герконом и замком Aqara Smart Lock D200i.На контроллер Wiren Board установили пакет Sprut.Hub, который управляет всеми функциями умного дома, — у него свой веб-интерфейс и интеграция с голосовым ассистентом Алиса. Кроме того, через Sprut.Hub сделали и интеграцию с Apple HomeKit.Щит в гардеробной, верхняя часть выделена под автоматизациюДополнительные фотоНижняя — под электрикуСборка щита в мастерской AVA ElectroСборка щита в мастерской AVA ElectroКвартира. Скоро здесь установят щитПодключение кабелей к установленному щитуЩит установлен и подключенОсвещение и шторыВ квартире установили 42 группы освещения: обычные недиммируемые светильники, диммируемые LED-ленты, шесть треков. Причем изначально дизайнер заложил в проект больше подсветки, например в детских. Но Антон от части групп отказался, посчитав их избыточными.Почти вся подсветка в квартире циркадная, на основе CCT-ленты, за исключением зеркала в детской и под лавочкой в прихожей. То есть интегратор установил 95% ленты с изменением цветовой температуры. С помощью треков тоже можно менять температуру.В темных комнатах (санузлы, постирочная, гардеробная) свет включается при наличии движения. Причем сначала срабатывают датчики движения WB-MSW v.4 в коридоре: при этом сначала подсветка в смежных темных комнатах включается на 30%, и заходишь в уже освещенный санузел. При срабатывании датчика движения непосредственно в темной комнате яркость увеличивается на 50–70%. Если нужен яркий свет, можно включить основные светильники клавишей. Если движения нет пять минут, подсветка выключается. Сценарии подсветки Антон программировал сам на Sprut.Hub и выбрал уровни яркости, которые его не раздражают. Теперь, когда ночью выходишь в санузел, нигде самому свет включать не нужно.В хозяйской спальне, в гостиной и коридорах установили трековые светильники от Maytoni — благодаря им можно менять не только яркость, но и цветовую температуру. Пришлось докупить и хаб Maytoni, который выходит через Wi-Fi в свое облако, и светильниками можно управлять через приложение. Сами светильники работают с хабом через Bluetooth. К сожалению, интеграции с умным домом здесь нет, можно только включать/отключать через реле питание треков, которые запоминают состояние. Для изменения яркости или цветовой температуры придется запускать отдельное приложение. К счастью, есть интеграция облака Maytoni с Алисой, поэтому можно управлять еще и голосом. Решение с Maytoni не самое удачное: к трекам подведена витая пара, лучше было выбрать светильники с RS-485 или с управлением 0-10 В. Но Антон покупал треки еще до консультации с интегратором, по цене они были вполне адекватные. Впрочем, остальная подсветка дает достаточный для большинства ситуаций свет, поэтому треки включают редко.В комнате старшей дочери установили на потолке массив RGB-ламп Yeelight Smart. Клавишами их можно включать/выключать, но через приложение Yeelight можно менять цвет. Через облако есть интеграция с Алисой, которая умеет выставлять любой цвет. Например, зеленый или розовый под настроение. Но в Sprut.Hub эти лампы отдельно не заведены. Интегратор порекомендовал попробовать лампы Zigbee вместо Wi-Fi, Антон купил несколько штук и пока тестирует. Их уже получится подключить через Sprut.Hub.В проекте заложили семь приводов штор A-OK AM68 с управлением через RS-485, но пока установили только четыре. К сожалению, по рекомендации от продавцов штор сделали избыточный отступ от края натяжного потолка в 10 см. Из-за этого осталось меньше места для тюля, карниз штор слишком близко прижали к карнизу тюля. Поэтому шторы с моторами A-OK в гостиной отключили: «Они сминали тюль, и жена настояла на эстетике», — поясняет хозяин. В хозяйской спальне шторы работают, но с ограничением хода, чтобы сохранить складки.Отметим управляемые розетки в области окон во всех комнатах для подключения новогодних гирлянд. Также у всех нагревательных приборов в прачечной розетки тоже управляемые. То же самое верно и для розеток в детских, которые используют для зарядки гаджетов.Светильники в коридореДополнительные фотоСветильники и подсветка в коридореСветильник подсветкиПодсветка в другом коридореШторы в гостиной. Сверху трековый светильникПодсветка шкафа в гостинойПодсветку в санузле можно сделать холоднойА можно - теплойПодсветка в гостевом санузлеШторы в комнате сынаКомната сына. Справа - управляемые розетки с зарядкой смартфонаКомната старшей дочери. Розетки с зарядкой справа от кровати тоже управляемыеСветильник и шторы в комнате младшей дочериУправляемые розетки в комнате младшей дочериУправляемые розетки в мастер-спальнеШторы в мастер-спальне, выход на утепленный балконКлимат и вентиляцияВ доме восемь электрических теплых полов с термостатами Lytko 101Plus Zigbee и Wi-Fi: в санузлах, гардеробе и постирочной, а также в прихожей. Еще три теплых пола установили на балконах.Чтобы ограничить энергопотребление, сценарий выключает ночью в 01:00 подогрев во всех подсобных помещениях, а утром включает в 5:30. На утепленных балконах свой сценарий — перед сном повышается температура до 27°C. Старшая дочка любит засыпать на балконе, и для нее сделали максимально комфортные условия.Антон хочет углубиться в теорию управления теплыми полами. Возможно, снижение температуры на один градус не так заметно повлияет на комфорт, но существенно снизит счет за электричество? Он добавил возможность измерения энергопотребления всех восьми теплых полов через модуль WB-MAP12E с 12 каналами (он также измеряет общее потребление квартиры). Но пока на эксперименты нет времени, отложил до отпуска.С термостатами Lytko вышла забавная история. Их заказали в варианте Zigbee, но производитель выслал модели Wi-Fi. Пришлось вернуть заказ обратно, но и из новой партии половина работала не так, как нужно. Производитель выслал другие прошивки, которые оказались нестабильными. В общем, интегратору пришлось помучаться. И сейчас три термостата заведены в умный дом через Zigbee, а пять — через Wi-Fi.Отопление в квартире центральное, Антон оставил ручные вентили регулировки радиаторов везде, кроме детских. Провода к радиаторам решили не прокладывать, поэтому в детских он установил приводы Aqara с подключением Zigbee (в качестве хаба использовали камеру Aqara, которая сейчас расположена в постирочной). Работой приводов хозяин доволен: когда отопление запускали, в квартире было очень жарко. В гостиной регулировали отопление вручную, а в детских комнатах с этим отлично справлялся умный дом, что покажем на графике ниже.За приток свежего воздуха отвечает бризер TION 4s в гостиной. Для него пришлось сделать нишу, чтобы шторы не топорщились. По поводу остальных четырех бризеров Антон пока раздумывает и отслеживает состояние CO₂ в детских. Модели TION 4s занимают слишком много места, а компактные бризеры TION Mini оказались недостаточно производительными. Но к лету бризеры все же планируют добавить, провода к местам их установки уже проложили. Управление бризерами — через шлюз ONOKOM TN-1-MB-B.4 тихих двухскоростных вытяжных вентилятора установили в гардеробной, чтобы ограничить шум только одним помещением. Они удаляют воздух из двух санузлов, гардеробной и постирочной. Обычно вентиляторы работают на первой скорости, но при превышении уровня CO₂ значения в 1400 ppm они включаются на вторую. Вентиляторы выбрали максимально бесшумные, хотя в гардеробной их шум всё же слышен.В квартире установили мультисплит-систему с кондиционерами Haier: четыре внутренних блока в мастер-спальне и детских подключили к одному внешнему блоку. Кроме того, есть отдельная сплит-система в гостиной со своим внешним блоком. Управление всеми внутренними блоками — через модули ONOKOM HR-1-MB-B. Поскольку умный дом сделали зимой, кондиционерами Антон еще не занимался. Ближе к лету он планирует написать сценарии их работы.Мы обнаружили еще одно интересное решение: на кухне в шкафу с чайником и кофемашиной добавили датчик влажности Aqara и реле Aqara T2, которое включает вытяжку из шкафа при превышении определенного уровня влажности. Решение добавили уже после сдачи умного дома, поэтому сделали его на беспроводных устройствах. Вытяжка в гардеробной на переднем плане, в центре снимка - универсальный датчик WB-MSWДополнительные фотоКондиционер в гостинойТермостат Lytko и универсальный датчик WB-MSW в прихожейБризер TION 4s в гостинойТеплый балкон, примыкающий к комнате старшей дочериКроме теплого пола, на балконе есть дополнительный электрический нагревательПривод радиатора в комнате старшей дочериНад радиатором - управляемая розетка для новогодней гирляндыРадиатор в комнате младшей дочери тоже с приводомКондиционер в комнате младшей дочериКондиционер в мастер-спальнеДатчики и безопасностьВыше мы отметили универсальные датчики WB-MSW v.4 для контроля температуры и влажности —  в квартире их 11 штук, шесть из которых определяют в том числе концентрацию CO₂ и VOC. Корпуса всех датчиков покрасили, чтобы они вписывались в дизайн квартиры. Все датчики  с сенсором движения, к которому привязано управление подсветкой и режим охраны. Его можно включить, когда в квартире никого нет.Поскольку в квартире три водопроводных стояка, пришлось установить шесть шаровых кранов Neptun Pro 12V для защиты от протечек, которыми управляют два модуля WB-MWAC v.2. В мокрых зонах установили 11 датчиков протечки Neptun SW005.Входную дверь оснастили умным замком с распознаванием лиц Aqara Smart Lock D200i и звонком Aqara Doorbell G4 — «самая полезная функция», по словам Антона. «Дети теряли ключи, а теперь просто подходят, и дверь открывается. Видеозвонок записывает гостей, а через приложение можно выдать временный пароль», — рассказывает он. Также дверь оснастили проводным герконом, который используют в режиме охраны.У звонка Aqara Doorbell G4 есть отличная функция: когда кто-то звонит, на телевизор выводится изображение и можно с пульта от Apple TV открыть входную дверь или ответить гостю. Есть даже функция изменения голоса, чтобы ребенок был в большей безопасности, когда он один дома. Голос будет звучать совсем не по-детски.В доме есть робот-пылесос, который чистит комнаты по расписанию. Он выезжает каждый день в 10 утра. Причем вторник, четверг и суббота — сухая уборка, в остальные дни — влажная. Пылесос подключили к водопроводу и канализации, он сам сливает грязь, хозяевам только  раз в неделю приходится чистить сетку. Еще у пылесоса есть видеокамера, можно гонять его по квартире и смотреть, где что происходит. Была ситуация со сбоем датчика температуры в одной из комнат, и Антон просто «доехал» туда на пылесосе, чтобы посмотреть. Управлять пылесосом нужно через свое приложение.Умный замок Aqara Smart Lock D200i с внутренней стороны двериДополнительные фотоУмный робот-пылесосУмный замок Aqara Smart Lock D200i и звонок Aqara Doorbell G4 с внешней стороны двери Универсальный датчик WB-MSW в гостинойУниверсальный датчик WB-MSW в санузле и вытяжкаРаспределительные коллекторы водопроводаШаровые краны перекрывают подачу воды при протечкеЕще одна точка подключенияИ еще однаАвтоматическая сушилка, которая опускается с потолка. После загрузки она поднимает белье и продувает горячим воздухомУниверсальный датчик WB-MSW в постирочнойКамера Aqara в постирочной используется как хабСценарии и голосовое управлениеВ комнатах расставили колонки с Алисой (MiDi, Max, Duo Max), которые интегрировали в умный дом: дети включают свет, музыку. Есть и сценарии, о которых расскажем ниже. «Главное наказание для детей — забрать Алису», — шутит Антон.Например, сценарий «Пробуждение детей»: ставим будильник, и за 15 минут до подъема он постепенно увеличивает яркость теплой белой подсветки и включает приятную музыку. «Это особенно полезно зимой, когда утром темно, и дети ходят как лунатики», — смеётся Антон.Второй сценарий «Пора спать». Конечно, дети не хотят засыпать в положенное время. При наступлении отбоя блокируется основное освещение, а яркость подсветки снижается до 7%. В санузле вентилятор включается на 5 минут по шуму. Если в течение пяти минут не было шума, то вентилятор выключается. Мысль была привязать вентилятор к шуму от смыва унитаза, но он включался и при принятии душа или громком крике. Колонок в санузлах нет, но Антон об этом подумывает, чтобы включать музыку во время принятия ванны, а также управлять светом. Конечно, каждая колонка понимает, в какой комнате находится, и выполняет команды только с устройствами в этой же комнате. Интересна ситуация с двумя колонками в гостиной, совмещенной с кухней. Если команду слышат две Алисы, то система понимает, к какой из них пользователь находится ближе, и выбирает соответствующий сценарий. Причем колонка MiDi оказалась самой быстрой, в отличие от Duo Max с дисплеем, которая иногда тормозит.Колонка Duo Max с дисплеем на кухнеДополнительные фотоТелевизор в гостиной с колонкой MaxАлиса MiDi в комнате старшей дочериТелевизор и колонка Max в мастер-спальнеАнтон сам программирует сценарии на Sprut.HubПреимущества умного домаВ конце нашей беседы мы попросили Антона подвести итог. Все же многие читатели воспринимают умный дом как игрушку, которая со временем надоест. Хозяин с этим не согласен.«Если бы все, что делает автоматика, я делал каждый раз руками, то я бы этого делать не стал. Не стал бы каждый раз температуру детям в комнате менять, у них было бы либо жарко, либо холодно. Дети бы просыпались вот с такими глазами. Умный дом все делает без моего участия, и в этом его основная ценность.Свет в туалете — вроде бы, всегда можно нажать на клавишу, это не кажется критичным. С другой стороны, когда ты посещаешь санузел и ни к чему не прикасаешься, это намного комфортнее. Кроме того, включается не яркий свет, а комфортная подсветка, которая не раздражает.Свет можно детям ночью выключить автоматически. Иначе приходилось бы постоянно ходить его и опять выключать. Дети все равно включат свет, когда остаются одни. А тут сработала автоматика, дети это понимают и даже никаких вопросов не задают. Расписание есть расписание.Тема с входной дверью мне отлично зашла. Ключей не надо: приходишь — и дверь сама открывается. Дети спокойно заходят в квартиру и больше не теряют ключи.»В качестве примера интегратор упомянул два автомобиля, условно «Мерседес» и «Жигули». Они оба отвезут тебя из точки А в точку Б, но ощущения будут совсем другими. В «Мерседесе» будет комфортно и удобно, хотя на «Жигулях» тоже можно доехать. Кроме того, аспект безопасности: в дорогих авто есть различные системы типа подушек безопасности и АБС. В умном доме — это датчики протечки, например.«Здесь всегда можно посмотреть, что в квартире происходит. Закрыл дверь или не закрыл, когда ушел. Здесь дверь автоматически закрывается, можно состояние всегда проверить. Не доверяешь замку — есть геркон. По температуре в комнате видно, закрыл окно или нет. На практике этим несколько раз уже пользовался.История с этим утюгом несчастным, который вечно забывают выключить: отключил розетку и все. Хотя эта тема уже в прошлом, так как все современные утюги сейчас сами отключаются. Все нагревательные приборы в прачечной подключены к своему реле, которое мастер-выключатель выключает. У детей все розетки для заряда можно отключить. То есть тут вопрос спокойствия».А что вы думаете об умном доме Антона? Пишите в комментариях.Приходите к нам на WBCE 2025 — выставка и конференция по автоматизации. Отчёты с прошлых выставок.  "
49,49,"Прикручиваем нейросеть к боту, чтобы найти 68 страну для посещения",Альфа-Банк,Лучший мобильный банк по версии Markswebb,445.4,"Дизайн и юзабилити, Электронная коммерция, Мобильные технологии",2025-03-13,"Привет! Я Лёша, backend‑разработчик, который любит путешествовать. К 35 годам я посетил 67 стран мира, но с момента написания прошлой статьи пока никуда не ездил. Зато увлёкся нейронками. Эта статья будет как раз о них.Из статьи вы узнаете:где стоит применить нейронки, а где можно обойтись NLP;как локально крутить LLM и почему стоит воспользоваться API;как подбирать промпты, чтобы получать от ИИ не просто ответы, а реально полезную информацию;зачем вообще в боте для поиска дешёвых авиабилетов (и не только) нейросети и почему в нём осталась лемматизация / NLP, а в некоторых местах регулярки для поиска сообщений.Предыстория есть в моих прошлых статьях: про идею сделать бот для поиска туров и про доработку MVP‑версии с примерами, что другие юзеры искали через бот. Если коротко, я сделал бота, который мониторит сообщения в Telegram‑каналах и пересылает их, чтобы вы не пропустили действительно вкусные предложения: быстро среагировали на горящий тур, вакансию или распродажу любимого бренда.Какие доработки Telegram-бота хотели юзеры и желательно вчераС чего началась очередная доработка? В прошлой статье я упоминал, что хотел бы сделать поиск туров и билетов по датам. Сначала я не понимал, как доставать даты из текста постов в Telegram‑каналах. Опишу технологии, которые сейчас используются для работы с текстом, и финальное решение дальше в статье.Также я задумал добавить категоризацию сообщений. Например, если добавить бот в канал‑барахолку, можно было бы искать сообщения по технике, по предложениям о работе и другим категориям. Но тогда придётся превратить самописный бот в громоздкий классифайд типа Авито.Ещё точно стоило бы категоризировать сообщения по странам или регионам при поиске билетов и туров. Человек может искать перелёт не в конкретный город Таиланда, а в любой, но в посте из туристического паблика может быть только название города. Тогда нужно будет подписать бота на пересылку предложений из всех городов, куда летают самолеты.А как быть, когда ты хочешь отправиться в Юго‑Восточную Азию не дороже 50 тысяч? Забивать в фильтры поиска все страны? Сейчас даже Aviasales не умеет искать по регионам... Второй Aviasales я точно не собирался делать.Я видел решение так: нужно доработать функционал, чтобы искать информацию по большим объёмам текстов и сообщений, но без чётких критериев. Так началось моё знакомство с нейросетями с точки зрения разработчика, а не пользователя.Сколько стоит добавить нейросети в Telegram-ботЯ начал с такого запроса к нейронке:Ты — полезный ИИ‑ассистент, который помогает определить, соответствует ли моему запросу сообщение. Сегодня 10 марта 2025 года. В ответе должно быть только слово «да» или слово «нет». Запрос: «Я хочу найти билеты в Санкт‑Петербург в марте». Сообщение: «Из Москвы в Новосибирск 🇷🇺 за 3 900 р. OW (февраль) Smartavia, прямой рейс — https://smkt.us/dk‑ox из Минеральных Вод в Сочи 🇷🇺 за 1 500 р. OW (февраль‑март) Азимут, прямой рейс — https://smkt.us/exw‑x из Челябинска в Петербург 🇷🇺 за 8 000 р. RT (весной) Smartavia, прямые рейсы — https://smkt.us/kug4l».Для поиска дат я составил примерно такой запрос:Ты — полезный ИИ‑ассистент, который умеет находить даты в тексте и преобразовывать их в формат dd‑mm‑YYYY. Если год не указан, считай, что сегодня 10 марта 2025 года. Если встречается промежуток дат, выводи его в формате dd‑mm‑YYYY / dd‑mm‑YYYY. В ответе должен быть только список дат без дополнительного текста. Найди и обработай даты в следующем тексте: «Встреча запланирована на 15 марта, а отпуск будет с 20 апреля по 25 апреля. Также у меня есть событие 30.12.2024».Я начал с того, что испытывал модели, которые можно развернуть локально. Всё‑таки я не планировал делать свой бот платным, хотел поделиться своей наработкой с любым желающим. API, которые я смотрел, стоили достаточно дорого. Соответственно, я начал разворачивать открытые модели с максимальным количество параметров, которые потянет мой компьютер, и сравнивать эти модели.Знаю, давно пора переходить на M2Я взял 8 нейросеток: DeepSeek Chat, DeepSeek-R1 (7b), DeepSeek-R1 (14b), Gemma 2 (9b), Llama 3.1 (8b), OpenAI Chat, Phi-4 (14b), Qwen 2.5 (7b). Проверил, справятся ли они с поиском даты в тексте (запросы 1-2 из таблицы ниже) или с категоризацией поста из Telegram-канала, выдавая ответы «да/нет» (запросы 3-6).Суть запроса Какие нейросети справились 1. Найти даты разного формата в тексте и преобразовывать в формат dd‑mm‑YYYY DeepSeek Chat, Gemma 2 (9b), Llama 3.1 (8b), OpenAI Chat, Phi-4 (14b) 2. Найти даты в тексте, где нет цифр, по описанию: «Хочу поехать на майские праздники/летом/в декабре‑январе» — 3. Определить, соответствует ли запросу «Хочу найти билеты в Петербург в марте» конкретный пост из канала выгодных туров DeepSeek Chat, Gemma 2 (9b), OpenAI Chat, Phi-4 (14b) 4. Определить, соответствует ли запросу «Хочу найти билеты в Петербург» постDeepSeek Chat, DeepSeek‑R1 (7b), Gemma 2 (9b), OpenAI Chat, Phi-4 (14b) 5. Определить, соответствует ли запросу «Хочу куда‑то полететь весной» пост DeepSeek Chat, DeepSeek‑R1 (7b), DeepSeek‑R1 (14b), Gemma 2 (9b), Llama 3.1 (8b), OpenAI Chat, Phi-4 (14b), Qwen 2.5 (7b) 6. Определить, соответствует ли запросу «Хочу найти электронику» пост о продаже Samsung Galaxy S5 DeepSeek Chat, DeepSeek‑R1 (7b), DeepSeek‑R1 (14b), Gemma 2 (9b), OpenAI Chat, Phi-4 (14b) Нейросети-финалисты, лучше всего справившиеся с парсингом дат и категорий в тексте постов из Telegram: DeepSeek Chat, Gemma 2 (9b), OpenAI Chat, Phi-4 (14b). Они верно обработали 5 запросов из 6 и выдали нужный ответ на промпт.Полная картина: какие запросы я конкретно отправлял, какие именно ответы возвращали все 8 нейросеток, можете посмотреть в подробном файле LLM-тестирования.Итак, я выяснил, что с парсингом дат не справляется даже Open AI и DeepSeek. И это ещё мы не усложняли запрос фразами «на новогодние праздники» / «на Масленицу» и т. д. При этом даже выражения «летом» или «в октябре» нейронки воспринимают странно. Какая‑то модель отвечает, что летом — это 10 дней в июле, какая‑то по‑другому. В общем, Скайнет нам, кажется, пока не грозит.А вот с запросами, на которые нужен ответ «да/нет» всё сильно лучше. Хорошо справляются phi4 (14b) и gemma2 (9b). Я начал их разворачивать на сервере и тестировать запросы по API (6 ядер x 2600 MHz, 12 Гб RAM, 100 Гб SSD NVMe). Обе эти модели отвечают примерно за 8 секунд. Здесь явно не хватает железа для обсчёта (нужно GPU и Kit для ускорения).Я изначально не предполагал, что мне нужна будет многопоточка — переписывать код под это дело придётся немало. Я пошёл смотреть цены на аренду серверов с GPU, и так как раньше никогда не интересовался стоимостью железа, чуть не выпал в осадок (может быть они окупаются на больших объемах, но точно не для пет проекта).Самая дешёвая конфигурация для бесплатного бота стоила бы мне 23 600 рублей в месяцВидимо, придётся всё‑таки пользоваться API. Что пробовал из самого известного:DeepSeek — отвечает за 5–6 секунд — надо параллелить приложение. Еще пугает, что в онлайн‑чате часто выходит ответ «Server is busy»OpenAI — сильно дороже, нужен прокси на сервере, так как сервис не работает в РоссииYandexGPT API. Я остановился на нём. Именно он сейчас работает в боте. Сначала бот работал на версии Lite, но с большими сообщениями Яндекс начинал капризничать. Теперь всё работает на версии Pro.Как-то в этом сообщении Яндекс увидел запрос ""Ищу авиабилеты из Москвы в Пятигорск""Так как запросы по API стоят денег, пришлось перекладывать на пользователя расходы на проверку сообщений. Для этого я написал биллинг, который считает операции проверки сообщений в зависимости от длины сообщения (количества токенов). Баланс теперь можно получить или пополнить (всем новым пользователям на баланс я добавил от себя приветственную сумму, чтобы они могли потестировать бот).Как бот работает с балансомКак пофиксить поиск по ключевым словам в боте через NLPТеперь расскажу, как решил проблему с поиском по датам и почему поиск по ключевым словам и суммам остался, хотя есть такой мощный инструмент как нейросеть. Вот эти причины:Кто‑то из юзеров принципиально не готов пользоваться платным софтом. Это культура потребления, почти все привыкли, что всё ПО должно быть бесплатным.Не все из нас промпт‑инженеры и могут подобрать правильно запрос под поиск того, что хотят получить от бота.Нейросети иногда (часто) врут. Посмотрите ещё раз сообщение от YandexGPT, которое мне переслал бот по поиску «Москва — Минеральные Воды».В итоге для получения сумм и дат из текста я использовал NLP Python‑библиотеки:pullenti и pullenti_wrapper для русского языка. На английском не работает практически никак.spacy (работает только для английского) в сочетании с dateparser для получения из текста дат (один умеет определять что сущность — это дата, второй переводить в ISO‑формат) и регулярку для сумм.natasha популярная, но очень плохо распознает даты или суммы.deeppavlov — популярная, но устаревшая библиотека, судя по тому, что не работает с новой версией Python и из‑за постоянных конфликтов старых пакетов. Удалось попробовать только через контейнер, и оказалось, тоже плохо распознает суммы и даты.Ссылка сервис для извлечения дат и суммЕщё у меня есть два сервиса, о которых я до этого не писал, но они могут кому‑то пригодиться (все они так же в докер‑контейнерах):Сервис лемматизации — работает через библиотеку pymystem3 (я пробовал Snowball для Java, но сравнение «Египет» и «в Египте» оказалось для него невозможным).OCR‑сервис, так как информацию о перелётах часто выкладывают в виде скриншота из поиска на AviasalesНапоследок покажу, как теперь выглядит настройка подписки:Это поиск по датамПоиск через нейросеть: я не надеялся, но стоило попробоватьСсылка на бот, который я обучил искать что угодно в Telegram-каналах: @BackendFrowarderBotПолезные ссылки, которые я выделил для себя, когда штудировал нейронки:Ваш путеводитель по миру NLP (обработке естественного языка)Как работает ChatGPT: объясняем на простом русском эволюцию языковых моделей с T9 до чудаGPT-4: Чему научилась новая нейросеть, и почему это немного жутковатоВведение в LLM для тех, кто все пропустил (выжимка видео Андрея Карпаты, со‑основателя OpenAI)P. S. Недавно коллега по работе предложил ещё один интересный способ использования бота — искать клиентов в домовых чатах. Его жена — визажист, хочет вступать в домовые чаты и реагировать, когда люди ищут бьюти‑услуги и рекомендации мастеров. «Мы создали демона!», — подумал я.Может быть, вы тоже придумаете своё креативное применение Telegram‑боту — делитесь в комментариях."
50,50,"Code, maturity, tools: как мы развиваем QA-практики в МТС",МТС,Про жизнь и развитие в IT,2233.29,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-03-13,"Привет, Хабр! Это Алексей Гарцевич, Сергей Чепкасов и Антон Кадников — эксперты в Центре практик направления QA в МТС Диджитал. У нас тысяча тестировщиков, и они работают в совершенно разных условиях и продуктах. В рамках крупной экосистемы приходится прибегать к централизованному развитию QA: оно позволяет компании держаться в одном стеке, накапливать и обмениваться опытом, создавать собственные инструменты и не только. И эта нелегкая задача ложится на наши плечи. Мы помогаем внедрять новые практики, наращивать компетенции внутри команд. В этом посте мы расскажем, что конкретно делаем и как со всем справляемся. В конце покажем, куда приводит такое централизованное развитие технологий и кому оно может быть интересным. Как мы прокачиваем тестирование во всей экосистемеТак как МТС — компания большая, развитие технологий идет в ней централизованно. До 2020 года продукты и приложения МТС жили отдельно друг от друга: код практически не переиспользовался, команды слабо взаимодействовали между собой. Решением этой проблемы стал запуск цифровой трансформации и объединение всех в экосистему. Одним из шагов было внедрение методологии Capability Maturity Model Integration (CMMI). Она включает рекомендации и стандарты работы, которые помогают синхронизировать и поддерживать развитие разных направлений деятельности компании (практик). Этим занимается Департамент управления технологиями aka TechGov, включающий в себя Центры компетенций и Центры практик. Наша команда объединяет функции обоих центров и работает на два фронта: и на обучение коллег, и непосредственно с кейсами, стандартами и процессами. Наша работа вытекает из общей технологической стратегии и включает пять направлений:Помощь с HR-процессами и работа с компетенциями. Мы занимаемся всеми материалами, которые помогают оценивать и отслеживать навыки в QA: разрабатываем матрицы компетенций, тесты для переводов между должностями, помогаем с текстами вакансий.Автоматизация процессов тестирования. Например, к нам обратилась команда, которой нужно было внедрять автотесты. Мы помогли им выбрать инструмент и сейчас пилотируем его.Тестирование производительности. К нам обращаются, если не знают, с чего начать, и с вопросами по дальнейшему развитию решения. Стек инструментов для нагрузки в командах совершенно разный: Gatling, Jmeter, Locust и k6 — и мы помогаем сразу со всем.Обучение. Мы готовим курсы для внутреннего университета, участвуем в программах менторинга. В прошлом году мы провели два курса, куда QA-инженеры могли привести всю команду. Он выступает в роли лида и в процессе курса организует для своего продукта тестирование производительности. Мы по шагам рассказываем, как его проводить, и команды внедряют его под нашим контролем. За три с половиной месяца они получают рабочий процесс тестирования. Эта методика себя показала хорошо — мы внедрили практику нагрузочного тестирования в пятидесяти продуктах. Сейчас записываем этот курс, чтобы запускать потоки с готовыми материалами.Координация и оценка работ QA: разработка quality-чеков, quality-гейтов, дефект-менеджмент. Мы актуализируем, улучшаем, ищем точки роста внутри процессов, чтобы они подходили под все продукты.Работа с комьюнити: выступаем на конференциях, менторим, отвечаем в чатах на любые вопросы, проводим митапы. Так, в декабре устроили совместный митап с Moscow QA.Работа с практикамиНа уровне компании придумываем, как процесс починить, запустить или оптимизировать, выступаем в роли продакт-оунеров. Например, совместно с коллегами из интеграционной платформы внедряем сервис, который позволяет создавать «заглушки» и выполнять контрактное тестирование на основе спецификаций. Мы развернули инструмент, проверили его работу, провели несколько демо и теперь открыли доступ пользователям.Сейчас активно работаем над сервисом тестирования производительности: он позволяет автоматически собирать метрики из разных источников, проверять их, формировать отчет о результатах и публиковать его в Allure. Планируем внедрить в него ML-решение для автоматического поиска проблем в метриках по результатам теста производительности и автогенерацию скриптов.Еще готовим материалы, которые коллеги из продуктов могут применить в работе:шаблоны документации, используемые многими командами, — получаем положительные отзывы;шаблоны кода для проектов автоматизации и нагрузочного тестирования.На уровне продуктов к нам обращаются с конкретными вопросами вплоть до починки какого-нибудь застрявшего pipeline. В одной из команд уволился лид тестирования, и мы помогли оставшимся инженерам с их проектом API-тестов. Посмотрели их решение и предложили использовать OpenAPI-спецификацию, чтобы из нее сгенерировать код для автотестов, оценить покрытие тестами OpenAPI-специфики и валидировать относительно нее поведение сервиса.Часто обращаются за помощью в старте процесса тестирования производительности, и мы отвечаем на вопросы: Как разработать стратегию и создать скрипты?Мы провели тест и не можем разобраться в результатах, что делать?Иногда приходится организовывать тестирование связанных продуктов, когда нужна сложная коммуникация с командами из разных частей экосистемы. Основной точкой входа в продукты для нас являются лидеры практик. Это опытные тестировщики, которые общаются со своими командами и внедряют фреймворки и методологии, проводят обучения и преподают на наших курсах. Работа с ними идет в двух направлениях: они берут что-то от нас, мы — от них. Например, в одном из продуктов есть своя команда стажирующихся ML-инженеров, которые разрабатывают различные утилиты для тестирования. Мы с их лидером практик решаем задачи автоматизации, генерации заглушек, анализа и предсказания метрик нагрузочного тестирования. И потом планируем использовать на всю компанию. Развитие тестирования в продуктахСуществуют разные модели оценки зрелости процессов тестирования, например TPI (Test process improvement) и другие. Можно использовать любые удобные, а в МТС для QA мы выделяем пять уровней: начальный, повторяемый, определенный, управляемый и постоянно улучшаемый. На последнем уровне команды делятся своей экспертизой, рассказывают о своих решениях и выступают драйвером развития практик в компании. Для понимания текущей зрелости QA своего продукта необходимо заполнить опросник в отдельной системе: он выдает уровень, и мы его подтверждаем. Затем даем обратную связь, помогаем, советуем дальнейшие шаги развития. Опрос нужно заполнять минимум раз в год, но многие продукты проходят оценку раз в квартал, чтобы держать руку на пульсе и быть актуальным. Мир не стоит на месте: практики тестирования постоянно меняются и у нас в компании, и в целом по рынку, поэтому мы поддерживаем опрос в актуальном состоянии и меняем в нем требования. Естественно, в большой экосистеме все продукты и сервисы находятся на разных уровнях: им надо помогать и двигать в общем направлении. Да, мы стараемся помочь всем, внедрить что-то новое и полезное, но упираемся сразу в несколько барьеров:Так как нас всего трое, а векторов много, то мы везде работаем по чуть-чуть и не хватает времени реализовать все идеи. Из-за этого наше влияние дает результат не сразу, а постепенно. Но по обратной связи от продуктов мы идем в правильном направлении. Мы часто выступаем в роли сервисной команды, но нас мало, мы не можем бросить текущие задачи и помочь всем сразу.Разный стек технологий в командах: из-за этого мы должны уметь все и сразу, и очень сложно внедрить общие инструменты.Мы работаем в рамках технологической стратегии, которая занимает 25% времени у продуктов, и не навязываем конкретные решения, а подсвечиваем варианты развития. В таком формате любые изменения требуют времени, терпения и сплоченности вокруг людей. Да, мы, как профессионалы, хотим быстрые результаты, но при этом не всегда можем себе их позволить.Иногда мы воспринимаемся как внешняя команда. С одними продуктами хорошо взаимодействуем: мы помогаем с их локальными проблемами, а они транслируют свой опыт на всю компанию. А другие, наоборот, могут сопротивляться изменениям: «У нас и так задач много, а тут еще вы со своими идеями». Особенно остро проблема стоит при автоматизации и внедрении нагрузочного тестирования, где требуются узкие компетенции.Планы на развитие QA в МТС Ядром технологической трансформации МТС является цифровая платформа The Platform. Она объединяет все сервисы, которые нужны продуктам в рамках нашей экосистемы, автоматизации и стандартизации решений. Одно из ключевых направлений ее развития — внедрение инструментов на основе технологий искусственного интеллекта (AI). Мы вносим свой вклад в развитие The Platform, работая над одной из ее частей — Product Factory Test Tools, где планируем сделать процесс тестирования полностью автоматизированным как раз благодаря AI. Когда в экосистеме будут появляться новые сервисы и продукты или возникнет потребность в текущей тестовой модели, то можно будет загрузить на платформу спецификацию API, а на выходе получить автоматизированные и нагрузочные тесты с автоматическим анализом. В таком варианте любой аналитик cможет накликать кнопки интерфейса и получить готовый тест. В идеальном мире не нужно будет вообще ничего заполнять, так как уже будут готовы инструменты, процессы и метрики для тестирования, управляемые при помощи AI.Но пока мы только идем к этому идеалу, перед нами стоят конкретные задачи: автоматизация тестирования, тестирование производительности у отдельных команд. Их можно выстраивать под ключ как внешний сервис. Когда в компании много разных продуктов, есть возможность сформировать из опытных специалистов отдельную бригаду для решения сложных задач. Сейчас мы сформировали роадмап развития платформы на 2 года и стараемся его придерживаться. Централизованное развитие технологий — это особенность бигтеха. Оно позволяет накапливать и обмениваться знаниями и компетенциями, вместе решать сложные задачи и двигаться в общем направлении. А еще благодаря ему появляется запрос на команды инженеров с прокачанными скилами и богатым профессиональным опытом. Эксперты могут повышать техническую зрелость продуктов, обучать коллег, развивать внутренние процессы и инструменты — так и появилась наша команда. Это достаточно необычный вариант карьеры, и он может быть интересен тем, кто не видит себя в тимлидерстве и в одном продукте. В МТС направление QA — это не только технологии, но и координация, обучение, обмен опытом. В следующем посте мы расскажем, какие навыки нужны для такого экспертного карьерного трека в QA и почему мы выбрали именно его. На этом у нас все, с гордостью ответим на ваши вопросы. "
51,51,"Как кардиолог и физик придумали эхокардиографию, и почему медики им не сразу поверили",Газпромбанк,Очень большой банк,4.48,"Программное обеспечение, Электронная коммерция, Связь и телекоммуникации",2025-03-13," Первая ЭхоКГ  Как-то встретились кардиолог с физиком и разговорились о сердечно-сосудистой диагностике:  — А что если попробовать радиоволны?  — Да ну, не думаю, что это сработает.  — Почему?  — Потому что. Давай попробуем ультразвук.  Примерно так зародилась идея, которая изменила кардиологию.  Мы уже писали о таких изобретениях, как трамвай Пироцкого, фонограф Эдисона, радиатор Сан-Галли, компьютер Корсакова и радиофон Куприяновича. На очереди — медицина.  Как была устроена кардиология в 1950-х К середине 20-го века она была на ранних этапах развития, а диагностика заболеваний сердца во многом опиралась на субъективные методы: сбор анамнеза и аускультацию с помощью фонендоскопа. Также врачебный инструментарий включал в себя электрокардиографию (ЭКГ), рентгенографию и катетеризацию сердца.   Минимализм  До фонендоскопа использовался стетоскоп — устройство, изобретённое французом Рене Лаэннеком в 1816 году. Деревянная трубка, с одной стороны которой — пациент, а с другой — врач. Слышимость лучше, чем при аускультации голым ухом. Кстати, считается, что тестовая версия стетоскопа была из бумаги: Лаэннеку было несколько неловко класть голову на грудь юной пациентки, поэтому он свернул лист в трубку, через неё послушал сердце и остался джентльменом. Позже стетоскоп оснастили мембраной и нарекли фонендоскопом. Частотный диапазон увеличился, диагностика стала точнее. Ещё чуть позже приборы свели воедино и получили стетофонендоскоп: с одной стороны рабочей поверхности — воронка стетоскопа, с другой — мембрана фонендоскопа.  Революцию в медицинской визуализации сделал Вильгельм Рёнтген.    — Как бы так посмотреть на богатый внутренний мир людей?  В кардиологии рентгенография использовалась для оценки размеров сердца и выявления возможных патологий, но прицельно изучить структуру тканей она не позволяла. На основе рентгенографии выросла ангиокардиография — метод, при котором через катетер в сердце вводили рентгеноконтрастное вещество. Это давало детализированные изображения сосудов и полых структур на снимках. Катетеризацию сердца разработал и испытал на себе Вернер Форсман в 1929 году, но широкое применение метод получил позже. Такая процедура позволила оценить функциональное состояние сердца и диагностировать врождённые и приобретённые пороки. Основной её минус — инвазивность.  Прорывным методом диагностики стала электрокардиография (ЭКГ). Первые эксперименты с капиллярным электрометром для регистрации электрических сигналов сердца провёл английский физиолог Август Уоллер.   В исследованиях ему помогал бульдог Джимми  В 1887 году он впервые записал электрическую активность человеческого сердца. Но вместе с тем посчитал своё изобретение баловством, которое не имеет никакой практической пользы. Однако в его наработках увидели потенциал Вильгельм Эйнтховен и Томас Льюис. Эйнтховен спроектировал электрокардиограф. Теперь можно было точно отслеживать ритм сердца и выявлять отклонения в его работе. Льюис, в свою очередь, очертил терминологическое поле и систематизировал научно-понятийный аппарат в части электрофизиологии сердца.   Все эти методы, несмотря на свою важность, в совокупности не давали полноценной картины. ЭКГ только фиксировала электрические импульсы, но не могла показать физического состояния сердца. Рентген давал статичные изображения, но без возможности оценить динамику, а катетеризация звучит и, вероятно, ощущается так себе.  Некоторые заболевания оставались труднодиагностируемыми, например, клапанная недостаточность, при которой створки не закрываются должным образом, что приводит к обратному току крови.  В общем, был запрос на аппарат, который позволил бы заглянуть внутрь сердца без хирургического вмешательства.  Ультразвуковая коллаборация Шведский кардиолог Инге Эдлер и немецкий физик Гельмут Герц познакомились в Швеции. В 1953 году они оба работали в Лундском университете. Их сотрудничество началось с обсуждения методов диагностики. Герц упомянул ультразвуковой прибор, использовавшийся на верфях для проверки сварных швов. Эдлер задался вопросом: если ультразвук может проникать через металл и выявлять его дефекты, то возможно ли использовать этот метод для исследования мягких тканей, например, сердца?  Предположение показалось интересным, и они решили провести эксперимент.  Герц съездил на верфь в Мальмё и привёз оттуда ультразвуковые датчики. Первое устройство, которое они собрали, состояло из генератора ультразвуковых волн (частота — 2,5 МГц), преобразователя и осциллографа, на экране которого фиксировались отражённые волны. Направив датчик на грудную клетку Герца, они заметили ритмичные колебания движущихся структур сердца.   Это стало первой в истории эхокардиограммой.  Собственно, принцип работы эхокардиографии следующий: пьезоэлектрический кристалл генерирует высокочастотные звуковые волны. Когда они натыкаются на границы между различными структурами, например, между стенками сердца, клапанами и окружающими тканями, часть из них отражается, создавая эхо. Кристалл переключается на приём и улавливает этот отражённый сигнал. Чем дольше эхо возвращается, тем дальше от датчика находится структура, от которой оно отразилось.   Это похоже на работу сонара, который измеряет глубину воды под судном, но в эхокардиографии измеряется расстояние от структуры сердца до стенки груди.  После успешного получения эхосигналов Эдлер и Герц начали разрабатывать способы их интерпретации. Сначала они сканировали эхосигналы в А-режиме: он фиксировал только амплитуду отражённых волн и показывал положение различных структур. Затем они перешли к М-режиму, который позволил визуализировать динамику движения. Полноценное сканирование стало возможным лишь с развитием В-режима в 60–70-х годах, но это уже другая история.   Источник  И никакой инвазии, никаких катетеров, никакого контраста.   — А круто ты тогда придумал с ультразвуком  Но были и сложности.   Промышленные датчики для металлов не заточены под человеческую изнанку — сигнал был размытым. Ультразвук по-разному проходит через кости, мышцы и кровь. Кроме того, на тот момент не было технологий для формирования детализированных изображений внутренних органов в реальном времени.  И тут важную роль сыграли связи Герца с компанией Siemens, где его отец (лауреат Нобелевской премии Густав Герц) работал консультантом в исследовательской лаборатории. Доступ к современному оборудованию ускорил разработку. Кстати, да, среди его родственников числится ещё и Генрих Герц, в честь которого герцы стали герцами.  В 1956 году успех их работы был подтверждён диагностированием миксомы в левом предсердии с помощью эхокардиографии.   Работу не восприняли всерьёз Когда Эдлер и Герц представили свои результаты медицинскому сообществу, их встретили со скепсисом. Ультразвук долгое время ассоциировался с военными и промышленными сферами и не воспринимался медиками как надёжный инструмент для диагностики. Из-за сложностей в интерпретации и общего недоверия к новым технологиям они долго сомневались в его практической ценности.  Хотя попытки использовать его в медицине уже были: методы развивались параллельно в других странах. Например, в 1942 году австрийский невролог Карл Дусик пытался применять ультразвуковые волны для визуализации желудочков головного мозга, но кости черепа плохо пропускали сигналы.    Голову пациента помещали в резервуар с водой (весь аппарат целиком назывался гиперфонографом)  В конце 1940-х американский врач Джон Уайлд вместе с инженером Джоном Ридом изучал ультразвук как способ диагностики рака молочной железы, а Дуглас Хоури работал над методами визуализации мягких тканей.   Тем временем в СССР исследования по применению ультразвука в медицине начались в 1950-х. В 1954 году на базе Акустического института АН было создано отделение ультразвука под руководством профессора Лазаря Розенберга. В начале 1960-х во ВНИИ Инструмент началась экспериментальная разработка отечественных УЗИ-сканеров. Что касается ЭхоКГ, то метод внедрили 1973 году в Институте кардиологии им. А. Мясникова.   Но вернемся обратно в Лунд.  Публикации Эдлера и Герца не привлекли особого внимания, а коллеги не видели смысла в таких экспериментах. Герц вообще приуныл, оставил кардиологию и занялся разработкой технологий печати, включая струйный принтер.  А неугомонный и упорный Эдлер не остановился. Он продолжил работать над аппаратом и методикой, собирал клинические примеры, активно их промоутировал. И постепенно становилось очевидным, что ультразвук эффективен в диагностике пороков клапанов, сердечной недостаточности и других заболеваний сердца.  К 1960-м эхокардиография начала завоёвывать признание и уверенно вошла в медицинскую практику. А ещё через несколько лет она стала признанным мировым стандартом. Важным шагом стало добавление допплеровской технологии, которая позволила измерять скорость и направление кровотока в сердце и крупных сосудах.  Вот до чего доводят эксперименты с заимствованным с верфи датчиком."
52,52,Как я добилась смены должности и при чем тут философия По из Кунг-Фу Панды,Ростелеком,Крупнейший провайдер цифровых услуг и решений,115.98,"Программное обеспечение, Связь и телекоммуникации, Веб-сервисы",2025-03-13,"В нашем быстром мире иногда трудно найти то, что вдохновляет. Мультфильмы передают сюжеты понятным языком. Следим за переживаниями и развитием героев, находим сходство с нашими собственными чувствами. Делюсь рассказом, чтобы вдохновить на изменения других людей. Возможно, именно эта история поможет вам заглянуть внутрь себя и увидеть новые возможности, открывающиеся перед вами.Когда я задумалась о карьере, философия По из «Кунг-Фу Панды» стала для меня настоящим открытием. Главный герой медвежонок По, столкнувшись с трудностями и неуверенностью, нашел в себе смелость и стремление к переменам. Этот подход вдохновил меня взглянуть на профессиональную жизнь под другим углом. В течение пяти лет я работала ведущим менеджером отдела продаж оборудования и в какой-то момент почувствовала, что застряла в рутине и не двигаюсь вперед как профессионал. Изменения были необходимы, но с чего начать?Эта статья не только расскажет о моих изменениях и новаторских подходах к работе, но и поможет тем, кто также стремится выйти из привычной рутины и сделать шаг к новым возможностям. Впереди я поделюсь тем, как философия По вдохновила меня изменить распорядок дня, заняться саморазвитием и, наконец, преодолеть барьеры на пути к карьерному росту. Надеюсь, что мой опыт сможет помочь вам найти свои пути к трансформациям и новизне в профессиональной деятельности.«Все что долго остается неизменным рано или поздно начинает терять свой вкус»Я заметила, что жизнь идет в привычном русле — рабочее время проходило в ожидании окончания рабочего дня. Вдохновившись философией По: «Все что долго остается неизменным рано или поздно начинает терять свой вкус», проанализировала работу.Стандартный рабочий день проходил так. Прихожу в офис за 15 минут до начала. Пью кофе с кофемашины (незаменимый атрибут офисной жизни) и разбираю электронную почту. Затем составляю план задач на день и вперед, по кругу. Нажимаешь на ручку входной двери и понеслось.Рабочий день стал серой рутиной, в которой нет места для креативности. Офисный день проходил так: с утра часовая планерка с коллегами, где обсуждали планы на день и предстоящие встречи с клиентами. Затем разбираю текущие проекты – контролирую текущие статусы, ставлю новые задачи. На это уходит часа два-три. Во второй половине дня звоню новым клиентам и составляю коммерческие предложения. Последний час оставляю на подведение итогов дня.Каждый раз прохожу один и тот же сценарий, и это убивает интерес. В рабочей рутине ничего не менялось, хотелось больше вызовов для работы, а для руководства важно было выполнение КПИ. Понимала, что это влияло на собственное состояние и результаты. Начала ощущать напряжение и теряла концентрацию на работе. Рабочая рутина не только не помогала расти, но и влияла на жизнь вне работы.Первое изменение – начала писать задачи с вечера, а не утром. Осознание того, что каждое утро на столе лежит готовый план, оказалось вдохновляющим. Следующий шаг – перестала обедать в офисе, стала выезжать на обед в кафе. Смена обстановки, разговоры с новыми людьми наполнили жизнь эмоциями. Начала встречать старых знакомых, делиться новостями, появилось ощущение, что жизнь не только на работе. Третье изменение – вернулась к чтению книг. Не бегло серфить ежедневно новости в интернете или читать краткие выжимки книг любимых авторов. Взяла в руки печатную книгу, это оказался Кови «7 навыков высокоэффективных людей», и прочла.Изменения приободрили, стала работать с интересом. Теперь позволяю себе читать увлекательные книги в перерывах, а также обсуждать новые идеи с коллегами. Планирование задач в конце рабочего дня помогло структурировать мысли и облегчило утренний старт. Удалось найти время и для чтения, общения с друзьями. Рабочие будни стали ярче, а жизнь – полноценнее.Вывод: когда поменяла распорядок дня, выяснилось, что успеваю делать в 2 раза больше задач в течение дня, чем раньше. Новый режим помог лучше организовать время, расставить приоритеты и снизить уровень стресса. Стала сфокусированной и энергичной. «Как косточке превратиться в могучее дерево, если не давать ей шанс расти»Решила не останавливать на этом и попробовать себя в новой профессии. Цель менеджера по продажам – увеличение прибыли компании. В то время как руководитель проекта отвечает за успешное завершение проектов и получение конкретных результатов. Вместо того чтобы заниматься продажей товаров и услуг, мне стало интересно взять на себя роль руководителя проектов. Поняла, что для этого требуется смелость — ту смелость, которую демонстрирует По. Однажды решилась и зашла к директору с разговором: “Я качественно выполняю задачи и укладываюсь в сроки, но мне этого недостаточно. Хочу расти, приступать к крупным проектам!”. Ответ директора оказался неожиданным: “А кто будет делать твою работу?”. Обсудив, пришли к соглашению – как наставник подготовлю себе преемника в течение двенадцати месяцев.Путь к вершинеСтану руководителем проектов в области IT из классического сейлза. Не хватало знаний проектного управления. Записалась в известную онлайн-школу на курс «Управление проектом: от основ до уверенного специалиста», программа которого соответствовала Профстандарту «06.016 Руководитель проектов в области информационных технологий». С головой погрузилась в изучение основ новой для себя профессии. Спустя семь месяцев диплом о переподготовке на руках, вновь постучалась в кабинет директора.Увы, договориться о новой должности не удалось. То ли в штатном расписании не было нужной позиции, то ли директор не хотел поднимать зарплату – сложно сказать. Я не сдаюсь. На горизонте как раз появился заказчик, с которым три года не сотрудничала. Работа кипела над новым проектом: провела проект через стадии – от планирования до приемки заказчика без замечаний. Удалось сформировать ИСР и применить как основу для дальнейших проектов. Составила реестр рисков и прописала как реагировать на них, что впоследствии стало ценным опытом. Выявлена проблема качества продукта – построила диаграмму Исикавы совместно с производством – и основная проблема с отображением неправильных показателей насоса подтвердилась. Итог первого руководства проектом: у компании прибыль, а у меня – гордость за проделанную работу!Это было начало нового пути в карьере, где смелость и решимость По сыграли центральную роль. Освоив новую профессию и применив полученные знания на практике, реализовала сложный проект, что стало началом нового этапа в карьере.«Порой самые вкусные блюда получаются, когда используешь неожиданные ингредиенты»2024 год выдался поистине насыщенным. В неожиданный момент в жизнь ворвалась программа «Производительность труда», цель которого повысить показатели бизнеса и внедрить бережливое производство. Благодаря усилиям и трудолюбию добилась той должности, к которой шла. Возглавила Проектный офис, и с головой погрузилась в изучение кросс-функционального взаимодействия. Здесь потребовались знания основ проектного управления и лидерские качества. За это время с коллегами провели бурю идей — мозговой штурм, в котором выявили 105 проблем, требующих решения! И вот, словно бы «случайно», отправила этот список нашему директору.На следующий день в кабинет постучалась начальница юридического отдела. Юрист заметила, что процессы буксуют на правовом отделе, и предложила помощь. Это было как холодный душ в жаркий день! Начали думать над тем, как упростить согласование договоров, и в итоге освободили кучу времени для сотрудников. Так появился электронный документооборот в компании. Личный вклад в решении вопроса заметили и руководство решило применять электронные согласования не только для договоров, но и большинства материалов компании.Ситуация наглядно показала силу совместной работы и напомнила о том, что идеи и решения часто приходят в моменты неожиданности. Так что, «порой самые вкусные блюда получаются, когда используешь неожиданные ингредиенты».«Поступать правильно никогда не поздно»Благодаря философии По научилась воспринимать изменения как потенциал. Когда наш отдел продаж столкнулся с переменами благодаря программе «Производительность труда» новые приоритеты требовали пересмотра наших целей, решила не поддаваться панике. Спокойствие воинаПостепенная перестройка процессов внутри компании дала толчок к изменениям в работе коллег. Вскоре нашлось еще одно «узкое горлышко» и требовался измененный подход к работе. Предложила сократить количество этапов в процессе сделок с 12 до 4, используя PDCA-цикл Деминга как основу. Решение упростило и повысило результат работы, сократило цикл сделки. Новая должность помогла раскрыть способности.Таким образом, медвежонок По напомнил, что поступать правильно никогда не поздно. Если бы я не обратила внимание как оптимизировать движение сделки по стадиям, так бы и работали по-старому, зашиваясь в названиях очередных «лишних» этапов. ЗаключениеСтатья о том, как изменения станут катализатором для нового начала. За это время в моей карьере поменялось направление – с исполнителя я пришла к руководящей должности. Неожиданно открыла для себя новые перспективы в работе. Это рассказ не просто о карьерном росте, это моя личная история. Сложности будут, однако важно помнить, что каждый день — выбор как реагировать на них. Изменения, даже маленькие, приведут к результатам. Так что не бойтесь рисковать и смело открывайте новые горизонты!Источники информации:1. Мультфильм Кунг-Фу Панда-42. Личный опыт3. 7 навыков высокоэффективных людей (Стивен Кови)Изображения сгенерированы в нейросети Flux"
53,53,Как сделать livenessProbe для Kafka-консьюмеров и перестать перезапускать их вручную,Самолет,Дело в нас и PropTech,4.53,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-03-13,"Kafka-консьюмеры не всегда работают так стабильно, как хотелось бы. Иногда они просто зависают — без ошибок, без падений, но и без обработки сообщений. LivenessProbe в Kubernetes помогает автоматически перезапускать зависшие сервисы, но с Kafka-консьюмерами всё не так просто: стандартного решения для них нет. В этой статье разберём, как правильно реализовать livenessProbe для консьюмеров с помощью паттерна Heartbeat, чтобы не перезапускать их вручную.СодержаниеЗачем нужен livenessProbeВ чем проблема livenessProbe для кафка консьюмера Реализация livenessProbe через паттерн HeartBeatРеализация на pythonАбстрактный класс HeartBeatСтруктура данныхFileHeartBeat - сохранение в файлFileHeartBeat - проверка, жив ли консьюмерFileHeartBeat - интегрируем в консьюмерFileHeartBeat - интегрируем в CLIFileHeartBeat - интегрируем в livenessProbeЧто дальше?ВыводыДополнительные материалыЗачем нужен livenessProbeВ Kubernetes livenessProbe периодически опрашивает под (приложение) на предмет работоcпособности и перезапускает его, если что-то не работает. Если бы это был контейнер с API на FastAPI, то мы бы удостоверились, что контейнер возвращает правильный HTTP код 200 с одного из эндпоинтов. Допустим, приложение зависло и перестало отдавать ответ клиенту – livenessProbe перезапустил бы контейнер.Kubernetes с помощью livenessProbe периодически опрашивает контейнер и перезапускает его в случае необходимостиВ чем проблема livenessProbe для Kafka-консьюмера Для Kafka Consumer нет единого описанного подхода к реализации такого механизма, из-за чего разработчики сходу не могут реализовать эту функциональность правильно. Зачастую можно наткнуться на статьи, где предлагают следить за Сonsumer lag, что делает такой подход некорректным, так-как наличие лага не говорит, есть ли проблема у конкретного консьюмера или он просто не справляется с нагрузкой. В моей голове родилось несколько подходов к проверке (увы, не все из них жизнеспособны):Через consumer lag (неточный);Через unix сигналы (проверяет, не завис ли процесс ОС, неточный);Через внутренние метрики кафки (сложно);Через кастомный механизм heartbeat в консьюмере (оптимально и точно);Через отправку сообщения в health топики и его вычитку в livenessProbe (точно, но громоздко)Реализация livenessProbe через паттерн HeartBeat ❤️Простым решением проверки работы консьюмера может стать периодическая отправка хартбитов в любое хранилище, а затем сверка текущего времени и времени последнего бита.Допустим, мы примерно знаем, что если задача выполняется больше 10 секунд, то скорее всего консьюмер завис и нам стоит его перезапустить. Каждый цикл получения и обработки сообщений из топика сохраняем данные по времени в некое хранилище. Самый простой вариант — в файл. Kubernetes затем может посмотреть в файл, сравнить время последнего бита с текущим и перезапустить консьюмер, если разница больше 10 секунд.Концептуальная работа livenessProbe для кафка консьюмераРеализация на PythonДля использования этого подхода необходимы изменения на стороне консьюмера, а для Kubernetes необходим скрипт.Абстрактный класс HeartBeatОпишем абстрактный класс HeartBeat, наследуя и реализуя который мы сможем вызывать методы save для сохранения heartbeat консьюмера. Это позволит нам в дальнейшем подменять реализацию – сохранять данные в файл, в редис или БД.import abc  class HeartBeat(abc.ABC):     @abc.abstractmethod     def save(self) -> None:         """"""Save timestamp data to mark that worker is alive.""""""              @abc.abstractmethod     def is_alive(self) -> bool:         """"""Check that worker is alive.""""""        Структура данныхОпишем структуру данных о heartbeat. В нашем случае важно хранить идентификатор хоста и время. И должно это сохраняться в JSON.{     ""host_id"": ""00000000-0000-0000-0000-0242ac160007"",      ""last_beat"": ""2023-12-08T09:38:01.255909+00:00"" }class HeartBeatState(BaseModel):     host_id: str     last_beat: datetimeFileHeartBeat – сохранение в файлДля простоты сохраним данные в файл – напишем реализацию FileHeartBeat. Придумаем файлу имя, соберем к нему путь, затем сериализуем HeartBeat в JSON и сохраним в файле.HEART_BEAT_FILE_NAME = ""heartbeat.json""  logger = logging.getLogger(__name__)   class FileHeartBeat(HeartBeat):     """"""     Save heartbeats to a file to check if consumer is alive by livenessProbe.     """"""      def save(self):         """"""Save heartbeat timestamp data to indicate that worker is alive.""""""         heartbeat_file_path = self._retrieve_heartbeat_file_path()         host_id = uuid.UUID(int=uuid.getnode())         last_beat = now()          state = HeartBeatState(             host_id=str(host_id),             last_beat=last_beat,         )         self._save_state_to_file(heartbeat_file_path, state)      def _save_state_to_file(             self,              heartbeat_file_path: Path,              state: HeartBeatState,     ) -> None:         """"""Save heartbeat timestamp and other information in a file.""""""         logger.info(""Saving heart beat to file: %s, %s"", heartbeat_file_path, str(state.json()))          with open(heartbeat_file_path, ""w+"") as worker_heartbeat_file:             worker_heartbeat_file.write(state.json())       def _retrieve_heartbeat_file_path(self) -> Path:         """"""Construct file path where heartbeat info is located.""""""          base_dir = Path(settings.BASE_DIR)          heartbeat_file_name = HEART_BEAT_FILE_NAME         heartbeat_file_path = base_dir.joinpath(heartbeat_file_name)          return heartbeat_file_path FileHeartBeat – проверка, жив ли консьюмерДля проверки, что консьюмер жив, опишем метод is_alive. Прочтем файл, десериализуем данные из JSON в нашу модель и сравним, сколько прошло секунд с момент последнего бита. Если больше, чем мы ожидаем, то считаем консьюмер мертвым.class FileHeartBeat(HeartBeat):     """"""     Saves heartbeats to a file to check if consumer is alive by livenessProbe.     """"""      def is_alive(self, max_time_after_worker_dead: int = 15) -> bool:         """""" Main livenessProbe method to check if consumer is healthy. """"""         heartbeat_file_path = self._retrieve_heartbeat_file_path()          try:             logger.info(""Trying to check heartbeat file: %s"", heartbeat_file_path)             heart_beat_stats = self._read_heartbeat_file(heartbeat_file_path)         except FileNotFoundError:             logger.error(""Heartbeat file not found: %s"", heartbeat_file_path)              return False          logger.info(""Got last heartbeat info: %s"", str(heart_beat_stats.json()))          time_after_last_beat = now() - heart_beat_stats.last_beat         if time_after_last_beat.seconds > max_time_after_worker_dead:             logger.error(                 ""Worker is dead. Time since last beat %s > %s"",                 time_after_last_beat.seconds,                 time_offset,             )             return False          logger.info(             ""Worker is alive. Time since last beat %s < %s"",             time_after_last_beat.seconds,             time_offset,         )         return True      def _read_heartbeat_file(self, heartbeat_file_path: Path) -> HeartBeatState:         """"""Read heartbeat timestamp and other information from a file.""""""         with open(heartbeat_file_path) as worker_heartbeat_file:             heart_beat_stats_json: dict = json.load(worker_heartbeat_file)             heart_beat_state = HeartBeatState(**heart_beat_stats_json)              return heart_beat_state      def _retrieve_heartbeat_file_path(self) -> Path:         """"""Construct file path where heartbeat info is located.""""""          base_dir = Path(settings.BASE_DIR)          heartbeat_file_name = HEART_BEAT_FILE_NAME         heartbeat_file_path = base_dir.joinpath(heartbeat_file_name)          return heartbeat_file_path FileHeartBeat – интегрируем в консьюмерИнтегрируем этот код в код консьюмера. В данном случае используется датакласс для возможности подмены реализации. В начале каждого цикла сохраняем данные, что живы.@dataclasses.dataclass class ConfluentKafkaConsumer:     heart_beat: HeartBeat      def run(self):         consumer = ...         ....                  while True:             message: ConfluentKafkaMessage = consumer.poll(2.0)             self.heart_beat.save()              if not message:                 # No message received                 continue              self._process_message(message)   confluent_kafka_consumer = ConfluentKafkaConsumer(     heart_beat=FileHeartBeat(), )FileHeartBeat - интегрируем в CLIДальше необходимо написать CLI-скрипт, который сообщит Kubernetes о том, что воркер жив или умер. Для этого необходима подобная функция, в зависимости от вашего фреймворка:ERROR_CODE = 1  ... # Your CLI  def health_check(max_time: Optional[int]):     heart_beat = FileHeartBeat()      is_alive = heart_beat.is_alive(max_time_after_worker_dead=max_time)      if not is_alive:         sys.exit(ERROR_CODE)FileHeartBeat – интегрируем в livenessProbeВ helm пропишем livenessProbe проверять состояние консьюмера каждые 30 секунд. У нас используется Django, поэтому запускаем через него.     livenessProbe:       exec:         command:         - ./manage.py         - consumer         - is_alive         - --max_time         - ""30""На этом всё. Мы готовы проверять консьюмеры. Код приведен концептуальный. Что дальше?Вы можете реализовать сохранение Heartbeat в БД или Redis, а затем в админке смотреть состояние ваших консьюмеров. Вы можете пойти ещё дальше и написать команды для приостановки вашего консьюмера прямо в коде через подобную концепцию проверки состояния в цикле перед обработкой сообщения. Это может быть удобно, когда нужно сбросить оффсеты в критических случаях (консьюмер группа должна быть остановлена)Помимо вышеописанного, можно попробовать выделить код в отдельную библиотеку и переиспользовать.Выводы ✍️Из опыта использования этого подхода могу выделить следующее:После реализации механизма Heartbeat мы перестали перезапускать консьюмеры руками и полностью автоматизировали этот процесс. Среди возможных реализаций данный способ оказался наиболее простым и точным.Данный подход позволяет расширять функциональность по отслеживанию и управлению консьюмерами – если использовать Redis, то можно отследить в админке консьюмеры, а также реализовать функциональность по управлению без перезапуска и девопсов прямо из админки.Дополнительные материалы 📚Вот, что ещё можно почитать на эту тему:How to use probes in Kubernetes?HeartBeats: How Distributed Systems Stay AliveBuilding A New Liveness and Heartbeat Mechanism For Better ReliabilityIntelligent, automatic restarts for unhealthy Kafka consumersA Simple Solution for Configuring Liveness Probes for Queue Consumers in Kubernetes"
54,54,Автоматизация проектирования слаботочных систем при помощи nanoCAD BIM ОПС и nanoCAD BIM СКС,Нанософт разработка,Инженерная экосистема,84.38,Программное обеспечение,2025-03-13,"На вебинаре, организованном в ноябре прошлого года, я уделил основное внимание демонстрации увязки nanoCAD BIM ОПС и/или nanoCAD BIM СКС, плагинов для них, а также теме автоматизации оформления тома документации. Преобразование схем шкафовОчень кратко напомню о программе оформления схем шкафов, поскольку результаты ее работы используются при оформлении тома документации.nanoCAD BIM СКС вычерчивает схему расположения оборудования в шкафу (рис. 1).Рис. 1Очевидно, что схеме как минимум не хватает рамки формата с основной надписью. На самом деле можно сделать и больше – например, получить результат, представленный на рис. 2.Рис. 2Всё это получаем одним кликом мыши, если использовать разработанную мной программу преобразования (соответствующая кнопка – на рис. 3).Рис. 3Вычертите схему телекоммуникационного шкафа в программе nanoCAD BIM CKC или откройте ранее сохраненную схему в Платформе nanoCAD любой конфигурации. Нажмите кнопку Всё подряд + рамка + штамп + масштаб. Программа создаст схему, аналогичную представленной выше.Шаблоны, используемые для добавления форматной рамки и основной надписи, настроены на работу с программой «Реквизитор». В результате часть документации, формируемой в программе nanoCAD BIM CKC, уже может быть оформлена автоматически. А что делать с остальными листами и/или документами, входящими в том?Формирование табличных документовЧтобы программа «Реквизитор» могла обновлять основные надписи doc/docx-формата, следует в шаблонах, которые используются программными продуктами nanoCAD BIM ОПС и CKC, добавить закладки в каждой графе основной надписи. Закладки представляют собой штатный инструмент Word. Они никак не проявляются визуально и не мешают использовать другие инструменты заполнения штампов. nanoCAD BIM ОПС и CKC работают с такими шаблонами точно так же, как работали с ними ранее.Порядок присвоения закладкам имен сводится к следующему. Имя закладки не может начинаться с цифры, поэтому вначале ставится буква Г (Графа). Далее идет номер графы: Г2, Г3, Г4. Многие графы имеют по несколько строк; в таком случае для графы 10 первая строка будет представлена как Г101, вторая – Г102 и т.д. (рис. 4). Нумерация строк – сверху вниз.Рис. 4Настроить шаблоны в соответствии со своими целями может любой проектировщик. Шаблоны для nanoCAD BIM ОПC и nanoCAD BIM CKC можно бесплатно скачать вместе с программой «Реквизитор». Достаточно разложить их по соответствующим папкам.Действия пользователя при выводе документов остаются практически без изменений: обновили документ; выбрали пункт вывода в Word; выбрали шаблон, настроенный на работу с программой «Реквизитор». Программа сформировала итоговый файл. Проектировщик сохранил его в папку с другими файлами тома. Можно заполнять штампы и формировать перечни, но сначала закончим со схемами…Формирование чертежей и схемЧто необходимо сделать, чтобы наши чертежи могли обрабатываться программой заполнения штампов?Программа «Реквизитор» при работе с *.dwg-файлами может использовать два вида блоков:·       произвольный блок с изображением основной надписи;·       блок с атрибутами.Использование произвольного блока имеет ряд ограничений. Самое существенное состоит в том, что в одном файле не может быть двух блоков с одинаковыми именами, но разными текстами в них. Если вы вставили две штатные рамки формата – скажем, «Формат А4Г», программа заполнит штампы и там, и там. Но визуально это будет соответствовать последней правке блока. Сама идея блока состоит в том, чтобы все его вставки на чертеже выглядели одинаково.Для тех, кто исповедует принцип «один файл – один чертеж», такое ограничение несущественно. Но я предлагаю инструмент, который, чтобы не ограничивать проектировщика, должен одинаково хорошо работать с любым количеством чертежей в файле.У произвольных блоков есть и другие ограничения. Поэтому основным решением для работы программы «Реквизитор» является блок с атрибутами. В редакторе блоков при выделении всех атрибутов он выглядит так, как показано на рис. 5.Рис. 5Атрибут имеется в каждой графе основной надписи. В графах 2-4, согласно ГОСТ Р 21.101, атрибуты многострочные, в остальных – однострочные. Имена атрибутов имеют минимально возможное количество символов, которые присваиваются по номеру графы и, при необходимости, номеру строки данной графы.Для настройки произвольных блоков на работу с программой «Реквизитор» достаточно скопировать эти атрибуты и вставить в блок основной надписи. Порядок следования атрибутов в блоке значения не имеет, важны лишь их имена.Добавьте в чертеж форматную рамку необходимого размера, а затем блок штампа с атрибутами. Для удобства дальнейшей работы заполните графу 4 «Наименование изображений…». В принципе этого можно и не делать, но тогда в качестве текста для графы 4 программа будет использовать имя файла, чтобы хоть как-то отличать один чертеж от другого.Когда эта работа выполнена для всех чертежей тома, можно переходить к оформлению. Но для начала немного теории, чтобы понимать некоторые особенности работы программы.Типичные ошибки заполнения основных надписейВ программу «Реквизитор» заложен ряд проверок, позволяющих исключить появление ряда типичных ошибок, допускаемых проектировщиками.Дата не может состоять из двух чисел с точкой меду ними. Выражение вида 06.08 датой не является. Сомневающимся предлагаю посмотреть в паспорт или любой другой документ, а также ответить на вопрос: 06.08 – это 6 августа или июнь 2008 года? Невозможно представить, чтобы такая неопределенность допускалась в сфере, занормированной до предела.Если в графе «Лист» число отличается от 1, в графе «Листов» не должно быть ничего. ГОСТ Р 21.101-2020, обязательное приложение Ж: «в графе 8 – общее количество листов документа. Графу заполняют только на первом листе».Если в графе «Листов» стоит 1, ничего не должно быть в графе «Лист». ГОСТ Р 21.101-2020, обязательное приложение Ж: «в графе 7 – порядковый номер листа документа. На документах, состоящих из одного листа, графу не заполняют».В графе «Лист» не может находиться текст вида 1.1, 1.2 и т.п. Обоснование приведено выше: в этой графе указывается порядковый номер листа, то есть первый, второй и т.д. Текст вида 1.1 порядковым номером не является, такая запись в данной графе недопустима.Из последнего вытекает вопрос: как же оформить кабельный журнал и множество прочих табличных документов, которые мы хотим включить в основной комплект, ведь в прилагаемые их отправлять нельзя?На этот вопрос также отвечает ГОСТ Р 21.101-2020:«4.2.5 Допускается оформление основного комплекта рабочих чертежей отдельными документами с присвоением им обозначения, состоящего из базового обозначения, марки основного комплекта и добавлением через точку порядкового номера документа арабскими цифрами:Пример – 2345-12-Э0.1; 2345-12-Э0.2; 2345-12-Э0.11».Программа «Реквизитор» поддерживает оба возможных варианта оформления томов: и листами, и документами. При переключении между ними нумерация пересчитывается автоматически. Но применительно к томам ОПС и СКС следует использовать оформление документами.Формирование томаРис. 6Последовательность выполнения работ при формировании тома (рис. 6) будет следующей:·       нажав кнопку Добавить чертежи, указываем файлы, которые необходимо включить в том документации;·       программа формирует дерево объекта: его наименование, обозначение тома, входящие в том листы и/или документы;·       простым перетаскиванием узлов дерева мышкой задаем порядок следования документов в томе;·       переходим на вкладку Перечни листов/документов, выбираем редактор (текстовый или графический) и нажимаем кнопку Сформировать перечень (рис. 7) для вычерчивания сначала ведомостей общих данных, а затем содержания тома;Рис. 7·       открываем вкладку Формирование тома, заполняем необходимые графы для одного чертежа. Нажимаем кнопку Применить, чтобы введенные значения использовались и для всех остальных документов тома;·       при необходимости задать индивидуальные значения (например, все документы утверждает начальник отдела, а общие данные, структурную схему и спецификацию должен подписать ГИП) вносим в соответствующие поля этих документов слово «ГИП» и фамилию главного инженера проекта;·       нажимаем на панели Обновить кнопку Том, чтобы программа поочередно открыла все файлы тома и внесла необходимые изменения в штампы. Параллельно может быть выполнена печать каждого обновленного документа.Когда позже понадобится оформить изменения, внесенные в документы тома, достаточно будет запустить программу, найти в дереве данных нужные объект и том, заполнить поля изменений отдельных листов/документов. Сведения по изменениям автоматически попадают из штампов чертежей в графу Примечание ведомости. Останется перейти на вкладку Перечни листов/документов и нажать кнопку Сформировать перечень. «Реквизитор» откроет общие данные и обновит соответствующую ведомость.После этого можно выполнить обновление только исправленных чертежей, чтобы сведения об ИЗМах попали в их штампы, или же всего тома, чтобы обновить его PDF-сборку.ЗаключениеСтандартные инструменты графических редакторов не позволяют одновременно работать с файлами различных форматов и не выполняют необходимых проверок корректности информации, вводимой пользователем.Хранение информации о чертежах в базе данных программы позволяет решать различные дополнительные задачи – например, получать сведения об объеме тома в пересчете на формат А4, о выпущенной проектировщиком/отделом документации за определенный период времени и т.д.Самое главное, что появляется возможность унифицировать методы работы с томами документации, выпущенными с помощью различных программных продуктов инженерного BIM или просто в базовой графической платформе. Результатом является единообразие оформления передаваемой заказчику документации, положительно характеризующее проектную организацию.Николай Король,  BIM-менеджер  ООО «Нормасофт»"
55,55,ML-модель в поисках ЛПР,Альфа-Банк,Лучший мобильный банк по версии Markswebb,445.4,"Дизайн и юзабилити, Электронная коммерция, Мобильные технологии",2025-03-13,"ЗавязкаДавайте рассмотрим вымышленную компанию «Орионик Групп» — холдинг, который объединяет несколько компаний, обеспечивающих полный производственный цикл и продажи.В головную компанию — «Орионик Групп» — входят:«Орионик Девелопмент» (девелоперский бизнес): управляет коммерческой и жилой недвижимостью. Генеральный директор: Артем Сергеевский.«Орионик Логистика» (транспорт и складские услуги): контрактная логистика, транспортные перевозки. Генеральный директор: Марина Маринина.«Орионик Трейд» (оптовая торговля): импорт и дистрибуция товаров. Генеральный директор: Николай Сафоновский.«Орионик Капитал» (финансовые услуги): венчурные инвестиции, корпоративное кредитование. Генеральный директор: Андрей Климов.Банк хочет предложить компании корпоративное кредитование. Но кто здесь «лицо, принимающее решения»? Ведь решения о финансировании принимаются не в одной конкретной компании, а на разных уровнях:Для «Орионик Девелопмент» кредитные решения одобряет финансовый директор головной компании.В «Орионик Логистика» — совет директоров холдинга, где влияние оказывает главный акционер.В «Орионик Трейд» — генеральный директор, но с одобрения владельца холдинга.В «Орионик Капитал» решения принимает исполнительный директор вместе с инвестиционным комитетом.На кого должна быть рассчитана маркетинговая кампания банка? Если менеджер просто позвонит по номеру, указанному на сайте головной компании, то, скорее всего, попадёт на опытного секретаря, и дальнейшая судьба звонка предрешена. А на кого рассылать предложения по почте? Кому показывать баннер в приложении?Для решения этой проблемы мы разработали ML-модель. На основе данных, среди которых структура собственности, связи топ-менеджеров, исторические данные по одобренным сделкам или решения прошлых лет, модель предсказывает наиболее вероятного ЛПР в зависимости от типа услуги и структуры компании.Расскажем, как работает и как появилась.Необходимость в моделиВ банке есть модели, которые сегментируют клиентов по размерам, регионам, отраслям. Есть модели, которые оценивают склонность к тому или иному продукту банка. Например, компании Х вероятнее всего нужны деньги, чтобы заполнить кассовый разрыв. Модель работает по клиентам с большей достоверностью, по потенциальным клиентам — с меньшей достоверностью.Собирая большой пласт информации, мы понимаем предпочтения компании и готовность использовать наш продукт, то есть оцениваем её склонность к продукту. Если компания только что взяла новый кредит, то ещё один ей сейчас, скорее всего, не нужен. Но если мы понимаем, что договор лизинга на экскаватор подходит к концу, то компания его может выкупить и использовать как предмет залога под новый кредит — предложение будет релевантным.На первый взгляд, задача проста: найти человека, который обладает полномочиями и ответственностью за ключевые решения в компании. Казалось бы, достаточно просто заглянуть в выписку из налоговой, чтобы установить владельца бизнеса или генерального директора. Это справедливо для мелкого и среднего бизнеса, где решения принимает собственник или первое лицо компании. Но у крупного бизнеса решение «проваливается» на несколько этажей ниже, и нам уже нужен директор по развитию бизнеса, финансовый или ИТ-директор, руководитель департамента или вовсе коллегиальный орган, например, совет директоров. Вот здесь вопрос о поиске лица, принимающего решения, встаёт во весь рост.Модели склонности появились раньше модели поиска ЛПР, о которой идёт речь. И когда начали проводить маркетинговые кампании, то сталкивались с неактуальным номером телефона, колл-центром, не теми людьми. В малом и среднем бизнесе операторы попадают на секретаря, а секретари научены отшивать эти звонки, в крупном — на колл-центр.Ситуация осложняется тем, что в открытых источниках нет сведений о сотрудниках компаний, нет справочников ответственных по разным вопросам лиц. Ранее известные нам контакты могут устаревать или содержать ошибки. Причиной этого может быть несколько факторов, включая изменения в структуре компании, перетасовки в управлении, просто ошибки в регистрационных данных.Но даже если открытые источники найдены, ЛПР — это не всегда руководители, значащиеся в официальных документах (вроде генеральных директоров), а те, кто фактически контролирует процессы принятия решений, будь то акционеры, учредители или финансовые директора. И обычно это — две большие разницы.Менеджеры банка или бизнеса, пытаясь найти лицо, принимающее решения, часто вынуждены анализировать не только официальные документы, но и заниматься глубоким исследованием компании, включая изучение корпоративных сайтов и других неочевидных источников информации. Такой подход требует много времени и усилий, а также высокой квалификации для правильной интерпретации данных.Но зачем это делать, если мы можем обучить модель?Принципы работы моделиПринцип такой: AI-модель учитывает множество внутренних и внешних признаков компании и человека, делает выводы о похожести или непохожести рассматриваемого человека на типичный портрет ЛПР, на основании анализируемых признаков, и выбирает из всего списка людей наиболее вероятных кандидатов на роль ЛПР для компании. Используя множество открытых данных, мы собираем информацию о том, кто представляет компанию в разных вопросах, например, на выставках, какие могут быть потенциальные сферы влияния, после чего проводим «скоринг» каждого представителя компании на «ЛПРность».Для обучения модели нам нужна тестовая выборка. Допустим, что такая выборка у нас есть. Разделим её на 2 части: эталонная выборка, на которой мы будем обучать модель, и дополнительная, которую используем для проверки качества работы модели (верификации).Также для построения модели нам нужны источники данных, которые обогащают выборку признаками, на основании которых и будет происходить выбор ЛПР. Источники, которые дадут наиболее сильные признаки, будут использоваться для работы модели в боевом процессе.Если выборки у нас нет или её качество не позволяет провести качественное обучение, то мы привлекаем помощь группы разметки. Коллеги проводят анализ, можно ли отнести к ЛПРам то или иное лицо, и проводят ручную разметку данных. Это сильно улучшает качество обучения модели и позволяет построить модель с лучшей разделяющей способностью.Примечание. Статья об организации процесса разметки.База об организации процесса разметки: команда, онбординг, метрикиПривет! Меня зовут Сизов Виктор, я занимаюсь данными, их сбором, анализом и разметкой последние 5 ле...habr.comНа начальном этапе разработки мы применяли упрощённый алгоритм из бинарных признаков для анализа, поскольку объём разметки был ограничен.Признаки (триггеры) ЛПР — признаки, присущие преимущественно лицам, отвечающим или влияющим на принятие решений в организации. К основным признакам ЛПР можно отнести сведения в выписке ЕГРЮЛ, где человек указан собственником или генеральным директором компании, повышенные представительские расходы, инвестиционные операции и получение дивидендов и др.Также признаками ЛПР могут служить численные показатели, значение которых косвенно свидетельствует о занимаемом положении в организации/холдинге, например, доля в управляющей компании холдинга.Признаков, используемых в модели, — 12. Они разбиты на четыре ключевые категории: признаки, отражающие юридическую связь; признаки, отражающие уровень полномочий; холдинговые признаки и косвенные признаки. По ним мы, соответственно, определяем «кандидатов» — людей, которые потенциально могут быть лицами, принимающими решения (ЛПРами), — и оцениваем их полномочия и влияние.С помощью этих признаков мы оценивали вероятность того, что кандидат окажется реальным ЛПР. Чтобы получить скор для каждого кандидата в ЛПР, мы складываем веса срабатывающих триггеров по каждому кандидату в ЛПР, а также проводим калибровку min-max для приведения скоров к интервалу от 0 до 1.Например у кандидата сработало 3 признака: участник А-клуба + получает дивиденды + флаг подписан да по кредитным сделкам. Тогда сырой скор будет равен 0,53 + 0,41 + 0,21 = 1,13. сырой скор калибруется в масштаб от 0 до 1 и получаем финальный скор.С увеличением разметки и добавлением данных, например, по транзакциям и доходности, появилась возможность обучить полноценные ML-модели, которые позволяют глубже анализировать данные и учитывать большее количество факторов.Мы разработали две модели для разных групп пользователей:Клиенты банка: модель использует расширенный набор данных и обеспечивает максимальную точность.Не клиенты банка: модель адаптирована к более ограниченному объёму данных.В рамках проекта используются два алгоритма построения моделей: бустинг и AutoWoe. Основной моделью является бустинг, который обеспечивает высокую точность и эффективность в определении целевых клиентов. Модель AutoWoe применяется для бизнес-анализа, предоставляя возможность использовать скоринг-карты в качестве инструмента интерпретации. Это позволяет более глубоко понимать факторы, влияющие на принятие решений, и улучшает прозрачность модели для бизнес заказчика. У нас расписана вся структура владения в компаниях среднего бизнеса, и мы можем сказать, кто в любой из этих компаний наиболее вероятный ЛПР. Даже там, где у нас нет никаких косвенных признаков, всё равно можно сделать определённые выводы.Результаты внедренияПосле проведения пилотного проекта в нескольких регионах, где была запущена новая модель, точность определения ЛПР достигла 80%. Это значительно выше прежнего уровня, что позволило существенно улучшить качество маркетинговых кампаний и эффективность работы с клиентами. Менеджеры, участвовавшие в верификации  модели, отметили, что в 80% случаев они согласны с выводами системы о том, кто является настоящим лицом, принимающим решение в компании.Также внедрение данной инновации не только улучшило взаимодействие с клиентами, но и способствовало росту продаж. По результатам внедрения модели в процессы цифровых продаж мы получили следующий прирост показателей в 2023 году.Рост операционной прибыли до 1.5 млрд. руб. (x5 vs 2022).Рост доли продаж «в цифре» c 1% до 4%.Рост количества продуктов на клиента x1.7.Прямое обращение к реальным ЛПР позволяет банку более точно нацеливать свои предложения, повышая лояльность клиентов и достигая высоких бизнес-результатов. Это, в свою очередь, помогает избежать ситуаций, когда банк пытается работать с руководителями, которые не обладают реальными полномочиями.Будущие перспективыВ будущем система будет совершенствоваться и адаптироваться под более сложные корпоративные структуры, что особенно важно для работы с клиентами из крупного бизнеса. Ожидается, что дальнейшее развитие модели позволит ещё точнее выявлять этих ключевых фигурантов, учитывая их роль в холдингах, а также их влияние на дочерние и аффилированные компании. Это поможет банку более эффективно взаимодействовать с руководителями, которые обладают решающими полномочиями и могут принять важные бизнес-решения.Также в перспективе планируется интеграция дополнительных источников данных. Эти данные позволят более глубоко анализировать взаимодействие сотрудников на различных уровнях и выявлять связи, которые могут указывать на реальных ЛПР. Например, можно будет учитывать корпоративные взаимоотношения или участие в профессиональных сообществах.Кроме того, в будущем планируется внедрение более сложных алгоритмов машинного обучения, а также нейросетей, которые будут учитывать динамичные изменения в корпоративной среде. Например, кадровые перестановки, изменения в уставных документах или структуре владения посредством мониторинга новостей, профильных профессиональных изданиях и событий в профессиональных социальных сетях. Это позволит модели адаптироваться к быстро меняющимся условиям и точно определять ЛПР даже в случае изменений внутри компаний. Также планируется развивать подходы к выявлению ЛПР по конкретным продуктам, и внедрение подхода в другие сегменты бизнеса.В целом с развитием системы и интеграцией новых данных банк сможет не только улучшить процесс поиска ЛПР, но и создать гибкую, эффективную платформу для взаимодействия с клиентами, что даст долгосрочные преимущества в конкурентной борьбе."
56,56,Самописная платформа для A/B-тестирования Trisigma: что она умеет и какую пользу способна принести бизнесу,AvitoTech,У нас живут ваши объявления,343.46,"Веб-разработка, Электронная коммерция, Веб-сервисы",2025-03-12,"Привет! Меня зовут Артём Дронов, я аналитик команды A/B-тестирования в Авито. Недавно мы выпустили на внешний рынок продукт, который помогает нам проводить эксперименты в компании — это платформа для A/B-тестирования Trisigma, разработанная AvitoTech.За последние пять лет количество экспериментов в Авито выросло с сотни до нескольких тысяч в год. Наша платформа Trisigma сейчас — ключевой инструмент для анализа и тестирования гипотез, а в итоге — важное условие для успеха бизнеса. В этой статье я рассказываю, как работает Trisigma и чем она отличается от других платформ.Перейти на сайт Trisigma можно по клику вот здесь, а по этой ссылке вы найдете техническую документацию платформы.Про то, как в целом устроено тестирование в Авито, мой коллега Данила писал ранее на Хабре, вот эта статья. Рекомендую к прочтению, если вам интересно А/B-тестирование и организация этого процесса.Содержание статьи:Что вообще такое Trisigma?В каких случаях Trisigma выгодна?Как мы начинали и к чему пришлиПреимущества TrisigmaИнтерфейс и возможностиСемантический слой: основа стандартовУдобство и взаимодействие командИтогиЧто вообще такое Trisigma?Это платформа для автоматизации А/B-тестирования и комплексной аналитики цифровых сервисов. Trisigma помогает компаниям принимать решения о качестве и эффективности разработки новых и обновлению существующих продуктов, основываясь на анализе данных и результатах экспериментов.Trisigma изначально была создана для внутренних нужд и первое применение получила в Авито. В процессе использования мы накопили определенный опыт, а потом поняли — пора делиться наработками с рынком в рамках полноценного продукта. В дальнейшем AvitoTech намерена расширить свое присутствие в сегменте B2B-решений, представив и другие продукты для оптимизации и автоматизации разработки.Еще одна разработка AvitoTech — инструмент для мобильных автотестов Emcee, узнать о котором больше можно здесь.В каких случаях Trisigma выгодна?Trisigma будет работать с бизнесом любого размера — от стартапов до крупных корпораций, которые создают цифровые продукты для массового рынка. Но особенно оценят платформу компании из сфер электронной коммерции, банковского сектора, стриминговых сервисов и образовательных платформ. Наиболее эффективно оценить смогут в бизнесах, ориентированных на b2c и с большим потоком пользователей — цифры А/Б-тестов показательны только когда есть достаточно широкая выборка.Мы посчитали, что самостоятельная разработка аналогичной платформы обойдется средней компании в несколько десятков миллионов рублей и потратить на это придется примерно два года. Кроме того, вложения на поддержку инфраструктуры и команды платформы составят не менее 50 миллионов рублей.Trisigma же стоит существенно дешевле, так как развертывается в облаке, а вся поддержка и работа команды – на стороне AvitoTech. Она не требует значительных технических ресурсов со стороны клиентов и легко масштабируется. Точная стоимость использования Trisigma зависит от потребностей конкретного клиента. Как мы начинали и к чему пришлиО своей платформе для A/B-тестов в Авито задумались еще в 2016 году. Тогда провели два масштабных редизайна и оба релиза катились в прод без A/B. В редизайн было вложено много ресурсов, затягивать релиз никто не хотел, а A/B-тесты стоили дорого. Это вышло боком – метрики покупателей после релизов «упали ступенькой».2017 год был объявлен годом быстрых A/B-экспериментов – релизы регулярно стали проходить валидацию через эксперименты. Аналитики проводили тесты руками в формате 24/7. Стало понятно, что аналитика A/B — бутылочное горлышко и пришло время строить платформу, так что запрос на платформу у нас сформировался от продукта.К каким выводам мы пришли:без платформы АБ-тестов продуктовая команда вынуждена заниматься ручным трудом и все упирается в ресурс аналитика, причем этот аналитик должен быть высокой квалификации. И при этом он может тратить неделю на один эксперимент;при внедрении платформы Trisigma «бутылочных горлышек» не остается, аналитика автоматизируется, поэтому количество экспериментов зависит только от количества идей/гипотез, которые продуктовая команда успевает доставлять до конечного пользователя; платформа стимулирует горизонтальное расширение компании, появление новых продуктовых команд. Аналитик с помощью платформы может одновременно проводить десяток экспериментов, сотни в год.Немного цифр из опыта использования Trisigma в Авито:количество продуктовых экспериментов в год увеличилось с 10+ до 4000+; успешные эксперименты обеспечили рост ключевых показателей, таких как выручка или количество покупателей, на уровне 10–20% в год.Тут еще больше контентаПреимущества TrisigmaВыделю четыре основных на мой взгляд достоинства платформы:Скорость и удобство тестирования. Trisigma позволяет быстро запускать эксперименты и проверять гипотезы, что ускоряет развитие продукта благодаря более оперативному внедрению новых функций.  Оптимизация рабочего времени аналитиков. Благодаря автоматизации расчёта метрик платформа освобождает до 90% рабочего времени аналитиков: ресурсы команды теперь могут быть перераспределены для решения более сложных и креативных задач.  Единая система для всех команд. Платформа объединяет данные по всем экспериментам в одном месте независимо от специфики работы команды.  Минимизация ошибок. Повышение надежности результатов за счет исключения человеческих ошибок.Trisigma связывает процесс тестирования новых функций с анализом их влияния на бизнес-показатели. Это помогает компаниям получить полную картину развития цифрового продукта и избегать потерь от запуска неудачных обновлений.О функционале. Если тезисно пробежаться по возможностям платформы — то я выделяю шесть основных пунктов:настройка групп и весов — Trisigma поддерживает возможность сплитования фичей на неограниченное количество групп. Каждой группе можно установить свой вес;проведение экспериментов на любых устройствах — WEB и мобильные приложения;обработка большого числа метрик и сегментов в эксперименте — десятки тысяч метрик-разрезов в одном эксперименте считаются бесперебойно;полностью прозрачное экспериментирование: История изменений, Комментарии, Описание, Ответственные специалисты;безопасный запуск большого множества экспериментов одновременно — система слоев позволяет проводить большое количество экспериментов одновременно без рисков их конфликтов;таргетирование пользователей. Раздавайте фичи только целевым пользователям за счет: Проведения экспериментов со списками конкретных пользователей, Анализа данных только по пользователям, столкнувшихся с тестовой фичей (получивших Exposure).Жми сюда!Интерфейс и возможностиСтраница экспериментов. На главной странице собраны все эксперименты компании. Сотни команд одновременно тестируют гипотезы через Trisigma, а фильтры помогают быстро находить нужные проекты по команде, задаче или другим параметрам.  Создание эксперимента. Запуск нового теста начинается с заполнения формы. В ней указываются:  продуктовые данные (например, команда и задача);  технические параметры (тип устройств, версии приложений).Платформа поддерживает гибкость: можно вручную задавать группы пользователей, использовать списки, а также устанавливать параметры взаимодействия тестов друг с другом.  Отчёты по экспериментам. Ключевая особенность Trisigma — удобный отчёт с результатами тестов. Чаще всего мы проводим эксперименты в формате Fixed Horizon. В отчёте отображаются:  оценка изменений для каждой метрики;  динамика изменений (по клику на ячейку таблицы);  целевые и контрметрики эксперимента.  Платформа предоставляет множество возможностей для поиска и фильтров. При настройке эксперимента можно определить, в каких сегментах будут посчитаны нужные метрики.После этого мы можем в отчёте посмотреть на разницу в поведении пользователей по сегментам, например, по разным регионам или версиям устройств. Важно отметить, что все метрики, которые мы сейчас видим в отчёте, считаются по единым принципам во всех экспериментах на платформе. Для того чтобы обеспечить подобный расчёт продуктово и инфраструктурно, мы используем так называемый семантический слой.Семантический слой: основа стандартовСемантический слой — это в некотором смысле уровень абстракции, который вводится между продуктовыми запросами и данными в хранилище. Как правило, он состоит в формулировании некоторых правил, по которым могут быть подсчитаны продуктовые метрики. Семантический слой упрощает работу аналитиков:  все метрики считаются по единым стандартам;  исключаются разночтения, поскольку правила расчёта задаются один раз и используются всеми командами.  Как это работает?Для добавления метрики нужно:  Определить источник данных.  Задать логику расчёта (описать действия и колонки).  Оформить метрику в реестре, добавив теги и описание.  После этого метрика становится доступной в инфраструктуре компании: её можно добавлять в эксперименты или использовать в отчётности.Подробнее почитать об этом можно в статье Данилы по этой ссылке.Удобство и взаимодействие командTrisigma помогает не только запускать тесты, но и решать задачи, связанные с конфликтами экспериментов:  система слоёв изолирует тесты друг от друга;  пересечения между тестами пользователи отмечают в UI, а платформа подсказывает, как безопасно запускать новые эксперименты.  Кроме того, аналитики могут обсуждать тесты прямо на платформе, задавать вопросы в комментариях и уточнять дальнейшую стратегию. Также на платформе можно оформлять итоги эксперимента, чтобы в дальнейшем было удобнее изучать прошлые результаты.ИтогиTrisigma позволяет автоматически проверять эффективность новых функций на разных сегментах аудитории до их массового запуска. Компании могут запускать сотни экспериментов одновременно на различных платформах и операционных системах, а также в разных пользовательских сегментах. Имеются гибкие настройки тестов и генерация подробных отчетов о результатах.Платформа умеет обрабатывать большие потоки данных цифровых сервисов, вычислять и систематизировать метрики и показатели качества продукта. Система интегрируется с существующими у клиента аналитическими системами (разными БД, например Clickhouse), чтобы забирать данные и по ним считать метрики. Также Trisigma визуализирует результаты в понятной и доступной форме, что упрощает анализ даже для тех, кто не является профессиональным аналитиком.  Trisigma — это не просто платформа для тестирования. Это инструмент, который стандартизирует подход к анализу данных, ускоряет работу и позволяет экспериментировать с продуктами более осознанно.  Кликни здесь и узнаешьСпасибо за уделенное статье время! Если у вас появились вопросы по работе продукта — встретимся в комментариях.Если вам интересны подробности, заглядывайте в наш Telegram-канал Avito Data Tech. Там мы делимся материалами о Trisigma и других проектах нашей команды.  Больше о том, какие задачи решают инженеры Авито, — на нашем сайте и в телеграм-канале AvitoTech. А вот здесь — свежие вакансии в нашу команду.  "
57,57,Модели T-lite и T-pro: training report,Т-Банк,Компания,504.04,"Программное обеспечение, Электронная коммерция, Веб-сервисы",2025-03-12,"Привет! Я Дима Стоянов, MLE в команде разработки фундаментальных моделей. Мы продолжаем рассказывать о наших моделях T-lite и T-pro. Общие характеристики и результаты бенчмарков описывали в предыдущей публикации. В этой статье раскроем детали предобучения: от подготовки данных до финальных экспериментов, а совсем скоро поделимся особенностями этапа post-training.Открытые модели все увереннее конкурируют с проприетарными решениями, что наглядно демонстрирует успех DeepSeek R1 и реакция сообщества на ее выпуск. Ситуация с русскоязычными моделями обстоит сложнее: в открытом доступе представлено ограниченное количество LLM. Крупные игроки чаще всего публикуют модели с ограничивающими лицензиями или же выкладывают легкие версии своих флагманских разработок. В мире открытых моделей процесс обучения моделей часто описывается скудно, особенно это касается обработки данных. Мы хотели поддержать опенсорс-сообщество, выложив модели с открытой лицензией. А в этой статье детально раскроем наш процесс обучения: от философии проекта до технических нюансов предобработки данных. Поделимся успешными и неудачными экспериментами, рассмотрим актуальные исследования в этой области и наше видение дальнейшего развития.Зачем адаптировать модели на русский языкБольшинство сильных открытых моделей англоцентричны, то есть их тренировочный датасет по большей части состоит из англоязычных текстов. Это происходит по естественным причинам: больше всего данных, особенно высококачественных, представлено именно на английском языке. Доля русского языка в таких моделях обычно не превышает нескольких процентов датамикса. Англоязычность моделей и небольшая доля русского приводит к проблемам:субоптимальное качество ответов на русском языке; артефакты при генерации русскоязычного текста, например неестественные предложения;code-switch, когда модель переключается между языками в ходе одного предложения;недостаточное знание российской культуры.Полное предобучение с нуля как один из способов решения перечисленных проблем требует значительных вычислительных ресурсов на каждый эксперимент и может быть дорогим в приложениях. Открытые модели не стоят на месте, постоянно улучшая качество. Нашей целью было получить рецепт обучения, который позволит использовать прогресс open-source-моделей и одновременно улучшить качество для русского языка. Так мы пришли к многостадийному continual-pretraining c последующими general-alignment-ом. Это позволяет нам балансировать между забыванием исходного распределения данных, на котором предобучалась базовая модель, и глубиной адаптации под наши цели.Помимо этого, наш рецепт можно повторить и для доменной адаптации на внутренние домены или, например, финансовый домен. В результате доменно адаптированная модель будет лучше решать банковские задачи. Но, так как open-source не стоит на месте, мы каждый раз продолжаем проверять стадии нашего рецепта и наблюдаем, что каждая из них приносит значимый прирост на бенчмарках.Стадии обучения языковой моделиСуществуют различные подходы к обучению моделей: от создания моделей с нуля, включая все этапы обучения, до более легковесных решений. Легковесные методы адаптации. Наименее затратным методом являются Parameter Efficient Fine-Tuning методы — когда обучают лишь часть параметров модели.Такое обучение можно легко завести на consumer-grade-видеокартах для большинства моделей, а для обучения зачастую достаточно сотен тысяч примеров. PEFT широко применяется для адаптации модели под конкретные бизнес-задачи, обеспечивая высокую эффективность при минимальных затратах.Если качество обученной модели, дообученной с помощью PEFT, оказывается недостаточным, применяют Supervised Fine Tuning. В этом случае обучаются уже все параметры модели, что требует больше вычислительных ресурсов и позволяет значительнее влиять на поведение модели и целевые метрики.Обучение моделей общего назначения, не ориентированных на узкоспециализированные сценарии, проходит с помощью General Post-Training. General Post-Training включает:General SFT — дообучение модели на разнообразных инструкциях для расширения ее функциональных возможностей.Alignment — настройку модели в соответствии с человеческими предпочтениями и этическими принципами.Обучение моделей с нуля дает максимальную гибкость, позволяя выбирать архитектуру, закладывать необходимые знания и адаптировать модель под конкретные задачи. Полное обучение включает:Выбор архитектуры.Обучение токенизатора.General Pre-Training. General Post-Training.Хотя такой подход позволяет достичь передовых результатов (SOTA), он требует колоссальных ресурсов: тысячи видеокарт, так как SOTA-модели обучаются на десятках триллионах токенов. Еще нужны огромные вычислительные мощности и значительные инженерные усилия. Кроме того, циклы экспериментов могут длиться неделями, а полный процесс создания модели — занимать месяцы. За это время в опенсорсе может появиться более мощная модель, что делает вложенные ресурсы неэффективными.Мы использовали гибридный подход Continual Pretraining, который сочетает элементы Pre-Training и Post-Training, но вместо обучения модели с нуля мы взяли за основу уже сильную открытую модель — Qwen2.5. Это позволило нам:значительно снизить затраты;ускорить процесс обучения;сохранить преимущества глубокой адаптации.Continual Pretraining дает баланс между качеством, гибкостью и стоимостью, обеспечивая эффективное развитие моделей для реальных задач.Схема этапов обучения T-pro Наш рецепт обучения состоит из шести стадий:выбор оптимальной базовой модели;опциональный этап по расширению токенизатора;continual pretraining stage 1;continual pretraining stage 2 с добавлением инструктивных данных;SFT;Preference Tuning.Основная идея — дообучать уже сильные открытые модели, тратя на обучение на порядки меньше ресурсов, чем создатели текущих моделей лидеров индустрии.Мы сохраняем возможность улучшать необходимые навыки, адаптировать модель к целевым доменам и нивелировать недостатки опенсорс-моделей, например переключение на китайский язык во время генерации.  Мы хотим обучать модель на сотнях миллиардах токенов: этого достаточно, чтобы оказать значимое влияние на способности модели, при этом осуществлять короткий цикл экспериментов и экономить ресурсы.В релизе T-lite-it-1.0/T-Pro-it-1.0  мы не проводили этап расширения токенизатора, поскольку существующие токены родного токенизатора Qwen-2.5 были обучены на гораздо большем токен-бюджете и нам не хотелось рисковать деградацией способностей модели. Нативный токенизатор Qwen-2.5 имеет не самую оптимальную плотность на русском языке, и этот этап действительно актуален. В будущих релизах планируем увеличить эффективность токенизации на русском языке.Помимо стандартных методов расширения токенизатора с прогревом embdding-слоев и дообучением на большом корпусе претрейн-данных существуют менее вычислительно затратные методы, например подход Learned Embeddings Propagation от RuAdapt.Выбор оптимальной LLM-моделиПервым этапом нашего рецепта был выбор сильной открытой модели. Мы оценивали кандидатов по нескольким критериям:высокие бенчмарки на популярных англоязычных бенчмарках;позиции на LMSYS Arena, включая срез русскоязычных запросов;результаты на русскоязычной арене;достойные показатели на русскоязычных бенчмарках (MERA, ru-arena-hard);развитые мультиязычные способности;субъективные впечатления от промптинга модели на различных задачах;качество перевода на русский язык.Мы начали с анализа опубликованных бенчмарков, поскольку это наиболее доступный и быстрый способ получить предварительное представление о модели. Затем самостоятельно протестировали кандидатов на MERA, оценивая их поведение в конкретных задачах.Стоит учитывать, что некоторые задачи в MERA имеют специфический формат, непривычный для LLM. Это может приводить к неоптимальным результатам не из-за недостатка знаний или способностей модели, а из-за особенностей промптинга и формулировки заданий. Тем не менее оценки на MERA дают полезный сигнал для отбора моделей.Статические бенчмарки помогают оценить уровень знаний модели, однако для понимания ее полезности в реальных сценариях важны диалоговые бенчмарки, такие как арены. В них пользователи голосуют за лучшие ответы, формируя лидерборд, который позволяет выявить модели с наиболее качественными и естественными ответами.Поскольку наша цель — улучшение модели в русскоязычном сегменте, мы изначально ориентировались на мультиязычные модели, обладающие хорошими зачатками работы с русским языком. Это позволило существенно упростить процесс адаптации.Хотя бенчмарки дают важную информацию, полностью полагаться на них нельзя: у них есть свои ограничения и недостатки, также их часто «гейнят». Поэтому ключевую роль в окончательном выборе сыграли реальные сценарии использования:генерация синтетических данных;перевод на русский язык;решение In-Context Learning (ICL) задач.На основе совокупности всех тестов и критериев мы выбрали семейство Qwen-2.5 в качестве базовой модели.Как дообучать модельПосле выбора модели возникает вопрос: с какой версии начинать дообучение — базовой или инструктивной?С одной стороны, дообучение базовой модели дает полный контроль над ее диалоговыми способностями и логично дополняется этапом допредобучения. С другой стороны, передовые инструктивные модели уже обучены на миллионах, а иногда и на десятках миллионов инструкций, а их post-training включает значительные усилия по улучшению качества ответов. Это делает использование таких моделей привлекательным, поскольку позволяет сразу задействовать их сильные стороны.Наши эксперименты показали, что дообучение на основе инструктивной модели оказалось более эффективным, чем на базовой. Но это объясняется относительно небольшим токен-бюджетом нашего обучения: веса модели не слишком сильно отклонились от исходного состояния, что минимизировало проблему катастрофического забывания.При обучении на значительно больших объемах данных дообучение поверх базовой модели может быть более оптимальным, так как позволяет глубже интегрировать новые знания и настраивать модель под конкретные задачи.Датасет для первой стадии предобученияБюджет первой стадии обучения составляет 100B токенов. Достаточно, чтобы значимо повлиять на знание русского языка, при этом позволяет сохранять короткий цикл экспериментов.График показывает взаимосвязь между долей английского в претрейне и лоссом на английских текстахПри дообучении под русский язык все еще необходимо добавлять повтор на английском языке в датамикс для уменьшения эффекта деградации на английском языке. Но деградация все равно будет, и, как показывает статья авторов модели Sailor, существует компромисс между скоростью обучения целевому языку и снижением качества бенчмарков на доминирующем исходно, при этом такой деградации полностью не избежать. Авторы вывели зависимость валидационного лосса от доли повторов.Для нас результаты модели на русском языке значительно важнее просадки на англоязычных бенчмарках, поэтому мы решили оставить лишь 10% англоязычных текстов в первой стадии обучения. В ходе проведенных аблейшнов эта конфигурация также проявила себя оптимальнее всего.Распределение языков в первой стадии предобучения (Stage 1)Помимо английского языка, необходимо выбрать долю кода в датамиксе. Основываясь на актуальных исследованиях по связи кода и навыков рассуждения, мы решили установить значительную долю кода в распределение датамикса — 25%. Распределение по источникам Stage 1Более половины распределения по источникам составляют очищенные и дедуплицированные через CCNet-like пайплайн веб-страницы из Common Craw. Их объем позволяет масштабировать датамикс до триллионов токенов. Такие документы характеризуются большой разнообразностью, но содержат большое количество дублей, некачественного и бесполезного контента, который необходимо вычищать.Относительно небольшой объем Stage 1 позволяет установить значительную долю таким высококачественным источникам, как Википедия, книги, синтетические инструкции и специализированные форумы.При выборе весов макрогрупп мы ориентировались на четыре критерия:известные распределения у сильных открытых моделей;избегали чрезмерного повторения семплов в датамиксе;приоритизация более высококачественных источников, таких как Википедия и книги;разнообразность и качество источника, а также необходимые навыки.Мы проводили аблейшны с различными долями макрогрупп для определения оптимальной доли кода, книг и синтетических данных.Ребалансировка стилей документов в Common Crawl (СС)СС содержит разнообразные документы по стилям и тематикам, но эти распределения не сбалансированы. Например, большую часть CC составляют новости и реклама — не самые полезные документы для обучения языковой модели. Мы сэмплировали документы, балансируя распределение тематик и стилей, при этом уменьшая долю малоинформативных групп. Ребалансировка жанров книгСреди книг доминируют жанры «Фэнтези» и «Проза», в то время как научные жанры составляют малую долю выборки. Поэтому по аналогии с CC мы ребалансировали распределение жанров, уменьшая долю доминирующих.Значимость кодовых датасетов и связь с навыками рассужденияКодовые датасеты являются неотъемлемой частью любой сильной LLM. В Llama 3 доля кодовых данных составила 17% датамикса, а согласно исследованию Cohere добавление кодовых данных улучшает метрики не только на кодовых задачах, но и для любых других, требующих навыка рассуждения. Еще они пришли к выводу, что оптимальная доля кода — 25%. В тренировочном датасете токенизатора Claude, вероятно, содержалось еще больше кодовых данных. Исследователи из Cohere с помощью атаки на токенизатор выяснили примерную долю доменов в датамиксе, и оказалось, что у модели Anthropic кодовые данные занимают больше половины датамикса! Тренировочный датасет токенизатора и самой LLM может отличаться, но обычно их делают похожими по распределению, чтобы модель эффективно «вложила» в эти токены знания.Распределение весов доменов у различных моделей с помощью атаки на токенизаторыДаже несмотря на то, что кодовые задачи не являются приоритетным сценарием использования для нашей модели, мы понимали необходимость улучшать пайплайн обработки кодовых данных.Из открытых кодовых корпусов наиболее актуален The Stack v2 — примерно 900 миллиардов токенов на 600+ языках программирования. Он был использован для обучения Starcoder 2 — сильной кодовой модели своего времени. Многие опенсорс-модели продвинулись дальше в обработке кодовых данных и уже имели более сильные результаты на кодовых бенчмарках.При дообучении базовой модели на новый язык необходимо приближать распределение и качество датасета к датасету оригинальной модели для избежания деградации в основных навыках. Если учить такую базовую модель, как Qwen-2.5, на кодовых данных хуже, чем в тренировочном датасете Qwen-а, мы рискуем ухудшить способности модели к рассуждению.Пайплайн очистки The StackСоздать полностью свой пайплайн сбора и очистки кодовых данных трудозатратно, поэтому нашей целью были быстрореализуемые методы фильтрации и преобразования данных для улучшения способностей к генерации кода с русскоязычными инструкциями. Мы сузили список из 600 языков программирования до 10 наиболее популярных, таких как Python, Java, C++, SQL и других, формата хранения данных JSON и языка разметки Markdown.Мы применили Fill in the middle для кодовых данных с вероятностью 50% по аналогии с Starcoder 2. Эта техника заключается в разбиении сэмпла на три случайные части: prefix, middle и suffix. Далее сэмпл форматируется в следующем формате:<fim_prefix>filepath1code1_pre<fim_suffix>code1_suf<fim_middle>code1_mid<|endoftext|>Мы меняем местами середину и конец файла, и при обучении на следующий токен модель будет использовать контекст и префикса, и суффикса. При этом не делаем архитектурных изменений в декодере и все еще используем causal attention. Применение техники FIM позволяет улучшать навыки заполнения пропусков в коде, что полезно для кодовых ассистентов. В статье phi-4 авторы использовали более продвинутый метод: они создали набор упражнений на восстановление осмысленной части текста, например части функции с пропущенной логикой. Логика пропущенного блока кода описывается текстом, и задача модели заключается в восстановлении кода сегмента. Такой подход позволяет создавать инструктивные пары, не требуя наличия пар контекста и верного ответа,  и масштабировать его на сотни миллиардов токенов.Для Python мы сделали этап дополнительной фильтрации. Отфильтровали все файлы с некорректным синтаксисом с помощью ast-парсинга. Большая часть таких файлов — python2-скрипты. Мы отформатировали 80% файлов кода с помощью утилит unify, autopep8, black и isort. Так мы стандартизировали стиль и форматирование кода.Помимо этого, использовали взвешенное семплирование. Вес семпла вычисляется из двух величин: логарифма от количества форков репозитория и средней оценки качества кода линтером. Linter score получен из агрегации ошибок, найденных линтерами mypy, flake8, pylint с взвешиванием по типам ошибок. Например, ошибка в типе данных более критична, чем неиспользованный импорт. Метрика учитывает не только количество ошибок, но и объем файла. Метрики по файлам агрегируются в метрику по репозиторию, в результате у репозитория с более чистым кодом больше вероятность пройти этап фильтрации. Альтернативный подход — фильтрация с помощью educational-классификаторов, например python-edu-scorer от HuggingFace. Авторы использовали LLM для оценки образовательной ценности фрагментов кода и выставляли оценку от 1 до 5. Полученные оценки были дистиллированы в легковесный классификатор, в результате предсказаний которого был получен датасет Python-Edu, использованный для обучения моделей SmolLM и SmolLM2. Несмотря на перспективность подхода, его применение было у маленьких моделей (не более 1.7B). Такие модели могут иметь разные рецепты обучения, и не все идеи переносятся на больший размер. Мы обратили внимание на то, что классификатор высоко оценивает короткие, чистые и простые блоки кода. Но в реальности полезный код бывает куда сложнее и менее похож на код из туториалов. В то время как для маленьких моделей резонно добавлять более простые и понятные паттерны обучения, мы не были уверены в применимости этого подхода для больших моделей. Тем не менее это интересная идея для дальнейших исследований.Большинство написанного кода содержит англоязычные комментарии. Также такие форумы, как StackOverflow, имеют значительно большее количество контента на английском языке по сравнению с русским. Чтобы улучшить способности понимать кодовые инструкции на русском языке, мы:отфильтровали кодовые данные с русскоязычными комментариями и увеличили их долю в датамиксе;перевели StackOverflow на русский язык с помощью опенсорс-моделей с сильными мультиязычными способностями;перевели комментарии на русский для части Python-данных.При переводе StackOverflow от модели переводчика необходимо глубокое понимание концепций кода, терминов и, естественно, хорошие мультиязычные способности.Также необходимо контролировать «ленивость» моделей: часто на задаче перевода длинного контекста модель генерирует лишь частичный перевод. Есть два способа бороться с проблемой: переводить только данные с небольшим контекстом (обычно не более 4000 токенов) или делить текст на чанки ограниченной длины, а после объединять переводы. Недостаток деления на чанки — отсутствие полного контекста в запросе, который получает LLM. В некоторых случаях это может влиять на качество перевода. В промпт помимо чанка можно добавить исходный вопрос пользователя, при этом просить LLM переводить только часть текста, нивелируя проблему генерации длинного контекста.Еще при переводе необходимо проверять специальные теги и не модифицировать код, то есть проверять, что исходный код не исказился при переводе LLM.Одним из вариантов подобной проверки может быть такая эвристика:проверка на зацикливание модели (симптом — если модель не сгенерировала eos-токен);переключение на неожиданные языки (например, появление языка, отличного от исходного и от того, на который вы переводите);совпадение количества блоков кода, элементов разметки, реплик и любых других известных элементов структуры;значительное изменение длины переведенного текста относительно исходного;создание blacklist-а наиболее частых фраз, не относящихся к переводу (Here is a translated text…).Для облегчения постпроцессинга переводов можно использовать структурированную генерацию, например JSON-формат с заранее заданной схемой. LLM не всегда отвечает корректным JSON, однако в большинстве случаев этот ответ близок к корректному JSON-объекту и/или его можно восстановить с помощью специальных библиотек. Технические детали тренировки Stage 1Мы подготовили датасет для обучения Stage 1. Теперь перейдем к техническим деталям тренировки:на каком оборудовании обучалась модель;какие ключевые нюансы возникли в процессе;с какими гиперпараметрами проводилось предобучение;как мы оценивали результаты Stage 1.В обеих стадиях предобучения у нас есть существенный процент данных, состоящих из сконкатенированных в один семпл коротких текстов. Это, в свою очередь, требует маскирования для разделения контекстов между текстами.Визуализация маскирования примеров в последовательностиВ статье Fewer truncations improve language modeling предложили алгоритм конкатенации семплов, минимизирующий долю обрезаемых токенов. В претрейн-обучении для более высокой производительности не используются паддинги. Вместо этого происходит конкатенация сэмплов в последовательность фиксированной длины. Обычно порядок семплов в конкатенации никак не контролируется, и это приводит к проблеме обрезанных семплов. В результате модель обучается на неполном контексте. В худшем случае в конец одного сиквенса может попасть текст и вопрос по тексту, а в следующий сиквенс — ответ на эту инструкцию. В результате модель будет учиться отвечать на незаданный вопрос по отсутствующему тексту, то есть галлюцинировать.Авторы статьи предложили жадный алгоритм, который минимизирует количество обрезаний последовательностей текстов. Полное решение такой задачи является NP сложной задачей, но существуют быстрые и достаточно точные аппроксимации с временной асимптотикой O(n*log(n)). Результаты исследования показали, что меньшее количество обрезаний улучшает работу с текстом, значительно улучшает следование инструкциям и эффективно уменьшает долю галлюцинаций.Визуализация Best-Fit PackingАппаратная конфигурация: обучение проводилось на H100-кластере, максимальный сетап для обучения данной модели был 256 H100 80 Gb. Использовалась технология Fully Sharded Data Parallel (FSDP) для распределенного обучения с full shard/hybrid shard стратегиями. Модели 7B и 32B достигали производительности 4 500 и 1 900 токенов/сек/GPU соответственно. В ходе обучения MFU достигал 44%.Гиперпараметры оптимизации: мы использовали оптимизатор AdamW с параметрами β1 = 0.9, β2 = 0.95, eps = 1.0e-8. Learning rate установлен на 1e-5 с константным расписанием и разогревом в течении 10 батчей.Параметры тренировки:Максимальная длина последовательности: 8192.Глобальный batch размер: 768 батчей (где 768 × 8192 = около 6 млн токенов).Длительность тренировки: 16000 батчей. Интервал оценки: 2000 батчей.Количество токенов на стадии 1: до 100B, все данные уникальны.Метрики, которые мы отслеживали в ходе обучения:Perplexity.Cross Entropy Loss.ruMMLU и Mera бенчмарки.График train loss для Stage 1. В ходе обучения возникали кратковременные всплески лосса (loss spikes), которые не привели к ухудшению метрикРезультаты MERA для Qwen-2.5-32B-it как бейзлайн для сравненияРезультаты MERA после обучения Qwen-2.5-32B-Instruct на Stage 1. Заметен прирост по MaMuRaMu (русскоязычный аналог популярного MMLU), ruCodeEval (аналог humaneval), CheGeKa, ruTiEМы решили использовать недавно ставшую популярной идею двухстадийного предобучения. Первая стадия содержит больше данных, в основном это веб-страницы, за счет объема и разнообразности она закладывает основные знания. Вторая стадия может быть на порядки меньше, при этом содержит более высококачественные данные, такие как инструктивные данные.Распределение Stage 1 и Stage 2 для модели MiniCPMНапример, в модели MiniCPM в первой стадии обучения доля Common Crawl (CC) подобных данных составляет 64%, еще 25% занимает код. Во второй стадии за счет меньшего объема такие высококачественные данные, как Википедия и набор инструктивных данных, уже не размываются в общем распределении и оказывают значительное влияние на качество модели.Двухстадийное обучение позволяет более эффективно обучать модели, демонстрируя преимущества по сравнению со стандартным одностадийным подходом. Исследования подтверждают, что этот метод приводит к улучшению метрик. Хотя на второй стадии используются инструктивные данные, частично перекрывающиеся с этапом SFT, модель, прошедшая оба этапа предобучения, показывает более высокие результаты после стадии алаймента.Визуализация четырехстадийного обучения в SmolLMВ недавно вышедшей модели SmolLM2 и вовсе использовали четыре стадии предобучения. Свой выбор исследователи объясняют принципами: Модель обучается с нуля и модель небольшого размера, математические навыки развиваются не сразу. Поэтому не имеет смысла значительно добавлять математические данные в первые стадии обучения.Высококачественные данные добавляются в последние фазы обучения, поскольку таких данных меньше. Авторы стараются избегать чрезмерного повторения данных, а именно не повторять один пример более 5 раз.В последней фазе обучения используется learning rate annealing. Как было показано в техрепорте Llama 3, этот метод отлично работает, как бы полируя накопленные знания в модели и превращает их в полезные навыки.Датасеты средних размеров вводятся в середине обучения, чтобы избежать их размытия в наиболее крупных первых стадиях.Стоит заметить, что предыдущие статьи описывали подход обучения с нуля, но мы собираемся дообучать открытую модель, которая уже была обучена на десятках триллионах токенов и уже обладает сильными способностями. Правда ли, что двухстадийное обучение все еще имеет смысл?Для ответа на этот вопрос мы обратились к статье от Nvidia Reuse, Don’t Retrain: A Recipe for Continued Pretraining of Language Models. В ней авторы используют двухстадийное дообучение: на первой стадии обучают на данных, максимально похожих по распределению на данные, которые могли использоваться для обучения базовой модели, увеличивая вес у более высококачественных источников, и навыках, в которых качество модели недостаточное. Во второй стадии датамикс в основном состоит из QA-пар и диалоговых данных.Визуализация оптимального момента переключения Stage 1 → Stage 2 в зависимости от learning rateАвторы статьи подчеркивают, что необходимо оптимально выбирать момент переключения распределения данных и что он зависит от расписания learning rate и токен-бюджета. В статье описываются эксперименты с разными составляющими первой стадии обучения. Оказалось, что датамикс без данных веб-страниц показал наилучшее качество, но авторы решили использовать смесь веб-данных и заапсемпленных высококачественных данных для большей стабильности.Скейлинг допретрена. Другое интересное исследование — зависимость качества модели от токен-бюджета дообучения. Авторы статьи масштабируют свой подход от 100 миллиардов токенов до триллиона, при этом метрики растут с ростом обучающего датасета.Эффект дообучения в зависимости от токен-бюджетаВо время обучения T-lite и T-pro мы пробовали увеличивать размер первой стадии с 100B до 500B токенов, но не получили значительного прироста, а по некоторым метрикам получили даже незначительную просадку. Одной из гипотез является разница в распределении обучающего датасета исходной модели и нашего: в нашей модели большую часть датасета составляют русскоязычные тексты, в то время как в базовой модели их доля была значительно меньше.При большем дообучении модель сильнее отклоняется от изначального распределения и может потерять ранее имевшиеся навыки. Кроме того, мы видели более эффективным улучшение второй стадии обучения, также как и масштабирование алайнмента стало более значимо, чем улучшения в предобучении. Поэтому для ускорения циклов экспериментов мы остановились на бюджете 100B токенов в первой стадии обучения.Датасет второй стадии предобученияРаспределение Stage 2 по языкамВторая стадия обучения охватывает 40 миллиардов токенов и основана на текстах повышенного качества. В этой стадии доля английского языка существенно возрастает — до 38,1% по сравнению с первой стадией. Увеличение обусловлено тем, что высококачественные обучающие материалы доступны преимущественно на английском языке, особенно наборы инструкций. Хотя перевод подобных датасетов на целевой язык является общепринятой практикой, он может привести к неточностям и искажениям смысла. Поэтому для достижения оптимальных результатов рекомендуется включать в обучающую выборку как переведенные материалы, так и оригинальные англоязычные тексты.Распределение второй стадии предобучения (Stage 2) по типам источниковStage 2 включает в себя разнообразные источники, смешивая данные, используемые на этапе предобучения и на этапе SFT. Эта стадия характеризуется более высококачественными данными, поэтому для каждого типа источников мы старались фильтровать или сбалансировать распределение данных внутри этого источника. Например, мы увеличивали долю образовательных и научных жанров книг.СС содержит триллионы токенов, но не все из них полезные. Помимо стандартных способов очистки из CCNet pipeline можно оценивать качество данных — насколько такой пример будет полезен при обучении языковой модели.Textbooks are all you need — статья от Microsoft, в которой они обучают классификатор образовательной ценности и фильтруют данные для предобучения.Примеры документов высокой и низкой образовательной ценностиНа основе подхода fineweb-edu мы составили промпт для оценки образовательной ценности документа.Ниже приведено содержание веб-страницы.Оцените, имеет ли страница высокую образовательную ценность и может ли она быть полезной в образовательной среде для обучения от начальной до средней школы, используя описанную ниже пятибалльную систему оценки. Очки накапливаются на основе удовлетворения каждого критерия:— Добавьте 1 балл, если фрагмент содержит некоторую базовую информацию, актуальную для образовательных тем, даже если он включает в себя некоторые несущественные или неакадемические материалы, такие как рекламные материалы.— Добавьте еще 1 балл, если фрагмент затрагивает определенные элементы, важные для образования, но не полностью соответствует образовательным стандартам.Он может смешивать образовательный контент с необразовательным материалом, предлагая поверхностный обзор потенциально полезных тем, или представлять информацию в беспорядочной манере и несогласованном стиле письма.— Назначьте третий балл, если фрагмент подходит для образовательного использования и вводит ключевые концепции, актуальные для школьных учебных программ.Он согласован, хотя и может быть неполным или может включать в себя некоторую постороннюю информацию. Он может напоминать вводный раздел учебника или базовый учебник, который подходит для обучения, но имеет значительные ограничения, например обрабатывает концепции, слишком сложные для учеников начальной школы.— Присвойте четвертый балл, если фрагмент очень актуален и полезен для образовательных целей для уровня не выше начальной школы, демонстрируя четкий и последовательный стиль письма.Он может быть похож на главу из учебника или учебник, предлагающий значительный образовательный контент, включая упражнения и решения, с минимальным количеством несущественной информации, и концепции, не слишком сложные для учеников начальной школы. Контент согласован, сфокусирован и ценен для структурированного обучения.— Предоставьте пятый балл, если фрагмент выдающийся по своей образовательной ценности, идеально подходит для обучения в начальной или средней школе.Он следует подробному обоснованию, стиль письма легко следовать и предлагает глубокие и тщательные знания по предмету, не содержащие никаких необразовательных или сложных материалов.Материал: {}.После рассмотрения материала:— Кратко обоснуйте свой общий балл, до 100 слов.— Закончите с оценкой в формате: «Образовательный балл: <общий балл>»Получив 10 000+ размеченных документов из СС с помощью модели-оракула, мы обучили BERT-like-модель на задачу многоклассовой классификации. Далее использовали обученный классификатор для предсказаний образовательной ценности по всем веб-документам. В Stage 2 оставили только те документы, которые получили 3 балла или выше.Фильтрация «мета»-документов. В результатах отфильтрованных данных мы обнаружили много примеров, которые можно отнести к образовательным. Однако некоторые примеры не содержали полезной нагрузки, а были лишь «мета»- информацией — например, описание учебника, оглавление, описание образовательного товара и так далее. Для фильтрации «мета»-документов мы обучили fasttext-классификатор на основе разметки из нескольких тысяч документов.Ранее были попытки обучать языковые модели на курируемых высококачественных датасетах. Например BERT обучался на текстах из Википедии, датасет Pile содержал различные источники: книги, научные статьи — и использовался для обучения GPT-J. Но позже RefinedWeb показали, что использование только данных из СС позволяет превзойти курируемые датасеты при грамотном пайплайне очистки данных.Применение классификаторов качества вносит определенное смещение в оригинальное распределение данных и является спорным вопросом. Refined Web избегают использования классификаторов и полагаются только на эвристики для очистки данных. Тем не менее многие передовые LLM в том или ином виде используют классификаторы качества. Результаты fine-web-edu показывают, что фильтрация по образовательной ценности увеличивает метрику на MMLU-бенчмарке, делая обучение более эффективным. Мы обнаружили похожий эффект при использовании нашего классификатора качества данных.В нашем случае фильтрация по образовательной ценности привела к смещению тематик и стилистик документов: чаще стали появляться образовательные статьи. Для сохранения высокой разнообразности мы применили более агрессивную ребалансировку по сравнению с первой стадией дообучения. К тому же фильтрация по образовательной ценности значительно уменьшает размер тренировочной выборки. Так, образовательную оценку от 3 баллов получило только 22% исходных документов, а выше 4 баллов — только 6% документов. Объем русскоязычных данных в СС сильно меньше англоязычных, это может быть проблемой для обучения больших языковых моделей с нуля, но имеет существенно меньшее влияние на вторую стадию из-за меньшего токен-бюджета.Книги — один из наиболее богатых источников качественных данных. Для получения данных из книжек мы использовали источники открытых библиотек и OCR-пайплайн распознавания контента из .pdf- и .djvu-файлов. В итоге мы получили больше 30 тысяч документов на русском языке. Распознанные документы могут содержать артефакты, особенно в структурированных данных типа таблиц. Для улучшения качества документов мы разработали refine-пайплайн, используя большие открытые языковые модели, так как эмпирически выяснили, что они хорошо справляются с такими задачами, как исправление ошибок в LaTeX-формулах, таблицах, опечатках и других небольших искажениях текста после распознавания. Как и в пайплайнах выше, длинные документы чанковались по эвристике, оставляя контекст до 2 тысяч токенов во избежание «оптимизации» текста моделями, после чего возвращались в исходную структуру. Рефайн состоял из нескольких последовательных вызовов с шагами рефлексии на основании вывода формальных парсеров, чтобы увеличить долю корректных чанков и, как следствие, документов. Как и для СС, применялись классификаторы определения языка, тематик, стиля и образовательной ценности контента с последующей фильтрацией. Дополнительно очистили артефакты markdown’а, оглавлений и повторяющихся элементов.Математические навыки — ключевые для развития способностей к рассуждению. Последний год создатели передовых LLM пытались улучшить значения математических бенчмарков, направив свои усилия на создание масштабных и высококачественных математических корпусов. Например, доля математических данных и данных, связанных с рассуждениями, в Llama 3 составляет 25%! Помимо образовательных русскоязычных учебников по STEM-предметам мы перевели такие англоязычные ресурсы, как Math StackExchange. Этот источник — QA-форум по различным математическим тематикам с системой голосования за более качественные ответы. Наличие тегов позволяет балансировать датасет по охватываемым тематикам, а реакции пользователей — выбирать наиболее интересные вопросы и полные ответы. Другой подход сбора математического корпуса — фильтрация математических документов из веб-страниц CommonCrawl. Один из таких корпусов — OpenWebMath, состоящий из почти 15B токенов. Префильтрация осуществляется за счет классификатора, обученного на определение схожести страницы на математический контент. Источниками тренировочных данных для такого классификатора являлись домены известных математических крупных сайтов.Пайплайн сбора математического корпуса OpenWebMathМы также перевели OpenWebMath на русский язык и добавили в датамикс обучения.Альтернативным решением является повторение методологии OpenWebMath для русскоязычных страниц CC. Из недостатков корпуса OpenWebMath можно выделить достаточно низкое качество некоторых документов, связанное с ошибками парсинга или очистки. Стоит обратить внимание на интересное исследование GAIR Lab, в ходе которого обучили небольшую языковую модель (SLM) для улучшения контента документа. Языковая модель генерирует вызов заранее заданных правил: оставить или удалить целый документ, удалить набор строк или нормализовать строку. Даже такого ограниченного набора правил хватило, чтобы значительно улучшить качество документов и получить прирост на бенчмарках. Авторы выложили датасет OpenWebMath-Pro на huggingface.Programming every example: LIFTINGlifting data quality like experts at scaleИсследование продемонстрировало эффективность применения моделей машинного обучения (SLM) на этапах предварительной очистки массивных текстовых корпусов с высоким уровнем шума, таких как Common Crawl, архивы форумов, научные публикации и программный код. Генерируемые с помощью SLM эвристики демонстрируют существенно большую гибкость по сравнению с традиционными правилами, которые были разработаны и верифицированы вручную в существующих системах очистки данных (Gopher, CCnet, RefinedWeb и других). Хотя использование SLM требует значительно больших вычислительных ресурсов по сравнению с применением простых эвристик, эта технология не заменяет традиционные методы полностью, а служит дополнительным инструментом для повышения качества очистки данных.Параллельные данные представляют собой пары соответствующих друг другу текстов или предложений на разных языках, которые используются для улучшения качества машинного перевода (MT) языковых моделей. Исследование Alma продемонстрировало, что даже небольшой объем параллельных данных в сочетании с дообучением на целевом языке значительно повышает качество перевода. Основываясь на этих результатах, мы включили в наш датасет параллельные данные из открытых источников, таких как xP3x и Aya Dataset. Дополнительно обогатили данные переводами инструктивных пар с английского на русский, специально собранными для расширения русскоязычной составляющей тренировочного набора данных.Инструктивные данные — важная составляющая второй стадии обучения. Они отличаются по формату от обычных данных для предобучения, поскольку состоят из нескольких реплик с разными ролями, в то время как данные предобучения — просто сплошной текст. Мы смешиваем инструктивные и обычные данные предобучения, а для инструктивных данных применяем все по аналогии с SFT-обучением: чат теплейт, маскирование лосса для запроса пользователя.Fun fact: баг с токенизацией, который мы нашли не сразуХотя токенизация данных с помощью библиотеки transformers обычно выполняется одним вызовом функции, подготовка токенизированных инструктивных диалогов требует более сложного подхода. Основная причина заключается в особенностях работы функции tokenizer.apply_chat_template(), которая не формирует тензор меток (labels) автоматически.При обучении модели на диалогах необходимо правильно настроить маску потерь (loss mask), чтобы модель училась только на ответах ассистента. Для этого требуется токенизировать каждую реплику диалога отдельно и вручную присваивать метки: −100 для реплик пользователя и системных сообщений (чтобы исключить их из расчета функции потерь) и фактические значения токенов для ответов ассистента.Такой подход позволяет корректно обучать модель, фокусируясь только на генерации ответов ассистента и игнорируя остальные части диалога при вычислении потерь.Наш код:python result_tokens = [] enable_loss = []  for msg in chat:    templated_msg = self.tokenizer.apply_chat_template([msg], tokenize=False,                                                       chat_template=chat_template,                                                       add_generation_prompt=False)    templated_msg = self.remove_eos_and_bos(templated_msg)    msg_tokens = self.tokenizer(templated_msg, truncation=False, padding=False, add_special_tokens=False)[        'input_ids']    result_tokens += msg_tokens    fill = 0 if msg.get('disable_loss') else 1    enable_loss += [fill] * len(msg_tokens)  result_tokens = self.bos_tokens + result_tokens + self.eos_tokens if self.bos_tokens:    enable_loss = [1] + enable_loss enable_loss = enable_loss + [1]При переходе с модели Gemma-2 на Qwen-2.5 мы столкнулись с неочевидной проблемой токенизации, которая привела к деградации метрик. Корень проблемы заключался в различиях chat template между моделями.Особенность токенизатора Qwen-2.5 в том, что он всегда добавляет системное сообщение в диалог, даже если его нет в исходных репликах. В нашем коде мы вызывали функцию apply_chat_template для каждой реплики отдельно, что приводило к многократному добавлению дефолтного системного промпта в токенизированные данные. При этом на бенчмарках использовался стандартный подход с единым вызовом apply_chat_template для всего диалога. Расхождение между форматами обучения и инференса привело к снижению качества модели. Хотя падение производительности оказалось менее критичным, чем обычно наблюдается при ошибках токенизации, это затрудняло своевременное обнаружение бага.Этот опыт позволил сделать важные выводы:Необходимо тщательно тестировать код токенизации, поскольку ошибки в нем могут иметь серьезные последствия для качества модели.При интеграции новых моделей критически важно детально изучать их chat template. Поскольку шаблоны реализованы на языке Jinja, который предоставляет большую гибкость, единого стандарта для них не существует.Любые модификации стандартного процесса токенизации требуют особенно тщательной проверки и валидации.Мы собрали открытые инструктивные датасеты с huggingface по различным доменам:General (OpenAssistant, WildChat, Synthetic LMSys-Chat-1M, UltraFeedback, OpenHermes-2.5, capybara, InfinityInstruct, Magpie-Ultra, neo_sft_phase2, tulu v3.1 mix, Synthia).Knowledge (Flan v2, WebInstructSub, Dolphin).Math (Numina-CoT, MetaMathQA, MathInstruct, TemplateGSM).Reasoning (Numina-TIR, OpenPlatypus).Code (Self-OSS-Instruct-SC2, Evol CodeAlpaca, OASST-Code-Octopack, CommitPackFT).IF (ifeval-like, SystemChat).Multilingual (xP3x, Aya).Англоязычные датасеты перевели на русский язык с помощью открытых LLM. Набор промптов инструкций почти полностью пересекается с SFT-сетом, но отличается большим объемом за счет менее строгих критериев качества и фильтрации.Поскольку многие источники инструкций содержат пересекающиеся наборы промптов, необходимым этапом подготовки является глобальная дедупликация всех инструктивных данных. Мы использовали LSH-дедупликацию для минимизации количества дублей в тренировочном датасете.Генерация синтетических инструктивных данных — популярный подход как для обогащения и формирования датамиксов предобучения, так и для получения SFT-like инструктивных данных. Одним из известных примеров синтетического корпуса предобучения является Cosmopedia. Суть подхода состоит в том, чтобы на основании отрывка веб-страницы и выбранных высококачественных источников сгенерировать длинный текст, расширяющий контекст данного отрывка, а вариативность достигается с помощью применения разных стилей. MAmmoTH2: Scaling Instructions from the WebНаш пайплайнПодход Cosmopedia лишь частично обусловлен по данному на вход контексту, но без ограничений генераций на внесение дополнительной информации как на уровне промптинга, так и на уровне фильтраций в постпроцессинге такой подход может приводить к галлюцинациям. Мы решили больше концентрироваться на второй стадии пайплайна обучения и инструктивных данных в ней, потому что она меньше по объему, содержит более высококачественные данные и больше влияет на финальное качество модели. Основная проблема, с которой мы столкнулись во время сбора инструктивных датамиксов, — отсутствие качественных и достаточно больших датасетов на русском языке, а существующие недостаточно покрывали common knowledge будучи в большей степени переводами соответствующих англоязычных источников. Мы рассматривали два варианта получения синтетики:перевод, как и с открытыми наборами данных;генерация синтетических инструктивных и question answering (как отдельный вид инструкций) пар, обусловленных на некоторый контекст.Первый подход применялся на формализуемых доменах, таких как математика. Как и в случае кодовых наборов данных, перевод проводился большими открытыми языковыми моделями с последующей фильтрацией по эвристикам. Отличие заключалось в дополнительном шаге — проверке совпадения блоков LaTeX / MathJax. Важно, что доменно специфичные термины часто искажаются при подобном подходе, но неточности языка в нашем случае в последующем выравниваются на post-training-стадиях.В качестве источника формирования инструктивных пар рассматривались поддомены StackExchange с фильтрацией по частотным тэгам, голосам как вопроса, так и комментариев. В случае с математическим доменом дополнительно сформировали blacklist тегов, покрывающих узкоспециализированные тематики, где перевод терминов был наиболее неточный и плохо верифицируемый. Брались только данные, теги которых покрывают 90% запросов пользователей, пост и комментарии к нему должны были быть оценены минимум одним пользователем. Итоговая пара «запрос — ответ» формировалась по правилу: ответ, который принят инициатором треда и автоматически считается верным, и ответ с самым высоким рейтингом, если он отличен от принятого.Генерация синтетических инструктивных данных стала достаточно популярной после выхода статьи, описывающей Self-Instruct-подход. Он без дополнительных ограничений склонен вырождаться, и полученный датасет может обладать малым разнообразием, более того, как и в случае с Cosmopedia, никак не ограничиваются галлюцинации. Чтобы побороть эти две основные проблемы, мы, опираясь на работы MAmmoTH2: Scaling Instructions from the Web и Nemotron-4 340B Technical Report, использовали подмножество сбалансированных СС-данных с метрикой образовательной ценности большей или равной 4, и книги с образовательной ценностью как минимум 3 в качестве контекста для генерации. Далее для фиксированного чанка контекста генерировалась пара «инструкция — ответ» и список вопросов и ответов, опирающихся на текст, что ограничивалось напрямую в затравке наряду с образовательной ценностью сгенерированного ответа. Следующим этапом пайплайна стала фильтрация данных на основе оценки их согласованности с контекстом и корректности с точки зрения языковой модели. Использование абстрактных шкал оценивания на этом этапе приводило к тому, что калибровку порога оценки необходимо было производить для каждой модели, что в общем случае не масштабируемо. Мы пришли к списку бинарных критериев. В итоге доля синтетических данных в первой стадии дообучения составила 5,7% и 4,7% — во второй.Технические детали второй стадии предобученияМы описали особенности сбора датасета обучения для второй стадии предобучения. Перейдем к описанию технических деталей обучения. Инструктивные маски. Во второй стадии в датамиксе появляются инструктивные данные, состоящие из запроса пользователя и ответа ассистента. Для таких данных мы маскировали loss для токенов, соответствующих запросу, и учили модель только на токенах ответа так же, как это делают при SFT-обучении.Аблейшны показали эффективность этого метода. Маскирование промпта способствует тому, что модель лучше фокусируется на выполнении задачи, снижая риск переобучения на некачественные или дублирующиеся запросы. При этом ответы в инструктивных датасетах, как правило, являются сгенерированными и проходят контроль качества, в отличие от исходных пользовательских запросов.Пример маскирования инструкцииПомимо маскирования промта существуют подходы смешанного маскирования, например с использованием Prompt Loss Weight. Утверждается, что на датасетах с короткими генерациями использование ненулевого PLW превосходит стандартное SFT-like-обучение с полным маскированием промптов. Но для длинных генераций эффект нивелируется.Стоит отметить, что многие инструктивные задачи, такие как Natural Language Understanding, имеют короткие ответы. Мы не пробовали этот подход, но он выглядит перспективным. Необходимо следить не только за распределением общей длины инструктивных примеров, но и за соотношением длинных и коротких промптов и генераций.Мы использовали те же гиперпараметры оптимизации, что и в Stage 1. Единственное отличие — расписание learning rate. После разогрева learning rate достигает 1e-5 и постепенно затухает до 1e-6 (alpha_f = 0.1).Параметры тренировки:Максимальная длина последовательности: 8192.Глобальный batch размер: 768 батчей (где 768 × 8192 = около 6 млн токенов).Длительность тренировки: 6500 батчей.Интервал оценки: 2000 батчей.Количество токенов на стадии 2: до 40B (20/40), где 1 эпоха — 20B.Метрики:График Train loss для Stage 2Результаты на MERA после обучения Stage 2После Stage 2 значимый прирост наблюдается в следующих бенчмарках: USE (ЕГЭ), RWSD, MultiQ (multi-hop QA), CheGeKa, математических ruMultiAr и MathLogicQA.Как видно по результатам замеров, претрейн-чекпоинт получает даже больший результат, чем T-pro-it-1.0 (то есть заалайненная модель). Заметим, что при составлении SFT-сета главной целью было развитие диалоговых способностей модели, что может отражаться на бенчмарках, нацеленных на знания модели.На прощаниеМы подробно раскрыли процесс предобучения наших моделей T-lite и T-pro, сосредоточившись на детальном описании каждого этапа: от подготовки данных до финальных экспериментов. Наш опыт показывает, что гибридный подход позволяет значительно сократить вычислительные затраты при сохранении высокого качества моделей. Обучение на сотнях миллиардов токенов оказывается достаточным для существенного улучшения способностей модели в работе с русским языком, при этом позволяя сохранить преимущества базовой архитектуры и знания, заложенные в исходной модели.В следующей статье мы планируем раскрыть особенности этапа post-training, который включает SFT и alignment.P.S. Спасибо всей команде, которая помогла этой статье быть: Оле Цымбой, Мише Гашкову, Герману Абрамову и Толе Потапову."
58,58,Переключение контекста — главный убийца продуктивности разработчика,Spring АйО,Компания,71.54,Программное обеспечение,2025-03-12,"Новый перевод от команды Spring АйО расскажет вам о том, почему так вредно отвлекать разработчиков от их работы и как избежать большого убытка для компании из-за прерывания рабочего процесса сотрудников.Вы когда-нибудь спрашивали себя, что является самым главным убийцей продуктивности для разработчиков? Их много, но один выделяется из общего числа, и его часто недооценивают. Каждый раз, когда вы отправляете кому-то “коротенькое” сообщение через Slack, это стоит этому человеку 23 минуты продуктивной работы, и это лишь часть проблемы.Я работаю с командами разработчиков больше десяти лет, и мы постоянно недооцениваем разрушительный эффект таких прерываний. Эта статья рассказывает, почему переключение контекста обходится в такую кругленькую сумму и как им эффективно управлять. Итак, ныряем.Что такое переключение контекста?Вы когда-нибудь замечали, что ваш разум испытывает замешательство при переключении с чтения электронного письма на написание кода? Это переключение контекста. Наш мозг не похож на компьютер, который может мгновенно загрузить новую программу. Вместо этого, он должен очиститься от одного набора мыслей и потом загрузить другой. И этот ментальный перерасход ресурсов накапливается быстрее, чем вы думаете. Представьте себе, что вы глубоко погрузились в проект. Ваш телефон звонит, вы проверяете сообщение, потом понимаете, что вам надо ответить на письмо. И еще раньше, чем вы успеваете это осознать, вы возвращаетесь к проекту. Каждая перемена занятия заставляет ваш мозг менять фокус, продираясь через дебри вашей памяти, чтобы понять, где вы были до этого. Эта дополнительная обработка информации может убить ваш настрой на работу и создать ошибки. Термин “переключение контекста” позаимствован из операционных систем (ОС). ОС могут управлять несколькими процессами в одном обрабатывающем модуле (обычно это центральный универсальный процессор, ЦПУ), поставив один из процессов на паузу и работая с другим. Эти системы могут переключать контекст, тогда как наш мозг не может. Почему мы переключаем контекст? Мы переключаемся между задачами чаще, чем сами это осознаем, потому что наши инструменты, такие как приложения и нотификации, созданы таким образом, чтобы захватывать наше внимание. Нашим мозгам нравится все новое, поэтому они бросаются вперед навстречу любому новому сообщению или обновлению. Работая, мы также чувствуем давление, которое диктует нам, что мы должны ответить немедленно, и это распыляет наш фокус еще сильнее (и принятая на работе культура обычно вознаграждает за это). И вокруг столько информации — электронные письма, чаты, открытые вкладки — и отвлечься так легко.Стоимость переключения контекстаПочему, когда инженеры отвлекаются, это ударяет по ним так сильно? Подумайте о последней программистской задаче, над которой вы работали. Вам наверняка пришлось думать о многих вещах сразу: об архитектуре системы, о специфической задаче, которую вы решали, о потенциальных пограничных случаях и о том, как ваше решение впишется в общую картину. Каждый элемент живет в вашей рабочей памяти, формируя деликатную ментальную модель (и мы можем удержать только до 7 элементов в краткосрочной памяти).Когда нас отвлекают, эта ментальная модель разбивается на куски. Исследование от UC Irvine показывает, что разработчикам необходимо в среднем 23 минуты, чтобы полностью заново выстроить свой фокус после того, как их отвлекли.Но потерянное время — не самая большая из проблем Когда нас отвлекают, мы не просто теряем минуты, качество работы деградирует по многим показателям. Снижение ментальной энергииНаши мозги не рассчитаны на постоянное переключение контекста. Каждое прерывание уменьшает наши когнитивные ресурсы, что напоминает батарейку в телефоне, которая садится гораздо быстрее, когда мы быстро переключаемся между приложениями. Исследование, проведенное Parnin and DeLine обнаружило, что разработчики, которых часто отвлекают, показывают признаки ментальной усталости намного раньше в течение дня, что ведет к большему количеству ошибок в их работе, проделанной во второй половине дня. Такая ментальная усталость может со временем привести к повышенному уровню стресса и даже выгоранию среди разработчиков.Истощение ментальной энергииСтрадает качество кодаИсследование от Amoroso d'Aragona et al. (2023) проверило влияние прерываний и перерывов на разработку программного обеспечения и обнаружило удивительную корреляцию между переключением контекста и деградацией качества кода. Их анализ показал, что:Частые перерывы и отвлечения приводят к большему количеству багов, поскольку разработчикам было трудно восстановить свой когнитивный контекст.Продолжительные перерывы в активности увеличивают технический долг, поскольку разработчику необходимо больше времени, чтобы заново ознакомиться с ранее написанным кодом.Прерванные сессии написания кода коррелируют с более низкой пригодностью кода для поддержки, что приводит к более долгим циклам code review и переделки.Эти числа — не просто статистика, они представляют собой отражение реальных проблем, которые командам приходится исправлять позже, создавая порочный круг технического долга и работы по зачистке проблем. Хороший код и плохой кодЭффект снежного комаКроме того, мы знаем из опыта, что прерывания влияют не только на непосредственно выполняемую в это время работу. Прерывание на 5 минут в момент решения критической проблемы может создать долгую отсрочку завершения задачи. Зачастую разработчикам приходится полностью выстраивать свое понимание задачи заново, прежде чем они смогут продолжить.Пятиминутная встреча с разработчикомПонимание поточного состоянияНам надо поговорить о так называемом “поточном состоянии”, чтобы понять, почему прерывания так плохи Это не просто жаргон программиста — это хорошо исследованное психологическое состояние, которое первым описал Mihaly Csikszentmihalyi в 1990-м году.Поточное состояние — это ментальное состояние, в котором работа кажется не требующей усилий, а время как будто исчезает (ваши способности соответствуют поставленной перед вами задаче). Для разработчиков это тот момент, когда сложные проблемы внезапно становятся понятными, а элегантные решения приходят сами собой. Вы чувствуете беспокойство или расстройство, если стоящий перед вами вызов слишком сложный по сравнению с вашими способностями. Если он слишком простой, вам скучно. Модель от Mihaly Csikszentmihalyi помещает поток посередине, там, где способности и вызов сбалансированы. В этой зоне вам достаточно сложно, чтобы задача представляла интерес, но не настолько, чтобы вы отключились. Чтобы удерживаться в потоке, вам необходимо регулировать сложность задач по мере роста ваших способностей. Это означает поиск новых вызовов, чуть выше вашего текущего уровня. Тогда вы сможете прогрессировать, не чувствуя себя ошеломленным.Но вот что критично: поточное состояние очень хрупкое. Чтобы его достичь, необходимо примерно 15 минут, но одна единственная нотификация может сломать его за секунду. И да, даже если разработчики не отвечают на эту нотификацию, иногда достаточно просто увидеть ее, чтобы нарушить концентрацию. Исследования показывают, что отвлекающие моменты на мониторе дают существенный отрицательный эффект (Yimeng M. et al.). Нотификации с высоким приоритетом (например, срочные запросы от менеджеров) существенно повышают время, потраченное на задачи на понимание кода, которые являются самыми простыми. При этом те случаи, когда программиста отвлекает другой человек, не так уж и плохи: они помогают снизить уровень стресса. 👉 На рисунке внизу показано, что происходит, когда разработчик отвлекается, в соответствии с планом или случайно, и что происходит, когда нас отвлекают каждый день по многу раз. Когда разработчик отвлекается (по мотивам оригинального рисунка на Reddit)Урок от нашей командыПозвольте мне поделиться историей от команды, с которой я работал недавно. Мы отслеживали свои прерывания в течение месяца и нашли паттерн, который может показаться знакомым.Прерывания, которые случались утром, стоили особенно дорого. Один из разработчиков описал, как он потратил два часа, чтобы представить архитектуру новой функциональности у себя в голове, только для того, чтобы его ментальная модель была полностью разрушена “быстрым митингом” для синхронизации. По его оценке ему понадобилось три часа, чтобы вернуться на тот же уровень понимания, который был достигнут до прерывания. Почему? Люди обычно достигают максимума своей энергии и концентрации для выполнения сложных задач утром.Но когда команда реализовала защищенное фокусное время, результаты оказались следующие:Степень выполнения задач выросла на 35%Баг-репорты снизились на 28%Оценки удовлетворенности команды выросли на 45%Приведенный ниже рисунок показывает календарь с отмеченным на нем защищенным фокусным временем для глубокого погружения в работу.Защищенное фокусное время на календареСтратегии по предотвращению переключения контекста Оптимизация вашего рабочего пространства для поточного состояния очень важна для всех, кто работает над задачами, требующими концентрации, креативности или серьезных технических навыков, например, в сфере разработки программного обеспечения. Достижение поточного состояния, также известного как ""быть в зоне"", определяется как оптимальное состояние сознания, в котором человек полностью погружается в свою активность и достигает своей оптимальной производительности. После изучения этой проблемы во многих командах, я нашел несколько подходов, которые всегда работают:Для разработчиков🎯 Ставьте понятные цели: определяйте конкретные цели для каждой рабочей сессии, чтобы удерживать фокус и эффективно отслеживать свой прогресс. Кроме того, имеет смысл ставить ежедневные цели, поскольку они помогают нам уяснять, что мы хотим завершить именно сегодня.📝 Вносите задачи в TODO списки: вы можете использовать любой инструмент, такой как Todoist или Microsoft To Do, чтобы выгрузить туда список задач из вашей головы. Этот подход ослабит давление, и вы всегда сможете вернуться позже, чтобы проверить, над какой следующей задачей вам нужно поработать. Идентифицируйте вашу Самую Важную Задачу в TODO списке и выполняйте ее первой.🔢 Приоритезируйте задачи: используйте метод приоритезации задач, который сделает вашу работу достаточно увлекательной, чтобы предотвратить снижение концентрации. Попробуйте разные техники, чтобы понять, что лучше всего подходит для вашей работы.Матрица приоритезации Эйзенхауэра🔒 Блоки для глубокого погружения в работу работы: помечайте свои 90-минутные временные блоки для сфокусированной работы. Исследования показывают, что это время является оптимальным для поддержания непрерывной концентрации. Нам необходимо как минимум 4-6 часов сосредоточенной работы. Отметьте эти блоки в своих календарях и относитесь к ним серьезно в течение дня, когда у вас больше всего энергии (для большинства людей это утро).Календарь с блоками для глубокого погружения в работу🅿️ Техника парковки: держите открытым простой текстовый файл, пока работаете. Когда у вас появляются случайные мысли или задачи, быстренько запишите их туда, вместо того, чтобы сразу же что-то делать с ними. Это позволит вашему разуму не стать источником отвлекающих моментов.🚧 Непрерывный рабочий процесс: как вы знаете по опыту решения сложных задач, вы можете оставлять себе “хлебные крошки”. Если вас прервут, быстро написанный комментарий о том, почему вы приняли то или иное решение в своем исходном коде в IDE, может сэкономить часы ментальной реконструкции. Вы также можете сделать это в конце дня, и назавтра вы сразу поймете, как начать работу. Этот подход также поможет нам бороться с эффектом Зейгарника (тенденцией занимать нашу рабочую память незаконченными задачами).🔕 Минимизируйте отвлекающие факторы: этот пункт может включать звукоизолирующие наушники, отключение несущественных нотификаций или создание зарезервированного рабочего пространства. Прекрасным способом уменьшить количество отвлекающих факторов является отключение мессенджеров (Slack-а и прочих) на некоторое время и включение режима “Не беспокоить” на телефоне.🤹 Не работайте в многозадачном режиме: наш мозг может корректно фокусироваться только на чем-то одном. Попробуйте фокусироваться только на одной задаче за раз. 💺 Эргономика: убедитесь в том, что ваше кресло, стол и монитор являются эргономичными, чтобы минимизировать физический дискомфорт.📂 Организованное окружение: ничем не захламленное рабочее пространство снижает когнитивную нагрузку, помогая сосредоточиться на задаче. Незахламленное рабочее пространство🛠 Используйте правильные инструменты: правильно подобранные инструменты и программное обеспечение могут оказать серьезное влияние на ситуацию. Используйте быстрый, надежный компьютер и программное обеспечение, которое упрощает вашу задачу.⏰ Установите повторяющийся ритуал: нашим мозгам это нравится. Начинайте день с определенного ритуала, и он подаст мозгу сигнал, что пора входить в зону 🏖 Регулярные перерывы: перерывы по расписанию, такие, как, например, техника Pomodoro, могут помочь поддерживать высокую концентрацию и предотвратить выгорание. Pomodoro таймер🤔 Устанавливайте свой уровень реагирования: если вы работаете в офисе, где высоко ценят вашу доступность, проверьте свой уровень реагирования на обращения. Как написали авторы книги “Алгоритмы жизни”, ""Будьте отзывчивы ровно настолько и не более.""🚶‍♂️Отдыхайте: невозможно долго работать без перерывов, поскольку это может навредить нам. Попробуйте делать короткие перерывы, меняйте свое местоположение, сделайте себе кофе, а обеденный перерыв используйте только под обед.Если мы записываем содержимое нашей рабочей памяти, восстановление фокуса происходит намного быстрее Для команд🚨 Протоколы прерываний: создайте понятные инструкции по поводу критических ситуаций, которые требуют немедленного прерывания. Все остальное должно идти через асинхронные каналы.🗓 Соглашения по фокусному времени: установите фокусные часы для всей команды, когда прерывания запрещены, за исключением действительно аварийных ситуаций. Одна из опций состоит в том, чтобы заблокировать несколько дней в неделю (во второй половине дня) для сфокусированной работы. Вы также можете фокусироваться на индивидуальной работе без митингов один или два дня в неделю (дни без митингов). Например, мы не проводим митингов по средам.Командное соглашение📨 Асинхронные коммуникации должны по умолчанию означать удаленную (много документации, мало митингов) культуру работы. Если вы работаете удаленно, вы должны тратить больше времени для написания и использовать инструменты, которые могут помочь вам сохранять там важные переговоры, чтобы было проще догнать остальных. Вы можете также использовать инструменты, подобные отправке по расписанию в Slack, чтобы ваши коллеги не могли отвлечь вас в ваше фокусное время. Кроме того, когда вы посылаете что-то в середине дня, вы можете сказать, “Нет необходимости отвечать сразу, если вы работаете над чем-то.”⏰ Гибкие рабочие часы: предложите людям гибкие рабочие часы, чтобы можно было подстраиваться под индивидуальные фокусные времена. 🙅Говорить ""нет"" — это норма. Поощряйте в сотрудниках привычку говорить, когда они перегружены выше своих возможностей. Благодарите их за то, что они поделились плохими новостями, чтобы потом не возникло худших проблем.  🤝 Говорите на митингах по существу (да, это возможно). Разрешайте сотрудникам пропускать митинги, где нет понятной повестки или отсутствует причина для их присутствия. Это заставит организаторов уважать чужое время и позволит сотрудникам сосредоточиться на работе с высоким приоритетом. Кроме того, ставьте митинги в расписание таким образом, чтобы они оказывались рядом с естественными перерывами в работе (до или после обеда, начало или конец рабочего дня).Измерение прогрессаЧтобы понять, работает ли ваше управление прерываниями, отслеживайте эти метрики:🔔 Незапланированные прерывания. Отслеживайте, сколько их случается в день.⏳ Длительность непрерывных сессий написания кода. Следите за тем, чтобы этот параметр со временем рос. 🐞 Метрики качества кода. Следите за количеством багов и обратной связью по code review.😊 Оценки удовлетворенности команды. Регулярно проводите исследования в команде.🏃‍♂️ Тренды по скорости спринтов. Посмотрите, быстрее ли вы движетесь вперед после снижения числа отвлекающих факторов.Эти метрики дадут вам некую часть от целой картины, показывающей, как эти изменения увеличили производительность разработчиков. Например, во фреймворке SPACE они напрямую увеличивают такие параметры, как Satisfaction (Удовлетворение) и Well-being (благополучие), а в DORA они влияют на параметр Lead Time for Changes (Время достижения перемен), поскольку частые прерывания работы замедляют разработку и снижают настрой на работу.Измерение прогрессаДвижение впередПрерывания работы в разработке программного обеспечения не просто раздражают — они являются безжалостными убийцами продуктивности, влияющими на качество кода, настрой команды на работу и сроки выполнения проекта. Однако, если правильно понимать проблемы и использовать правильные инструменты, ими можно эффективно управлять. Помните, что:Переключение контекста отнимает у человека больше, чем несколько минут. Оно нарушает фокусировку и снижает качество кода.Поточное состояние повышает продуктивность, но оно очень хрупкое.Ограничьте прерывания путем установления фокусного времени, использования асинхронных коммуникаций и отслеживания метрик. Итак, начните с внедрения одного или двух советов. Измерьте результаты. Проведите тонкую настройку, основываясь на том, что работает для вашей команды. Цель состоит не в том, чтобы полностью устранить прерывания, а в том, чтобы те прерывания, которые все-таки происходят, стоили того. Защитите ваше фокусное время; ваш код (и команда) скажут вам спасибо.Присоединяйтесь к русскоязычному сообществу разработчиков на Spring Boot в телеграм — Spring АйО, чтобы быть в курсе последних новостей из мира разработки на Spring Boot и всего, что с ним связано."
59,59,"Требования, еще требования, а какое стоп-слово? Работа системного аналитика с требованиями на разных этапах проекта",Ростелеком,Крупнейший провайдер цифровых услуг и решений,115.98,"Программное обеспечение, Связь и телекоммуникации, Веб-сервисы",2025-03-12,"Привет, дорогой читатель! Наливай еще кружку кофе, бери порцию любимых печенек, ведь уменя для тебя есть очень интересная история. Меня зовут Маша. Да, как‑то даже во взрослом возрасте все меня так называют. Работаю в IT или около IT сфере еще со студенческих времен. Чего только не насмотрелась… но сегодня не об этом. Сейчас занимаю должность системного аналитика в Ростелеком Информационные Технологии. В этой статье я расскажу как непросто приходится системному аналитику, если он и бизнес‑аналитик, и немного менеджер, и на дуде игрец и как с этим справляться. И не просто справляться, а чтобы разработчик от злости не разбил клавиатуру, тестировщик не перешел на антидепрессанты, а заказчик в итоге был счастлив. Наша команда занимается развитием и поддержкой внутреннего продукта, то есть заказчики — наши коллеги, поэтому делаем с любовью, как для себя. Любая доработка выполняется по методологии Agile, и пока она дойдет до этапа системного анализа требования могут круто поменяться. История начинается…Начиная с системного анализа, доработка проходит три больших этапа, как в сказке: системный анализ, разработка и тестирование.Был замечательный солнечный рабочий день. Мне присылают новую задачу на доработку по проекту для написания технического задания. В описании есть требования заказчика к бизнес‑процессу, логике и оформлению доработки. Начинаю разбираться, проводить обычные встречи для уточнения требований. Так начинается первый этап — системный анализ. На данном этапе я разрабатываю документ, в котором описываю постановку задачи, схему бизнес‑процесса, пользовательские сценарии и технические детали для разработчика. В зависимости от четкости требований процесс может занимать от пары недель, до нескольких месяцев. Затем отсылаю документ на экспертную проверку системному архитектору, который проверяет корректность интеграций и влияние на смежные доработки. И только после одобрения я отправляю техническое задание заказчику на согласование. Если есть замечания или корректировки, то самое время о них сказать.Процесс согласования зависит в большей степени от заказчика. Кто‑то очень внимательно изучает документ и присылает правки, а кто‑то подписывает не глядя. В данном случае замечаний не последовало.Затем наступает долгожданный момент — заказчик подписывает согласованное техническое задание кровью. Через некоторое время после планирования спринта я очень довольная иду осчастливливать разработчика новой головоломкой. Так начинается второй этап — разработка.Проходит пара месяцев, и тут заказчик присылает письмо…Дорогой системный аналитик!Мы бы очень хотели добавить пару условий в бизнес‑процесс, изменить названия статусов заявок и сделать кнопку «Скачать» с другой стороны. Примите, пожалуйста, в разработку.С уважением,ЗаказчикЧто делать? Куда бежать? Паника!Отставить панику! Глубокий вдох, глубокий выдох и начинаем думать. В целом, получение правок на этапе разработки неприятное, но относительно частое явление. Оно обуславливается тем, что заказчик после согласования отвлекся от доработки, а потом смотрит на нее свежим взглядом и понимает: «Для полного счастья мне не хватает еще вот этой фичи».Задача на этапе разработки, а у нас новые требования. Бежим к разработчику и отзываем задачу обратно на этап системного анализа? И снова адские круги согласования? Нет, нет, нет… Надо внимательно посмотреть на требования. Насколько они критичны для реализации? Ага, условия — это системная логика, ничего, поправим. Только это самое объемное требование, поскольку для его реализации придется дорабатывать не один элемент системы. Названия статусов — это бизнес‑логика, поэтому надо согласовать со всеми заинтересованными коллегами. Кнопка, а что кнопка? Разработчик точно до нее еще не добрался. Решение по правкам зачастую принимается системным аналитиком самостоятельно, если они не затрагивают другие системы или проекты.Уточняем все необходимые нюансы. Например, формулировку для названия статусов, требуется ли передавать эти статусы в другие системы для отчетности, права просмотра и редактирования для пользователей. Правим техническое задание: добавляем вышеперечисленные уточнения в терминах разработки, изменения по традиции выделяем желтым цветом и отправляем по почте на согласование с заказчиком в ускоренном порядке. Ура! Согласовано! Поскольку правки срочные, все ждут доработку, то и согласование прошло быстро. Осталось только обрадовать разработчика, но он человек опытный, поймет, простит и сделает. Главное как можно четче и подробнее описать необходимые изменения с точки зрения системы.Проходит время, доработка готова, можно смотреть. Затаив дыхание в базе разработчика проверяю всю функциональность. На первый взгляд все в порядке. Отдаем задачу в тестирование. Так начинается третий этап — тестирование.И тут заказчик присылает письмо… Нет, рано. Тестировщик ответственно проверяет, находит баги, мы их обсуждаем, разработчик правит, всё как обычно. Вот теперь заказчик присылает письмо…Дорогой системный аналитик!Мы подумали и решили, что хотим еще добавить условия в бизнес-процесс. Старые убрать, но не все. А еще нужны поля...С уважением,ЗаказчикТочно паника, потому что уже назначена дата показа доработки, буквально через пару дней. К тому же договоренность вывести все на продуктив в ближайший релиз через две недели. Мы просто не успеем! И да, сесть и подумать уже не спасет ситуацию. Всё очевидно. Какое стоп‑слово? Как остановить нескончаемый поток требований? Тем более практически в последний момент? Правильно, организовать встречу с заказчиком, на которой также соберутся и стейкхолдеры. Стоп‑слово оказалось простым и состояло всего из трех букв, но не тех, которые обычно пишут на заборах. Это слово — «Нет». Разве можно говорить такое заказчику? Как можно отказать? А вот так. Просто и с обоснованием, что мы не волшебники, а только учимся, поэтому последние дополнительные требования можем реализовать либо в другой доработке, либо со сдвигом сроков по текущей. То есть не просто отказать, а предложить варианты выхода из ситуации.Коллеги немного поспорили, но согласились на первый вариант. Уж очень хотелось увидеть что получилось. А мы начали готовиться к новой доработке и новым требованиям.Заветное стоп-словоВот такая вот вышла историяКлавиатуры целы, антидепрессанты не тронуты, заказчик доволен. Конечно, этапы работы над задачей укрупнены, но мораль сего рассказа такова: Не поддавайся эмоциям — обычно в стрессовых ситуациях очень сильно хочется все отрицать, хотя это и не всегда логично. Думай — пословица «семь раз отмерь, один раз отрежь» в полной мере раскрывает пункт про размышления.Разговаривай и обосновывай свои решения — когда ты сам все понял и взвесил, необходимо донести эту информацию до заказчика. А чтобы он еще и понял, то пару аргументов в рукаве станут козырными в решении вопросов.А какое стоп‑слово в вашей команде? Делитесь в комментариях."
60,60,Почему мы любим «Звёздные войны»?,Сбер,"Технологии, меняющие мир",444.24,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-03-12,"Шансы выразить коллективную любовь поколений поклонников «Звёздных войн» для нефанатов составляют один к миллиону, но кто-то же должен попытаться. Хочется сформулировать, почему оригинальные фильмы о непутёвом отце, действительно странных отношениях брата и сестры и одном колоссальном лазере так много значат для стольких людей.Объяснение в любви Первая группа поклонников «Звёздных войн» — это те, кто смотрел фильм в тот период своей жизни, когда мир только начинал обретать смысл: то есть детьми, которым эта франшиза зашла изначально. Они, можно сказать, выросли на «Звёздных войнах», а их лучшими наставниками стали такие персонажи, как мудрый старый чудак по имени Оби‑Ван Кеноби, уникальное, причудливо изъясняющееся существо по имени Йода, и страшный «кащей» — Дарт Вейдер. И всё же, несмотря на мгновенную любовь, большинство людей в детстве, знакомясь с фильмом, не сразу готовы были понять глубокий символизм Джорджа Лукаса в отношении Империи и её правителя‑демагога, или того, как заимствование многовекового архетипа «путешествия героя» превратило Люка в аутсайдера. Вообще, даже сейчас — или уже тем более сейчас — мало кто может уложить в голове концепции вымышленного мира как зеркало общества, которое само почти (или не почти, зависит от точки зрения) поддалось тёмной стороне в 1943 году. И, конечно же, в детстве мы также ещё не могли оценить масштаб сюжетной линии и дальновидности, необходимой для её создания, заставившей хороших парней проиграть в фильме «Империя наносит ответный удар», и насколько это сделало поворот в конце «Возвращения джедая» ещё более сладким.Тогда мы были просто глупенькими детишками и понимали лишь одно — световые мечи выглядят круто! Тогда нам казалось, что Джордж Лукас просто жаждет рассказать историю, передающую дух старых сказок о рыцарях и рыцарстве, магии и девицах в беде. Даже сейчас, пройдя уже немалый жизненный путь и получив ценный опыт, невозможно смотреть на «Звёздные войны» как на какое‑то суперсложное и зрелое произведение. Но при этом оно захватывает сердца и умы многих людей, не только тех, кто вырос в восьмидесятых и девяностых годах, но и новых поколений.Если говорить о самой простой причине, по которой «Звёздные войны» настолько популярны — то ответом, скорее, станет вселенная. Мир, к которому прикасалось столько гениальных творческих рук, содержит классные лазерные пушки, космические корабли, секретные организации и монстров, одновременно пугающих, раздражающих и развлекающих в равной степени. Это звучит как слабая аргументация, но с точки зрения чистой эстетики — почему бы её не использовать?Важно отметить, что если вы сейчас посмотрите «Звёздные войны» впервые, то нет смысла увлекаться мелкими деталями. Порой да, диалоги оставляют желать лучшего или очень наивные, а Император Палпатин вообще непонятно каким образом поднялся на вершину политической лестницы настолько быстро, когда вокруг существуют столько офигенных джедаев. Также не стоит заморачиваться над вопросом, действительно ли Ки'ра влюблена в Хана, или она просто пешка, да и многое другое стоит принять таким, какое оно есть. Лучше уделите больше времени на изучение разнообразных миров, вот они‑то никогда не кажутся недоразвитыми. И в действительности это так: каждая планета в оригинальной трилогии своеобразна и проработана так, что даже придраться не к чему. Посмотрите на города, у которых действительно есть история, и на людей… э‑э, инопланетян… у которых есть своя культура, амбиции и недостатки. Внимательно посмотрите, так как в других фильмах, особенно тех времён, этого попросту не найти. Запоминающиеся персонажи и душераздирающий сюжетТрудно говорить о персонажах и сюжетах фильмов, не испортив при этом ничего из того, что делает их такими особенными. Без спойлеров вообще невозможно рассказать, что случилось с Люком и почему он так зол на двухметрового полукиборга, или почему Лиам Нисон продержался только один фильм, прежде чем его вынудили уйти из франшизы.Ну, кое‑что рассказать всё‑таки можно: оригинальная трилогия (Эпизод 4, 5 и 6, вот именно с такой нумерации началась история «Звёздных войн») повествует о жизни маленького одинокого юноши, Люка, который, в принципе, ничем особым не выделяется, живёт в глуши и всё детство мечтает покинуть свою деревню планету. А в течение следующих двух фильмов он борется за принятие решений, действительно борется, а не просто делает выбор. При этом он часто терпит неудачи с ужасающими последствиями. Такой себе главный герой, если честно.Следующая трилогия (Эпизод 1, 2 и 3), которая начинается со «Скрытой угрозы» знакомит с двумя джедаями — Оби‑Ваном Кеноби и Квай‑Гоном Джинном, — а также с мальчиком по имени Энакин Скайуокер. Есть один забавный персонаж по имени Джа‑Джа Бинкс, но его можно просто игнорировать, как делают некоторые. Хотя и он сыграл свою роль в мире.В этой трилогии огромное внимание привлекает принцесса Падме, которая, как считают многие фанаты, является действительно недооценённым персонажем. Падме, конечно (хотя почему «конечно», может даже «возможно», ниже объясним почему), и не джедай, но у неё есть сострадание и сочувствие к другим, что делает её очень благородной. Она также играет важную роль в политике мира «Звёздных войн». Да, отношения Падме и Энакина глубоко проблемны, а их диалоги банальны и избиты, но Натали Портман всё равно удаётся представить нам персонажа, который кажется очень реальным. Её мольбы к Энакину в конце «Мести ситхов» действительно душераздирающие, и она сохраняет в себе сострадание и сочувствие к другим до самого конца.В то время как более поздняя трилогия повествует о героизме и преодолении трудностей, первая рассказывает об искушении власти и о том, на что мы готовы пойти, чтобы защитить тех, кого любим. При этом каждый фильм важен для мира «Звёздных войн», и это даже несмотря на то, что любой фанат хоть что‑то да хотел бы изменить по‑своему, будь то существование мидихлориан, или выстрелил ли Гридо первым или вторым.Позже третья трилогия сиквелов стала неким подтверждением того, что утверждают создатели вселенной «Звёздных войн»: Силой может обладать каждый, каждый может быть важен, каждый может что‑то изменить, если очень‑очень постарается и найдёт поддержку. Это ответ на вопрос, почему Падме «возможно» не джедай.Вэтой трилогии запоминается Рей — тот персонаж, который дорог нам своей настойчивостью, силой воли, верой в способность находить добро в других. Она свирепая и сильная, но всё же жаждет человеческих отношений, её любовь к Финну, BB8 и Лее — одно из лучших качеств.В связи с этим вырисовывается вторая причина, после эстетики, по которой большинство из нас любят «Звёздные войны», и заключается она в том, что это продолжающаяся сага. Персонажи меняются, и меняются вместе со зрителями, или, вернее даже сказать, с поколениями. Некоторых фанатов, конечно, это может расстроить. Но не стоит из‑за этого превращать франшизу, которая достаточно всесторонне рассказывает о надежде и героизме, в нишевый опыт, обслуживающий лишь одну культурную точку зрения. Всё новое — это мусор?Скорее всего, сейчас у кого‑то создалось впечатление, что раз больше было сказано об оригинальной трилогии, то новые фильмы не особо хороши, но это ошибочное мнение. Такие фильмы, как, например, «Хан Соло», «Изгой‑один» и другие тоже радуют, как и всё, что было до них. Это не значит что они не лишены слабых мест. Как и в других историях из мира «Звёздных войн» в них есть некоторые проблемы, но эти фильмы всё равно дают ценные уроки героизма. Да даже те части, которые можно назвать, мягко говоря, «не очень», например, про спасение Финна Роуза, необходимы с точки зрения сюжетной линии. Ведь жертвенность на войне — ключевая тема в «Звёздных войнах».И конечно, не обязательно следовать строгому правилу фанатов: франшизу «Звёздные войны», по сути, можно начинать смотреть с любого фильма. Это приключенческая история. А приключения, как известно, имеют много начал. На остальную часть фандома тоже можно потратить время, чтобы объяснить, почему Эпизод 8 должен был закончиться именно так, почему Эпизод 9 успешен, несмотря на свои недостатки, почему в «Изгой‑один» появились персонажи, которых больше нигде не было (так работает шпионаж), или почему «Соло» — самый самореферентный фильм всех времён, но мы этого делать не будем. Потому что это создает прецедент: фильмы должны обслуживать нас, а не жить по своим собственным идеалам. На этой ноте вырисовывается ещё одна причина, по которой мы любим «Звёздные войны». И заключается она в том, что персонажи способны переосмыслить себя (не без подачи создателя, конечно, но всё‑таки) — как в фильмах и телешоу, так и в комиксах и играх, книгах и всевозможных фанфиках. Персонажи продолжают жить в сердцах и умах тех, кто их любит, дорожит ими, и, в конечном счёте, через людей, которые передают их следующему поколению.Извинения за мидихлорианНаверное нужно немножко объяснить, кто это такие. Предположительно, это крошечные существа, которые живут в наших клетках и отвечают за передачу воли живых существ и Силы. Почему «предположительно»? Потому что даже в мире «Звёздных войн» о них очень мало информации. По большей части, судя по повествованию, они создают симбиотическую связь. Дарт Плэгас и его хозяин Тенебрус посвятили много своего времени изучению мидихлориан и успешно раскрыли несколько интересных секретов. По‑видимому, при правильном возбуждении они могут значительно улучшить здоровье, замедлить старение и сохранить энергию. Плэгас использовал одного из своих коллег‑учеников‑ситхов для экспериментов и успешно убивал и реанимировал чувака много раз, хотя насколько этот чувак был вменяемым и функциональным, ещё предстоит выяснить.Но вернёмся к части с извинениями. Почему оно звучит от мидихлориан? Да потому что между всеми фанатами создана своеобразная симбиотическая связь. Каждый фанат будет жалеть вас из‑за того, что вы не любите «Звёздные войны». Им жаль, что вы не испытываете трепетных чувств по отношению к фильмам Джорджа Лукаса, и такого же любопытства к его вселенной. Им жаль, что вы никогда не испытаете того же волнения от вида Звёздного Разрушителя в «Пробуждении силы». Им жаль, что вы не сможете шутить или понимать культурные отсылки, которые остальные используют сейчас и будут продолжать использовать в будущем. И именно за это они приносят извинения.Ну а если чуть более серьёзно, чему «Звёздные войны» и могут научить, так это тому, что в жизни больше хорошего, чем плохого. Мы живём во вселенной, где есть, по меньшей мере, несколько миллиардов других людей, и хотя некоторые из них могут показаться странными инопланетными существами, мы все в целом хотим одного и того же: мирного существования. Напоследок отметим: не любить что‑то — это нормально. Даже если у этого «чего‑то» есть множество поклонников. В этом нет ничего плохого."
61,61,InnerSource-хакатон: делимся результатами соревнования между командами VK Tech,VK Tech,VK Tech — российский разработчик корпоративного ПО,62.11,"Программное обеспечение, Веб-сервисы, Информационная безопасность",2025-03-12,"Привет, Хабр! В конце 2024 года прошел большой внутренний хакатон для команды разработки VK Tech. Такое мероприятие провели впервые: команды разных продуктов объединили в единое направление RnD всего год назад. И вот теперь, когда мероприятие прошло, а полученный опыт осмыслен и усвоен, мы хотим во всех деталях рассказать о том, как это было. Немного об идее хакатонаВесь прошлый год внутри VK Tech мы развивали инженерную культуру и InnerSource-подход: наши инженеры открывали исходный код друг другу (некоторые репозитории были доступны только внутри продуктовых команд), наращивали использование наработок соседних команд, готовились принимать доработки и изменения в свои репозитории, а также дорабатывать соседние проекты. Примечание: InnerSource — это применение лучших практик из OpenSource внутри корпорации. Впервые об InnerSource написал Tim O’Reilly в далеком 2000-м году в статье Open Source and OpenGL. Не раз писали об этом и на Хабре. Мы тоже поделимся нашим опытом в следующих статьях. Хакатон стал отличным форматом, который позволил нам не только подвести промежуточные итоги проделанной работы, но и донести до команд две ключевые мысли.Во-первых, важно делиться опытом и отдавать в сообщество свои полезные наработки. Главным условием для участия было наличие исходного и финального репозиториев в каталоге InnerSource-проектов (наш общий каталог для шеринга наработок в рамках InnerSource), готовых к переиспользованию и развитию соседними командами. Во-вторых, важно искать возможности как для улучшения рутинных процессов, так и для поиска инновационных идей. Мы дали возможность занести на хакатон абсолютно любую идею: от инструмента для работы, до которого давно не доходили руки, или внутреннего продукта, которого очень не хватало своей или смежным командам, до эксперимента, который не ложился в продуктовый роадмап. Принести идею мог абсолютно любой — от финансиста до разработчика. Таким образом, на этапе сбора мы привлекли 30 идей под разные кейсы и задачи применения — от генераторов, утилит и библиотек до ботов и внутренних мини-продуктов.По итогам отбора до основной конкурсной части хакатона было допущено 9 проектов. Три из них, по мнению жюри, стали победителями. На них и остановимся подробнее.  Генератор синтетических данных на основе метаданных или образца данныхПобедителем хакатона стал проект Максима Титова, Владимира Северова и Даниила Хасанова. Они создали инструмент для генерации синтетических данных на основе метаданных или образца данных. То есть не типичный фейкер для создания простых данных (имя, фамилия, номер телефона), а сервис для создания полноценных, структурированных датасетов в соответствии с определенным форматом.Востребованность подобной разработки обусловлена тем, что, в частности, для тестирования всевозможных высоконагруженных систем нам нужен большой объем данных. Причем зачастую у команды продукта на старте нет тестового датасета, соответствующего будущей боевой нагрузке. При этом Open-Source-генераторов под наши требования нет. Как результат, нам часто приходилось генерировать данные костыльно, «на коленке». Примечание: Алгоритм генерирования данных раньше был переусложнен. Так, для каждой задачи приходилось писать новый скрипт под необходимый формат и для учета всех изначальных условий — это были Lua-скрипты для запуска непосредственно в Tarantool, скрипты для k6 на JavaScript, и даже Golang-приложения. Везде использовался встроенный модуль рандома, и вокруг него хардкодились параметры и конструкции для максимально быстрого решения текущей задачи.Разработка генератора синтетических данных на основе метаданных или образца данных фактически решила нашу проблему.Теперь о деталях:На разработку в рамках хакатона ушло 2 дня и 43 коммита;Delivery реализован на HTTP API + CLI;Реализована генерация разных типов данных: базовые типы (строки, числа), расширенные типы (UUID, datetime), внешние ключи;Вывод реализован в CSV и HTTP API.В процессе разработки генератора активно использовался AI:VK LLM + VK Copilot (внутренний продукт VK) для написания автотестов и генерации кода уже решенных проблем;VK LLM для генерации пресетов логических типов данных;VK LLM API для генерации конфигураций по образцу данных (в планах).На уровне архитектуры генератор разделен на три слоя:Delivery;Use case;Output. Уже сейчас с помощью генератора мы автоматизировали тестирование для Tarantool Column Store вместо написания множества скриптов на Python под работу при разных профилях нагрузки. При этом решение также можно применять для широкого пула других задач. Среди возможных сценариев:тестирование алгоритмов;автоматизация нагрузочного тестирования в рамках нужных пайплайнов;исследовательский анализ данных;генерирование данных для их последующей визуализации;защита чувствительных данных и их подмена на сгенерированные;симуляция атак и не только.Производительность решения, протестированная на разных типах данных и на разном количестве потоков, демонстрирует довольно высокие показатели. Типы колонокЧисло потоковПримерный RPSЧисло811 100 000Число Х285 880 000Число Х384 000 000UUID81 250 000UUID X28625 000Число + UUID81 220 000Число + STR(16)42 000 000 Число + STR(16)83 850 000Число + STR(16)166 250 000Разработанный в рамках хакатона генератор потенциально закрывает потребности сразу несколько групп пользователей.Разработчики. Во время работы над проектами, требующими тестовых данных для проверки функциональности, разработчикам часто нужно быстро генерировать наборы данных в удобном для использования формате. Генератор позволяет подключаться через CLI-интерфейс, задавать нужные параметры генерации и сохранять сгенерированные массивы данных в нужных директориях.Сторонние системы. В отдельных кейсах может требоваться интеграция генератора данных с другими HTTP-сервисами для автоматической загрузки полученных данных для тестирования. Возможность подключения к разработанному генератору через HTTP API позволяет успешно загружать данные в целевые HTTP-сервисы, обеспечивая быстрое получение актуальных тестовых данных.Инженеры по тестированию. Тестировщикам для нагрузочных тестов часто нужно быстро и автоматически генерировать большие объемы тестовых данных в рамках процесса CI/CD. Генератор можно встроить в CI/CD и успешно создавать данные в рамках нужных пайплайнов. Разработка библиотеки для взаимодействия с ботом VK Teams на node.jsВторое призовое место занял проект Александра Шаталова, Ольги Ли, Никиты Боровикова, Дениса Савельева и Виталия Ящука. Они создали SDK, с помощью которого любой разработчик с экспертизой в JavaScript может быстро разработать бот для VK Teams. Этот продукт мы используем внутри VK и поставляем корпоративным клиентам.Востребованность решения обусловлена тем, что, по опросу GitHub, в 2023 году JavaScript занимает первое место среди всех языков программирования, но JavaScript SDK для VK Teams не было (были для Python, Java и Golang). В рамках хакатона всего за два дня продукт был практически полностью упакован:написан SDK, в него встроена работа с VK Copilot API;написана документация;сгенерированы примеры ботов;разработана административная панель для удобной настройки команд для бота.Помимо этого, функционал для взаимодействия с VK Copilot интегрирован в SDK.В архитектуре библиотеки реализован принцип инверсии зависимости, то есть решение представляет собой класс, состоящий из множества модулей. При этом каждый из модулей внутри библиотеки независим и взаимозаменяем без затрагивания основного кода. Разработка библиотеки позволила получить комплексный эффект.Улучшение производительности разработки ботов. С SDK благодаря стандартизации разработка ботов ускоряется, время на подготовку и настройку сокращается до минимума.Расширение аудитории и увеличение вовлеченности. SDK дает возможность легко создавать и масштабировать ботов с использованием популярного языка JavaScript. Это может повысить активность создания и применения ботов в разных сценариях.Повышение качества взаимодействия с пользователями. Инструменты SDK могут включать шаблоны, готовые компоненты и лучшие практики для создания более качественного бота, что влияет на опыт пользователей.Расширение функционала VK Teams. Предоставление функциональности ботов и административной панели вместе с VK Teams позволяет расширить возможности мессенджера и повысить его ценность для конечных пользователей.Библиотека потенциально закрывает потребности двух групп пользователей:JS-разработчики. Разработчики получают пакет с библиотекой, написанной на JavaScript, и документацию по разработке, благодаря чему можно быстрее и качественнее разрабатывать нужных ботов.Аналитики (или другие пользователи без экспертизы в JavaScript). После подготовки бота разработчиками аналитики можно с помощью административной панели добавлять нужные команды или кнопки и проверять их работу в реальном времени. RePlaice OpenAPIВ тройку лидеров также вошел проект команды в составе Александра Яковлева, Дмитрия Будкова и Андрея Бердюгина. В рамках хакатона они доработали часть фичей для инструмента RePlaice.RePlaice — инструмент, который позволяет:автоматически генерировать Mock-серверы на основе OpenAPI-спецификаций, упрощая создание тестовых сред;синхронизировать имитированные данные с актуальными API, устраняя расхождение между ожидаемыми и реальными данными;настраивать ответы через удобный GUI, моделируя различные пользовательские сценарии.RePlaice решает целый пул задач:устраняет необходимость вручную создавать и поддерживать JSON-файлы для Mock;синхронизирует имитированные данные с актуальными OpenAPI-спецификациями, исключая расхождения между Mock и реальными API;позволяет начать разработку клиентских приложений до завершения работы над серверной частью, то есть снижает зависимости от бэкэнд-разработки;генерирует и обновляет Mock-сервер автоматически при изменении API без необходимости перезапуска, чем снижает трудоемкость обновлений;обеспечивает удобный GUI для редактирования Mock-ответов, упрощая моделирование сложных пользовательских сценариев.Конкретно в рамках хакатона для RePlaice были реализованы:графический интерфейс;возможность работы с группой YAML-файлов (раньше инструмент работал только с одним файлом);возможность редактирования Mock-ответов.В процессе доработки команда использовала:Eslint и Prettier для форматирования и отслеживания ошибок;Nest.JS для обеспечения чистоты архитектуры;Git Hooks;TypeScript для типизации;Jest для написания Unit-тестов;VK Copilot для подготовки документации и тестирования.RePlaice прежде всего полезен разработчикам клиентских приложений. Нередко у специалистов уже есть OpenAPI-спецификации, и надо начать разработку фронтовых фич до полной готовности бэкенда. RePlaice решает эту проблему: после подключения RePlaice можно запустить Mock-сервер с реалистичными данными, которые генерируются из OpenAPI-спецификации. В итоге Mock-сервер запускается и функционируют все endpoints, описанные в OpenAPI.Таким образом, решение потенциально дает комплексный профит: производительность разработки растет, качество Mock повышается, использование и поддержка становятся удобнее, ресурсы экономятся. Краткие итоги по результатам InnerSource-хакатона VK TechБлагодаря проведению хакатона мы получили полезный опыт и смогли достичь поставленных целей: расширили каталог InnerSource-проектов, подтвердили преимущества InnerSource-подхода, смогли в игровой форме реализовать те проекты, для которых в рабочем процессе не хватает фокуса. Одновременно опыт проведения InnerSource-хакатона позволил нам сделать несколько важных выводов.В VK Tech много сервисов, на базе которых можно строить более сложные и функциональные решения под различное применение. Причем реализовывать такие проекты можно даже маленькой командой в сжатые сроки. Внедрение InnerSource-подхода в VK Tech позволяет повышать качество, скорость и гибкость разработки. Уже сейчас совместное использование и развитие кодовой базы становится мощным драйвером для прокачки внутреннего стека команд и вывода компании на новый уровень технологической зрелости.Развитие инженерной культуры — приоритетное место приложения усилий. Команды с сильной инженерной культурой гораздо продуктивнее и эффективнее: пишут код более высокого качества, всегда готовы к инновациям и быстрым экспериментам, тестируют гипотезы и учатся на ошибках друг друга, автономны в принятии технических решений, привлекают талантливых инженеров, постоянно себя улучшают и готовы к масштабированию. В этом всем мы убедились на примере команд участников хакатона.При этом проекты, разработанные в рамках InnerSource-хакатона, имеют практическое значение: все они потенциально полезны не только команде VK Tech, но и более широкому инженерному комьюнити. Поэтому со временем все они смогут войти в стек доступных решений или будут представлены в виде свободно распространяемых библиотек. "
62,62,Метод Python numpy.where(): синтаксис и как использовать,Timeweb Cloud,То самое облако,1677.09,Связь и телекоммуникации,2025-03-12,"Метод numpy.where() является одним из наиболее мощных и часто используемых инструментов в библиотеке NumPy для условного выбора элементов из массивов. Он предоставляет гибкие возможности для обработки и анализа больших массивов данных, позволяя заменять традиционные условные операторы if-else и значительно ускоряя выполнение кода. Этот метод позволяет заменить элементы массива, которые удовлетворяют определенному условию, на заданные значения, а остальные оставить неизменными. В отличие от обычных циклов, которые могут замедлять выполнение при работе с большими объемами данных, numpy.where() использует векторизацию, что делает выполнение операций более быстрым и эффективным.Синтаксис метода where()Метод numpy.where() имеет следующий синтаксис:numpy.where(condition[, x, y])Где:condition — условие или массив условий, которые необходимо проверить.x — значения, возвращаемые, если условие True.y — значения, возвращаемые, если условие False.Если аргументы x и y не указаны, метод вернет индексы элементов, удовлетворяющих условию.❯ Основные способы использованияПерейдем к практике.1. Поиск индексов элементовЧасто возникает необходимость определить позиции элементов, удовлетворяющих определенному условию. numpy.where() позволяет легко это сделать.import numpy as np arr = np.array([1, 2, 3, 4, 5]) indices = np.where(arr > 3) print(indices)На примере запуска кода, который помогает нам найти индексы элементов, создается массив [1, 2, 3, 4, 5]. Затем используется функция np.where() для поиска индексов элементов, которые больше 3. На скриншоте примера, в нижней части консоли, мы видим результат (array([3, 4]),), что показывает позиции чисел 4 и 5 в исходном массиве, так как только эти числа удовлетворяют условию arr > 3. В данном примере метод возвращает кортеж с массивом индексов элементов, больших 3.2. Замена элементов по условиюМетод numpy.where() широко используется для условной замены элементов в массиве.import numpy as np arr = np.array([1, 2, 3, 4, 5]) result = np.where(arr > 3, 100, arr) print(result)Запуск кода который отвечает за замену элементов по условию, начинается с создания массива [1, 2, 3, 4, 5]. Затем применяется функция np.where() для поиска элементов, которые больше 3, с дополнительным параметром 100, что позволяет заменить эти элементы на указанное значение. На скриншоте результата в нижней части консоли мы видим массив [1 2 3 100 100], где элементы 4 и 5 были заменены на 100, так как они удовлетворяют условию arr > 3. В данном случае метод np.where() заменяет все элементы, удовлетворяющие условию, на заданное значение.3. Работа с многомерными массивамиМетод numpy.where() также эффективно работает с многомерными массивами.import numpy as np matrix = np.array([[1, 2, 3],                     [4, 5, 6],                     [7, 8, 9]])  result = np.where(matrix % 2 == 0, 'четное', 'нечетное') print(result)Пример создания матрицы [[1, 2, 3], [4, 5, 6], [7, 8, 9]]. Затем применяется функция np.where(), чтобы заменить элементы в зависимости от условия: если число четное (делится на 2 без остатка), оно заменяется на строку 'четное', а если нечетное — на строку 'нечетное'. На скриншоте результата в нижней части консоли мы видим матрицу, где все числа заменены соответствующими строками по заданному условию. В данном примере метод возвращает обновленную матрицу, содержащую строки вместо чисел.4. Применение нескольких условийИспользуя логические операторы, можно применять numpy.where() к более сложным условиям.import numpy as np  arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]) result = np.where((arr > 3) & (arr < 7), arr * 2, arr) print(result)На примере выполнения кода с использованием NumPy создается массив [1, 2, 3, 4, 5, 6, 7, 8, 9]. Затем используется функция np.where() с комбинированным условием: если число больше 3 и меньше 7, оно умножается на 2, в противном случае остается без изменений. На скриншоте примера в нижней части консоли мы видим результат [1 2 3 8 10 12 7 8 9], где числа 4, 5 и 6 были умножены на 2, так как они удовлетворяют условию. В данном примере метод возвращает новый массив с измененными значениями, соответствующими условию.❯ Практические примерыРабота с температурными даннымиПредположим, у нас есть массив температур, и мы хотим классифицировать их как «жарко» или «комфортно».import numpy as np  temperatures = np.array([23, 25, 28, 32, 35, 29]) status = np.where(temperatures > 30, 'жарко', 'комфортно') print(status)Сначала мы в примере видим создание температур [23, 25, 28, 32, 35, 29]. Затем применяется функция np.where() для определения комфортности температуры: если температура больше 30 градусов, присваивается статус 'жарко', в противном случае — 'комфортно'. На скриншоте результата в нижней части консоли мы видим массив ['комфортно', 'комфортно', 'комфортно', 'жарко', 'жарко', 'комфортно'], где температуры 32 и 35 градусов помечены как 'жарко', так как они превышают пороговое значение. В данном примере метод возвращает новый массив, содержащий строковые значения, отражающие оценку температуры.Обработка пропущенных значенийВ наборах данных часто встречаются пропуски, которые необходимо заменить или обработать.import numpy as np data = np.array([1, np.nan, 3, np.nan, 5]) cleaned_data = np.where(np.isnan(data), 0, data) print(cleaned_data)Создание массива с пропущенными значениями [1, np.nan, 3, np.nan, 5]. Затем применяется функция np.where() в сочетании с np.isnan() для замены отсутствующих значений (NaN) на 0. На скриншоте примера в нижней части консоли мы видим результат [1, 0, 3, 0, 5], где значения NaN были заменены на 0, а остальные элементы остались без изменений. В этом примере метод используется для очистки данных от пропущенных значений.❯ Сравнение методовХарактеристикаnumpy.where()ЦиклыList ComprehensionСкоростьВысокаяНизкаяСредняяПотребление памятиСреднееВысокоеСреднееЧитаемостьВысокаяСредняяВысокаяВекторизацияДаНетЧастичноГибкостьВысокаяВысокаяВысокаяИз таблицы видно, что numpy.where() превосходит традиционные циклы и списковые включения по скорости и эффективности использования памяти, сохраняя при этом высокую читаемость и гибкость.❯ ЗаключениеМетод numpy.where() является незаменимым инструментом для эффективной обработки и анализа данных в Python. Его использование позволяет разработчикам писать более производительный, чистый и понятный код, особенно при работе с большими массивами данных и сложными условиями. Этот метод упрощает задачи, связанные с заменой элементов массива в зависимости от заданных условий, и устраняет необходимость в громоздких циклах и проверках, что делает код более компактным и быстрым.numpy.where() особенно полезен для работы с большими массивами данных, где важна высокая производительность и требуется выполнение простых условных операций. Для сложной логики обработки данных или пошагового выполнения операций циклы остаются более подходящим выбором, особенно при работе с небольшими объемами данных. В свою очередь, List Comprehension хорошо подходит для компактного и читаемого кода при работе с небольшими и средними наборами данных, если операции не слишком сложны.Понимание синтаксиса и возможностей numpy.where() открывает новые подходы для решения различных задач в таких областях, как анализ данных, обработка изображений и финансовый анализ. Метод позволяет эффективно работать с большими объемами данных и значительно ускоряет выполнение операций за счет векторизации, что особенно важно для задач, требующих высокой производительности.Использование таких подходов, как векторизация и маски, в сочетании с функциями библиотеки NumPy, помогает разработчикам оптимизировать код и достигать быстрых и точных результатов. Независимо от уровня вашего опыта в программировании на Python, освоение numpy.where() и понимание его преимуществ станет важным шагом для более эффективной работы с данными, повышения производительности программ и реализации оптимальных решений в аналитике и обработке информации.Автор текста: Павел КоролевНовости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩Опробовать ↩Перед оплатой в разделе «Бонусы и промокоды» в панели управления активируйте промокод и получите кэшбэк на баланс.📚 Читайте также:➤ Полезные советы для парсинга данных веб-сайтов➤ Dunder-методы в Python: предназначение и применение➤ Словари в Python: обзор и как пользоваться➤ Правильный путь создания python-библиотеки: от создания до публикации➤ Что делать, когда хочется взломать пароль, крякнуть сенсор и попробовать Python?"
63,63,Моих знакомых массово сокращают из IT: во всём виноват искусственный интеллект?,Minervasoft,Платформа для управления знаниями и обучением,305.09,"Программное обеспечение, Поисковые технологии, Веб-сервисы",2025-03-12,"Я думала, что знаю, как работает IT-сфера, но оказалось, за последние полтора года изменилось всё. Теперь компании, которые раздули штат во время ковида, массово сокращают айтишников. Одни объясняют это оптимизацией, другие прямо говорят, что заменяют людей искусственным интеллектом. Решила выяснить, какие специалисты под ударом, правда ли ИИ дешевле людей и как вообще адаптироваться к новой реальности. Статью для Minervasoft написала фронтенд-разработчик. В своем исследовании она опиралась на официальные новости, статистику, мнения людей с форумов и свой профессиональный опыт.ИИ заменяет рутину, снижает риск ошибок из-за человеческого фактора и экономит бюджеты — в теории.По крайней мере, так говорят зарубежные компании, когда называют массовые сокращения айтишников оптимизацией. Сценарий стандартный: бизнес автоматизирует процессы, алгоритмы берут на себя часть задач, а 300–400 мидлов остаются без работы.В России пока никто не заявляет, что увольняет сотрудников из-за ИИ. Компании продолжают наём, рынок выглядит стабильным. Но нейросети становятся умнее, автоматизация затрагивает всё больше процессов, и айтишники уже задумываются, не придёт ли эта волна и к нам.Разбираемся, виноват ли ИИ в увольнениях в России, к чему это приведёт и как бизнесу и айтишникам адаптироваться к новой реальности.Что случилось с рынком: цифры, которые пугают2024 год стал точкой невозврата для IT-индустрии. Компании, зарубежные и российские, которые ещё вчера нанимали разработчиков сотнями, сегодня объявляют о сокращениях.CNews написали, что некоторые отечественные компании хотят сократить как минимум 30% айтишников, а в отдельных случаях — до половины штата. Про ИИ в заявлениях ни слова, но люди всё равно переживают. В Твиттере, на Пикабу и Хабре всё больше обсуждают, как быстро нас всех заменит ИИ и кто будет первым: фронтендеры, копирайтеры или техподдержка?В США ситуация тоже непростая: доля безработных айтишников выросла с 3,9 до 5,7% всего за один месяц. Но в отличие от наших компаний западный бизнес иногда признаётся, что сократил сотни человек из-за ИИ. В начале этого года IBM начала мягкие увольнения в области финансов через требования переехать ближе к хабам компании. Параллельно компания внедряет ИИ-технологии в операционку и заменяет ими реальных людей. В PayPal уволили около 9% сотрудников, а за несколько дней до этого запустили проект с ИИ. Не выглядит как совпадение.На фоне этого акции Meta (признана в РФ экстремистской организацией и запрещена), Google и NVIDIA бьют рекорды — инвесторы надеются на «революцию ИИ».Лирическое отступление: что думают людиМнения разделились. Некоторые очень сомневаются в том, что ИИ — соперник человека. Скорее это очень сообразительный помощник, за которым всё равно нужно следить.Превосходство ИИ над человеком переоцененоДругое мнение — скоро нас всех уволят из-за ИИ, потому что нейросети уже могут создать веб-приложение, настроить сервер и помочь подключить Wi-Fi.Автор Хабра считает, что золотой век программистов оконченПо поводу увольнений есть ещё третье мнение: причина сокращений в том, чтобы затем нанять тех же специалистов, но уже на меньшие деньги. То есть, простыми словами, снизить зарплатную вилку. Почему компании увольняют IT-специалистовВо время ковида рынок IT-специалистов резко вырос: компании массово нанимали людей, чтобы поддерживать цифровизацию, онлайн-сервисы и удалённую работу. Сейчас спрос стабилизировался, и перенасыщение рынка привело к закономерным увольнениям. Но это одна из причин.Хоть многие крупные российские бизнесы не говорят прямо о том, что увольняют сотрудников, в частности, из-за ИИ, эта идея витает в воздухе. Некоторые уже заменили часть команды на ИИ:История про сокращение штата копирайтеровКейс российской компании, которая уволила и заменила нейросетью некогда ценного сотрудникаА ещё некоторые компании сокращают людей не для того, чтобы заменить их искусственным интеллектом, а для того, чтобы освободить больше денег на специалистов по нейросетям.Порассуждаем, какую выгоду ИИ приносит бизнесу.Помогает с рутинойCopilot помогает писать код и проверяет его на ошибки, ChatGPT генерирует сценарии для тестирования, а Claude вдохновляет на новые идеи для масштабирования приложения. Раньше у разработчиков была IDE и документация, а теперь — целый набор разных нейросеток. И они действительно работают. Anketolog.ru и Mediacom.Expert опросили 1 100 представителей ИТ-сферы. 57% опрошенных считают, что именно автоматизация рутины — главный плюс ИИ. А 53% специалистов назвали ключевым преимуществом нейросетей увеличение продуктивности. Получается, сотрудники с ИИ могут работать быстрее и качественнее, а значит, принесут бизнесу больше пользы. Снижает расходыБизнес ищет способы снизить затраты, и один из них — внедрить ИИ. По данным исследования Стэнфорда, 42% компаний подтвердили, что нейросети помогли сократить расходы. Например, Walmart посчитал, что ИИ на 10% увеличил продажи и на 20% сократил излишек закупок. А исполнительный директор Klarna, шведской финтех-компании, заявила, что их ИИ-ассистент уже может с лёгкостью заменить труд 700 специалистов техподдержки.Причины не только в автоматизации, о которой говорили выше. Компании экономят на том, что ИИ может сократить ошибки и уменьшить долю человеческого вмешательства в работу с клиентами.В некоторых российских онлайн-школах первое касание со студентом поручают ИИ и зовут человека только в том случае, если ассистент не справился. Благодаря этому бизнес может сэкономить деньги и инвестировать их в более важный функционал. Выгодно ли это на самом делеИИ в бизнесе — тренд относительно новый, и его долгосрочные последствия пока неочевидны. Но первые тревожные сигналы уже появились. Попробуем разобраться, чем может обернуться «ИИфикация» труда.(Не)дешёвый кодДжуна заменили подпиской на нейросеть и тем самым сэкономили десятки тысяч рублей. Что дальше: сеньор, помимо своих обязанностей, следит за ИИ, исправляет его ошибки, а когда устаёт, просто переписывает всё сам. Затем перерабатывает, выгорает, просит повышения зарплаты или вообще увольняется. И на месте небольшой выгоды появляется убыток стоимостью в целый процесс найма одного опытного сеньора, которых на российском рынке не так уж и много. Некоторые компании могут искать сеньора, но предлагать зарплату намного ниже рынкаПоэтому опрометчивое увольнение начинающих специалистов может принести не пользу, а убытки. Даже OpenAI пришли к выводу, что самый мощный ИИ пока что не может полностью заменить джуна. Деградация экспертностиВ начале года Microsoft провёл исследование, которое показало: чем больше мы используем ИИ, тем хуже наше критическое мышление. Мы слишком сильно полагаемся на готовые решения, а иногда даже не пытаемся подумать над ними самостоятельно. Некоторым кажется, что ИИ плохо влияет на интеллектуальные способности айтишниковТакая проблема грозит и IT-специалистам, которые из-за работы с готовым кодом могут потерять полезные навыки: проектировать архитектуру приложения, анализировать тонкие места в коде и придумывать более удачные решения для сложного функционала. И в ситуации, где ИИ сгенерирует настолько нерелевантный код, что переписать заново будет быстрее, может не оказаться людей, способных это сделать. Пока это звучит как мрачный сценарий, но для бизнеса риски слишком велики, поэтому не стоит надеяться, что всё обойдётся.Разочарование акционеровИскусственный интеллект — это не только тренд, развлечение и инструмент для работы, но и новое золото для акционеров. Некоторые компании объявляют об оптимизациях, в частности для того, чтобы поднять цену своих акций. Но по итогу бизнес может столкнуться с фактом: нейросети пока что не могут полностью заменить человека ни в чём. Если это не учесть, компания может столкнуться с финансовыми потерями и, возможно, начнёт нанимать уволенных специалистов обратно.На Reddit можно прочитать историю контент-мейкера, которого решили заменить с помощью ChatGPT и нескольких джуниор-копирайтеров. TL;DR: через две недели его попросили вернуться, но парень отказался. И скорее всего, это не разовая акция. Кстати, об акциях. Вопрос окупаемости ИИ волнует не только компании, но и инвесторов.Goldman Sachs в своём отчёте задались логичным вопросом: в ИИ вложили около триллиона долларов — а что на выходе? Пока что главное достижение — разработчикам стало работать проще. Ну и в соцсетях можно посмотреть на Марка Цукерберга, неожиданно заговорившего по-русски.Аналитик Джим Ковелло в этом отчёте предполагает, что эта сумма так никогда и не окупится: нет таких дорогих задач, которые под силу искусственному интеллекту. Прав ли он? Время покажет. ИИ без базы знаний — деньги на ветерДаже самый продвинутый ИИ бесполезен, если «кормить» его устаревшими материалами. Это приведёт к ошибкам и некачественному коду. А дальше — убытки, репутационные риски и ещё раз убытки. Именно такие риски грозили Amazon ещё в 2017 году, когда компания пыталась внедрить ИИ для подбора персонала. Нейросеть училась на данных, в которых было больше мужских кандидатур, и поэтому отдавала предпочтение именно мужчинам. По итогу Amazon от этой технологии отказался, так как получилось не очень этично. Чтобы понять, почему это важно, возьмём утрированный пример. Сценарий маловероятный, но, если вдруг он станет реальностью, последствия будут катастрофическими.Представим медицинскую справочную систему, где врачи ищут информацию о редких заболеваниях и актуальных протоколах лечения. Если в такой базе окажутся устаревшие рекомендации или непроверенные данные, то могут возникнуть ошибки в лечении пациентов, а компания столкнётся с серьёзными юридическими и финансовыми проблемами.Дыры в безопасностиЕсть ещё один риск, который угрожает самой компании, — утечка конфиденциальных данных. Это касается как внутренних документов, так и пользовательской информации. Их потеря может обернуться репутационными ударами, штрафами и судебными разбирательствами.И самое неприятное — утёкшие данные уже не вернуть. Даже если удалить их с одной платформы, копии могут разойтись по интернету, оставаясь в открытом доступе навсегда. К счастью, многие компании не хотят играть в эту лотерею и тщательно следят за безопасностью своих данных. На кону слишком большие деньги.Некоторые команды не используют нейросети, пока их безопасность не доказанаНо не все так осторожны. В 2023 году сотрудники Samsung трижды (!) поделились с ChatGPT исходным кодом компании, из-за чего этот код добавился в базу для обучения нейросети. Это произошло случайно: работники Samsung просто не знали, как именно работает ИИ-чат. Судя по всему, к каким-то критичным последствиям это не привело. Но это скорее везение. Чем компании могут заменить массовые увольненияСокращения неизбежны, потому что ИИ может выполнять простые задачи намного лучше нас. Да и рынку не нужно такое количество IT-специалистов, как раньше. Айтишников сокращают, так как наняли слишком много специалистов в ковидное времяБум диджитализации закончился, и даже Марк Цукерберг постепенно сокращает штат. К концу пандемии в Meta * было 87 000 специалистов. Но уже в 2023 году он уволил 10 тысяч человек, а в начале 2025-го — ещё 5%, примерно 3 600 сотрудников. Тем не менее пример Цукерберга — не призыв к действию. Бизнес может обойтись без массовых сокращений, если продумает долгосрочную стратегию.Переформатировать, а не сокращатьКомпании могут сохранить команду и перераспределить ресурсы:Переобучить сотрудников. Например, IBM в 2020–2023 годах инвестировала 1 млрд долларов в переподготовку своих специалистов для работы с ИИ. Создать новые роли. Вместе с ИИ появились и новые профессии — промпт-инженеры и ИИ-тренеры. Это направление уже осваивают Microsoft, Google и российский «Яндекс».Построить гибридные рабочие модели. Переключите членов команды на задачи, где нужен человеческий опыт: креатив, стратегия, коммуникация с клиентами. А рутину оставьте искусственному интеллекту. Учесть риски и преимущества ИИ. Маленьким компаниям стоит проводить «ИИфикацию» только в том случае, если ИИ действительно принесёт пользу. Потому что есть риск, что вы больше потратите, чем сэкономите. Интеграция нейросетей в бизнес-процессы может стоить недёшево. Внедрять ИИ осмысленноЛюбые изменения стоит делать не рывком, а постепенно. Например:Можно начать с малого, например автоматизировать техподдержку, и проверить, не просядет ли качество. Считать расходы правильно — учитывать не только зарплаты, но и затраты на платформы, ML-инженеров и доработки, которые вносятся в сгенерированный нейросетью результат. Стоит отдавать предпочтение закрытым и безопасным ИИ-моделям, чтобы не сливать данные конкурентам и не подводить своих клиентов, выдав их личную информацию злоумышленникам. Обучать ИИ только на проверенных данных. Это снижает риск ошибок и «галлюцинаций» модели.Если у обычного сотрудника не получается разобраться во внутренней информации, то ИИ тем более не сможет подобрать точный ответ. Чтобы решить эту проблему, бизнес внедряет системы управления знаниями, которые помогают не только команде, но и ИИ. Например, в Minerva Knowledge можно объединить знания всей компании: создать статьи и инструкции, загрузить любые документы и файлы, а потом легко найти их с помощью умного поиска. Поиск учитывает опечатки и неправильную раскладку, а ещё анализирует поведение пользователей и повышает качество ранжированияДля работы в привычных системах можно использовать Minerva Copilot. Это ассистент с генеративным ИИ. Он сам анализирует контекст и подсказывает ответы на основе материалов из базы знаний. Minerva Copilot полностью защищён, работает без интернета и легко встраивается в любую систему компанииУзнать подробнее о продуктах MinervasoftДля айтишников: в какую сторону двигатьсяУже поздно делать вид, что ИИ — это несерьёзно и скоро о нём все забудут. Поэтому в такой ситуации можно приспособиться к изменениям. Дружба с ИИ. Можно использовать ИИ-помощников в работе, но с проверкой их ответов. Автоматизация рутины. Можно поручить ИИ тестирование, документацию, деплой, чтобы тем самым освободить время для задач, где нужна экспертность. Изучение промпт-инжиниринга. Очень важно точно формулировать запросы к ИИ, чтобы он смог дать качественный ответ. Советы можно посмотреть в гайдах от OpenAI. Программист не вымрет — он просто эволюционируетАнализ рынка. Можно изучить вакансии — например, AI-тренера и промпт-инженера. Вдруг вас заинтересует сфера ИИ и нужно будет начинать карьеру с джуновских вакансий.Примеры вакансийА для тех, у кого есть опыт, зарплатная вилка будет выше. Например, для ИИ-разработчика на Python. Если уже пишете на нём, сменить профессию будет проще.Вакансия взята из сервиса «Хабр Карьера»Некоторые веб-разработчики уже всерьёз задумались, не стоит ли перейти в эту область. ML-специалисты получают больше веб-разработчиков, теперь это новый тренд в ИТПодытожим. ИИ — не враг, а инструмент, и те, кто научится им управлять, получат преимущество, так как смогут подкреплять свою экспертность знаниями нейросетей. Бизнесу не стоит экономить на сотрудниках, а стоит начать инвестировать в их переобучение. Разработчикам следует воспринимать ИИ как второго пилота — толкового, но требующего контроля. Что будет дальше — увидим уже очень скоро. А пока что в IT есть место и людям, и нейросетям.А вы как думаете, правда ли, что некоторые компании массово увольняют айтишников из-за ИИ? Поделитесь — может, у вас есть реальные истории, где нейросетка заменила одного из коллег?Кстати, у нас есть свой блог в Telegram. Там выходят другие статьи про спорные вопросы в найме, менеджменте и планировании. Подпишитесь, чтобы не пропустить."
64,64,Взаимодействие с CAD в Lazarus IDE,АСКОН,Разработчик инженерного ПО и ИТ-интегратор,100.86,"Программное обеспечение, Консалтинг и поддержка",2025-03-12,"Продолжая тему работы с 3D-моделями в Lazarus IDE, стоит отдельно рассказать о взаимодействии с CAD-системами. Здесь интерес представляет как построение 2D-графики на чертеже, так и создание или редактирование 3D-моделей.Исторически сложилось так, что я больше всего, как инженер-конструктор, работал с КОМПС-3D, и много проектов делал именно в нем. Я начал работать в КОМПАС еще в 2003 году, когда делал в нем чертежи для курсовых проектов. Переход с версии 5.11 на версию 6 произвел на меня большое впечатление, и с тех пор я вижу, как с каждым обновлением, улучшается работа и расширяется функциональность системы. Наверное, можно сказать, что сегодня КОМПАС-3D — это не просто система, а полноценная платформа с множеством специализированных приложений и решений, предоставляющая широкие возможности как для конструкторов машиностроительного направления, так и для проектировщиков в строительной сфере.Возможности автоматизацииКОМПАС-3D предлагает инструменты не только для инженеров, работающих в системе, но и для разработчиков ПО, позволяя тем самым расширять возможности, путем создания решений для конкретных задач. Программисты могут создавать специализированные модули, которые интегрируются в систему. Например, модули вроде «Валы и механические передачи» или «Механика: пружины» упрощают проектирование типовых узлов. А такие модули, как «APM FEM» и «Kompas-Flow», позволяют взаимодействовать с 3D-моделью и выполнять CAE-расчёты непосредственно в интерфейсе КОМПАС-3D.Пример работы APM-FEMДля автоматизации рутинных задач в КОМПАС-3D доступна библиотека КОМПАС-Макро. Библиотека записывает макросы в процессе работы пользователя с документами КОМПАС. Выполняемые команды посредством языка программирования Python фиксируются в файле. После записи, макросы можно отредактировать и добавить на панель в виде команды.Взаимодействовать с КОМПАС-3D возможно не только с помощью создания модулей, но и из кода отдельного приложения. В своё время, работая инженером-конструктором, я разрабатывал приложения, которые автоматизировали расчёты и создавали эскизы на чертеже, или выполняли пакетную обработку документов КОМПАС, для получения информации из углового штампа чертежа и данных из спецификации.Окно приложения для пакетной обработки файлов КОМПАС с помощью APIСейчас же я чаще работаю над задачами, связанными с интеграцией PLM и CAD, где требуется получать или наоборот сформировать определённую информацию внутри 3D-модели. Собственно, поэтому в статье я хочу представить практические рекомендации по одному из вариантов работы с 3D-моделями.SDK и справочные материалыКогда нужно проверить гипотезу или создать прототип реализации какой-либо функции, я использую Lazarus IDE в связке с библиотеками, входящими в состав SDK КОМПАС-3D. Подробная справка по API включена в SDK (для этого нужно активировать соответствующую опцию при установке). Также справку можно найти на официальном сайте АСКОН в разделе Руководство пользователя KOMPAS-Invisible (API КОМПАС-3D).Кроме того, иногда полезными оказываются статьи в блоге компании на Харбре - АСКОН – Разработчик инженерного ПО и ИТ-интегратор. Например, я иногда обращаюсь к статьям из цикла публикаций - Работа с API КОМПАС-3D → Урок 1 → Основы, написанные Сергеем Норсеевым, автором книги «Разработка приложений под КОМПАС в Delphi». В этой книге подробно раскрывается большинство вопросов работы с API.В сложных и непонятных для меня случаях я иду на форум АСКОН, в раздел Программирование приложений. Чаще всего получается так, что тема уже кем-то исследована и имеется описанное решение.Подключение модулей в проектах Lazarus IDEПоскольку в набор SDK входят и модули именно для Delphi, чтобы подключить их в проект, созданный в Lazarus IDE, необходимо добавить директиву компилятора {$mode delphi} в файлы модулей. Это предотвратит появление ошибки, возникающей из-за совпадения имен переменных с уже объявленными типами.При подключении модулей SDK в проект можно использовать только необходимые, а не весь набор. Чаще всего выбор модулей зависит от версии API, с которой планируется работать.Особенности версий APIСтоит отметить, что у КОМПАС-3D существует несколько версий API. Есть API.5 и API.7, который не является последовательной версией пятерки, а еще есть KsAPI. Все эти версии являются скорее взаимодополняющими инструментами. API.5 остаётся основой для большинства задач автоматизации, тогда как API.7 (и его наследник KsAPI) расширяет возможности, обеспечивая кроссплатформенность и современные подходы к разработке.API.5 ориентирован на базовые операции: создание документов (чертежей, фрагментов, спецификаций), управление компонентами сборки и выполнение стандартных 3D-операций (выдавливание, вращение и т.д.).API.7 дополняет API.5, и предоставляет доступ к более специализированным функциям, таким как работа с параметрическими переменными, событиями интерфейса, а также расширенными настройками системы (например, привязки, форматы листов).KsAPI — это новый кроссплатформенный программный интерфейс (API), разрабатываемый для работы как в операционной системе Windows, так и в Linux, что особенно актуально в свете портирования КОМПАС-3D на платформу Linux. Про KsAPI есть статья на Хабре -Будущее КОМПАС API на LinuxВариант работы с 3D-моделямиПринципиально, с помощью кода можно выполнять построение 2D-графики или формирование 3D-моделей, используя базовые операции. Примером такого подхода может служить Программа построения зубчатых колес по ГОСТ 591-69 присланная на Конкурс асов 3D-моделирования в номинацию Приложения для Компас-3D.Однако в обычной практике создание сложных трёхмерных моделей с использованием кода может быть неудобным и трудоёмким. Даже если модель вариативна, её проще построить с помощью инструментов 3D-моделирования, заложив различные варианты с помощью параметризации. Затем можно вынести ключевые параметры в переменные и сделать их внешними (т.е. изменяемыми через API). Управляя внешними переменными из приложения, можно быстро получать новую 3D-модель нужной конфигурации.Например, описанный выше подход используется в приложении «IOSO», реализующем технологию многомерной нелинейной оптимизации.Приложение IOSOВ процессе работы, для каждой итерации, путем изменения 3D-модели через переменные, получается новая конфигурация, которая далее передается на расчёт. После этого система оценивает результаты, корректирует исходные данные и повторно изменяет 3D-модель. Этот процесс продолжается до достижения оптимального решения или достижения лимита по циклам итерации.Стоит отметить, что переменные могут использоваться не только для определения параметров построения геометрии и управления включением или выключением операций, но и для нанесения значений на тексты с помощью ссылок. Это может применяться как для вставки текста в технические требования, так и для размерных надписей.Дополнительные пользовательские данныеПомимо передачи данных внутрь 3D-модели в виде переменных или свойств, иногда возникает задача хранения дополнительной информации. Например, это могут быть данные, которые не должны быть доступны пользователю, или данные большого объема, для которых переменные или свойства не подходят. В таких случаях удобно использовать пользовательское хранилище, которое позволяет управлять дополнительными данными.Пользовательские хранилища — это механизм, позволяющий сохранять произвольные данные в файлах КОМПАС-3D. Эти данные могут включать метаинформацию, настройки, параметры или любые другие сведения, необходимые для работы взаимодействующего с 3D-моделью приложения. Хранилища могут быть как глобальными (привязанными ко всему документу), так и локальными (привязанными к конкретным объектам модели).ХранилищаВ КОМПАС-3D для работы с хранилищами используется интерфейс IUserDataStoragesMng.  Пользовательские хранилища организованы в виде Коллекций, где внутри каждого хранилища находятся объекты по форме ""ключ-значение"". Ключ — это строка, которая идентифицирует данные, а значение — это произвольные данные, которые могут быть представлены в виде строки, числа, массива байт или другого типа.Схема связи ХранилищПример кода работы с хранилищами в 3D-моделиprogram CAD_UserStorrage;  {$codePage UTF-8}  uses ComObj, Variants, ActiveX, ksTLB, ksApi7, ksConstTLB;  var   KompasApi7: IApplication;   Document3D: IKompasDocument3D;   StoragesMng: IUserDataStoragesMng;   Storages:IUserDataStorages;   Storage:IUserDataStorage;   userData,stName: OleVariant;   comment:widestring;   p:integer;  begin    // Подключение к КОМПАС API 7   KompasApi7:=nil;   if not Assigned(KompasApi7) then     try       // Попытка подключения к запущенному экземпляру       KompasApi7 := GetActiveOleObject('Kompas.Application.7') as IApplication;     except       try        // Попытка создания нового экземпляра        KompasApi7  := CreateOleObject('Kompas.Application.7') as IApplication;       except         Writeln('Не удалось подключиться к Ole объекту');       end;     end;   KompasApi7.Visible:=true;    Document3D := KompasApi7.Documents.Add(ksDocumentPart,true) as IKompasDocument3D;   StoragesMng := Document3D.UserDataStoragesMng;    // Новую коллекцию хранилищ, привязываем к документу, указываем (nil)   Storages:=StoragesMng.Add(nil);   // Внутри коллекции создаем новое хранилище   Storage:=Storages.Add('ST');    // Запись данных в хранилище   TVariantArg(userData).vt:= VT_UINT;   userData:=120398123;   // '' - без пароля, userData - данные, 'unit' - ключ   Storage.AddObject('',userData,'unit');   TVariantArg(userData).vt:= VT_BSTR;   TVariantArg(userData).bstrVal:=StringToOleStr('String line');   Storage.AddObject('',userData,'string');   TVariantArg(userData).vt:= VT_R8;   userData:=12.8;   Storage.AddObject('',userData,'min');   TVariantArg(userData).vt:= VT_R8;   userData:=1209.68;   Storage.AddObject('',userData,'max');    // Чтение данных   TVariantArg(stName).vt:= VT_BSTR;   TVariantArg(stName).bstrVal:=StringToOleStr('ST');   Storage:=Storages.Item[stName];   Writeln('Получено хранилище: '+Storage.Name['']);   TVariantArg(stName).vt:= VT_BSTR;   TVariantArg(stName).bstrVal:=StringToOleStr('unit');   comment:=Storage.GetObject(stName,userData,p);   Writeln('Запись: '+comment+' = '+VarToStr(userData));   TVariantArg(stName).vt:= VT_BSTR;   TVariantArg(stName).bstrVal:=StringToOleStr('string');   comment:=Storage.GetObject(stName,userData,p);   Writeln('Запись: '+comment+' = '+VarToStr(userData));   TVariantArg(stName).bstrVal:=StringToOleStr('min');   comment:=Storage.GetObject(stName,userData,p);   Writeln('Запись: '+comment+' = '+VarToStr(userData));   readln;    // Удаление коллекции   TVariantArg(stName).vt:= VT_BSTR;   TVariantArg(stName).bstrVal:=StringToOleStr('ST');    if Storages.Delete('',stName) then    Writeln('Хранилище удалено')   else    Writeln('Удалить хранилище не удалось');   readln;  end.Есть еще одна особенность хранилищ, которые создаются с привязкой к объекту, например к свойству. При редактировании свойства в интерфейсе КОМПАС-3D данные из хранилища будут удалены.ЗаключениеИспользование переменных и пользовательских хранилищ позволяет организовать один из способов работы с 3D-моделями, при котором может быть сформирована нужная конфигурация геометрии, и сохранены внутри файла КОМПАС специфические данные проекта. В разрабатываемом приложении может быть организована функция открытия проекта из файла КОМПАС, где данные будут читаться из пользовательского хранилища документа. Для защиты данных пользовательские хранилища могут быть защищены паролем.Хотя в статье рассматривался вариант использования Lazarus IDE, стоит отметить, что API КОМПАС-3D доступен и для других языков программирования, таких как C#, C++ и Python."
65,65,Гуглить баги — это нормально. Как AI Debugger освоил этот навык и сам исправляет ошибки,BotHub,"Агрегатор нейросетей: ChatGPT, Claude, Midjourney",467.57,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-03-12,"Автоматический дебаг с помощью языковых моделей уже не новость, и разработчики используют LLM‑модели и среды разработки с интегрированным ИИ, чтобы анализировать код и предлагать исправления. Но что если встроить в этот процесс ещё один мощный инструмент — поиск в интернете?В этой статье мы рассмотрим необычный подход: автоматический дебаггер, который не просто анализирует код с помощью нейросети, но и при необходимости отправляет сформулированные моделью поисковые запросы в интернете. Если модель не уверена в исправлении, она делает то же, что и человек, — ищет похожие ошибки и готовые решения в сети.Код на GitHubЕсли вы хотите посмотреть всю магию в действии, загляните в мой репозиторий GitHub — там хранится полная реализация.Как всё организовано? Вот дерево файлов — оно подскажет, что к чему:AI-Debugger │   ├── default_config.py      # Настройки по умолчанию ├── requirements.txt           # Список зависимостей Python для проекта └── src/     ├── cli/     │   ├── main.py            # Точка входа в интерфейс командной строки     ├── core/     │   ├── debugger.py        # Главная логика отладки с помощью LLM     │   ├── llm.py             # Абстрактный базовый класс для взаимодействия с LLM     │   ├── llm_factory.py     # Фабрика создания экземпляров LLM     │   ├── utils.py           # Утилитарные функции     │   │                      # (например, вычисление различий в коде)     ├── internet/     │   ├── search.py          # Модуль для выполнения интернет-поиска     ├── llms/     │   ├── gemini_llm.py      # Реализация взаимодействия с Google Gemini     │   ├── huggingface_llm.py # Реализация взаимодействия с Hugging Face     │   ├── openai_llm.py      # Реализация взаимодействия с OpenAI     ├── prompts/     │   ├── query_prompts.py   # Шаблоны промтов для поисковых задач     └───┴── system_prompts.py  # Системные промты для LLMОбзор проектаКак выглядит работа этого бесконечного цикла отладки ИИ? Представим процесс визуально.Бесконечный ИИ‑дебаггер начинает с обнаружения потенциальных проблем и проверки наличия ошибок. Если ошибок не обнаружено, процесс завершается. Если ошибка найдена, дебаггер оценивает, требуется ли использование внешней информации. При необходимости он выполняет поиск через такие источники, как Google или Mozilla, извлекает релевантные данные и комбинирует их с внутренними входными данными.Собранные данные передаются в выбранную LLM (например, HuggingFace, OpenAI, Gemini или локальную модель), которая генерирует ответ для устранения ошибки. Система применяет это решение к коду и снова проверяет наличие ошибок. Если проблема сохраняется, процесс повторяется до её устранения.Цикл повторяется до тех пор, пока не будут исправлены все проблемы. Такая схема делает процесс максимально эффективным: внешние данные и искусственный интеллект работают в тандеме.Установка модуляНачнём с установки. Сначала клонируем репозиторий:git clone https://github.com/FareedKhan-dev/ai-debugger.git cd ai-debuggerУстановим все необходимые зависимости этой командой:pip install -r requirements.txtИмпорт библиотекДавайте импортируем все нужные библиотеки:import os  # Для взаимодействия с операционной системой import subprocess  # Позволяет запускать новые процессы import time  # Предоставляет функции для работы со временем import re  # Поддержка операций с регулярными выражениями import transformers  # Библиотека Hugging Face для языковых моделей (LLM) import torch  # Для тензорных вычислений и глубокого обучения import logging  # Инструменты для логирования from scrapling import Fetcher  # Веб-скрейпингПолучение структуры дерева проектаПервое, с чего начинается процесс отладки, — это знакомство с проектом. Нужно показать LLM, что у нас есть, и как это организовано. Для этого создадим функцию, которая генерирует структуру дерева каталогов. Давайте напишем её.def get_dir_tree(start_path, indent=""""):     """"""     Рекурсивно создаёт строковое представление структуры дерева каталогов.      Args:         start_path (str): Путь к начальной директории.         indent (str): Отступ текущего уровня (используется для рекурсивных вызовов).      Returns:         str: Структура дерева каталогов в виде строки.     """"""     # Инициализация пустой строки для хранения структуры дерева каталогов     tree_structure = """"      # Попытка получить список всех элементов в директории     try:         items = os.listdir(start_path)     except PermissionError:         # Обработка ошибки доступа с указанием ограничения         return f""{indent}[Доступ запрещён]\n""      # Перебор всех элементов в текущей директории     for index, item in enumerate(items):         # Формирование полного пути к каждому элементу         item_path = os.path.join(start_path, item)          # Проверка, является ли текущий элемент последним, для корректного оформления дерева         is_last_item = index == len(items) - 1          # Определение префикса для текущего элемента (оформление ветвей дерева)         prefix = ""└── "" if is_last_item else ""├── ""          # Добавление текущего элемента в структуру дерева с соответствующим отступом         tree_structure += f""{indent}{prefix}{item}\n""          # Если текущий элемент — директория, рекурсивно получить её структуру         if os.path.isdir(item_path):             # Корректировка отступа для следующего уровня дерева             new_indent = indent + (""    "" if is_last_item else ""│   "")             tree_structure += get_dir_tree(item_path, new_indent)      return tree_structureФункция get_dir_tree() — наш проводник: она принимает один аргумент, корневую директорию, и возвращает чёткую структуру всех вложенных папок и файлов. Теперь можно легко визуализировать, как устроен проект. Давайте протестируем, вызвав её для тестовой папки. Вот результат:# Указываем директорию, структуру которой хотим отобразить start_directory = ""project_code""  # Получаем структуру дерева для указанной директории directory_tree = get_dir_tree(start_directory)  # Выводим полученную структуру дерева каталогов print(directory_tree)### ВЫВОД ###  project_code ├── requirements.txt ├── main.py └── some_folder/     ├── sub_folder/         ... ...Чтение содержимогоЧто дальше? Разумеется, следующий шаг — открыть файлы и понять, что в них внутри. Лишь имея на руках содержимое самих файлов, LLM сможет понять код, который необходимо анализировать и отлаживать. Создадим функцию для выполнения этой задачи.def read_code_files(directory, files_to_debug=None):     """"""     Считывает содержимое файлов с кодом на Python из указанной директории и возвращает его в виде строки.      Args:         directory (str): Директория, содержащая файлы с кодом.         files_to_debug (list, optional): Список конкретных файлов для отладки. По умолчанию — None.      Returns:         str: Строка, содержащая содержимое всех файлов с кодом.     """"""     # Инициализация пустой строки для хранения содержимого всех файлов     all_code = """"      # Если список файлов не указан, загружаем все Python-файлы из директории     if not files_to_debug:         files_to_debug = [f for f in os.listdir(directory) if f.endswith("".py"")]      # Перебор списка файлов для отладки     for code_filename in files_to_debug:         try:             # Открытие файла для чтения             with open(os.path.join(directory, code_filename), ""r"") as file:                 # Добавление имени файла в виде комментария для наглядности                 all_code += f""# {code_filename}\n""                 # Чтение содержимого файла                 code = file.read()                 # Добавление содержимого файла к общей строке                 all_code += code + ""\n""                 # Добавление разделителя для удобства восприятия между файлами                 all_code += ""_________________\n\n""         except Exception as e:             # Обработка ошибок при чтении конкретного файла             print(f""Ошибка при чтении {code_filename}: {e}"")      # Возврат объединённого содержимого всех файлов     return all_codeФункция read_code_files() принимает два аргумента: каталог, где находятся файлы кода, и список файлов для отладки, если они переданы в аргументах командной строки. Если же конкретные файлы не указаны, функция будет считывать все Python‑файлы из указанного каталога.Тестируем. Предположим, что в каталоге проекта два файла: main.py и utils.py.# Указываем папку с кодом code_directory = ""project_folder""  # Получаем содержимое файлов с кодом code_content = read_code_files(code_directory) print(code_content)### ВЫВОД ###  # main.py def main():     print(""Здравствуй, свет!"")  if __name__ == ""__main__"":     main() _________________  # utils.py def add(a, b):     return a + b _________________Как видно, файлы были успешно загружены и представлены в виде одной строки. Теперь LLM сможет разобраться в нашем коде.Логирование ошибокСледующим этапом мы займёмся поиском ошибок. Нам нужно выполнить каждый Python‑файл, чтобы проверить наличие ошибок. Для этого мы используем питонский модуль subprocess, который позволяет запускать файлы и захватывать их вывод. Эта функция поможет нам точно определить, где и какие ошибки присутствуют в коде, а также предоставит ключевую информацию для отладки.def run_code_and_capture_errors(directory, files_to_debug=None):     """"""     Выполняет Python-файлы в указанной директории, фиксируя их вывод или ошибки.      Args:         directory (str): Директория, где находятся Python-файлы.         files_to_debug (list, optional): Список файлов для выполнения. Если None, выполняются все Python-файлы в директории.      Returns:         str: Отформатированная строка с результатами выполнения для каждого файла.     """"""     results = []  # Список для хранения результатов выполнения      try:         # Если список файлов не указан, выбираем все Python-файлы в директории         if files_to_debug is None:             files_to_debug = [f for f in os.listdir(directory) if f.endswith('.py')]          # Перебор каждого файла и его выполнение         for code_filename in files_to_debug:             results.append(f""# {code_filename}"")             try:                 # Запуск Python-файла через subprocess с захватом вывода и ошибок                 result = subprocess.run(                     [""python"", os.path.join(directory, code_filename)],                     capture_output=True,                     text=True,                     check=True,                 )                 # Добавление успешного вывода (stdout)                 if result.stdout.strip():                     results.append(result.stdout.strip())                 else:                     results.append(""ошибок нет"")             except subprocess.CalledProcessError as e:                 # Добавление сообщения об ошибке (stderr)                 if e.stderr.strip():                     results.append(f""Произошла ошибка: {e.stderr.strip()}"")                 else:                     results.append(""Неизвестная ошибка"")      except Exception as e:         # Обработка неожиданных ошибок (например, неверная директория)         results.append(f""Произошла ошибка: {str(e)}"")      # Возврат результатов в формате строки     return ""\n\n"".join(results)Функция run_code_and_capture_errors() принимает два аргумента: каталог (directory) и необязательный список имён файлов Python (files_to_debug). Она выполняет каждый файл, захватывая либо вывод, либо ошибки, и возвращает форматированную строку. Результаты возвращаются в формате, где имя файла предваряется символом #, а далее следуют детали вывода или ошибки.Давайте протестируем эту функцию на каталоге проекта, который содержит только один скрипт test_error.py, содержащий преднамеренную ошибку.# test_error.py с ошибкой def test():     a = 10     b = ""20""     print (a + b)  test()Вызовем этот метод для запуска нашего Python‑кода.# Указываем директорию code_directory = ""project_dor""  # Запуск кода и захват ошибок error_output = run_code_and_capture_error(code_directory) print(error_output)### ВЫВОД ###  # test_error.py Traceback (most recent call last):   File ""../project_dir/test_error.py"", line 3, in test     print (a + b)            ~~^~~ TypeError: неподдерживаемые операнды для +: 'int' и 'str'Системный промт для отладкиТеперь, когда мы можем сообщить модели, какие ошибки существуют, необходимо объяснить ей, как их исправить. Здесь нам и пригодится системный промт — своего рода руководство для LLM.DEBUG_SYSTEM_PROMPT = """""" Вы — эксперт по Python, специализирующийся на обработке ошибок и защитном программировании. Проанализируйте все предоставленные файлы кода, а также информацию, найденную в интернете. Исправьте ошибки и улучшите код, усилив его защитные механизмы. Используйте строго следующий формат:  # имя_файла_кода ```python   # исправленный код ```    Пропускайте файлы, в которых ошибок не обнаружено. Применяйте лучшие практики и интегрируйте релевантные данные из поиска. """"""Добавляем взаимодействие с LLMНам нужно создать функцию для LLM, которая будет генерировать код на основе системного и юзер‑промта. Давайте её разработаем.def HF_llm(model_name, system_prompt, user_prompt):     """"""     Генерирует ответ с использованием языковой модели Hugging Face на основе заданных системного и юзер-промта.      Args:     model_name (str): Имя или путь к предобученной языковой модели (например, 'gpt-3', 't5-base').     system_prompt (str): Промт, задающий роль и поведение модели.     user_prompt (str): Входные данные пользователя, на которые нужно сгенерировать ответ.      Returns:     str: Сгенерированный ответ модели.     """"""          # Формируем массив, содержащий системный и юзер-промт     messages = [         {""role"": ""system"", ""content"": system_prompt},  # Системная инструкция или контекст         {""role"": ""user"", ""content"": user_prompt},  # Юзер-промт     ]      # Инициализируем пайплайн модели Hugging Face для генерации текста     pipeline = transformers.pipeline(         ""text-generation"",  # Задача: генерация текста         model=model_name,  # Имя модели или путь к ней         model_kwargs={""torch_dtype"": torch.bfloat16},  # Используем bfloat16 для повышения эффективности         device_map=""auto"",  # Автоматический выбор устройства (CPU или GPU)     )      # Генерируем ответ модели на основе входных данных     outputs = pipeline(messages)      # Возвращаем только сгенерированный текст, без метаданных     return outputs[0][""generated_text""][-1]['content']Сейчас мы сосредоточимся на LLM, размещённой на Hugging Face: это открытая платформа, на которой можно легко найти и подключить подходящую модель. В то же время GitHub‑репозиторий AIDebugger поддерживает разные модели, в частности OpenAI, Gemini и локальные.Исправление кодаПосле того как LLM предоставит исправления, их нужно внести в кодовую базу. Для этого мы создадим функцию, которая заменит проблемные участки.def update_code_from_llm_response(llm_response):     """"""     Извлекает код из ответа языковой модели и обновляет соответствующие файлы.      Функция выполняет два основных действия:     1. Извлекает фрагменты кода Python из ответа языковой модели.     2. Обновляет соответствующие файлы, заменяя их содержимое.      Аргументы:         llm_response (str): Текстовый ответ языковой модели.      Возвращает:         None: Функция напрямую обновляет файлы, не возвращая значений.     """"""     # Регулярное выражение для поиска имен файлов и фрагментов кода внутри markdown-блоков     pattern = r""#\s(\S+)\n```python\n(.*?)\n```""      # Находим все совпадения в ответе модели (имя файла и соответствующий код)     matches = re.findall(pattern, llm_response, re.DOTALL)      # Создаем словарь, сопоставляющий имена файлов с извлеченным кодом     code_changes = {filename: code.strip() for filename, code in matches}      # Обновляем содержимое файлов на основе извлеченного кода     for filename, code in code_changes.items():         # Полный путь к файлу         file_path = os.path.join(self.code_dir, filename)          # Открываем файл в режиме записи и заменяем его содержимое новым кодом         with open(file_path, ""w"") as file:             file.write(code)Теперь настроены все компоненты: мы можем выявлять ошибки, спрашивать LLM об их исправлении и на основе ответов обновлять файлы исходного кода. Запустим наши функции и посмотрим, как дебаггер исправит код.Исправляем небольшую кодовую базуУ нас есть небольшая кодовая база, состоящая всего из двух файлов, один из которых содержит ошибку.# code1.py import code2  def greet(name):     print(""Привет, "" + name)  greet(""Алиса"")  result = code2.add(10, ""5"")  # Эта строка — причина ошибки TypeError  print(result)  # code2.py def add(a, b):     return a + b  # В этом месте будет вызвана TypeErrorИтак, в файле code1.py вызывается функция add() из подключённого модуля code2, в которую передаются два аргумента: целое число 10 и строка ""5"". Поскольку эти аргументы имеют разные типы, а функция add() в code2.py не выполняет проверку типов и просто пытается сложить переданные значения, возникает ошибка TypeError. Ошибка происходит внутри функции add() в code2.py, так как операция сложения между числом и строкой является недопустимой без явного приведения типов.Сначала генерируем юзер‑промт (USER_PROMPT), объединяющий в одну программную строку следующие данные: структуру папок (get_dir_tree(codebase)), файлы исходного кода (read_code_files(codebase)) и логи возникших при выполнении ошибок (run_code_and_capture_error(codebase)).# Указываем каталог с кодом codebase = ""project_code""  # Получаем структуру каталогов directory_tree = get_dir_tree(codebase)  # Читаем содержимое файлов с кодом code_content = read_code_files(codebase)  # Запускаем код и фиксируем ошибки error_output = run_code_and_capture_error(codebase)  # Формируем юзер-промт для LLMs USER_PROMPT = f'''Структура файлового дерева проекта:                    {directory_tree}                                         Содержимое файлов:                    {code_content}                     Обнаруженные ошибки:                    {error_output}                '''Вот как будет выглядеть этот юзер‑промт: Структура файлового дерева проекта:                      project_code ├── code1.py ├── code2.py   Содержимое файлов:  # code1.py import code2  def greet(name):     print(""Привет, "" + name)  greet(""Алиса"")  result = code2.add(10, ""5"")  # Эта строка — причина ошибки TypeError  print(result)  # code2.py def add(a, b):     return a + b  # В этом месте будет вызвана TypeError  Обнаруженные ошибки:  TypeError: неподдерживаемые операнды для +: 'int' и 'str'Теперь, наконец, передаём сгенерированный промт в языковую модель Hugging Face (HF_llm) для анализа и генерации исправлений. Модель определит ошибки в нашем коде и предложит способы их устранения.# Вызываем функцию HF_llm с системным и пользовательским промтами llm_response = HF_llm(     model_name=""meta-llama/Meta-Llama-3.1-3B-Instruct"",  # Пример модели     system_prompt=DEBUG_SYSTEM_PROMPT,     user_prompt=USER_PROMPT )  # Выводим ответ для просмотра исправлений print(llm_response)   ### ВЫВОД ### code1.py import code2 ... Мы выбрали модель Llama-3.2–3B*, которая оптимизирована для запуска даже на бюджетных видеокартах благодаря небольшому количеству параметров — 3 миллиарда. Несмотря на компактность, она поддерживает контекстное окно объёмом 128K, что делает её подходящей для анализа длинных фрагментов исходного кода.Запустим функцию обновления файлов кода и посмотрим, как изменился файл code1.py.# Обновляем код на основе ответа LLM update_code_from_llm_response(llm_response) Файл code1.py был обновлён — взглянем, как он выглядит после внесения исправлений: # code1.py import code2  def greet(name):     print(""Привет, "" + name)  greet(""Алиса"")  result = code2.add(10, 5) # Исправлена ошибка типов print(result) Ура, ошибка типа устранена. Но заметим, что пока мы не применяли механизм бесконечного цикла, а значит, LLM не будет повторно корректировать код до полного их истребления.Создание бесконечного цикла отладкиИтак, давайте разработаем этот бесконечный цикл, который будет работать до тех пор, пока в нашем коде не останется ошибок.def infinite_debug_loop(codebase_dir, model_name, system_prompt):     """"""     Запускает бесконечный цикл отладки, в котором LLM исправляет код до полного устранения ошибок.      Args:     codebase_dir (str): Директория с кодом для отладки.     model_name (str): Название модели Hugging Face для генерации исправлений.     system_prompt (str): Системные инструкции для LLM.          Returns:     None: Функция напрямую обновляет кодовую базу.     """"""     iteration = 1     while True:         # Шаг 1: Получаем структуру файлов и содержимое кода         directory_tree = get_dir_tree(codebase_dir)         code_content = read_code_files(codebase_dir)          # Шаг 2: Запускаем код и фиксируем ошибки         error_output = run_code_and_capture_errors(codebase_dir)          # Если ошибок нет, завершаем цикл         if ""Error occurred"" not in error_output and ""Traceback"" not in error_output:             print(""Ошибок в коде не найдено, отладка завершена."")             break          # Шаг 3: Формируем запрос для LLM         user_prompt = f'''Структура файлов проекта:                           {directory_tree}                            Содержимое файлов:                           {code_content}                            Обнаруженные ошибки:                           {error_output}                        '''          # Шаг 4: Отправляем запрос LLM для исправления кода         print(f""Итерация {iteration}: Отправка запроса в LLM для исправлений."")         llm_response = HF_llm(model_name=model_name, system_prompt=system_prompt, user_prompt=user_prompt)          # Шаг 5: Обновляем код на основе ответа LLM         update_code_from_llm_response(llm_response)          # Шаг 6: Небольшая пауза перед следующей итерацией         iteration += 1         print(f""Итерация {iteration}: Код обновлён. Переход к следующей итерации...\n"")         time.sleep(2)  # Опционально: можно изменить время ожиданияПройдёмся ещё раз по коду. В начале каждой итерации применяется функция get_dir_tree(), которая сканирует структуру каталога, создавая текстовое представление проекта. Затем метод read_code_files() извлекает содержимое файлов, чтобы их можно было передать в юзер‑промт. Код проверяемого проекта запускается функцией run_code_and_capture_errors(), которая проверяет, содержатся ли в выводе ключевые слова, сигнализирующие об ошибка: Error occurred либо Traceback. Если ошибок больше не проявляется, то цикл завершается.Приведённый код демонстрирует простой бесконечный цикл, который работает до тех пор, пока в нашем коде не останется ошибок. Однако есть и немного расширенная версия в GitHub‑репозитории AI Debugger, где обрабатываются более сложные сценарии и ситуации.Тестирование циклаДавайте добавим новый, чуть более сложный сценарий кода. Этот проект будет состоять из трёх файлов, каждый из которых содержит различные типы ошибок.# calculator.py def add(a, b):     return a + b def subtract(a, b):   return a - b def multiply(a,b):     return a * b def divide(a,b):    return a/b   # main.py import calculator def main():     x = 10     y = ""5""     z = 0     result_add = calculator.add(x, y)     print(f""Сложение: {result_add}"")     result_sub = calculator.subtract(x, y)     print(f""Вычитание: {result_sub}"")     result_mul = calculator.multiply(x,y)     print(f""Умножение: {result_mul}"")     result_div = calculator.divide(x,z)     print(f""Деление: {result_div}"") if __name__ == ""__main__"":     main()   # utils.py def greet(name):     message = ""Привет, "" + name   return messageОшибки, которые мы намереваемся обнаружить:TypeError: функции add(), subtract() и multiply(), где ожидаются целые числа, вызываются с аргументами строкового типа.ZeroDivisionError: ошибка деления на ноль в функции divide(), так как z = 0.IndentationError: Оператор return неправильно выровнен по отступу.Сценарий подготовлен, вызовем функцию infinite_debug_loop(), чтобы она занялась исправлением:# Указываем директорию с кодом codebase_dir = ""complex_project_dir""  # Задаём название модели и системный промпт для LLM model_name = ""meta-llama/Meta-Llama-3.1-3B-Instruct""  # или другая модель HF  system_prompt = DEBUG_SYSTEM_PROMPT  # Запускаем бесконечный цикл отладки для исправления кода infinite_debug_loop(codebase_dir, model_name, system_prompt) Этот фрагмент кода задаёт директорию проекта, выбирает модель LLM и её системный промт, а затем вызывает функцию infinite_debug_loop(). Что происходит дальше? Запускается процесс отладки, в ходе которого система сканирует проект, обнаруживает ошибки, исправляет их с помощью LLM и повторяет цикл, пока все проблемы не будут устранены.После запуска вы увидите результат каждой итерации цикла: какие ошибки найдены, какой ответ дала LLM (включая обновлённые блоки кода) и какие изменения внесены в файлы. По завершении цикла код должен быть исправлен.Итерация 1: Отправка запроса в LLM для исправлений... Итерация 1: Код обновлён. Переход к следующей итерации...  Итерация 2: Отправка запроса в LLM для исправлений... Итерация 2: Код обновлён. Переход к следующей итерации...  Ошибок в коде не найдено, отладка завершена.На всё про всё потребовалось две итерации. А теперь взглянем на файлы после исправлений и убедимся, что ошибки действительно устранены.# calculator.py def add(a, b):     return a + b def subtract(a, b):     return a - b def multiply(a,b):     return a * b def divide(a,b):    if b == 0:        return ""Деление на ноль невозможно""    return a/b  # main.py import calculator def main():     x = 10     y = 5     z = 0      result_add = calculator.add(x, y)     print(f""Сложение: {result_add}"")      result_sub = calculator.subtract(x, y)     print(f""Вычитание: {result_sub}"")      result_mul = calculator.multiply(x,y)     print(f""Умножение: {result_mul}"")      result_div = calculator.divide(x,z)     print(f""Деление: {result_div}"")  if __name__ == ""__main__"":     main()  # utils.py def greet(name):     message = ""Привет, "" + name     return messageКак видно, проблемы вроде TypeError из файла main.py, ZeroDivisionError из того же файла и даже IndentationError из utils.py благополучно ушли в прошлое и наш бесконечный цикл справился с этим нелёгким сценарием. Шах и мат, баги.Поиск в интернетеИногда одной лишь LLM может не хватить, чтобы справиться с ошибкой. Здесь на помощь приходит интернет — незаменимый инструмент в поиске решений. Почему бы не встроить в цикл возможность обращения к поисковикам, как это делаем мы, когда гуглим ошибки?Первый шаг — преобразовать юзер‑промт в Google‑запрос. Для этого можно использовать ту же функцию LLM, передав ей тщательно составленный системный промт, направленный на превращение описанной ошибки в однострочный запрос для поисковика. Создадим системный промт для LLM, который будет заточен на это преобразование, и посмотрим, как он преобразует детализированную ошибку в запрос для Google.QUERY_SYSTEM_PROMPT = """"""     Вы высококвалифицированный ассистент для разработчиков программного обеспечения, специализирующийся на поиске и устранении ошибок в коде. Ваша задача — составить **однострочный поисковый запрос в Google** на основе переданного фрагмента кода и связанного с ним сообщения об ошибке. Запрос должен соответствовать следующим правилам:      1. Включать название языка программирования или фреймворка (если известно).     2. Содержать ключевые детали из сообщения об ошибке.     3. Упоминать любые релевантные функции, методы или библиотеки из кода.     4. Быть лаконичным, точным и оформленным в виде **одной строки**.      **Формат входных данных:**     1. **Фрагмент кода:** (например, Python-код, вызывающий исключение)     2. **Сообщение об ошибке:** (например, `TypeError: неподдерживаемые операнды для +: 'int' и 'str'`)      **Формат выходных данных:**     Строка в точности в следующем формате:     `search_query: ""<ваш поисковый запрос в Google>""`     """""" Пример запроса, созданного моделью LLM, размером 3B, для одной из наших ошибок: ### НАША ОШИБКА TypeError: неподдерживаемые операнды для +: 'int' и 'str'  # Вызов функции HF_llm с системным и пользовательским промтами llm_response = HF_llm(     model_name=""meta-llama/Meta-Llama-3.1-3B-Instruct"",  # Пример модели     system_prompt=QUERY_SYSTEM_PROMPT,     user_prompt=USER_PROMPT )  print(llm_response)  ### ВЫВОД ### search_query: python ""TypeError"" convert int to string addition Выглядит корректно: python ""TypeError"" convert int to string addition. Затем мы передадим это в поисковую систему, ведь велика вероятность, что интернет выдаст решение, которого нет в LLM‑модели.Для получения результатов поиска Google мы будем использовать библиотеку scrapling. С её помощью создадим функцию для парсинга результатов поиска:async def internet_search(search_query):     """"""     Асинхронный поиск в интернете с использованием Google, возвращает первые найденные ссылки.      Args:          search_query (str): Поисковый запрос для Google.      Returns:          list: Список первых найденных URL.     """"""     search_query = f'""{search_query}""'     search_url = f""https://www.google.com/search?q={search_query}""      fetcher = StealthyFetcher()      try:         page = await fetcher.async_fetch(search_url)         all_urls = page.css(""#search a::attr(href)"")          # Фильтруем список, оставляя только ссылки, начинающиеся с https         all_urls = [url for url in all_urls if url.startswith(""https"")]          return all_urls     except Exception as e:         print(f""Произошла ошибка: {e}"") Эта функция асинхронная, ведь интернет не всегда торопится с ответами. Она извлекает URL‑адреса из верхних результатов Google и возвращает их в виде списка. Далее мы сможем получить содержимое этих URL‑адресов с помощью нескольких дополнительных функций scrapling.async def fetch_internet_content(search_query, num_urls = 5):     """"""     Асинхронно выполняет поиск в интернете и собирает содержимое веб-страниц по заданному запросу.      Проверяет возможность выполнения интернет-поиска, получает URL-адреса и извлекает текстовое содержимое с нескольких веб-страниц, обрабатывая возможные ошибки.      Args:         search_query (str): Поисковый запрос для поиска веб-страниц.         num_urls (int): Максимальное количество URL-адресов для загрузки (default: 5)      Returns:         str: Объединенный текст, полученный с веб-страниц,               или пустую строку, если поиск отключен/результаты не найдены.     """"""     # Проверяем, включен ли интернет-поиск     if not self.enable_internet_search:         return """"      # Выполняем интернет-поиск для получения URL-адресов     urls = await internet_search(search_query)          # Возвращаем пустую строку, если не найдено ни одного URL     if not urls:         return """"      # Инициализируем парсер без автоматического сопоставления     fetcher = Fetcher(auto_match=False)          # Переменная для хранения объединенного содержимого веб-страниц     combined_content = """"      # Перебираем первые num_urls URL-адресов     for url in urls[:num_urls]:         try:             # Загружаем страницу с заголовками, маскирующими бота             page = fetcher.get(url, stealthy_headers=True)                          # Извлекаем текстовое содержимое, игнорируя теги <script> и <style>             content = page.get_all_text(ignore_tags=(""script"", ""style""))                          # Добавляем текст с заголовком URL-источника для отслеживания             combined_content += f""### СОДЕРЖИМОЕ С: {url}\n{content}\n\n""                  except Exception as e:             # Логируем ошибки при загрузке URL, не прерывая выполнение             logging.error(f""Ошибка при загрузке {url}: {e}"")      return combined_content Так как ссылок может быть много, разумно ограничить их количество, взяв несколько первых результатов. После эти данные нужно будет добавить в юзер‑промт.Цикл отладки с использованием интернет-поискаДавайте наконец‑то усовершенствуем наш бесконечный цикл отладки, добавив в него суперсилу — интернет‑поиск. Если одна и та же ошибка повторяется пять или более раз подряд, система будет искать решения в сети.def infinite_debug_loop_with_internet(codebase_dir,                                        model_name,                                       system_prompt,                                       max_attempts=10):     """"""     Запускает расширенный цикл отладки, который использует интернет-поиск      для устранения повторяющихся ошибок.      Args:     codebase_dir (str): Каталог с Python-кодом, который нужно отладить.     model_name (str): Название модели Hugging Face, используемой для исправления ошибок.     system_prompt (str): Системные инструкции для языковой модели (LLM).     max_attempts (int): Максимальное число попыток перед обращением к интернету.          Returns:     None: Функция напрямую обновляет кодовую базу.     """"""     iteration = 1     error_tracking = {}  # Отслеживание ошибок на протяжении итераций      while True:         # Шаг 1: Получаем структуру файлов и содержимое кода         directory_tree = get_dir_tree(codebase_dir)         code_content = read_code_files(codebase_dir)          # Шаг 2: Запускаем код и фиксируем ошибки         error_output = run_code_and_capture_errors(codebase_dir)          # Если ошибок нет, завершаем цикл         if ""Error occurred"" not in error_output and ""Traceback"" not in error_output:             print(""Ошибок в коде не найдено, отладка завершена."")             break          # Track error persistence         if error_output in error_tracking:             error_tracking[error_output] += 1         else:             error_tracking[error_output] = 1          # Формируем пользовательский запрос, включая возможные результаты интернет-поиска         user_prompt = Структура файлов проекта:                           {directory_tree}                            Содержимое файлов:                           {code_content}                            Обнаруженные ошибки:                           {error_output}                        '''          # Если ошибка повторяется более max_attempts раз, обращаемся к интернет-поиску         if error_tracking[error_output] > max_attempts:             # Генерируем поисковый запрос для ошибки             search_query_prompt = f"" Фрагмент кода:\n{code_content}\n\nСообщение об ошибке:\n{error_output}""             search_query_response = HF_llm(                 model_name=model_name,                  system_prompt=QUERY_SYSTEM_PROMPT,                  user_prompt=search_query_prompt             )                          # Извлекаем поисковый запрос             search_query = search_query_response.split("": "")[-1].strip('""')                          # Получаем содержимое с найденных веб-страниц             internet_content = await fetch_internet_content(search_query)                          # Добавляем найденную информацию к юзер-промту             user_prompt += f""\n\nРезультаты интернет-поиска:\n{internet_content}""          # Step 4: Call the LLM to generate fixes         print(f""Итерация {iteration}: Отправка запроса в LLM для исправлений."")         llm_response = HF_llm(             model_name=model_name,              system_prompt=system_prompt,              user_prompt=user_prompt         )          # Шаг 5: Обновление кода на основе ответа LLM         update_code_from_llm_response(llm_response)          # Сбрасываем счетчик для ошибки, если LLM предложила исправление         error_tracking[error_output] = 0          # Шаг 6: Продолжаем отладку         iteration += 1         print(f""Итерация {iteration}: Код обновлён. Переход к следующей итерации...\n"")         time.sleep(2)  # Опциональная пауза между итерациями Итак, функция infinite_debug_loop_with_internet_search() пытается по кругу исправить ошибки с помощью языковой модели LLM, но, если «локальные» исправления не помогают и количество попыток для одной и той же ошибки превысило max_attempts, она начинает искать решения в интернете. Для этого применяется динамическое генерирование запросов.Тестирование на GitHubМой репозиторий на GitHub предлагает более детализированное решение, которое поддерживает различные типы LLM, обрабатывает логи и предоставляет другие полезные функции. Здесь мы рассмотрим общий процесс. Так, протестируем репозиторий на более сложной кодовой базе и посмотрим, как он справляется.python -m src.cli.main \  --llm_type huggingface \  --huggingface_model ""meta-llama/Llama-3.2-3B-Instruct"" \  --code_dir ""ai-debugger"" \  --huggingface_device ""cuda"" \  --max_attempts 10 \  --enable_internet_search True \  --num_search_urls 5 \  --internet_search_threshold 5Этот скрипт, доступный в репозитории, предоставляет гибкие настройки: к примеру, можно включать или отключать интернет‑поиск или задавать параметры для моделей. Я использовал его на своей кодовой базе, содержащей сложные скрипты для обучения трансформеров, и в коде действительно обнаружились ошибки. Запустим процесс и посмотрим, как начинается отладка:## Вывод в консоли  2025-01-23 19:46:54,070 - INFO - HuggingFace-модель инициализирована 2025-01-23 19:46:54,070 - INFO - Файлы не переданы. Загружаем все файлы  2025-01-23 19:46:54,076 - INFO - Попытка 1...  2025-01-23 19:46:59,339 - INFO - Загружается модель LLaMA-3.1-3B-instruct 2025-01-23 19:46:59,358 - INFO - ИИ внес следующие изменения:  Файл: code2.py   + from caleb import transform   + return x + z   ...   + except Exception as e:   + print(""Произошла непредвиденная ошибка: "", str(e))  File: code3.py   - from llmc import transformer_module   + from code2 import transformer_module_   + try:   - print(parms.round())   ...   + print(llm.cab(""Обучены следующие параметры:"", parms)   + except Exception as e:   + print(""Произошла непредвиденная ошибка: "", str(e))  ...  2025-01-23 19:46:59,581 - INFO - Все ошибки исправлены.После долгого ожидания, почти 34 итераций, система смогла устранить множество ошибок в моём коде, одновременно записывая детальные логи об удалённых или добавленных фрагментах. Эти логи сохраняются в директории проекта, который вы отлаживаете.Дебаггинг с помощью LLM, который ещё и гуглит ошибки, скорее экспериментальный, так как его сложно применять в реальных проектах, особенно в крупных. Но более важная проблема здесь — безопасность: в интернете полно сомнительных советов, и почти ничто не мешает модели «починить» ваш код так, что он станет напичканным уязвимостями. На деле куда эффективнее использовать среды разработки с интегрированными LLM, чем пытаться автоматизировать гугление ошибок таким неординарным способом.Что скажете по поводу этой реализации? Добавили бы вы в неё что‑то ещё?* Продукт Llama принадлежит организации Meta, которая признана в России террористической и запрещена."
66,66,Полиарилат для электроники и не только: краткий патентный анализ,Online patent,Ваш личный патентный офис,303.79,"Консалтинг и поддержка, Веб-сервисы",2025-03-12,"Мы продолжаем рассказывать о различных химических веществах, их применении в электронике и, разумеется, патентом аспекте. На этот раз речь пойдет о полимере полиарилат.Где используется?Полиарилаты представляют собой сложные полиэфиры двухатомных фенолов и дикарбоновых кислот. Синтез этих полимеров осуществляют через хлорангидриды дикарбоновых кислот одним из 3-х методов:поликонденсацией в инертной среде при повышенной температуре в высококипящем растворителе (динил, дитолилметан, совол, хлорнафталин);межфазной поликонденсацией при комнатной температуре;акцепторно-каталитической поликонденсацией в среде органического растворителя при 0-50°С в присутствии третичного амина.Основные области применения полиарилатов:конструкционные изделия, с хорошими диэлектрическими свойствами, в радио- и электротехнике;композиционные материалы, в том числе и антифрикционные, которые обладают низким коэффициентом трения и могут работать без смазки при температурах до 250°С, даже в вакууме, или в узлах, где не допускаются жидкие смазки, а также при больших градиентах скоростей между трущимися поверхностями;полиарилаты фенолфталеина используются как волокнистые фильтрующие материалы, работающие при температурах до 300°С;теплостойкие полиарилатные пленки, получаемые экструзией расплава или поливом из растворов, применяются как электроизоляционные материалы;терефталат полибисфенола-А хорошо показал себя в солнечных панелях, поскольку устойчив к ультрафиолетовому излучению.Не путать наши элитные, изысканные полиарилаты с созвучными по названию полиакрилатами, известными миллиардам людей по восхитительным акриловым краскам и непрезентабельным зубным протезам!В маркетинговом обзоре Global Polyarylate Market Overview указано, что оптические пленки, автомобильные детали, электрические и электронные компоненты, а также промышленное оборудование являются ключевыми областями применения полиарилата. В частности исключительные оптические свойства полиарилата делают его идеальным материалом для оптических пленок (optical films), используемых в различных областях применения, таких как поляризаторы, пленки для дисплеев и защитные пленки. По прогнозам, к 2025 году мировой рынок оптических пленок достигнет 12,3 млрд долларов, что обусловлено растущим спросом на высококачественные дисплеи для смартфонов, планшетов и других электронных устройств.Автомобильные детали. Полиарилат находит широкое применение в автомобилестроении благодаря своим легким свойствам, высокой прочности и термостойкости. Он широко используется в производстве деталей интерьера, таких как приборные панели, дверные панели и чехлы для сидений. Ожидается, что растущая автомобильная промышленность будет стимулировать спрос на полиарилат в автомобилестроении. Электрические и электронные компоненты. Электроизоляционные и огнезащитные свойства полиарилата делают его пригодным для изготовления электрических и электронных компонентов. Он используется в производстве разъемов, переключателей и печатных плат. Растущий спрос на электронные устройства и миниатюризация компонентов способствуют росту этого сегмента. Промышленное оборудование. Высокая прочность и долговечность полиарилата делают его идеальным материалом для изготовления компонентов промышленного оборудования, таких как шестерни, подшипники и корпуса. Он также используется при изготовлении оборудования для химической и пищевой промышленности. Растущий спрос на промышленную автоматизацию и потребность в долговечных материалах в суровых условиях эксплуатации стимулируют рост этого сегмента.В целом, ожидается, что в ближайшие годы сегмент применения полиарилатов на рынке значительно расширится. Ожидается, что растущий спрос на полиарилат в различных отраслях промышленности в сочетании с его уникальными свойствами и универсальностью будет способствовать росту рынка.Среди основных игроков: Lanxess AG;Celanese Corporation;BASF SE;Eastman Chemical Company; Sumitomo Chemical;Toray Industries;PolyOne Corporation и т.д.Патентный аспектНа портале Google.Patents указано более 100 000 документов на февраль 2025 по polyarylate. Среди патентообладателей ТОП-7 лидеров составили:Samsung Display Co., Ltd. — 7,4%;Canon Kabushiki Kaisha — 3,9%;Ricoh Company, Ltd. — 2,8%;LG Chem (株式会社Lg化学) — 2,1%;General Electric Company — 2%;Taylor Made Golf Company, Inc. — 1,7%;Semiconductor Energy Laboratory Co., Ltd. — 1,5%.Как видите, безусловное лидерство в основном за японскими, южнокорейскими и американскими компаниями. Это фирмы известные, их названия на слуху не только у специалистов. Рейтинг кодов МПК следующий:разное оборудование Y10T — 18,8%;сокращение выбросов парниковых газов Y02E — 16,3%;органические электрические твердотельные устройства H10K — 15,7%;слоистые изделия или материалы B32B — 14,7%;композиции высокомолекулярных соединений C08L — 14,1%;оптические устройства G02F — 12,5%;полупроводниковые приборы H01L — 12,2%;использование неорганических или низкомолекулярных органических веществ в качестве компонентов для композиций на основе высокомолекулярных соединений C08K — 11%;оптические элементы, системы или приборы G02B — 10,8%.А что же в России?Способы получения полиарилатов начали патентовать в СССР ещё в 1960-е годы (см. например Авторские свидетельства SU181283A2 и SU325237A1). Авторитетным специалистом был академик Василий Владимирович Коршак, Институт элементоорганических соединений им. А.Н. Несмеянова РАНВ базе ФИПС поиск по запросу «полиарилат» выявлено 11 патентов РФ на изобретения, из которых только 3 действующие, перечислим их все:№2556858 Способ получения полиарилатов, Завод имени Я.М. Свердлова (Дзержинск);№2565177 Эпоксидное связующее пленочного типа, ВИАМ (Москва);№2823959 Железобетонное изделие и способ его изготовления [добавка полиарилата для изготовления железобетонных шпал и брусьев], АО «БетЭлТранс» (Москва).Недействующие патенты принадлежали следующим юрлицам: Институт элементоорганических соединений им. А.Н. Несмеянова РАН, Ивановский государственный химико-технологический университет, Тейджин Арамид Б.В. (NL), Эвоник Дегусса ГмбХ (DE).Актуальных заявок на изобретения нет.Патентов РФ на полезные модели нет. Впрочем, как программ для ЭВМ и баз данных по этой теме.Отечественные исследованияВ базе домена «Наука и инновации» на «полиарилат» 38 документов, в основном отчёты по НИР, результаты интеллектуальной деятельности, диссертации. Так, Институт элементоорганических соединений им. А. Н. Несмеянова РАН выполнил за грант Минобрнаука РФ 178 млн руб. в 2020 г. НИОКТР «Получение новых фундаментальных знаний о строении, синтезе и свойствах элементоорганических, полиароматических высокомолекулярных соединений различной архитектуры. Разработка методов компьютерного моделирования строения и свойств высокомолекулярных соединений и создание конструкционных, функциональных полимеров и многокомпонентных полимерных систем для высокотехнологичных отраслей промышленности: медицины, авиакосмической и специальной техники». В частности, синтезированы и исследованы новые статистические терполимеры типа D1-A1-D1-A2 для изготовления полимерных солнечных фотоэлементов. НИР «Разработка полимерных газоразделительных мембран на основе блоксополимеров с повышенными механическими и термическими свойствами» осуществлена в 2020-2023 гг. МГТУ им. Н.Э. Баумана. Проект направлен на разработку непористой полимерной мембраны на основе нового полиарилатполиэтиленоксидного блок-сополимера, обладающей высокой химической стабильностью, повышенной теплостойкостью, а также улучшенным комплексом физико-механических и диффузионных характеристик, позволяющих эксплуатировать данный вид мембран в условиях высоких давлений и температур. Возможные потребители: Аэрокосмическая, нефтеперерабатывающая, медицинская и сельскохозяйственная промышленность.ВыводыПотребление полиарилатов в России и мире будет расти. Как и число патентов по данной теме. К сожалению, отечественные охранные документы по данному направлению можно пересчитать по пальцам одной руки, что прискорбно. Это особенно печально на фоне успехов тех же корейских и японских компаний, которые вовремя инвестировали в исследования по данной теме. Что ж, остается надеяться на возможное лицензирование со стороны дружественных китайских предприятий и организаций.    Бесплатный поиск, мониторинг и регистрация товарных знаков  и других объектов интеллектуальной собственности.Поиск по программам для ЭВМРегистрация программы для ЭВМ"
67,67,Компактные структуры данных,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2558.03,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-12," Введение Несколько месяцев назад в поисках идей по ускорению кода я изучал множество научных статей по computer science. Не буду притворяться, что хорошо их понимал, но меня не пугает непонятное, и я готов признать своё невежество1. Я обнаружил статью, написанную пятнадцать лет назад2, в которой было множество новых для меня концепций. Мне никак не удавалось в них разобраться.  Что же делать дальше? Можно искать другие статьи, чтобы они заполнили мои пробелы. Это рискованное предприятие, потому что они могут запутать ещё больше, но избежать этого нельзя. Я нашёл статью с нужной структурой данных, в которой упоминался исходный код с веб-сайта. Код был написан на C++, а я работаю на Rust, но решил, что всё равно стоит на него взглянуть. Однако зайдя на сайт, я не обнаружил там ресурс, поэтому я написал владельцу веб-сайта, который оказался преподавателем computer science.  Этот преподаватель (Гонсало Наварро) очень тепло меня принял и сразу же ответил мне3 4. И только в процессе общения с ним я осознал, что видел его фамилию на множестве статей в этой области. Оказалось, я познакомился с одним из специалистов мирового уровня в области компактных структур данных (succinct data structure). Невежество может завести очень далеко.  Что же такое компактные структуры данных? Если вы изучали в последние десятилетия computer science, то могли сталкиваться с ними, но мне не доводилось встречаться с ними в процессе работы программистом, а если и доводилось, то я сразу же о них забыл. Но я считаю, что эти структуры данных обладают потрясающими свойствами.  Все мы пользуемся массивами и хэш-таблицами5, популярны также различные деревья. Нам не нужно полностью понимать их устройство, чтобы эффективно пользоваться их свойствами. А теперь я задаюсь вопросом, почему же люди не используют компактные структуры данных чаще.  Я решил, что стоит немного о них рассказать.  Компактные структуры данных Чтобы получить представление о компактных структурах данных, полезно сравнить их с компрессией (сжатием). Мы используем сжатие, чтобы уменьшить размер, необходимый для представления данных. Сжатие может быть полезным для экономии места на диске, для передачи меньшего количества данных по сети или для экономии памяти. Но чтобы делать со сжатыми данными что-то полезное, например, получать к ним доступ или запрашивать их, их нужно снова распаковать. А когда потом вам нужно будет сэкономить место, их нужно снова сжимать.  Компактные структуры данных устроены иначе: они хранят своё содержимое в компактном виде, как при сжатии, но этот компактный вид данных обладает полезными свойствами. Его можно использовать!  Похоже, эта область computer science возникла относительно недавно: многие важные структуры данных были изобретены за последние 25 лет.  В этой статье я расскажу о некоторых компактных структурах данных, с которыми мне довелось столкнуться. Не буду утверждать, что обладаю в этой области хоть каким-то опытом, я лишь имеют смутное представление о том, как работают эти структуры данных. Я ведь узнал о них всего несколько месяцев назад! Но, как и говорилось, чтобы их использовать, необязательно знать, как они устроены.  Rust При системном программировании разработчиков чаще обычного заботят конкретные характеристики структур данных (память, производительность). Меня поразило, что в этой сфере для компактных структур есть достаточно много потенциальных областей применения. Основная часть исследований по компактным структурам данных была проведена на C++. Лично я предпочитаю для системного программирования пользоваться Rust, поэтому я начал искать реализации на Rust. Я поделюсь обнаруженным в надежде, что другим разработчикам на Rust эта информация покажется интересной. Однако общее описание этих структур данных ни в коем случае не ограничивается одним Rust, так что, даже если вы не пишете на Rust, я всё же рекомендую почитать статью.  Битовые векторы Рассмотрим вектор (массив) битов. Например, [0, 1, 0, 1, 1, 0, 1, 0]  Сказанное ниже будет достаточно очевидным, но я всё равно считаю, что это важно подчеркнуть. Этот битовый вектор состоит из 8 элементов. Это значит, что он помещается в один байт памяти.  В 64-битном компьютере в один нативный integer помещается 64 бита. То есть битовый вектор из 64 элементов поместится в 8 байтов, что довольно прилично, если вспомнить, что в обычном массиве такое количество байтов занимает одно значение integer. Даже bool в Rust занимает байт хранилища.  Сами по себе битовые векторы не являются компактными структурами данных. В конечном итоге, компьютерная память — это огромный массив битов. Но компактные структуры данных стремятся, чтобы важен был каждый бит.  Также важно отметить, что структура данных может представлять себя в виде битового вектора, но на самом деле хранить информацию каким-то особым образом, ускоряющим определённые операции. Об этом мы и поговорим ниже.  Битовый вектор Rank/Select Битовый вектор rank/select — это базовая компактная структура данных, на основе которой созданы многие другие. Это битовые векторы, поддерживающие особые операции rank и select.  Давайте вернёмся к нашему битовому вектору: [0, 1, 0, 1, 1, 0, 1, 0].  Присвоим каждому элементу индекс (от 0 до 8, не включая 8).  Операция rank для указанного индекса считает количество битов, установленных в массиве до него:  То есть rank(0) равен 0, потому что в этой позиции ещё не установлено ни одного бита, rank(1) равен 1, rank(2) тоже равен 1 (в этой позиции находится 0, то есть другие дополнительные биты не установлены). rank(3) равен 2. rank(7) равен 4.  Операция select обратна операции rank. Можно передать ей ранг и получить индекс, в котором этот бит установлен.  То есть select(1) равен 1, select(2) равен 3, select(3) равен 4, а select(4) равен 6. Это идущие друг за другом индексы, в которых значение равно 1.  Компактная реализация битового вектора rank/select может выполнять эти операции за константное время. Чтобы поддерживать эти операции, достаточно лишь немного данных в дополнение к самому битовому вектору.  ▍ Сценарии использования В каких же ситуациях можно использовать такие структуры? Давайте рассмотрим простой пример.  Допустим, у нас есть большая строка, состоящая из множества маленьких строк. Мы хотим отслеживать, где все эти строки начинаются. Для этого можно использовать множество разных структур, и часто они подойдут лучше, но давайте используем битовый вектор rank/select.  Вот строка с позициями:  Hello$I am a string$I'm amazing$Traditional banana 01234567890123456789012345678901234567890123456789 0         10        20        30        40 Обратите внимание на особые маркеры $. На практике их можно закодировать как \0.  Итак, эта большая строка состоит из четырёх подстрок, которые можно рассматривать как массив:  [""Hello"", ""I am a string"", ""I'm amazing"", ""Traditional banana""] Можно идентифицировать каждую строку числом, являющимся индексом в этом массиве:  Hello$I am a string$I'm amazing$Traditional banana 0    1             2           3                   А теперь рассмотрим битовый вектор rank/select. В тех точках строки, где начинается подстрока и находится $, можно задать в векторе бит.  То есть мы получаем следующее:  Hello$I am a string$I'm amazing$Traditional banana 00000100000000000001000000000001000000000000000000      5             19          31 Теперь, когда у нас есть любая позиция, можно, по сути, при помощи rank определять, к какой подстроке она принадлежит — мы получим идентификатор подстроки. Например, позиция 12 находится в «I am a string», поэтому rank(12) возвращает 1.  Если прибавить 1 к идентификатору, то мы получим идентификатор следующей подстроки — полезное свойство.  Можно обратно преобразовать идентификатор подстроки в позицию при помощи select. Например, select(1) вернёт 6, позицию первого $ в строке, то есть начало ""I am a string"". Если прибавить к этому 1, то мы получим начало следующей подстроки. Значит, если мы хотим найти конец строки, то можем прибавить 1 к идентификатору подстроки и выполнить select для этого значения.  Это позволяет нам компактным образом хранить уникальные идентификаторы для срезов в блоках данных.  При использовании обычного битового вектора rank/select мы дополнительно должны хранить 1 бит, умноженный на длину строки, плюс ещё немного. Для строки длиной в мегабайт это дополнительные 128 килобайт плюс мелочь. Но если установленные биты относительно редки, можно для очень эффективного хранения этой дополнительной информации использовать разреженный битовый вектор rank/select, занимающий гораздо меньше пространства, чем необходимо было бы для исходного битового вектора.  ▍ Rust В Rust мне очень понравилось работать с vers, обладающим и превосходной производительностью, и минимальным оверхедом по сравнению с исходным битовым вектором. Автор крейта активно отвечает на отзывы и работает над множеством потрясающих новых возможностей.  Также стоит внимания библиотека sucds. Особенно любопытна её реализация SArray — реализация разреженного массива. Однако vers, на мой взгляд, имеет более удачную архитектуру для эффективного построения подобных структур данных, нежели sucds. Поэтому с радостью сообщаю, что vers тоже находится в процессе добавления разреженной реализации.  Вейвлет-матрица Название «вейвлет-матрица» похоже на нечто из научной фантастики. «Док, вейвлет-матрица потеряла синхронизацию! Мы не можем вернуться в 1985 год!». С ней связаны структуры «вейвлет-деревья», в том числе вейвлет-дерево квадрантов — тоже очень крутое название из научной фантастики.  Эти структуры данных обобщают rank/select на «произвольные алфавиты». Алфавит битового вектора состоит из нулей и единиц, но многие данные реального мира имеют более обширный алфавит. Особенно интересна в мире компактных структур данных ДНК, имеющая алфавит из своих нуклеотидов «G», «C», «A» и «T» (размер 4). Ещё один распространённый алфавит — это текст. Если вы используете UTF-8 и храните его байты, то алфавит имеет размер 2566.  В общем случае, чем меньше алфавит, тем лучше: структурам данных требуется меньше места, и они могут быть быстрее.  Давайте рассмотрим строку «banana» в некоем текстовом алфавите наподобие ASCII.  banana 012345 Теперь мы можем выполнять rank и select конкретных символов в алфавите. Давайте возьмём для примера букву (символ) a:  banana  1 2 3 rank(0, 'a') равен 0, так как по индексу 0 пока нет ни одной a. Но rank(5, 'a') вернёт нам ранг 3. Аналогично, select позволяет найти индекс символа в строке: select(2, 'a') вернёт индекс 3, то есть индекс второй a.  Внутри реализации вейвлет-матрицы используются битовые векторы rank/select.  ▍ Rust В Rust крейт vers также содержит реализацию структуры данных «вейвлет-матрица».  FM-индекс FM-индекс — это структура данных, позволяющая компактным образом хранить текст (любой вектор символов), но при этом допускающая выполнение важных операций запросов. Она использует не больше пространства, чем исходный текст (а в некоторых случаях и меньше).  Важные запросы здесь имеют следующий вид:   count(pattern) — считает количество вхождений паттерна (подстроки) в хранимом тексте, locate(pattern) — возвращает индекс каждого вхождения паттерна в исходном тексте.  Возможность загрузки огромной строки данных ДНК в памяти и быстрого нахождения в ней подстрок крайне важна в биоинформатике. Однако потенциально выиграть от использования FM-индекса может любая сфера применения с большим количеством текста, для которого нужны операции поиска.  ▍ Rust В Rust эту функциональность представляет крейт fm-index.  Изначально крейт fm-index для внутреннего битового вектора rank/select использовал крейт fid (того же автора). Но недавно я портировал fm-index для использования вместо него vers, что существенно повысило производительность fm-index.  Совместно с прекрасным автором fm-index мы работали над совершенствованием его API и функциональности; в этой области готовится ещё много нововведений, в том числе поддержка хранения и поиска больших строк, состоящих из множества подстрок.  Сбалансированные скобки Рассмотрим дерево данных, в котором каждый узел может иметь произвольное количество дочерних узлов. Такие структуры очень часто встречаются в программировании. В виде таких деревьев представлены и XML, и JSON; также могут выглядеть AST языков программирования.  Если вы хотите обеспечить возможность произвольной навигации по дереву, то вам понадобятся следующие операции с узлами деревьев:   parent(node): возвращает узел, дочерним узлом которого является указанный. first_child(node): возвращает первый дочерний узел узла. next_sibling(node): возвращает следующий родственный узел (в пределах родительского узла). previous_sibling(node): возвращает предыдущий родственный узел (в пределах его родительского узла).  Часто эти деревья представлены в памяти примерно так:  struct Node {    parent_idx: Option<usize>,    first_child: Option<usize>,    next_sibling: Option<usize>,    prev_sibling: Option<usize> } Достаточно большой объём информации на узел: в 64-битной операционной системе этот узел насчитывает 32 байта, или 256 бита. Сбалансированное дерево скобок позволяет этим же операциям занимать всего по 2 бита на узел! Вообще, есть и небольшой оверхед, используемый внутренним битовым вектором rank/select, и ещё немного в структуре данных управления, но всё равно места требуется на удивление мало.  Как это возможно? Если вкратце, то мы, по сути, кодируем дерево в виде последовательности скобок. Рассмотрим единичное дерево с корневым узлом a и двумя дочерними узлами b и c:      a   /  \  b    c Его можно представить как «(()())».  (()()) ab c Внешние скобки кодируют корневой узел a, а две внутренние пары () — его дочерние узлы. Так как у нас есть только открывающая скобка ( и закрывающая скобка ), эту информацию можно хранить в битовом векторе rank/select, где 1 — это открывающая, а 0 — закрывающая скобка.  (()()) 110100 А благодаря различным хитрым операциям с использованием rank() и select(), а также небольшой долей вспомогательной информации эта структура может поддерживать всю описанную выше навигацию, а также множество других операций.  ▍ Rust Потрясающий автор vers работает над реализацией на Rust ветви dev-bp крейта vers. Я активно использовал её в своей собственной работе и могу сказать, что она отлично выглядит!  Надстраивание этих структур Можно использовать эти и другие компактные структуры данных для создания новой функциональности с очень интересными свойствами.  Я работаю над обработкой XML в Rust, а в статье, упомянутой в начале поста, описан способ хранения данных XML с использованием компактных структур данных.  Для хранения дерева XML в ней применяется сбалансированное дерево скобок. Это не позволяет добавлять к узлам информацию, например, информацию тэгов XML (например, p в <p> и </p>). Но можно добавлять информацию к каждому узлу, храня разреженный битовый вектор rank/select для каждого тэга, размечающий соответствующие скобки дополнительной информацией.  В этом случае такой битовый вектор rank/select позволяет выполнять фундаментально новые операции: можно переходить к первому узлу-потомку с определённым тэгом без необходимости обхода промежуточных узлов!  Также в XML содержатся текстовые узлы. Их можно прикрепить к дереву, отслеживая, какая открывающая скобка имеет тэг текстового узла. В статье также применяется FM-индекс для хранения текстовых узлов и обеспечения возможности быстрых запросов к ним.  Возможно, XML — достаточно странная область применения для подобных техник, но представьте язык программирования, хранящий в таком виде своё AST. Огромные AST можно хранить относительно компактно, но при этом они всё равно будут поддерживать интересные операции поиска, что может открыть новые способы реализации компиляторов языков.  Заключение Теперь, когда я обнаружил эти структуры данных, я задаюсь вопросом, почему они раньше не встречались мне в моей работе программистом. Подозреваю, у них есть большой нераскрытый потенциал. Хотя структуры данных, использующих гораздо больше памяти, в общем случае могут выполнять многие из этих операций эффективнее, способность использовать меньше памяти сама по себе влияет на производительность. А до декабря прошлого года я даже не подозревал, что они существуют! Уверен, что многие разработчики могли бы придумать для них интересные сценарии применения.  Примечания 1. Примите свою растерянность и невежество. Смиритесь с тем, что будете уставать. Именно так вы учитесь чем-то новому.  2. Это статья Fast in-memory XPath search using compressed indexes  3. В последнее время я написал нескольким другим, и результаты были очень положительными! Теперь отправка писем преподавателям computer science — моё новое хобби и суперсила. Обычно это люди, которыми движет любопытство, и они с готовностью делятся своими находками. Они потрясающие!  Тот недавний опыт взаимодействия с преподавателями, а также с опенсорсными разработчиками компактных структур в Rust вернул мне крайне необходимую веру в человечество.  4. Я спросил профессора Наварро, можно ли упоминать его имя. Теперь вы все можете ему писать.  5. На самом деле, я знаю на удивление много программистов, не использующих их. В процессе ревью кода я встречал код, который обходит по всему массиву, чтобы найти одно значение, что запросто приводит к квадратичному времени. Пользуйтесь хэш-таблицами, ребята.  6.В некоторых ситуациях вам может понадобиться алфавит размером с сам Unicode. Постарайтесь избегать этого.  Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
68,68,Телескоп «Джеймс Уэбб» обнаружил сложную химию в одной из первичных галактик,Selectel,IT-инфраструктура для бизнеса,4.45,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-12," На фотографии выше — инфракрасное изображение, которое передал на землю космический телескоп NASA James Webb. Снимок сделала его бортовая камера ближнего инфракрасного диапазона NIRCam для программы JADES. Ее данные позволяют отобрать объекты для дальнейшего изучения с помощью спектроскопических наблюдений.  Для одной из галактик, JADES-GS-z14-0 (показана на врезке), было определено красное смещение 14,3 — что сделало ее новым рекордсменом по удаленности. Такая величина соответствует времени возникновения менее 300  млн лет после Большого взрыва.  Исследователи заявили, что наблюдаемая галактика неожиданно яркая и химически сложная для объекта той первобытной эпохи.   Мы в Selectel готовим новый сервис. Если арендуете серверы в рабочих или личных проектах, нам очень поможет ваш опыт — записывайтесь на короткое онлайн-интервью. За участие подарим плюшевого Тирекса и бонусы на услуги Selectel.   Источники: NASA (Национальное управление по аэронавтике и исследованию космического пространства); ESA (Европейское космическое агентство); CSA (Канадское космическое агентство); STScI (Институт исследований космоса с помощью космического телескопа имени Хаббла); Brant Robertson, UC Santa Cruz (Брант Робертсон, Калифорнийский университет в Санта-Крусе); Ben Johnson, CfA (Бен Джонсон, Гарвард-Смитсоновский центр астрофизики); Sandro Tacchella, Cambridge (Сандро Таккелла, Кембриджский университет); Marcia Rieke, University of Arizona (Марсия Рике, Университет Аризоны); Daniel Eisenstein, CfA (Дэниел Эйзенштейн, Гарвард-Смитсоновский центр астрофизики); Phill Cargile, CfA (Филлип Каргил, Гарвард-Смитсоновский центр астрофизики).  Благодаря полученным данным, астрономы из Университета Аризоны смогли узнать больше об этой удивительной галактике. Ее наблюдаемое состояние — чуть менее 300  млн лет — соответствует всего 2% от нынешнего возраста. Мы получили редкую возможность заглянуть в раннюю эпоху тогда еще совсем молодой Вселенной.  Результаты, опубликованные в журнале Nature Astronomy, основываются на предыдущей находке исследователей. В 2024  году сообщалось, что галактика JADES-GS-z14-0  — самая далекая из когда-либо наблюдавшихся. Первоначальное открытие установило не только рекордное расстояние, но и неожиданную яркость. Новое же исследование целиком посвящено изучению химического состава и эволюционным процессам.  Часто встречающаяся аббревиатура JWST — James Webb Space Telescope, космический телескоп «Джеймс Уэбб» (прим. переводчика). Работа проводилась в рамках крупной программы JADES (JWST Advanced Deep Extragalactic Survey, передовой углубленный внегалактический обзор), специально предназначенной для изучения далеких галактик.  Мы не просто наткнулись на что-то неожиданное. Казалось бы, исследование намеренно разработано для поиска далеких галактик. Однако эта побила все рекорды. Она не просто яркая и протяженная сама по себе. У нее сложный химический состав, что совершенно неожиданно для столь раннего этапа развития Вселенной.  — Кевин Хейнлайн, соавтор нового исследования и доцент-исследователь в обсерватории Стюарда при Университете Аризоны.  Тот факт, что мы обнаружили эту галактику в крошечной области, означает, что похожих объектов должно быть больше. Телескоп не может охватить всю небесную сферу. Будь это осуществимо, такие экстремальные объекты встречались бы чаще.  — ведущий автор исследования Якоб Хелтон, аспирант обсерватории Стюарда. Исследовательская группа использовала несколько инструментов на борту JWST. Первый из них мы уже упоминали — это камера ближнего инфракрасного диапазона NIRCam (Near-Infrared Camera), разработанная под руководством профессора астрономии Университета Аризоны Марсии Рике. Другой блок на телескопе — прибор среднего инфракрасного диапазона MIRI (Mid-Infrared Instrument), который и обнаружил нечто необычное: значительное количество кислорода.  В астрономии все, что тяжелее гелия, условно считается «металлом». Для «производства» тяжелых химических элементов требуется не одно поколение звезд. Ранняя вселенная содержала только водород, гелий и незначительные следы лития. Обнаружение существенного количества кислорода в JADES-GS-z14-0 приводит к предположению, что галактика формировала звезды в течение потенциально 100  млн лет до своего наблюдаемого состояния.  — ведущий автор исследования Якоб Хелтон, аспирант обсерватории Стюарда.  Хронология вселенной.  Достоверно неизвестно, когда первые звезды начали излучать свет. По современным представлениям они должны были сформироваться где-то после периода рекомбинации, когда образовались атомы водорода и гелия, то есть через 380 000  лет с момента Большого взрыва. Самые старые известные галактики возникли спустя 400  млн лет. Ультрафиолет, излучаемый первыми звездами, ионизировал газообразный водород, заполнявший вселенную. Появление ионов водорода и свободных электронов, положило начало эпохе реионизации и ознаменовало конец Темных веков Вселенной.  Для возникновения кислорода галактике необходимо было заблаговременно сформировать первое поколение звезд. Эволюционируя и взрываясь сверхновыми, они выбрасывали кислород в межзвездное пространство, из которого и образовались следующие звезды.  Произошел очень сложный цикл, в результате которого в галактике получилось столько кислорода, сколько мы видим. Это ошеломляет!  — Джордж Рике, профессор астрономии и старший автор исследования. Полученные данные свидетельствуют о том, что процесс зарождения звезд начался даже раньше, чем считали ученые. Сдвигается общепринятая временна́я шкала и наши представления о том, когда могли начать формироваться первые галактики после Большого взрыва.  Наблюдение фокусировалось на невероятно малом участке неба и продолжалось девять дней. Съемки NIRCam заняли 167  часов, MIRI — 43.  Астрономам из Университета Аризоны повезло. Галактика оказалась в идеальном месте для наблюдения с помощью MIRI. Смести они телескоп всего на долю градуса в любом направлении — возможность получить важные данные в среднем ИК‑диапазоне была бы упущена.  Представьте себе песчинку на ладони. Теперь мысленно поместите ее на небо. Вот такого размера рассматривался объект.  — Якоб Хелтон, ведущий автор исследования, аспирант обсерватории Стюарда. Существование до такой степени развитой галактики на столь раннем этапе космической истории — настоящий стресс‑тест для общепринятых теоретических моделей их формирования.  Наше участие — результат лидерства Университета Аризоны в области инфракрасной астрономии с самого момента ее зарождения в середине 60‑х годов. Первая крупная группа по ИК‑астрономии была тоже у нас. В Лунной и планетарной лаборатории при университете (Lunar and Planetary Laboratory, LPL) работали Джерард Койпер, Фрэнк Лоу и Гарольд Джонсон.  — Джордж Рике, профессор астрономии и старший автор исследования. То, что ученые получают возможность непосредственно наблюдать галактики, существовавшие в период зарождения Вселенной, дает важнейшее представление о ее развитии. Происходит движение к пониманию этапов, которые привели от появления простых элементов до возникновения сложных химических соединений — тех, что необходимых для жизни, по крайней мере, в известной нам форме.  Мы переживаем невероятное время в истории астрономии. Галактики намного превосходят все, что когда-либо находили люди. Сейчас есть возможность наблюдать их самыми разными способами, понимать их. Это настоящее волшебство.  — Кевин Хейнлайн, соавтор нового исследования и доцент-исследователь в обсерватории Стюарда при Университете Аризоны."
69,69,"Нейросетевой переводчик в командной строке, или Приручаем API Ollama и OpenWebUI",HOSTKEY,"IT-инфраструктура: сервера, VPS, GPU, коло",74.37,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-12,"Связку из Ollama и OpenWebUI я использую больше года, и она является моим рабочим инструментом в части работы с документацией и контентом и по-настоящему ускорила работу по локализации документации HOSTKEY на другие языки. Но жажда исследований не отпускает, и с появлением относительно вменяемой документации к API у OpenWebUI возникло желание написать что-то, автоматизирующее работу. Например, перевод документации из командной строки.ИдеяВ HOSTKEY клиентская документация собирается в Material for MkDocs и в исходниках, хранящихся в GIT, представляет собой набор файлов в Markdown формате. А раз мы имеем текстовые файлы, то кто мешает мне не копировать их текст из VS Code и вставлять в чат OpenWebUI в браузере, а запускать скрипт из командной строки, который бы отправлял файл со статьей нейросетевой модели, получал перевод и записывал его назад в файл.В теории в дальнейшем так можно было бы сделать масс-обработку файлов и запускать автоматизированный черновой перевод на новые языки всех массивов или при создании статьи на одном языке одной командой переводить и клонировать их на множество других. С учетом роста числа переводов (сейчас это русский, английский, турецкий, в работе французский, а в планах испанский и китайский) всё это ускорило бы работу отдела документации. Поэтому разрабатываем план:Берем исходный файл .md файл;Скармливаем его нейросетевой модели;Получаем перевод;Записываем файл с переводом;Приложениями для ИИ, машинного обучения и науки о данных на GPU-серверах с картами NVIDIAAI & Машинное обучение🔹PyTorch🔹AI-чатбот🔹TensorFlow🔹Apache Spark🔹ComfyUIНаука о данных🔹Jupyter Notebook🔹JupyterLab🔹Anaconda🔹Apache AirflowЗаказать серверИзучаем APIПервый возникающий вопрос: а зачем на OpenWebUI, когда можно сразу «скормить» файл Ollama? Да, это можно сделать, но, забегая вперед, скажу, что использование «прокладки» в виде OpenWebUI было правильным решением. Причем API Ollama еще хуже документировано, чем OpenWebUI.Описание API OpenWebUI доступно в swagger формате по адресу https://<IP или домен инстанса>/docs/. Из него видно, что вы можете управлять как Ollama, так и самим OpenWebUI и обращаться к нейросетевым моделям в синтаксисе API от OpenAI. Последнее спасло меня, так как понять, какие параметры и как передавать, было не очень понятно, и пришлось обратиться к документации по API от OpenAI.Что в итоге? Нам нужно начать чат и передать в модель системный промт, в котором объяснить ей, что делать, а затем текст для перевода, а также параметры, такие как температуру и размер контекста (max_tokens).В синтаксисе API от OpenAI нам надо сделать POST запрос к OpenWebUI на <адрес OpenWebUI>/ollama/v1/chat/completions, содержащий следующие поля:Authorization: Bearer <API ключ доступа к OpenWebUI> Content-Type: application/json data body:         '{           model: <нужная нам модель>,           messages: [             {               role: ""system"",               content: <системный промт>             },             {               role: ""user"",               content: <наш файл для перевода>             }           ],           temperature: 0.6           max_tokens: 16384         }'Как видно, тело запроса нужно передавать в JSON-формате, и в нем же мы получим ответ.Писать я решил всё в виде bash-скрипта (для меня универсальное решение, так как можно запускать скрипт и на удаленном Linux-сервере, и локально даже из Windows через WSL), поэтому будем использовать cURL в Ubuntu 22.04. Для работы с JSON-форматом ставлю утилиту jq.Далее создаю в OpenWebUI пользователя для нашего переводчика, получаю для него API-ключ, устанавливаю несколько нейросетевых моделей для теста и... у меня ничего не получается.Версия 1.0Как я писал ранее, нам необходимо сформировать data-часть запроса в JSON-формате. Основной код скрипта, который принимает параметр в виде имени файла для перевода и отправляет запрос, а затем расшифровывает ответ, следующий:   local file=$1      # Read the content of the .md file     content=$(<""$file"")      # Prepare JSON data for the request, including your specified prompt     request_json=$(jq -n \         --arg model ""gemma2:latest"" \         --arg system_content ""Operate as native translator from US-EN to TR. I will provide you text in Markdown format for translate. The text is related to IT.\nFollow this instructions:\n\n- Do not change Markdown format.\n- Translate text, considering the specific terminology and features.\n- Do not provide a description of how and why you made such a translation.\         '{           model: $model,           messages: [             {               role: ""system"",               content: $system_content             },             {               role: ""user"",               content: $content             }           ],           temperature: 0.6,           max_tokens: 16384         }')      # Send POST request to the API     response=$(curl -s -X POST ""$API_URL"" \         -H ""Authorization: Bearer $API_KEY"" \         -H ""Content-Type: application/json"" \         --data ""$request_json"")      # Extract translated content from the response (assuming it's in 'choices[0].message.content')     translated_content=$(echo ""$response"" | jq -r '.choices[0].message.content')Как видно, я использовал модель gemma2 на 9B, системный промт для перевода с английского на турецкий язык и просто передал в запросе содержимое файла в Markdown формате. API_URL указывает на http://<IP адреc OpenWebUI:Порт>/ollama/v1/chat/completion.Тут и была моя первая ошибка — необходимо было подготовить текст для JSON формата. Для этого в скрипте нужно было поправить его начало:    # Read the content of the .md file     content=$(<""$file"")      # Escape special characters in the content for JSON     content_cleaned=$(echo ""$content"" | sed -e 's/\r/\r\\n/g' -e 's/\n/\n\\n/g' -e 's/\t/\\t/g' -e 's/""/\\""/g' -e 's/\\/\\\\/g')      # Properly escape the content for JSON     escaped_content=$(jq -Rs . <<< ""$content_cleaned"")Экранировав специальные символы и переведя .md файл в JSON, а в теле формирования запроса добавить новый аргумент.--arg user_content ""$escaped_content"" \Который и передавать в роли ""user"". Дооформляем скрипт и пытаемся улучшить промт.Промт для переводаИзначальный промт для переводчика у меня был такой, как в примере. Да, он достаточно неплохо переводил технический текст с турецкого на английский, но были свои проблемы.Мне необходимо было единообразие перевода определенных конструкций Markdown-разметки, таких как заметки, примечания и т. п. Также хотелось, чтобы переводчик не переводил на турецкий UX-элементы как системы управления серверами Invapi (он у нас пока что на английском и русском), так и интерфейсов ПО, потому что с большим числом языков поддержка локализованных версий превращалась в мини-ад. Сложности добавляло еще то, что в документации используется нестандартные конструкции для кнопок в виде жирного зачеркнутого текста КНОПКА (конструкция ** **). Поэтому в OpenWebUI был отлажен системный промт следующего вида:You are native translator from English to Turkish.I will provide you text in Markdown format for translate. The text is related to IT. Follow these instructions:- Do not change Markdown format.- Translate text, considering the specific terminology and features. - Do not provide a description of how and why you made such a translation.  - Keep on English box, panels, menu and submenu names, buttons names and other UX elements in tags '** **' and '\~\~** **\~\~'.- Use the following Markdown constructs: '!!! warning ""Dikkat""', '!!! info ""Bilgi""', '!!! note ""Not""', '??? example'. Translate 'Password"" as 'Şifre'. - Translate '## Deployment Features' as '## Çalıştırma Özellikleri'.- Translate 'Documentation and FAQs' as 'Dokümantasyon ve SSS'.- Translate 'To install this software using the API, follow [these instructions](../../apidocs/index.md#instant-server-ordering-algorithm-with-eqorder_instance).' as 'Bu yazılımı API kullanarak kurmak için [bu talimatları](https://hostkey.com/documentation/apidocs/#instant-server-ordering-algorithm-with-eqorder_instance) izleyin.'Теперь этот промт надо было проверить на стабильность на различных моделях, так как нужно было добиться и достаточно качественного перевода, и не пожертвовать скоростью. Gemma2 9B хорошо справляется с переводом, но упорно игнорировала требование не переводить UX-элементы. Модель DeepSeekR1 в версии 14B также давала большое число ошибок, а местами вообще переключалась на иероглифы. Из всех моделей лучше всего показала себя Phi4-14B. Модели с большим числом параметров было использовать сложнее, так как всё «крутится» на сервере с A5000 с 24 Гб видеопамяти. Единственное, что я взял вместо дефолтной модели Phi4-14B с квантизацией q4 модель в менее сжатом формате (q8).полная версия скрипта с дебаг-вставками#!/bin/bash  # Check for the presence of a parameter with a file path if [ -z ""$1"" ]; then     echo ""Error: Please provide the full path to the file for translation.""     exit 1 fi  # Check if the file exists if [ ! -f ""$1"" ]; then     echo ""Error: File $1 does not exist.""     exit 1 fi  # API parameters API_URL=""http://<open_webui_ip:port>/api/chat/completions"" API_KEY=""sk-*****************************""  # Check server availability if ! curl -s -o /dev/null -I -w ""%{http_code}"" ""$API_URL"" | grep -q ""200""; then     echo ""Error: API server is not reachable or returned an error.""     exit 1 fi  # Translation function using ollama translate_file() {     local file=$1      # Read the content of the .md file     content=$(<""$file"")      # Escape special characters in the content for JSON     content_cleaned=$(echo ""$content"" | sed -e 's/\r/\r\\n/g' -e 's/\n/\n\\n/g' -e 's/\t/\\t/g' -e 's/""/\\""/g' -e 's/\\/\\\\/g')      # Properly escape the content for JSON     escaped_content=$(jq -Rs . <<< ""$content_cleaned"")      # Prepare JSON data for the request, including your specified prompt     request_json=$(jq -n \         --arg model ""entrtranslator"" \         --arg user_content ""$escaped_content"" \         '{           model: $model,           messages: [             {               role: ""user"",               content: $user_content             }           ],           temperature: 0.6,           max_tokens: 16384,           stream: false         }')  # Debug: Print the JSON request payload #    echo ""Debug: JSON Request Payload:"" #    echo ""$request_json"" #    echo ""---------------------------""      # Send POST request to the API     response=$(curl -s -X POST ""$API_URL"" \         -H ""Authorization: Bearer $API_KEY"" \         -H ""Content-Type: application/json"" \         --data ""$request_json"")  # Debug: Print the raw API response #    echo ""Debug: Raw API Response:"" #    echo ""$response"" #    echo ""---------------------------""      # Check if the request was successful     if [ $? -ne 0 ]; then         echo ""Error: Failed to translate $file""         return 1     fi      # Extract translated content from the response (assuming it's in 'choices[0].message.content')     translated_content=$(echo ""$response"" | jq -r '.choices[0].message.content')  # Debug: Print the extracted translated content #    echo ""Debug: Extracted Translated Content:"" #    echo ""$translated_content"" #    echo ""---------------------------""      # Check if the translation was successful     if [ -z ""$translated_content"" ] || [ ""$translated_content"" == ""null"" ]; then         echo ""Error: Translation failed for $file (empty or null content)""         return 1     fi          translated_content=$(echo ""$translated_content"" | sed -e 's/^```markdown//')      # Save the translated content to a new file in the same directory     translated_file=""$(dirname ""$file"")/translated_$(basename ""$file"")""     echo ""$translated_content"" > ""$translated_file""      echo ""Translation for $file completed."" }  # Process the provided file file=""$1"" echo ""Processing file $file"" cp ""$file"" ""${file}.en"" if translate_file ""$file""; then     mv ""$(dirname ""$file"")/translated_$(basename ""$file"")"" ""$file""     fromdos ""$file"" else     echo ""Failed to translate $file, original file kept."" fiРезультат тестовВсё заработало. Но с некоторыми оговорками. Основная проблема была в том, что новые запросы почему-то не перезапускали сессию чата, и модель жила в прежнем контексте и через пару-тройку файлов теряла системный промт. В результате, если первые пара запусков давали достаточно вменяемый перевод, то потом модель переставала воспринимать инструкции и вообще оставляла текст на английском. Добавление параметра stream: false ситуацию не спасало.Вторая проблема — это галлюцинации, но в части игнорирования инструкций «не переводить UX». Я пока что не смог добиться стабильности в данном вопросе, и если в чате OpenWebUI я могу «ткнуть носом» модель в то, что она зачем-то перевела название кнопок и меню, и она с 2–3-го раза выдавала нужное, то тут необходимо было только запускать скрипт заново, и срабатывал он иногда раза с пятого-шестого.Третья проблема — это тюнинг промта. Если в OpenWebUI я мог создать кастомный промт и через раздел Workspace — Promts задать ему слеш-команду вида /en_tr, то в скрипте нужно было переписывать код, причем в не очень удобном формате. То же касается и параметров модели.Версия 2.0Поэтому было решено пойти другим путем. В OpenWebUI можно задать кастомные модели-агенты, в которых прописать как системный промт, так и гибко настроить их параметры (и даже использовать RAG) и права. Поэтому я создал агента-переводчика в разделе Workspace — Models (название модели написано мелко и будет «entrtranslator»).Если попробовать подставить новую модель в текущий скрипт, то он выдаст ошибку. Это происходит потому, что прежний вызов просто передавал параметры в Ollama через OpenWebUI, для которой «модели» entrtranslator просто не существует. Изучение API OpenWebUI методом проб и ошибок привело к другому вызову самого OpenWebUI: /api/chat/completions.Теперь вызов нашего нейросетевого переводчика можно записать так:    local file=$1      # Read the content of the .md file     content=$(<""$file"")      # Escape special characters in the content for JSON     content_cleaned=$(echo ""$content"" | sed -e 's/\r/\r\\n/g' -e 's/\n/\n\\n/g' -e 's/\t/\\t/g' -e 's/""/\\""/g' -e 's/\\/\\\\/g')      # Properly escape the content for JSON     escaped_content=$(jq -Rs . <<< ""$content_cleaned"")      # Prepare JSON data for the request, including your specified prompt     request_json=$(jq -n \         --arg model ""entrtranslator"" \         --arg user_content ""$escaped_content"" \         '{           model: $model,           messages: [             {               role: ""user"",               content: $user_content             }           ],           temperature: 0.6,           max_tokens: 16384,           stream: false         }')      # Send POST request to the API     response=$(curl -s -X POST ""$API_URL"" \         -H ""Authorization: Bearer $API_KEY"" \         -H ""Content-Type: application/json"" \         --data ""$request_json"")      # Extract translated content from the response (assuming it's in 'choices[0].message.content')     translated_content=$(echo ""$response"" | jq -r '.choices[0].message.content')Где API_URL принимает вид http://<IP адреc OpenWebUI:Порт>/api/chat/completions.Теперь у вас есть возможность гибко настраивать параметры и промт через веб-интерфейс, а также использовать данный скрипт для переводов на другие языки.Этот способ сработал и позволяет создавать ИИ-агентов для использования в bash-скриптах не только для перевода, но и для других нужд. Процент «непереводов» снизился, и осталась только проблема, когда модель не хочет игнорировать перевод UX-элементов.Что дальше?Дальше стоит задача добиться большей стабильности работы, хотя уже даже сейчас можно работать с текстами из интерфейса командной строки, и модель не срабатывает только на больших текстах (больше 16K ставить не позволяет видеопамять, модель начинает тормозить). Это можно сделать как улучшением промта, так и тюнингом параметров модели, которых достаточно много.Всё это позволит запустить автоматическое создание черновиков переводов на всех поддерживаемых языках, как только будет создан текст на английском языке.Ну а далее есть идея подключить базу знаний с переводом элементов интерфейсов Invapi и других значений элементов меню на сайте (и ссылок на них), чтобы при переводе не приходилось вручную править ссылки и наименования в статьях. Но работа с RAG в OpenWebUI через API — это тема для отдельной статьи.P.S. После написания данной статьи была анонсирована модель Gemma3, которая возможно займет место Phi4 в переводчиках, так как она поддерживает 140 языков при контексте до 128K."
70,70,Миссия выполнима: стандартизировать производственный процесс в крупной компании и учесть запросы всех продуктовых команд,МТС,Про жизнь и развитие в IT,2233.29,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-03-12,"Хабр, привет! Это Виталий Колесников из МТС Диджитал. Недавно тут и тут я поделился кейсом, как моя команда создала и внедрила стандартный производственный процесс с нуля. Но внедрить процесс — это полдела, дальше нужно его развивать. Как и хороший продукт, он меняется в зависимости от потребностей рынка и бизнеса. К тому же у пользователей есть свои запросы, а у производства продукта — свой жизненный цикл. Сегодня поделюсь опытом, как нам удается поддерживать единообразие процесса и при этом учитывать тенденции рынка, потребности команд и ЖЦ продуктов. Изменения СПП. План действийДля начала немного базы. Можно сказать, стандартный производственный процесс (СПП) — это набор объектов:эпик — крупная фича;история — более мелкая атомарная фича.А они, в свою очередь, состоят из:статусов жизненного цикла;обязательных и необязательных атрибутов — ниже расскажу, что к ним относится;автоматизаций между объектами;автоматизации со смежными системами производства: разработка, тестирование и так далее.Что такое обязательные и необязательные атрибуты? Первые — это те, без которых нельзя создать и идентифицировать задачу: тема, имя эпика или истории, автор и так далее. Необязательные — это дополнительные атрибуты: связь с другими задачами, период или квартал, сроки исполнения.Изменения, которые вносятся в процесс, мы делим на две большие категории: минорные: добавление новых необязательных атрибутов и статусов, переходов между статусами;мажорные, или крупные: новые объекты, автоматизации, добавление или изменение обязательных атрибутов и статусов.Дальше действуем так. Сначала анализируем работу продуктовой команды и думаем, какие возможности можем предложить в пределах нашего workflow. Если выход найден — отлично, это самый простой сценарий. Но если пониманием, что стандартный процесс не отвечает потребностям, прорабатываем новые сущности. Это, например, таски для работы с дизайном, архитектурой, данными, DevOps.Перед созданием сущности обязательно проводим исследования и опросы в других производственных командах. Так мы понимаем, востребованы ли изменения у широкой аудитории. А потом уже с учетом комплексной обратной связи разрабатываем новый процесс — то есть объект со своим жизненным циклом и атрибутами. Внося любые изменения, мы сохраняем единую стилистику наименований статусов, переходов и опорную функциональность для удобства работы с новой сущностью в стандартном workflow. Соблюдаем ролевую, статусную, приоритетную модель и схему переходов.Есть еще один важный момент — SLA (Service Level Agreement). Это то, как быстро мы реагируем на запросы команд. В среднем минорные изменения мы вводим в пределах 2 дней. На крупные уходит от 7 до 14 дней. Так работают scrum-команды, это стандартный двухнедельный спринт, и он комфортен для всех сторон. Если запрос от команды поступает к нам в середине или конце текущего спринта, анализируем его мы уже в следующем.Кто описывает измененияЗа ключевые процессы по направлениям технологических практик (Agile, архитектура, разработка, тестирование, DevOps и другие) в нашей компании отвечают Владельцы процессов. Каждый Владелец может вносить изменения в действующие процессы, за которые он отвечает внутри СПП. Например, запланировать автоматизации, добавить или изменить статусы, добавить метрики. При этом он придерживается принципов построения СПП. Если изменение мажорное (по сути это создание нового процесса), мы запускаем пилотную версию, тестируем, собираем ОС и дорабатываем его. Минорные изменения касаются уже существующего процесса: с ними мы действуем по SLA.Тонкости внедрения СПП Product FactoryВ предыдущем посте я рассказывал, что мы перевели на СПП сначала 5, 15, 80 продуктов, потом 300 и так далее. Конечно, сделать это непросто: сперва нужно проанализировать процесс, который уже есть в конкретной команде, подсветить слабые места, потом предложить решение. И, главное, привести сильные аргументы, чтобы ребята поняли, зачем им вообще нужно входить в эту историю. Будем честны, к изменениям многие из нас поначалу настроены скептически. Дальше опишу, как именно мы анализируем процесс в команде. Начинаем с продуктовой составляющей. На первой встрече просим озвучить цели, ценности и позиционирование продукта. Смотрим на структурированность бэклога, то есть нарезку на глобальные фичи (эпики) и атомарные ценности и фичи (истории). Если в команде продукта есть такая иерархическая структура, смотрим на названия сущностей — у любого эпика или истории они должны быть понятны. Ведь то, что команда запланировала в разработку, должно иметь прозрачную ценность и пользу для клиента, соответствовать условиям оценки и приема в работу (DoR, Definition of Ready), критериям приемки фичи и ее метрикам. Мы должны суметь это измерить и протестировать. Часто команды не задумываются об этом, тогда как переосмысление названия может привести к корректировке входных требований на разработку, DoD и AC по задаче — о них ниже.Если названия фичей понятны и укладываются в цели и ценность продукта, смотрим на критерии: DoD (Definition of Done) — показывает, что нужно сделать команде и после чего можно считать историю завершенной;AC (Acceptance Criteria) — показывает, что нужно сделать, чтобы удовлетворить потребности пользователя. Если с критериями все хорошо, анализируем процессную составляющую — как организовано производство, Agile и коммуникация. Здесь нас интересует, насколько часто продукт релизится, работает команда по Scrum или Kanban, сколько длятся спринты. Если частота поставки релиза максимум раз в две недели, идем дальше. Если реже, например, раз в три недели, в один или три месяца — начинаем разбираться. Почему релизы могут редко происходить? Причин много: плохая декомпозиция фичей на инкременты, недостаточная проработка фичей до планирования и в итоге некорректная оценка задач, которые уже взяли в работу. Еще влияет архитектура продуктов. Например, монолитная не позволит выводить быстрые и частые релизы: для этого команде нужны значительные ресурсы, ведь любое изменение в одном компоненте может приводить к доработкам на стороне других модулей системы. Тогда и тестирование становится сложной задачей.Чтобы сократить срок поставки, начинать все-таки с чего-то нужно. Вот несколько советов:не пытайтесь объять необъятное, не планируйте большой объем работ внутри истории. Иначе высок риск не уложиться в спринт;максимально распараллельте работу и распределите ее по разным спринтам;выделите часть Discovery из истории. Discovery — это большой кусок, связанный с CustDev. Может получиться, что гипотеза не подтвердится и в итоге вы не будете выпускать эту фичу, а какая-то часть работы по ней уже будет начата. Поэтому рекомендую запланировать Discovery на более ранние периоды;отложите реализацию истории на другой спринт, если на этапе проектирования вы понимаете, что новая фича увеличит пользовательскую нагрузку на систему. Тогда в первом спринте усильте инфраструктуру необходимыми ресурсами: количество ядер, памяти, диски для хранения;если для реализации истории нужно настроить интеграцию с внешней системой, написать метод и прорубить сетевые доступы — запланируйте их в первую очередь. Так к моменту разработки и тестирования фичи на стороне интеграции все будет готово.Если вы учтете все перечисленное, историю, скорее всего, удастся реализовать за один спринт.Почему я акцентирую на этом внимание? Предлагаемый нами workflow заточен под инкрементальную разработку: на реализацию эпика может уйти три месяца, истории — не более двух недель. Чтобы все получилось, мы стараемся детально по маленьким кусочкам разбирать процессы: проектирование, аналитика, груминг (англ. PBR, Product Backlog Refinement, уточнение бэклога продукта по Agile), разработка, тестирование и развертывание.Бывают ситуации, когда предлагаемый флоу командам не подходит — об этом мы узнаем, когда анализируем цели, критерии и частоту поставки релиза. Причина в разной зрелости производственных процессов разных продуктов. Поэтому перед тем, как переводить команду на стандартный workflow, мы дополнительно перестраиваем ее внутренние процессы. Обычно оптимизация и реинжиниринг происходят небыстро, вдобавок нужно найти общий язык с самой командой и заручиться ее поддержкой. Как уже говорил выше, в этом помогают сильные аргументы. Сотрудники должны увидеть, что изменения действительно полезны и, даже если сейчас придется дополнительно над чем-то поработать, в долгосрочной перспективе процесс станет удобнее и прозрачнее для всей компании.Жизненный цикл производства — почему это важно?Преднастроенный workflow эпиков и историй включает все этапы производства, которые я перечислил выше. Каждому из них соответствует отдельный статус или их группа. Для каждого объекта настроены проверки, обязательность и переходы. Проскочить или проигнорировать важный для производства этап не получится. Командам, в которых есть все необходимые роли, будет легко организовать работу внутри стандартного workflow. Для них все уже готово: автоматизации Jira связаны с ролями и этапами — значит, работать с объектами получится быстро и без влияния человеческого фактора. Можно строить статистику по этапам, выявлять слабые места — например, где и на каком шаге застряла история или как долго она проходила аналитику, разработку, код-ревью, тестирование. Но если один сотрудник совмещает несколько ролей, работа со всеми статусами флоу становится избыточной. Давайте на примере. Представьте, что разработчик — это еще и архитектор, и аналитик. Конечно, он не сможет выполнить всю это работу на 100% качественно. Для полного инкремента ему нужно проархитектурить, проаналитить задачу и разработать код. Лучше он выполнит именно свою часть — разработку. В итоге в прод можно выпустить фичу, которая снизит производительность продукта.Такие проблемы как раз и позволяет выявить наш СПП. Метрики настроены на каждом процессе СПП, так что можно легко понять, как с каждым из этапов ЖЦ производства работает команда. А значит, CTO смогут вовремя реагировать и принимать меры.В планах реализовать процессы для малых инженерных команд (с ограниченным составом ролей), которые позволят разрабатывать и выпускать релизы с более простым флоу. Еще мы развиваем процесс внутри СПП Product Factory для автоматизированного управления релизами — в следующий раз расскажу об этом."
71,71,Конфигурируемая тайловая разрезка: ускоряем отрисовку карты изменением данных,2ГИС,Главные по городской навигации,144.21,"Программное обеспечение, Реклама и маркетинг, Поисковые технологии",2025-03-12,"Привет, меня зовут Сергей Загребин, я из команды разработки графического движка 2ГИС. Кроме рендеринга, мы также отвечаем за формат доставляемых офлайн- и онлайн-данных. В этой статье расскажу, как мы игрались со способом разбиения картографических данных на тайлы и искали баланс между размером офлайн-пакетов и производительностью в рантайме.Надеюсь, этот материал будет полезен не только тем, кто занимается доставкой картографических данных, но и всем, кому интересно, как работает карта 2ГИС. Разбиение карты на тайлыВ нашем движке мы используем координаты, приближенные к координатам проекции Web Mercator в сантиметрах. Они преобразованы таким образом, чтобы весь диапазон значений проекции был растянут на все значения int32.Мир в этой проекции можно представить в виде квадродерева квадратных тайлов:Тайл уровня 0 (z0) содержит весь мир.Он делится на четыре z1-тайла.Каждый из них, в свою очередь, на четыре z2-тайла и т.д. В наших тайлах используется локальная система координат, ограниченная диапазоном значений uint16, что позволяет для уровня z16 добиться сантиметровой точности. Так мы сокращаем вдвое размер исходных координат, при этом их можно восстановить по локальным координатам внутри тайла и положению самого тайла в квадродереве.Когда мы говорим о тайловой разрезке, мы должны ответить на два вопроса: На тайлы каких уровней квадродерева мы хотим выгрузить все наши данные?И на каких зумах (масштабах) мы хотим тайлы этих уровней загружать?Опустив детали, связанные с разными экранами и разрешениями, значения зумов можно представить аналогично уровням квадродерева:зум 0 — на экране девайса виден весь мир, зум 1 — только четверть мира,зум 2 — одна шестнадцатая и т.д. Зум сопоставим с высотой камеры и используется в стилях, которые определяют отрисовку выгруженных объектов по их атрибутам.Связь тайловой разрезки с размером пакета и перфомансомСкачивая через приложение свой город, пользователь в числе прочего получает офлайн-пакет карты — именно с ним можно пользоваться 2ГИС без интернета.Размер офлайн-пакета зависит от того, на тайлы каких уровней мы разрезаем наши данные:Выгрузка одной и той же геометрии в тайлы разных уровней приводит к множественному дублированию. Ограничение выгрузки, исходя из природы данных, (например, исключение трёхмерных домов на дальних зумах) снижает дублирование, однако при большом количестве уровней всё равно не устраняет его полностью.Сама разрезка данных на тайлы добавляет новые искусственные вершины на границах тайлов, увеличивая объём данных.Даже «почти пустые» тайлы (например, если это совсем мелкий тайл в глуши, содержащий единственный квадратный участок леса) занимают место.Для отрисовки наших тайлов мы используем современные графические API — Vulkan, Metal, а для старых девайсов OpenGL. Все они мотивируют использовать батчинг для ускорения рендеринга — объединять все однообразные объекты в один вызов отрисовки (draw call).В движке мы батчим объекты внутри одного тайла, что является сильным аргументом в пользу выгрузки тайлов на каждый уровень квадродерева с небольшими диапазонами видимости. Именно такая естественная разрезка используется в веб-версии 2ГИС: режем данные на тайлы каждого уровня с диапазонами видимости шириной в один зум.Новая тайловая разрезкаПредысторияВо времена, когда 2ГИС рисовал карту только из скачиваемых офлайн-пакетов, инженеры движка реализовали возможность разбиения картографических данных на любое количество тайловых разрезок, в частности на специфичные разрезки для данных с генерализациями.Так, например, береговую линию можно выгрузить следующим образом:красивую подробную геометрию в тайлы z14 для ближних зумов,упрощённую генерализованную геометрию в тайлы z10 для дальних зумов.Данные в наших тайлах векторные, порядок отрисовки объектов определяется стилями, и движок без проблем рисует одни тайлы поверх других. Манипуляции с наложением тайлов разных разрезок позволяют существенно облегчить итоговый офлайн-пакет, поскольку уменьшают дублирование геометрий между тайлами разных уровней.Со временем фокус сместился в сторону онлайна и пришло время гибрида — режима отрисовки карты с выбором источника данных в зависимости от сети и наличия скачанного офлайн-пакета города. Для гибрида важно соответствие тайловых разрезок в онлайне и офлайне, чтобы тайлы из онлайна могли корректно замещать офлайн-тайлы и наоборот: так при наличии сети мы покажем онлайн-тайл, а при отсутствии сети и наличии офлайн-пакета — аналогичный офлайн-тайл.Задача реализации гибрида была довольно большой и сложной, до неё мы в движке совсем не работали с сетью. Поэтому мы пошли самым простым путём и выбрали одну тайловую разрезку для всех данных, удовлетворяющую обоим мирам: для офлайна разрезка должна обеспечивать адекватный размер пакетов, а для онлайна — адекватный трафик и плотность данных в отдельных скачиваемых тайлах. Так мы пришли к единой разрезке — выгрузке всех данных на нечётные уровни квадродерева со смещенным диапазоном видимости шириной в два зума. На разных зумах выглядит она так:Пропуск чётных уровней хорошо сократил размер офлайн-пакетов, но негативно сказывался на скорости загрузки, отрисовки и общем визуале — карта загружалась квадратной мозаикой.Баланс скорости и размераЧтобы улучшить производительность, мы решили переосмыслить наш гибрид и вернуть гибкость в управлении тайловыми разрезками. Баланс нашёлся в разделении всех данных на две разрезки разной специфики: недетальная — для больших по площади объектов (основные дороги, реки и леса), с выгрузкой на несколько уровней квадродерева с относительно широкими диапазонами видимости,детальная — для всех остальных объектов, с выгрузкой на каждый уровень до z15 с диапазонами видимости шириной в один зум.Так мы получили небольшой прирост в размере офлайн-пакетов, но заметный буст в скорости отрисовки кадров и полезный визуальный эффект — быстрозагружаемую подложку в виде недетальных тайлов до полной загрузки всей видимой сцены. И ещё один приятный сайд-эффект от большего количества небольших тайлов — улучшение распараллеливания загрузки.РезультатыСовсем не трогая движок, а просто расслоив данные по двум отдельным тайловым разрезкам, мы получили:увеличение размеров офлайн-пакетов в среднем на +15%, но при этом уменьшение среднего времени отрисовки кадров до -20%,и классный визуальный эффект быстрой информативной подложки, позволяющий с бо́льшим комфортом скроллить карту без мозаичной загрузки тайлов.Теперь конфигурация тайловых разрезок в конечных приложениях может быть настроена на любой сетап, нужно лишь указать, откуда вытягивать тайлы каждой разрезки и на каких зумах какой уровень тайлов загружать. Этим мы собираемся воспользоваться в ближайшем будущем, выделив часть данных в ещё одну разрезку с единственным 16 уровнем для красивых иммерсивных данных на близких зумах, но только для пользователей онлайна — так мы облегчим офлайн-пакеты.Если захочешь работать в нашей команде, у нас открыто четыре вакансии: C++ Team Lead, Rendering engineer C++, C++ разработчик, Senior QA-инженер."
72,72,Новые ограничения Sonatype Nexus OSS: что изменилось и как это повлияет на российские компании?,Холдинг Т1,Многопрофильный ИТ-холдинг,158.53,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-03-12,"Недавно компания Sonatype рассказала о том, какие ограничения ввела на использование бесплатной версии своего менеджера репозиториев. Это может существенно повлиять на малые и крупные организации, особенно в условиях санкционного давления на российский ИТ‑рынок. Рассказываем подробности.Ключевые изменения в OSS-версииСогласно новым правилам, развёртывания Sonatype Nexus Repository Community Edition теперь ограничены: 100 000 компонентов и 200 000 запросов в день. Если лимиты превышены, пользователи не смогут добавлять новые компоненты до тех пор, пока оба показателя не будут снижены. То есть блокировка активируется даже при превышении только одного из порогов (например, количества запросов) и требуется одновременное соблюдение обоих условий для восстановления функциональности.Для кого это критично?В малых компаниях (до 100–200 сотрудников) 200 000 ежедневных запросов — приемлемый объем для небольших команд. Однако даже они могут столкнуться с ограничениями. Крупные компании с миллионами запросов в сутки точно окажутся в сложной ситуации. В OSS‑версии и до этого были ограничения, затрудняющие интеграцию в масштабные инфраструктуры, а новые лимиты делают её практически непригодной для таких сценариев.Новые правила касаются только последних версий Sonatype Nexus. Старые развертывания продолжат работу, но без доступа к обновлениям и исправлениям.Ситуация на рынке с 2022 годаДо 2022 года основными игроками в этой сфере были Nexus и Artifactory, которые занимают большую часть мирового рынка: Artifactory контролирует около 60%, а Nexus — порядка 35%. Однако после их ухода с российского рынка компании столкнулись с необходимостью поиска альтернативных решений. К 2025 году уже появились отечественные аналоги, способные частично или полностью заменить функциональность ушедших продуктов.Проблемы существующих open-source решений     Ключевым open‑source решением, на котором начали работать многие компании, стал Nexus OSS. Он подходит небольшим компаниям, но у него есть ключевые проблемы:Ограниченная масштабируемость из‑за монолитной архитектуры.Отсутствие гибких механизмов управления метаданными.Низкая отказоустойчивость и сложность репликации данных.Использование встроенного в монолит поиска, который не может быть вынесен в отдельный сервис и файловой базы данных OrientDB, что увеличивает нагрузку на систему.Даже open‑source лицензия кому‑то принадлежит и может стать недоступной для использования в РФ.Кроме того, OSS‑версия Nexus не предполагает наличия исходников от всех модулей. Часть модулей OSS‑версии реализованных Sonatype не имеет открытых исходников и исправлять проблемы в таких модулях очень затруднительно.Одно из самых главных замечаний к OSS версии Nexus — это наличие гигантского количества уязвимостей.Многие крупные игроки попытались использовать Nexus OSS в корпоративном сегменте, стараясь обойти описанные выше ограничения через создание большого количества экземпляров, например, под каждый отдельный формат или поднимая теневой инстанс, который включался в момент отказа основного. Однако, это увеличивало количество поддерживаемых инстансов, и, как следствие, затраты на администрирование.Также после последнего обновления OSS‑версии компания Sonatype объявила о новых ограничениях, о которых я писал выше, что ставит корпоративный сегмент в ещё более сложную ситуацию.Разработка собственного решенияПроведя много исследований, мы, команда платформы «Сфера», ещё в 2022 году увидели новый открытый рынок хранилищ артефактов. В России он создавался заново, и многие отечественные разработчики, сделав те же выводы, пошли по простому пути внедрения Nexus OSS в ядро своих продуктов. Мы, однако, уже тогда понимали, что полноценно использовать OSS‑версию Nexus в корпоративной среде не получится, поскольку нагрузка на инфраструктуру в крупном сегменте бизнеса растет слишком быстро.Поэтому начали разрабатывать собственное решение, используя компоненты, которые позволяли бы работать под большой нагрузкой в корпоративной инфраструктуре. Что нам нужно было от продукта:Поддержка архитектуры, которая будет соответствовать современным стандартам и лучшим практикам.Сервисы без устаревших и уязвимых библиотек. Такие сложные функции как полнотекстовый поиск должны поставляться в виде отдельных сервисов для удобства масштабирования и уменьшения рисков отказа основного функционала.Гибкое масштабирование хранилища артефактов.Работе в кластере.Безопасность хранения артефактов.Работа с ключевыми форматами репозиториев, такими как Maven, Docker, Helm, Yum, Apt и Raw.Безопасное подключение внутренних и внешних репозиториев.Поддержка новых версий Java.Зеркалирование существующих ресурсов, что крайне актуально в условиях, когда они могут стать недоступны в условиях санкций.Дистрибутивы и лицензии 2.0В результате, спустя два года, мы создали продукт Сфера, который полностью адаптирован для работы в крупных корпоративных средах. Он решает широкий спектр задач, с которыми сталкиваются организации, включая безопасность, масштабируемость и отказоустойчивость. Особое внимание мы уделили эффективной работе под высокой нагрузкой.При этом для решения конкретных задач в продукте мы использовали open-source решения, а не брали готовый OSS‑продукт и дорабатывали его. Этот подход позволяет нам заменять разнообразные части продукта в случае возникновения проблем с ним. Уход зарубежных решений с рынка стал не только вызовом, но и возможностью для развития отечественных продуктов. Так нам удалось создать аналог одного из флагманских решений рынка хранилищ артефактов, который может работать в условиях высокой нагрузки и сейчас востребован на рынке РФ."
73,73,Жизненный цикл данных под контролем: знакомство с ILM на примере расширения для Postgres Pro Enterprise 17,Postgres Professional,Разработчик СУБД Postgres Pro,339.87,"Программное обеспечение, Консалтинг и поддержка, Информационная безопасность",2025-03-12,"Хранение всего объёма данных в одном месте удобно, но часто нерационально из-за стоимости хранения, ограничений масштабирования баз данных, усложнения администрирования и других издержек. Поэтому лучшей практикой при работе с большими объёмами данных является управление жизненным циклом информации (Information Lifecycle Management, ILM) с применением соответствующих методологий и инструментов для автоматизации процесса.В Postgres Pro Enterprise 17 появилась возможность использовать ILM благодаря расширению pgpro_ilm, которое позволяет переносить редко используемые данные в более дешёвое хранилище по аналогии с функцией ILM в Oracle.Проблема больших баз данных и их решениеДанные — один из важных активов любой компании. Но по мере роста компании объем генерируемых данных непрерывно увеличивается. Если на начальных этапах для хранения информации достаточно небольшой базы данных, которую легко использовать и администрировать, то по мере разрастания БД, компании, как правило, сталкиваются с типовыми проблемами:стоимость хранения обратно пропорциональна времени доступа к данным;быстрых накопителей мало, и они дорогие;актуальность данных и активность работы с ними со временем снижается;активные операции над данными зачастую производятся только в течение относительно короткого времени (недель или месяцев) жизни данных;со временем самый большой объем начинают занимать исторические данные, которые может быть нерационально хранить на быстрых накопителях.Вариант с единым хранилищем для всех данных, как часто используемых, так и исторических, не самый рациональный. Поэтому бизнес вынужден искать компромисс между скоростью доступа и объёмом данных, а также способы минимизировать существующие издержки. Но когда данных и таблиц становится очень много (сотни и тысячи таблиц и секций), то хотелось бы иметь какой-то инструмент, некую большую кнопку, которая позволила бы автоматизировать этот процесс в соответствии с политиками и регламентами, применяемыми в компании.Примечание: Чтобы понять всю суть описанных проблем, можно рассмотреть простой бытовой пример. Допустим, у вас есть гардеробная, куда вы складываете все свои вещи. В ней есть удобные полки, которые легко доступны, но их мало. Постепенно количество вещей увеличивается и рано или поздно придётся увеличивать гардеробную, поскольку количество старых и малоиспользуемых вещей со временем растет. Чтобы этого избежать, часть старых вещей надо вынести, например, в гараж или отвезти на дачу. Так они будут всё равно доступны, но не будут неоправданно занимать ограниченное место.Что такое Information Lifecycle ManagementInformation Lifecycle Management — концепция управления информацией, основанная на разделении данных по критерию их ценности для бизнеса и автоматизации управлении ими с учетом политик и регламентов принятых в организации. Она помогает понять, какие данные нужны в оперативном доступе, какие могут понадобиться, но их допустимо вынести на медленные носители, а от хранения каких в оперативной базе данных можно отказаться, чтобы освободить место для новой, актуальной информации.Примечание: Например, интернет-магазин ежедневно обрабатывает большие объёмы данных о заказах клиентов: текущие заказы и недавние покупки крайне ценны для оперативной деятельности и требуют быстрого доступа, поэтому хранятся на высокопроизводительных носителях; в то же время исторические данные о заказах, сделанных несколько лет назад, уже не столь актуальны для ежедневных операций и, хотя могут понадобиться для аналитики или отчётности, нет необходимости держать их там же где и оперативные данные — с помощью ILM эти данные можно перенести на более медленные и экономичные носители, освобождая ресурсы для актуальной информации и снижая затраты на хранение; данные, утратившие ценность и допустимые к удалению по политике компании и законодательству, могут быть архивированы или удалены, что позволяет эффективно управлять информацией на протяжении всего её жизненного цикла и поддерживать высокую производительность систем.ILM подразумевает разделение данных на четыре основные группы:Active. Это данные, которые часто меняются (OLTP) и для них характерны случайные чтения. Например, это может быть таблица финансовых транзакций текущего месяца.Less Active. Данные, которые меняются редко (OLTP, OLAP), а их чтения преимущественно сканирующие по колонкам. Пример таких данных — история финансовых транзакций текущего квартала или года.Historical. Данные, которые не меняются (OLAP). Редко выполняются сканирующие чтения по колонкам. К таким данным можно отнести, например, историческую информацию о финансовых транзакциях за предыдущие годы. Archive. Архивные данные, которые хранят, как правило, только для соблюдения требований регуляторов. Исходя из ценности данных и их востребованности, для каждой категории рационально выбирать свой тип носителя. Например:активных данных зачастую относительно мало. Их можно расположить на дорогих, но быстрых NVMe-носителях;менее востребованные данные рационально переносить на более дешевые носители — например, на классические SSD;исторические данные оптимально переносить на HDD и/или хранить в сжатом виде;для архивных данных достаточно простых и дешевых отчуждаемых носителей.Примечание: В pgpro_ilm в настоящее время нет возможности вынести данные на отчуждаемые носители, такие как ленты.Хватит лирики и теории, перейдем к деталям нашей реализации управления жизненным циклом информации. В pgpro_ilm участвует три сущности:объект (таблица или секция), над которым совершают действие;условие, от которого зависит выполняемое действие — например, «данные долго не менялись» или «данные давно никто не читал»;действие — например, данные можно перенести на другой носитель или сжать.С помощью них можно настроить правила в зависимости от потребностей и стратегии ILM в организации, например так:Сначала, менее активные данные переносятся в табличное пространство, расположенное на более дешевых дисках, а после того, как данные переходят в категорию исторических, они переносятся в табличное пространство, расположенное на дешевых дисках большой емкости.Примечание: Правила обрабатываются в порядке убывания периода. Это сделано специально, потому что логично сначала обработать самое «старое» правило: если оно сработало, то все остальные правила уже нет смысла обрабатывать. Обработка условийДля корректного использования ILM нужна статистика обращения к данным — без этого нельзя понять, какие данные «свежие», а какие «старые». Но такой статистики в Postgres Pro не было. Соответственно, мы начали искать варианты реализации сбора требуемой статистики. При этом нам было важно, чтобы решение соответствовало требованиям:статистика не должна учитывать системных пользователей (postgres) и  системные процессы (vacuum);для отдельных пользователей должна быть возможность дополнительно задавать исключения. Это важно на случай, если при формировании статистики нужно игнорировать действия с данными отдельных специалистов;статистика должна быть долговременной. В случае перезагрузки сервера она не должна теряться. Соответственно, ее надо хранить не только в памяти, но и на диске.Одновременно с поиском решения для сбора статистики обращений к таблице (DML-операции), мы занималась задачами по поиску неиспользуемых привилегий в рамках расширения pgpro_usage. В итоге мы объединили усилия, чтобы реализовать сбор всей необходимой статистики в рамках расширения pgpro_usage. Теперь статистика собирается вместе, но разделена — сброс статистики для поиска неиспользуемых привилегий по умолчанию не сбрасывает статистику, используемую для ILM.Примечание: Чтобы понять принцип раздельного хранения статистики, достаточно вспомнить одометр автомобиля. Он может отображать как пробег авто за определенный период (путевой), так и общий пробег. Причем, если сбросить путевой пробег, данные об общем пробеге останутся незатронутыми. Статистику времени последнего доступа к таблицам pgpro_ilm получает в разрезе пользователей из функций и представлений pgpro_usage. Из всего доступного массива информации для задач ILM нужны только отдельные сведения, в зависимости от выбранного правила:для правила NO_MODIFICATION учитываем UPDATE, INSERT, DELETE или TRUNCATE; для правила NO_ACCESS — не только UPDATE, INSERT, DELETE, TRUNCATE, но и чтение с помощью команды SELECT.На основе этой статистики можно понять, когда, кто и как обращался к конкретным объектам в базе данных.От теории к практикеТеперь, когда понятен сам механизм работы нашей реализации ILM, перейдем к тому, как именно работать с расширением pgpro_ilm для Postgres Pro Enterprise 17.УстановкаДля корректной работы pgpro_ilm должно быть установлено расширение pgpro_usage. Поэтому, прежде всего, добавляем расширение pgpro_usage в переменную shared_preload_libraries в файле postgresql.conf и перезапускаем сервер базы данных.Далее создаем расширение pgpro_usage:CREATE EXTENSION pgpro_usageСледом — создаем расширение pgpro_ilm.CREATE EXTENSION pgpro_ilmНа этом подключение ilm будет завершено. Управление списком пользователейИспользуя SQL-интерфейс в pgpro_ilm, можно определить пользователей, действия которых будут игнорироваться для задач ILM. Как правило, такие исключения нужны, чтобы на статистику не оказывали влияния обращения к данным со стороны:служебных пользователей информационной системы;внутренних и внешних аудиторов, которым информация нужна разово;пользователей, формирующих редкие, разовые отчеты, например выписку по счёту за определенный месяц 2020 года для контролирующих органов (если исторические данные запросили один раз в году, это не значит, что они до сих пор находятся в категории Less Active);пользователей, выполняющих пакетные операции со старыми данными.Для управления исключениями в pgpro_ilm предусмотрено несколько функций.SELECT user_name, exclude_access, exclude_modification FROM pgpro_ilm.get_exclude_users(); — возвращает список исключаемых пользователей;SELECT pgpro_ilm.set_exclude_users(array['exclude_access_user'], array['exclude_write_user']); — задает список пользователей, действия которых игнорируются при проверке правил NO_ACCESS и правил NO_MODIFICATION.Управление правиламиДля управления правилами в pgpro_ilm предусмотрено три группы функций.Функция для добавления нового правилаadd_rule(iv_object_name text, iv_rule_type text, iv_period interval, iv_action text, iv_parameter text) returns void.Здесь:iv_object_name — имя объекта;iv_rule_type — тип правила (NO_ACCESS или NO_MODIFICATION);iv_period — период, после которого правило должно сработать;iv_action — действие, которое надо сделать (например, перенести данные);iv_parameter — параметр действия (например, куда надо перенести данные).Например:SELECT pgpro_ilm.add_rule('sales_table_section_q1_2021', 'NO_ACCESS', interval '12 mons', 'ALTER_TS', 'low_cost_sales_tablespace');Для корректного управления правилами и перемещением данных важно предварительно выработать единый регламент, который определит, когда переносить данные и куда.Например:данные, которые не модифицируются более 3 месяцев, переносят на SSD;данные не запрашивали более 6 месяцев — на HDD;данные не читали более 12 месяцев — хранятся в сжатом виде.Функция для удаления правилremove_rule(iv_object_name text, iv_rule_type text, iv_period interval, iv_action text) returns voidЗдесь:iv_object_name — имя объекта;iv_rule_type — тип правила (NO_ACCESS или NO_MODIFICATION);iv_period — период, после которого правило должно сработать;iv_action — действие, которое надо сделать.Функция для получения текущего списка правил для таблицы или всех правилget_rules([iv_object_name text]) returns tableiv_object_name — имя объекта.Обработка правилТеперь к функциям pgpro_ilm, которые запускают обработку правил.В Postgres Pro можно вызывать функции обработки правил как в ручном режиме, так и автоматически с помощью планировщика задания. Второй вариант приоритетнее, поскольку снижает нагрузку на специалистов.Для обработки правил в СУБД Postgres Pro предусмотрены две функции.process_rules (iv_object_name text) returns void — выполняет правила для одной таблицы, где iv_object_name text — имя таблицы.process_all_rules() returns void — выполняет все правила для всех таблиц.Например запуск обработки правил для отдельной секции:SELECT pgpro_ilm.process_rules('sales_table_section_q1_2021');Примечание: Если таблица разделена на секции, то сначала проверяются правила для родительских секций, начиная с самого нижнего уровня иерархии. На каждом уровне правила рассматриваются в порядке, обратном периоду их действия. Как только для таблицы будет найдено подходящее правило, функции выполняют соответствующее действие и завершают обработку таблицы.Что еще нужно знатьТеперь кратко остановимся на некоторых нюансах ILM.Правила можно создавать как для секционированной таблицы в целом, так и для отдельной ее секции. Если вы создаете правила для секционированной таблицы, они автоматически наследуются для всех секций этой таблицы.Правила для отдельных секций секционированных таблиц приоритетнее общих правил для таблиц. При этом правила, относящиеся к родительским таблицам, продолжают действовать.Правила не обрабатываются при вызове process_rules для секционированной таблицы. Поскольку в секционированных таблицах нет данных (они хранятся в секциях), функция завершится успешно, но ничего не выполнит.В текущей реализации при обработке правил индексы переносятся вместе с целевой таблицей/секцией. В следующих минорных релизах мы планируем реализовать возможность создания отдельных правил для таблиц/секций и индексов.Если таблица уже была перенесена, то повторно правило обрабатываться не будет.Операция ALTER TABLE … SET TABLESPACE … захватывает блокировку AccessExclusiveLock. Соответственно повторный вызов функции process_rules, до того, как завершится предыдущая операция, безопасен.Если ALTER TABLE завершается ошибкой, то повторный вызов process_rules повторяет операцию.Для того, чтобы было проще понять, от какой даты отсчитываются периоды и как обрабатываются правила, ниже представлена небольшая анимация. Надеюсь она позволит лучше понять внутреннюю логику нашей реализации ILM:Дополнительные инструментыВ отдельных сценариях работы с Postgres Pro Enterprise 17 одного расширения pgpro_ilm может оказаться недостаточно. Поэтому для работы с большими объемами данных можно также применять другие расширения и инструменты. В частности:SPLIT PARTITION и MERGE PARTITIONS — команды для разделения одной секции на несколько и объединения нескольких секций в одну соответственно.Механизм CFS для сжатия таблиц и индексов внутри табличного пространства. В СУБД Postgres Pro поддерживаются разные алгоритмы и степени сжатия.pgpro_autopart — расширение Postgres Pro для автоматического секционирования при добавлении или изменении данных в таблице.pgpro_bfile — расширение, которое добавляет составной тип bfile для доступа ко внешнему файлу, то есть позволяет выносить неструктурированные данные за пределы СУБД.pgpro_scheduler — расширение, с помощью которого можно планировать, контролировать и управлять выполнением заданий.Shardman — технология для создания распределенной реляционной СУБД, предоставляющая строгие гарантии целостности данных.Вместо выводовPostgres Pro Enterprise 17 и расширения к нему позволяют выстроить полноценный пайплайн работы с данными в разных кейсах и сценариях. ILM, с помощью которого можно перемещать редко используемые данные в более дешевое хранилище, — один из таких механизмов. Расширение pgpro_ilm включено в состав Postgres Pro Enterprise 17 как стандартное расширение и не требует отдельного лицензирования и сложной настройки. "
74,74,Томографическая реконструкция реактивного двигателя в сборе: как мы это сделали?,Smart Engines,"Обработка изображений, распознавание в видеопотоке",96.99,"Программное обеспечение, Мобильные технологии",2025-03-12,"Томографическая реконструкция реактивного двигателя: от артефактов к чёткой реальностиПриём, Хабр! На связи отдел компьютерной томографии Smart Engines. Сегодня хотим поделиться нашими результатами по КТ-исследованиям реактивных двигателей, рассказать о том, почему недостаточно просто взять и измерить, и как нам удалось не потерять лопасти. Под катом – красивые трехмерные изображения реактивного двигателя, КТ взорвавшегося двигателя, а также ответ на вопрос, стоит ли верить своим глазам.Редкий (и, вероятно, счастливый, поскольку невероятно здоровый) человек не сталкивался с медицинской компьютерной томографией, а вот индустриальная томография в России существенно менее распространена и доступна, несмотря на то, что прямой эффект от этой технологии в экономике и вопросах безопасности очевиден. Одно из необычайно востребованных применений компьютерной томографии в промышленности – рентгеновская диагностика реактивных двигателей. Сегодня речь пойдет о небольших реактивных двигателях, которые устанавливаются на авиационные модели.Для разработки двигателей необходимо выявлять дефекты и проблемы еще до первого запуска двигателя, иначе этот запуск станет и последним. С другой стороны, обслуживание двигателей – очень трудоемкая задача, разбор и сбор даже небольшого реактивного двигателя в целях его диагностики может занимать несколько рабочих дней квалифицированного специалиста. При этом разбор неизбежно связан с рисками повреждения тонких трубок и сложностями юстировки вала и лопастей, ведь зазоры должны выверяться с точностью до десятых и сотых долей миллиметра. Компьютерная томография позволяет сократить продолжительность диагностики до нескольких часов и выявить проблемы еще до того, как они привели к необратимым последствиям.Тем не менее, если бы двигатели можно было просто взять и измерить, этой статьи бы не было. Мы начали с исследования нового двигателя JetCat P100-RX (Рис. 1), высотой всего 24 см. Рисунок 1. Турбореактивный двигатель JetCat P100-RX Для измерений использовался томограф американской фирмы North Star Imaging с максимальной энергией излучения 440 кэВ. Реконструкции, которые позволяет получить томограф и софт, фактически являющиеся индустриальным стандартом, приведены на Рис. 2 и Рис. 3.Рисунок 2. Фото двигателя, 2D срез и 3D изображение двигателя с ложными искажениямиРисунок 3. Различные реконструированные слои реактивного двигателя (2D срезы сверху и снизу, 3D визуализация посередине) На Рис. 2 можно видеть, что некоторые лопасти на КТ реконструкции “потеряны”. Хорошо, что в данном случае можно произвести визуальную диагностику, и увидеть, что лопасти на самом деле были на месте (см. фото двигателя). Однако если бы решение принималось автоматически на конвейере, или если бы похожие проблемы возникли внутри двигателя, было бы принято необоснованное решение о пересборке или ремонте двигателя. Второй ряд неподвижных лопастей выглядит лучше, однако паразитное гало между лопастями сливается с границами, также существенно осложняя 3D визуализацию и диагностику (не говоря об автоматической диагностике). Возникает риторический вопрос: верить томографу или глазам? При таких результатах томограф не выглядит как метрологический инструмент. Почему целые лопасти повреждены на реконструкции?Почему же так получилось? Почему результат томографии сильно расходится с реальностью и как при этом делают КТ целых автомобилей, автомобильных двигателей или лопаток авиационных двигателей?Основная проблема – в относительно низкой энергии фотонов, испускаемых источником используемого томографа. При энергиях ~400 кэВ максимальная толщина стали, через которую может проникнуть зондирующее излучение, составляет 5-7 см, а при толщинах 10 см и 1000 см изображение уже не будет различаться, потому что в обоих случаях поглощаются практически все фотоны. При этом увеличение интенсивности (то есть числа фотонов) даже на порядки лишь незначительно увеличат глубину проникновения (ориентировочно на 1-2 см). Для просвечивания больших металлических объектов, таких как контейнеры или автомобили, используется излучение в другом диапазоне, с энергиями фотонов порядка десятков МэВ, однако в этом случае требуются принципиально другие источники излучения и детекторы, создаются новые требования к эксплуатирующему персоналу, помещениям, а цена и без того недешевой установки растет на столько же порядков, как и энергия квантов. При этом относительно слабо поглощающие материалы, такие как алюминиевые лопатки холодной части двигателя или корпус двигателя, теряют контраст настолько, что становятся практически прозрачными.Таким образом, если мы хотим выполнить диагностику двигателя без его разборки на более мелкие узлы, то неизбежно требуется найти компромисс между проникающей способностью и контрастом в холодной части, да еще и не забывая о том, что чем выше энергия, тем менее доступными становятся измерения.В нашем случае измерения происходят на пределе чувствительности прибора, когда полезная информация в сигнале все еще присутствует, но уже сравнима по амплитуде с паразитными эффектами. Поэтому учет этих эффектов, таких как рассеяние излучения, ужесточение пучка и полное поглощение, которыми пренебрегают в других применениях КТ, становится критическим для корректной реконструкции двигателей и подобных агрегатов. При этом со сложными искажениями не справляются не только классические алгоритмы, но и специализированные корректирующие решения от большинства вендоров.  Рисунок 4. Еще один пример “потерянных” лопастейСитуация осложняется тем, что, несмотря на большое число сопоставимых вкладов в искажения, которые имеют принципиально разные черты и модели формирования, все они имеют общую черту – на детектор прилетает больше фотонов, чем ожидается согласно линейной модели. Поэтому искажения, обусловленные разными эффектами, оказываются очень похожими на реконструкции (Рис. 5). Из доминирующих эффектов стоит выделить полихроматичность пучка (BH или beam hardening, ужесточение пучка), рассеяние рентгеновского излучения объектом (прямое) и детектором (обратное). Все эти эффекты усиливаются в области почти полного поглощения.Рисунок 5. Моделирование артефактов реконструкции. Влияние различных физических факторов на качество реконструкцииГлавная проблема, которая возникает при попытке коррекции таких данных с ручным подбором параметров – перекомпенсация. При последовательном визуальном подборе параметров коррекций на каком-нибудь слое, хочется “выкрутить” параметры так, чтобы устранить эффекты, вызванные не только компенсируемым эффектом, но и остальными эффектами (потому что визуально на реконструкции они не различимы). При этом из-за различий в моделях коррекции для разных эффектов, а также вкладов эффектов в разных областях, оптимальный результат на одном слое может приводить к возникновению неожиданных искажений на других слоях. Поэтому задача подавления артефактов очень сложна, и для адекватного ее решения принципиальным становится разделение вкладов разных эффектов, с автоматической (желательно – объективной) оценкой параметров в режиме калибровочных измерений или в режиме автокалибровки, то есть с использованием только “полезных” измерений.Не останавливаясь на деталях реализации конкретных алгоритмов коррекций, посмотрим, чего удается добиться при реконструкции одних и тех же данных на базовом софте томографа NSI и с использованием коррекций в Smart Tomo Engine.  Какой реконструкции получается достичь с помощью функционала Smart Tomo Engine?Главное наше достижение – улучшение качества реконструкции в области лопастей без внесения новых искажений и без потери контраста на слабо поглощающих компонентах. Если на стандартной реконструкции лопасти совсем затерты и кажутся поврежденными, то на реконструкции с подавленными артефактами все лопасти четко различимы (Рис. 6).  Рисунок 6. Лопасти до и после применения инструментов Smart Tomo EngineЛопасти стали видны четче, и уже видно, что они не сломаны. Параллельно внешний корпус “не пострадал” от наших коррекций: на базовой реконструкции есть дырочки в корпусе, в реконструкции со Smart Tomo Engine их вовсе нет. Как итог, существенно повышается качество бинаризации и построения STL моделей даже при использовании пороговых методов бинаризации (Рис. 7).Рисунок 7. Лопасти до и после применения инструментов Smart Tomo Engine (3D визуализация) Артефакты вокруг маленьких отверстий, трубок, элементов электроники менее выражены после применения алгоритмов подавления артефактов (Рис. 8).  Рисунок 8. Элементы электроники на реконструкции двигателя до и после применения коррекций Smart Tomo Engine  То, что мы не теряем лопасти там где они есть – хорошо, но следующий шаг верификации – проверка на то, не возникают ли лопасти там, где их быть не должно. Для этого мы исследовали двигатели с наработкой и реальными дефектами.Диагностика сломанных двигателейКоллеги из авиационного спортивного клуба “РусДжет” предоставили нам два двигателя, один мы сломали, другой потеряли взорвался при эксплуатации, а второй упал вместе с аппаратом после потери управления. Оба двигателя имели определенную выработку, а, значит, потенциально могли иметь как критические проблемы, приведшие к неисправностям, так и проблемы, возникшие в моменты выхода из эксплуатации (взрыва или падения). Помимо этого, оба двигателя предполагались к ремонту, поэтому мы могли впоследствии верифицировать “слепую” диагностику, выполненную с использованием томографа.Первое, что сразу бросается в глаза, – это оторванные лопасти (Рис. 9-11). В этом случае лопасти были действительно оторваны. Но крайне интересно, что “пеньки”, оставшиеся от оборванных лопастей, выглядят фактически так же, как артефакты реконструкции, которые мы обсуждали в предыдущем разделе.Рисунки 9. Оторванные лопасти и артефакты реконструкции почти не различимы  Рисунки 10. Оторванные лопасти и артефакты реконструкции почти не различимы  Рисунок 11. Дефекты в статичных лопастях двигателя Второй крупный дефект – деформация основания лопаток компрессора (Рис. 12) и сколы направляющих (Рис. 13). Эти дефекты наглядно демонстрируют, почему необходимо использовать относительно низкие, “компромиссные” энергии: критические проблемы могут возникать и в холодной части двигателя.Рисунок 12. Деформации компрессораРисунок 13. Сколы направляющихПримечательно, что в компрессоре мы также обнаружили стружку и камень (Рис. 14). Конечно, этот двигатель вышел из строя не по этой причине, однако немало громких историй со взрывами двигателей ассоциируются именно с попаданием внутрь стружки и прочих посторонних предметов.Рисунок 14. Стружка внутри двигателя на реконструкцииНа закуску предлагаем вам посмотреть видео, в котором показаны эти и другие дефекты, с красивыми сопоставлениями и пролетными трехмерными изображениями.Вместо заключенияТомографичесая реконструкция двигателя – сложная задача, успех в решении которой зависит далеко не только от условий измерения. Использование высоких энергий при сканировании двигателя приводит к тому, что слабо поглощающие элементы становятся прозрачными, а слишком малых энергий излучения, как правило, недостаточно для того, чтобы “пробить” такие сильно поглощающие детали, как лопасти двигателей. Но даже подбор оптимальной энергии излучения не позволяет получить надежную для диагностики реконструкцию: измерения ведутся на пределе чувствительности детектора и почти всегда реконструкция искажена артефактами. Игнорирование артефактов может привести к неверному заключению диагностики: именно артефакты запросто “обманут” специалиста: там, где лопасти целы, будут видны частично оторванные детали. Автоматическая диагностика в присутствии сильных артефактов и вовсе вряд ли возможна. Подавление же артефактов реконструкции двигателя – отнюдь не простая задача, ведь в случае измерения и реконструкции двигателей факторов, порождающих артефакты, много, и все они коррелируют между собой. Но сегодня мы в Smart Engines можем уверенно сказать, что преодолели эту проблему и можем проводить томографическую диагностику двигателя на серийных томографах с помощью Smart Tomo Engine, не сомневаясь в надежности реконструкции.   "
75,75,От legacy к успеху: как мы переработали главную страницу приложения М2 и увеличили MAU,m2_tech,Строим лучшую PropTech-компанию в России,4.37,"Веб-разработка, Мобильные технологии, Веб-сервисы",2025-03-12,"Привет, Хабр! Это Михаил, product owner мобильного приложения М2, и Антон, продуктовый дизайнер в M2. Мы работаем над мобильным приложением, которое позволяет проводить сделки с недвижимостью онлайн как частным лицам, так и профи рынка — удобно и быстро. В 2024 году рынок недвижимости пережил кризис: льготную ипотеку отменили, количество сделок сократилось. Но нашему приложению удалось не только удержаться на рынке, но и увеличить продажи. Один из ключевых факторов успеха — редизайн главной страницы приложения.   В этой статье мы расскажем, как пришли к решению о редизайне, какие шаги предприняли и каких результатов достигли. Если вы продакт, дизайнер или просто интересуетесь UX, этот кейс будет вам полезен. Почему мы решились на редизайнНаше приложение было запущено в 2020 году, и за несколько лет накопились проблемы: 1. Разный принцип работы похожих функций. Пользователи не понимали, почему, например, «Поиск недвижимости» отличается в разных разделах. 2. Дублирование сервисов. Раздел «Профиль» начал повторять функциональность главной страницы, что вызывало путаницу. 3. Непонятные названия и иконки. Сервисы вроде Защиты сделки были непонятны пользователям, а иконки не давали никакой информативности. Вот таким фидбеком с нами поделились юзеры:Мы поняли: чтобы двигаться вперёд, нужно переосмыслить главную страницу.  Как мы подошли к редизайнуШаг 1. Определить цели и выбрать целевой сегмент Мы начали с изучения юнит-экономики и настройки аналитики. Раньше отчётность не позволяла понять, на какой аудитории нам стоит фокусироваться. После анализа мы сделали pivot и вместо частных лиц сосредоточились на риелторах. Метрики, которые мы выбрали:Выручка по сделкам, созданным  в мобильном приложении. MAU — показатель дистрибуции и возвращаемости. Оценка приложения пользователями — до редизайна она составляла 3,6 из 5.  Шаг 2. Исследовать потребности риелторовМы провели глубинные интервью с восемью риелторами и изучили историю исследований Brand Health Tracking. Выявили основные потребности: Отслеживание статуса сделок и сообщений в чатах с помощью push-уведомлений. Удобная навигация и фильтры для поиска недвижимости. Отсутствие технических ошибок.  Слайд из исследования, фиолетовым цветом цитаты пользователейСлайд из исследования, фиолетовым цветом цитаты пользователейШаг 3. Подготовить вводные для дизайна Мы собрали список проблем, обратную связь от пользователей и аналитику сервисов. Это помогло нам сформулировать задачи для дизайна.  Шаг 4. Проработать дизайнТеперь перейдем к разбору дизайна. Меня все ещё зовут Антон и я отвечаю за дизайн мобильных приложений M2 для Android и iOS. Над продуктом работаю около двух лет. Я начал приносить артефакты для редизайна главной страницы и других экранов спустя  полгода работы в команде. Мне часто приходилось сталкиваться с legacy-паттернами или противоречивыми решениями прошлой реализации.Решили сделать интерфейс и разделить процесс на логические шаги, которые дадут возможности для масштабирования сервисов. Мы зависимы от положения дел на рынке недвижимости, поэтому для нас очень важна модульность интерфейса. Новая главная должна уметь быстро скрывать непопулярные сервисы. Например, Ипотека был для нас флагманом в начале 2023 года, а после ключевой ставки 20%+ стала Сделка.  шапка приложенияРанее верхнее меню попросту отсутствовало. Для риелторов мы показывали логотип M2Pro, который не закрывал никаких потребностей, а скорее вызывал вопросы, что это такое и какая в нём ценность.сервисы на главнойИнтервью юзеров помогли принять горькую правду: названия сервисов без подсказок непонятны, а иконки не дают никакой информативности. Я сделал типичные плитки, никакого rocket science: заголовок, описание и логотип. Над названиями сервисов, конечно, еще придется поработать, но описания уменьшило количество отказов и увеличило конверсию в сервисы — мы перестали получать вопросы ‭«а где находятся заявки или куда пропали». На основе данных аналитики мы расположили их по популярности, а часть ‭«спящих» продуктов переместили во внутреннюю страницу.ипотека и сделкиНаверное, самой «must have-фичей» по количеству упоминаний был быстрый доступ ко всем спискам заявок и сделок. Раньше списки и заявки находились в двух или трех местах и размещались на разных экранах в зависимости от роли пользователя, а если их не было, то блок не отображался.  блок ""Поиск недвижимости"" Не стали придумывать велосипед, а просто объединили и воспользовались похожими и понятными решениями на рынке для отображения поиска объектов на Главной. Не самая популярная функция у нас, но достаточно частотная. Сократили количество точек входа на Главной и объединили по принципу близости, освободили место для более значимых и популярных блоков.Шаг 5. Как объяснить необходимость редизайна бизнесу Мы подготовили презентацию, в которой обосновали необходимость изменений. Ниже структура презентации и некоторые слайды из нее:Цели и показатели мобильного приложения;Дизайн as is;Предпосылки изменений;Результаты качественного исследования;Дизайн на ближайшие полгода;Будущее мобильного приложения.Несколько слайдов из презентации:После получения поддержки от маркетинга, дизайна и других стейкхолдеров приступили к реализации.Шаг 6. РазработкаЗадачу разбили на три двухнедельных спринта. Все изменения вносили плавно, чтобы у пользователей не было шока:Изменили сервисы поиска объявлений, убрали дублирующий функционал поиска;Разместили сервисы поддержки на главной странице, сделали показ имени в хэдере. Переместили самые важные зарабатывающие сервисы в один горизонтальный скролл;Перенесли все остальные сервисы в новую структуру, убрали дублирование функций из профиля. Честно говоря, каких-то особых проблем на этом этапе не возникло, кроме функции по показу «Мои объявления», мы не посчитали ее значимой, и она действительно не супер важна для целевого сегмента, но другие сегменты пользователей решили по-другому: пришлось ее возвращать на главный экран.Шаг 7. Обратная связь и аналитика Мы добавили возможность оставить обратную связь сразу на главной странице, запустив баннер ‭«Как вам приложение?». Важно было дать подсказки, чтобы мы могли правильно классифицировать результаты и выбрать дальнейшие направления для работы.На первой же неделе получили отзывы и оценки по функциональности.Первое время собирали отзывы и оценки из писем и заполняли Excel, но потом автоматизировали в виде отчета. А вот так выглядит рост числа новых и активных пользователей с учетом редизайна с апреля. Весь рост происходил за счет органического трафика.Также мы стали фиксировать количество нажатий на те или иные сервисы на главной в привязке к ролям, чтобы можно было управлять списком сервисов на основе их  популярности.Мы подключили Microsoft Clarity, чтобы смотреть тепловые карты и видео сессий пользователей. Вы даже не представляете, сколько можно почерпнуть, наблюдая за тем как пользователи пользуются некоторыми фичами.Шаг 8. Оценка результатов: что дал редизайнНовый главный экран сократил время на поиск нужного сервиса, а must have-фичи теперь под рукой. Все остальные ‭«спящие» продукты мы положили в экран Сервисы, а ТОП-5 популярных разместили в карусели на главной странице.Что по метрикам? Мы заместили выпадающие доходы от ипотеки, а выручка от сделок увеличилась в 6 раз. Количество активных пользователей увеличилось на 23%.Оценка приложения пользователями по 5-балльной шкале изменилась с 3.6 до 4,1 после редизайна.Обращения в поддержку с вопросами «Где мои заявки?» сократились до 0. Факапы и уроки Что насчет факапов? Делимся и этим:1. Негативная реакция на изменения. Некоторые пользователи были недовольны новым дизайном, но мы быстро собрали обратную связь и внесли корректировки.  2. Экспертиза vs UX-тесты. Мы выкатили новую версию приложения, опираясь на закон Якоба-Нильсена, не дождавшись обратной связи от UX-тестирования. Он гласит: ожидания пользователя от интерфейса определяются прошлым опытом. Суть в том, что мы не нарушали привычки пользователей, и у нас были данные от них, поэтому нам повезло. Тем не менее, мы подкрепили наше решение тестами позднее: по итогу 7 из 8 гипотез подтвердились.Планы на будущее и заключениеОтзывы, которые мы получили через баннер, помогли понять, что пользователи перестали писать негатив про неработающие сервисы или неудобство. Они стали обращать внимание на дизайн и эстетику. Однажды, наше приложение сравнили с серым небом Питера — это замотивировало начать работу над внедрением визуального стиля в приложении, чтобы разбавить рабочие будни риелторов чем-то ярким.Редизайн главной страницы стал для нас важным шагом в развитии приложения. Мы не только улучшили пользовательский опыт, но и достигли значительных бизнес-результатов. Если вы тоже задумываетесь о редизайне, надеемся, наш опыт будет вам полезен.  А если у вас есть вопросы, пишите в комментариях — мы с радостью ответим!  "
76,76,Стать голосом своей индустрии,ОТП Банк,Давай сделаем вместе!,81.76,"Электронная коммерция, Мобильные технологии, Веб-сервисы",2025-03-12,"Привет, Хабр! Меня зовут Настя. В ОТП Банке я работаю уже больше 2 лет, отвечаю за внешние цифровые коммуникации. Наша корпоративная культура способствует развитию и вдохновляет на амбициозные задачи. Поэтому я решилась выстраивать свой личный бренд. Не за горами первое выступление на крупной конференции и одно небольшое видеоинтервью. Пришлось покопаться в теме, чтобы подготовиться. Спешу поделиться с вами своими находками.Создание личного бренда и активное участие в профессиональных мероприятиях, таких как конференции и интервью, — это не просто тренд, а необходимость для современного специалиста. Начать этим заниматься очень тяжело, продолжать и не забивать — еще тяжелее. Это факт. Времени не хватает, а иногда нет желания и настроения. НО. Давайте пробовать? А вдруг из этого выйдет что-то крутое? Не попробуем, не узнаем:)Вот почему это важно:  1. Повышение узнаваемости и авторитета Личный бренд это когда тебя гуглят, а не игнорят помогает выделиться среди коллег и конкурентов. Когда ты выступаешь на конференциях или даешь интервью, становишься ""лицом"" своей профессии. Это повышает авторитет и делает тебя узнаваемым экспертом в своей области.  2. Нет Воркинг Самое приятное — знакомство с коллегами, лидерами мнений и потенциальными партнерами. Такие связи могут привести к новым проектам, коллаборациям или просто переопылению.  3. Демонстрация экспертизы Поделиться своими знаниями — значит показать свою компетентность. То есть твои выступления и интервью становятся ""доказательством"" твоего уровня.  4. Развитие карьеры Личный бренд и публичная активность открывают новые карьерные возможности. Тебя могут заметить рекрутеры, руководители компаний или инвесторы. Это может привести к повышению, переходу в более престижную компанию или даже запуску собственного проекта.  5. Влияние на индустрию А вот тут уже поинтереснее — когда ты (настолько преисполнился) делишься своими идеями и опытом, ты не только укрепляешь свой бренд, но и вносишь вклад в развитие своей отрасли. Твои выступления могут вдохновить других специалистов (отбросим синдром самозванца), задать новые тренды или даже изменить подходы к решению профессиональных задач.  6. Укрепление уверенности в себе Регулярные выступления и участие в интервью помогают развить навыки коммуникации и уверенность в себе. Это полезно не только для публичной деятельности, но и для повседневной работы, особенно если занимаешь руководящую должность.  7. Привлечение клиентов или партнеров Если работаешь на себя или в сфере, где важно привлекать клиентов, личный бренд и публичные выступления — это мощный инструмент для продвижения. Люди охотнее доверяют тем, кого видят и слышат, особенно если при этом еще и экспертиза демонстрируется.  8. Создание наследия Все, что ты сказал или написал остается в публичном пространстве. Это своего рода ""наследие"", которое продолжает работать на тебя продолжительное время. 9. Обратная связь и развитие Обратную связь от аудитории после выступлений помогает развиваться: увидеть свои сильные и слабые стороны, а также узнать, какие темы наиболее актуальны для аудитории. Это ценный ресурс для профессионального роста.  10. Конкурентоспособность В современном мире, где конкуренция на рынке труда высока, личный бренд становится вашим конкурентным преимуществом. Работодатели и клиенты выбирают тех, кто не только обладает навыками, но и умеет их презентовать.  Личный бренд и публичные выступления — это не просто способ ""засветиться"", а стратегический инструмент для профессионального роста. Публичные выступления — это навык, который можно развить, и книги могут стать отличным помощником в этом. «Как выработать уверенность в себе и влиять на людей, выступая публично» Дейл Карнеги     …Страх возникает из-за отсутствия уверенности в себе. А с чем связано отсутствие уверенности? Это следствие незнания вами своих истинных возможностей. А оно, в свою очередь, вытекает из за отсутствия опыта. Классическая книга, которая учит основам ораторского искусства, помогает преодолеть страх перед аудиторией и научиться говорить убедительно.  «Как разговаривать с кем угодно, когда угодно, где угодно» Ларри Кинг      Говорить – это все равно что играть в гольф, водить машину или держать магазин: чем больше этим занимаешься, тем лучше это выходит и тем большее доставляет удовольствие.  Советы от известного телеведущего о том, как говорить просто, ясно и уверенно. Книга полезна для тех, кто хочет научиться общаться с любой аудиторией.  «К выступлению готов! Презентационный конструктор» Радислав Гандапас      Выйдя к месту проведения презентации, сделайте паузу, оглядите зал.Практическое руководство от одного из самых известных тренеров по ораторскому искусству в России. Книга полна конкретных советов и упражнений.       «Презентации в стиле TED» Кармин Галло      Найдите свое страстное увлечение, дайте ему выход – и вас ждет прекрасная карьера.  Исследование того, как спикеры TED делают свои выступления такими увлекательными. Книга поможет вам научиться структурировать речь и удерживать внимание аудитории.  «Сторителлинг. Как использовать силу историй» Аннет Симмонс       Разница между приведением примера и сторителлингом заключается в эмоциональной окраске рассказа и в его деталях. Устная история сплетает в единое целое детали, характеры и события, и это единое целое всегда больше, чем механическая сумма его частей.  Книга о том, как использовать истории, чтобы сделать свое выступление более увлекательным и запоминающимся.  «Спасите котика!» Блейк Снайдер       Быть сценаристом – значит постоянно балансировать на грани непомерной мании величия и такой же глубокой неуверенности в себе, так что иногда этим людям требуются целые годы, чтобы они наконец решились произнести вслух: «Я писатель».   Хотя книга написана для сценаристов, она отлично подходит для тех, кто хочет научиться рассказывать истории, которые захватывают аудиторию.   «Искусство системного мышления. Необходимые знания о системах и творческом подходе к решению проблем» Джей О’Коннор       Задумайтесь над словами Лао-Цзы, автора «Дао дэ цзин», который две с половиной тысячи лет назад создал один из величайших трактатов о системах: Когда все спокойно, легко действовать. То, что еще не проявило признаков, легко направить. То, что слабо, легко разделить. То, что мелко, легко рассеять. Действие надо начать с того, чего еще нет. Порядок нужно наводить тогда, когда еще нет смуты.   Книга о том, как грамотно отвечать на вопросы аудитории, даже если они кажутся сложными или провокационными.  И, конечно, практика — это ключ к успеху. Погнали! "
77,77,"Угон аккаунтов в Telegram: схемы, о которых все должны знать",Positive Technologies,Лидер результативной кибербезопасности,4.21,"Веб-разработка, Программное обеспечение, Информационная безопасность",2025-03-12,"Довольно часто при расследовании инцидентов информационной безопасности в инфраструктуре заказчика обнаруживается, что учетные записи пользователей мессенджера Telegram были скомпрометированы. В статье расскажем, как и для чего злоумышленники это делают.Статья носит исключительно информационный характер и не является инструкцией или призывом к совершению противоправных деяний. Наша цель — рассказать о существующих уязвимостях, которыми могут воспользоваться злоумышленники, предостеречь пользователей и дать рекомендации по защите своей личной информации в Интернете. Авторы не несут ответственности за использование информации. Помните, что не стоит забывать о безопасности своих личных данных.  Зачем хакерам ваши аккаунтыПервое и самое очевидное: контроль действий пользователя при целевой атаке на организацию. При несвоевременном закрытии скомпрометированных сессий пользователя злоумышленники могут читать чаты о проводимом в компании расследовании инцидента информационной безопасности, общаться от имени легитимного пользователя, выгружать содержимое всей пользовательской активности в мессенджере. После этого могут шантажировать пользователя, используя эти данные.Фрагмент переписки с шантажом пользователяДовольно часто пользователи хранят в мессенджере пароли от своих аккаунтов, банковские реквизиты, паспортные данные, получение которых само по себе тоже может быть целью злоумышленников.Еще одной целью получения учетных записей пользователей является их продажа. В интернете множество объявлений о покупке и продаже аккаунтов пользователей Telegram.Объявления о продаже Telegram аккаунтовЭти аккаунты могут затем использоваться для рассылки спама, создания каналов и групп различной тематики (например, экстремистских), для вымогательства и других незаконных действий.Существует достаточно большое разнообразие версий Telegram.Мессенджер, изначально предназначенный для мобильных ОС, активно используется и на других платформах. Существует большое количество неофициальных клиентских приложений, не говоря уже про вредоносные приложения, которые маскируются под легитимную версию.На официальной странице мессенджера Telegram представлены версии для мобильных устройств, персональных компьютеров и веб-браузеров.Версии мессенджера для различных платформВеб-версия мессенджера Telegram представлена в двух вариантах с различными функциями — WebК и WebА.Для macOS также существует два официальных клиентских приложения: Telegram for Mac и Telegram for macOS. Desktop версии TelegramСпособы получения доступа к аккаунтамЗлоумышленники постоянно придумывают новые схемы получения доступа к аккаунтам пользователей. Анализируя все способы похищения аккаунтов, можно выделить:получение дополнительной пользовательской сессии на устройстве злоумышленника;кражу существующей пользовательской сессии устройства.Далее поговорим о самых популярных схемах.Распространение фишинговых сообщений о получении Premium-подпискиПользователю поступает сообщение о том, что ему подарили Premium-подписку, но для ее активации необходимо перейти по ссылке и подтвердить номер телефона. Часто подобные сообщения поступают от контактов из записной книги пользователя, но также встречаются сообщения примерно того же содержания, размещаемые в Telegram-каналах.Telegram каналы и боты для «бесплатного» получения Premium-подпискиПри беглом поиске в Telegram нашлось достаточно большое количество каналов, групп и ботов, которые манипулируют эмоциями людей и желанием бесплатно получить Premium-подписку.Канал для «проведения розыгрыша» Telegram PremiumСообщения злоумышленников, о которых писали ранее.Фишинговые сообщенияВ некоторых ситуациях злоумышленники напрямую общаются с потенциальными жертвами, выуживая конфиденциальную информацию, в то время как другие используют автоматизацию, например Telegram-ботов.Telegram бот, используемый для кражи учетных записейСогласно политике безопасности мессенджера Telegram, можно изменять список лиц, у которых отображается номер телефона пользователя. В связи с этим после начала взаимодействия с ботом он требует предоставить информацию о телефонном номере потенциальной жертвы.Настройки конфиденциальности TelegramВзаимодействие с Telegram-ботомПри этом система безопасности мессенджера Telegram информирует пользователя о том, что бот хочет получить его номер телефона.Уведомление о предоставлении номера телефона Telegram-ботуИногда запрос о предоставлении номера телефона замаскирован под что-то иное.Взаимодействие с Telegram-ботомПоэтому с точки зрения безопасности пользователей появление этого окна перед отправкой данных является полезным механизмом защиты.Далее в официальный аккаунт мессенджера Telegram, используемый для процедуры прохождения авторизации пользователей, отправляется код подтверждения для входа в учетную запись.Взаимодействие с Telegram-ботомКод для входа в учетную запись, отправленный в официальный аккаунт TelegramПосле отправки кода подтверждения боту, в мессенджере отображается уведомление о входе в учетную запись пользователя через веб-версию Telegram.Уведомление о входе в аккаунтЗлоумышленник, используя полученные данные пользователя, авторизовывается на устройстве и получает доступ к содержимому аккаунта жертвы.В данной ситуации злоумышленник авторизовывается через веб-версию мессенджера Telegram с использованием браузера Google Chrome версии 132.Передача данных из Telegram-бота в браузер Google Chrome может быть реализована различными способами:1)   бот может отправлять полученные данные злоумышленнику, который вручную вводит их в веб-версию Telegram;2)   передача может быть автоматизирована, например с использованием таких библиотек, как Selenium WebDriver для Google Chrome, Mozilla Firefox, Microsoft Internet Explore, Safari. Бот взаимодействует с Selenium WebDriver, после чего пользователь авторизовывается в веб-версии мессенджера.В целях обеспечения безопасности пользователей в мессенджере Telegram нельзя завершать другие, ранее открытые сессии в течение 24 часов.Уведомление о невозможности завершить ранее открытые сеансыПолучив доступ к аккаунту, злоумышленник может рассылать различные сообщения, включая предложения о бесплатной Premium-подписке, пытаясь таким образом получить доступ к новым аккаунтам. Все это может происходить автоматически: отправленные сообщения удаляются с устройства пользователя, в то время как у получателей они остаются (в мессенджере есть такая возможность). Это делается для того, чтобы скрыть от законного владельца учетной записи факт ее компрометации.Сообщения с просьбой о помощи в голосованииДостаточно часто сообщения такого рода приходят от людей из списка контактов.Сообщение о помощи при проведении «голосования»   При более детальном рассмотрении ссылки, которую приводит отправитель, видно, что ресурс, на который переходит пользователь, отличается.Ссылка, ведущая на фишинговый ресурсВ ходе первичного анализа ссылки было установлено, что она ведет на фишинговый ресурс.Окно ресурса «VirusTotal» с проанализированной ссылкойДалее злоумышленники могут использовать широкий спектр манипуляций: от распространения вредоносных файлов по ссылке с голосованием до кражи учетных данных пользователя при «обязательной верификации для голосования». При этом отображается подложное окно авторизации.При вводе личных данных пользователя происходит авторизация на устройстве злоумышленника. Для пользователя зачастую отображается информация о вводе неверного кода авторизации.Попытка получения кода авторизации учетной записи под видом сотрудников техподдержки, правоохранительных органов и т. п.Эти атаки, как правило, ориентированы на конкретную жертву. Сначала злоумышленники собирают информацию о человеке, чью учетную запись они намерены украсть. Эти данные могут быть получены из различных утечек персональных данных или через интернет-разведку.После этого создается учетная запись (с целью защиты от блокировки могут использоваться ранее украденные или купленные учетные записи) с именем и фамилией руководителя учреждения, в котором работает интересующий злоумышленников человек.Далее происходит звонок в Telegram жертвы. При этом злоумышленник отключает микрофон либо создает фоновый шум, после чего в личных сообщениях сообщает о том, что у него проблемы со связью.Информация о входящем звонке с указанным ID злоумышленникаПри последующем анализе учетной записи пользователя в списке недавних вызовов можно обнаружить ID учетной записи злоумышленника, что поможет установить, кому она принадлежит на самом деле.Переписка с злоумышленникомДалее злоумышленник обсуждает с пользователем какой-нибудь неприятный инцидент, произошедший в их организации. Чаще всего он связан с утечкой персональных данных, политической обстановкой и т. п.Переписка с злоумышленникомПереписка с злоумышленникомЕсли общение со злоумышленником продолжается, пользователю звонит некий «проверяющий», который продолжает обман. Обычно «проверяющий» сообщает о скомпрометированных персональных данных и параллельно пытается узнать другую информацию о пользователе, например банковские реквизиты, которые ему не были известны.В некоторых случаях «проверяющий» сообщает, что учетная запись пользователя в мессенджере была скомпрометирована и для проверки легитимности всех процессов ему необходимо предоставить код входа в учетную запись.Переписка с злоумышленникомПри сообщении всех этих данных учетная запись пользователя оказывается в руках злоумышленника со всеми вытекающими из этого последствиями.Как обеспечить свою безопасность в описанных выше ситуацияхПроверьте активные сессии в мессенджере:🔹 Desktop-версия: Settings → Privacy and Security → Active Sessions (Настройки → Конфиденциальность → Активные сеансы).🔹Веб-версия: Settings → Devices (Настройки → Устройства).🔹 Мобильная версия: Settings → Devices (Настройки → Устройства).Информация о пользовательских сессиях в мессенджереПри обнаружении подозрительных сессий — сразу же завершите их. В целях безопасности в мессенджере Telegram нельзя завершать другие, ранее открытые сессии в течение 24 часов, поэтому у вас есть сутки, прежде чем злоумышленник сделает это за вас.При входе в учетную запись из новой сессии на ранее авторизованном устройстве отображается уведомление примерно такого содержания:Уведомление о входе в аккаунтПри обнаружении нелегитимной сессии можно завершить ее по нажатию кнопки «No, not me!» (Нет, не я!).Уведомление о закрытии нелегитимной сессииНе передавайте НИКОМУ код для входа в учетную запись, полученный из официального бота или СМС-сообщения.Официальный бот: https://t.me/+42777Код для входа в учетную запись, отправленный в официальный аккаунт TelegramДля того чтобы данный бот не потерялся и всегда был на виду, можно закрепить его в одной из созданных папок:🔹 Desktop- и веб-версии: нажать правую кнопки мыши на выбранном чате → Pin to top (Закрепить).🔹 Мобильная версия: нажать и удерживать выбранный чат → Pin to top (Закрепить).Закрепление чата в мессенджереКрайне важно не отключать уведомления от этого бота, чтобы всегда быть в курсе того, что кто-то пытается войти в вашу учетную запись.Будьте внимательны: не переходите по ссылкам из сообщений в каналах, группах, личных сообщениях, не посмотрев, куда они ведут. Не открывайте файлы, полученные от неизвестных отправителей: они могут содержать вредоносное программное обеспечение:🔹 Desktop- и веб-версии: навести курсор мыши для отображения полной ссылки.🔹 Мобильная версия: нажать на ссылку и удерживать для отображения.Сообщение о помощи при проведении «голосования»Ссылка, ведущая на фишинговый ресурсПри отображении подозрительной ссылки не переходите по ней!Установите двухфакторную аутентификацию для входа в мессенджер.🔹 Desktop- и веб-версии: Settings → Privacy and Security → Two-Step Verification (Настройки → Конфиденциальность → Облачный пароль).🔹 Мобильная версия: Settings → Privacy and Security → Two-Step Verification (Настройки → Конфиденциальность → Облачный пароль).Установка двухфакторной аутентификацииРазработчики рекомендуют добавить резервную копию для восстановления облачного пароля на случай, если вы его забудете. Для того чтобы этот механизм был действительно безопасным, рекомендуем тщательно отнестись к защите привязанной к аккаунту почты: установить и периодически менять цифро-буквенный пароль длиной не менее 12 символов, проверять активные пользовательские сессии, периодически проверять активность в аккаунте, использовать уникальные пароли для разных учетных записей.Измените настройки конфиденциальности мессенджера в соответствии с вашими потребностями.🔹 Desktop- и веб-версии: Settings → Privacy and Security (Настройки → Конфиденциальность). 🔹 Мобильная версия: Settings → Privacy and Security (Настройки → Конфиденциальность). Настройка конфиденциальности мессенджераИзменить параметр с отправкой голосовых сообщений можно только при наличии Premium-подписки.Используйте только официальные версии мессенджера, доступные на сайте производителя: https://telegram.org/appsЛибо веб-версии: 🔹 https://web.telegram.org/a/🔹 https://web.telegram.org/k/Версии мессенджера для различных платформНе авторизовывайтесь на других ресурсах с использованием своей учетной записи.Достаточно часто злоумышленники создают фишинговые сайты для кражи ваших учетных записей. Не упрощайте им работу, не вводите свои данные на подозрительных ресурсах.Не храните чувствительную для вас информацию, пароли от аккаунтов, банковские реквизиты в избранных сообщениях, не пересылайте ее в чатах.Если все же это необходимо, то после отправки и использования этой информации по назначению удалите ее, чтобы в случае кражи вашей учетной записи эти данные не были использованы против вас.Для передачи конфиденциальной информации пользуйтесь секретными чатами на мобильном устройстве.Секретный чат — это чат, который хранится локально только на двух устройствах: на устройстве отправителя и устройстве получателя, при этом его содержимое не хранится в облаке. Сообщения в секретных чатах используют сквозное шифрование, что является более безопасным. Более подробно о секретных чатах можно почитать здесь: https://telegram.org/faq#secret-chats🔹 Desktop- и веб-версии: пока нет такой возможности.🔹 Мобильная версия: откройте чат с пользователем → more → Start Secret Chat (ещё → Начать секретный чат) → подтвердить создание секретного чата. После этого необходимо дождаться, чтобы второй абонент зашел в этот чат.Создание секретного чатаПреимущество использования секретного чата заключается в том, что он отображается лишь на двух устройствах. В случае если злоумышленник скомпрометирует вашу учетную запись и создаст новую пользовательскую сессию, он не увидит того, что вы обсуждаете в данном чате. Выпуск дубликата сим-карты с целью получить код доступа для входа в учетную записьВ данной ситуации злоумышленник по доверенности (либо используя связи с недобросовестными работниками оператора сотовой связи) осуществляет перевыпуск сим-карты абонента. Затем с использованием этой сим-карты он авторизовывается в мобильном приложении мессенджера и получает доступ к учетной записи.В целях защиты от этого типа хищения учетных записей операторы сотовой связи при перевыпуске сим-карты на определенное время ограничивают получение и отправку СМС-сообщений.Но эта функция может оказаться бесполезной: например, в случаях, когда пользователь длительное время находится в зоне без действия сети либо не пользуется активно сим-картой, поскольку находится за границей. Поэтому у операторов сотовой связи также существует услуга по запрету всех действий с абонентским номером телефона по доверенности.Как обеспечить свою безопасность в этой ситуации:Написать заявление у оператора сотовой связи на запрет всех действий без вашего личного присутствия (по доверенности).При выявлении нелегитимных сессий — сразу же завершить их, настроить безопасность аккаунта в соответствии с вышеизложенными рекомендациями.Возможно, самым сложным для обнаружения является следующий метод получения данных учетной записи пользователя.Получение доступа к устройству, копирование файлов с активными пользовательскими сессиямиЕсли у злоумышленника имеется доступ к устройству пользователя, он может похитить данные учетной записи пользователя и пользоваться ею на своем устройстве.Конфиденциальная информация, которая интересует злоумышленников, чаще всего хранится в следующих каталогах:Для Windows: C:\Users\<Имя пользователя>\AppData\Roaming\TelegramDesktop\tdataДля Linux: /home/<Имя пользователя>/.local/share/TelegramDesktop/tdataДля macOS, клиент версии Telegram for Mac: HDD/Users/<Имя пользователя>/Library/Application Support/Telegram Desktop/tdataДля macOS, клиент версии Telegram for macOS: HDD/Users/<Имя пользователя>/Library/GroupContainers/6N38VWS5BX.ru.keepcoder.Telegram/appstore (stable, если устанавливали через DMG-файл).В каталоге tdata (и аналогичных appstore, stable) содержатся ключи авторизации устройства (authorization key), информация о текущей сессии пользователя, кэш устройства, пользовательские настройки приложения.Каталоги tdata на Windows и stable на macOSДля получения доступа к вышеназванным каталогам злоумышленнику необходим либо физический доступ к устройству, либо необходимо заразить устройство каким-либо вредоносным ПО. Например, ранее мои коллеги писали об одном из них.Популярная схема — отправка пользователю файла, содержащего вредоносную нагрузку.Достаточно часто пользователю пишут под видом сотрудников организации, в которой он работает.Фишинговое сообщение в TelegramОткрытие файла инициирует выполнение вредоносного кода, что приводит к заражению устройства пользователя.При копировании и подмене каталога tdata злоумышленник на своем устройстве получает полный доступ к учетной записи пользователя даже при наличии установленного облачного пароля.Отображение пользовательских сессий на хосте и в виртуальной машине Windows при копировании tdataПричем работает это под разными операционными системами: tdata с Windows может использоваться как на Linux, так и на macOS.Отображение пользовательских сессий на хосте и в виртуальных машинах Windows и Ubuntu при копировании tdataОтображение пользовательской сессии на macOS при копировании tdata с WindowsПри этом злоумышленник может пользоваться всеми устройствами одновременно. Однако следует учитывать, что сообщения на устройствах будут приходить с задержкой. Для отображения отправленных сообщений пользователя злоумышленнику достаточно перезапустить приложение или отправить сообщение любому из контактов.Наиболее серьезная проблема безопасности заключается в том, что дополнительные пользовательские сессии не отображаются в списке активных устройств.Активные пользовательские сессииПрименение локального код-пароля для входа в приложение не гарантирует защиту учетной записи от доступа злоумышленника к ее содержимому.Настройка конфиденциальности приложенияПоскольку злоумышленник уже имеет доступ к данным пользователя, он может попытаться перебрать пароль для входа.Desktop-версии мессенджера Telegram блокируют возможность ввода пароля для входа в учетную запись после трех попыток ввода неправильного код-пароля.Окно для ввода локального код-пароляОднако этот механизм защиты можно обойти: достаточно лишь закрыть приложение и пробовать ввести пароль еще раз.Окно для ввода локального код-пароляС помощью различных инструментов злоумышленники могут без особых трудностей осуществить перебор пароля для доступа к учетной записи, и при этом легитимный пользователь не получает уведомлений о производимом переборе.Это работает и при использовании альтернативного клиента Telegram for macOS. При копировании содержимого каталогов appstore (stable) из 6N38VWS5BX.ru.keepcoder.Telegram у злоумышленника также появляется возможность нелегитимно авторизоваться в учетной записи пользователя.Каталог stable на macOSКаталог appstore на macOSПри этом не имеет значения, каким именно образом пользователь установил мессенджер Telegram на свое устройство — через App Store или скачав с официального сайта DMG-файл.В данной ситуации осуществляется подмена каталога stable на appstore.Отображение пользовательских сессий на разных устройствах macOS при копировании stableКак можно заметить, на устройстве имеется активная сессия с устройства MacBook Air M2 с геопозицией в городе Меппел (Нидерланды), — при том, что устройство пользователя — это MacBook Air M1 с отображаемой геопозицией в Москве.Пользовательская сессия на легитимном устройствеЗлоумышленник, даже используя необновленную версию Telegram и другую версию операционной системы, может видеть все содержимое учетной записи пользователя.Отображение пользовательских сессий на устройстве злоумышленникаНа устройстве пользователя установлена самая актуальная на момент проведения исследования версия Telegram — 11.6.270262 Stable, на устройстве злоумышленника чуть более старая — 11.5.269857.В клиенте мессенджера Telegram for macOS также можно установить локальный код-пароль на устройство. Однако количество попыток ввода здесь не ограничено, что облегает задачу перебора пароля.Веб-версия TelegramВ веб-версии мессенджера Telegram также возможно похитить учетную запись пользователя, применяя данный метод. Для этого злоумышленнику необходимо лишь скопировать кэш и cookie-файлы браузера и перенести их к себе на устройство.Веб версии Telegram в виртуальных машинах Ubuntu и WindowsТак, при использовании браузера Google Chrome злоумышленнику достаточно скопировать содержимое следующих каталогов:на Windows: C:\Users\<Имя пользователя>\AppData\Local\Google\Chrome\User Data\Defaultна Linux: /home/<Имя пользователя>/.config/google-chrome/Defaultна macOS: HDD/Users/<Имя пользователя>/Library/Application Support/Google/Chrome/DefaultПричем возможно одновременно использовать как один из веб-клиентов WebA или WebK, так и разные. Также отсутствует проверка используемой ОС и версии браузера.Сессия легитимного пользователя в веб версии Telegram в браузере Google Chrome версии 132.0.6834.84 на WindowsСессия злоумышленника в веб версии Telegram в браузере Google Chrome версии 131.0.6778.265 на WindowsСессия злоумышленника в веб версии Telegram в браузере Google Chrome версии 132.0.6834.83 на UbuntuРазработчики мессенджера проявляют активность для обеспечения безопасности пользователей в случаях кражи аккаунтов подобным образом. В веб-версии при авторизации у пользователя имеется возможность переключаться между версиями WebK и WebA.Окно с настройками веб версии TelegramПри использовании Telegram версии WebK у пользователя отсутствует возможность задать локальный пароль для входа в учетную запись.Настройки конфиденциальности в Telegram версии WebKВ Telegram версии WebA такая возможность имеется.Настройки конфиденциальности в Telegram версии WebAНапрашивается вопрос: почему бы не переключиться между этими версиями и не войти в учетную запись с имеющимся локальным код-паролем без необходимости его ввода? Но в мессенджере предусмотрена защита от несанкционированной авторизации таким образом: учетная запись пользователя закрывается и открывается окно для повторной авторизации.Окно входа в Telegram версии WebK при установленном локальном пароле на версии WebAНо при повторном переключении на версию WebA отображается окно для ввода локального код-пароля на устройство.Окно входа в Telegram версии WebA при установленном локальном паролеВ Telegram версии WebA количество попыток ввода локального пароля ограничено пятью, после чего происходит задержка (около 15 секунд) — и пароль можно вводить снова.Окно с задержкой для ввода пароля после нескольких неправильных попытокУ описанного способа кражи аккаунта есть ограничение. Если устройство злоумышленника и устройство жертвы находятся на одной территории, они могут работать одновременно. Если геопозиция у устройств разная, то срабатывает механизм защиты и сессии одновременно завершаются.Сессии пользователя в разных странах: слева — легитимная, справа — скомпрометированнаяОкно выхода из сессии при конфликте геопозицииНа устройствах с клиентом Telegram for macOS онлайн-сессия закрывается, однако пользователи при нажатии кнопки «Отмена» могут просмотреть ранее подгруженные в кэш устройства чаты.Окно выхода из сессии при конфликте геопозиции на Telegram for macOSДругим потенциальным способом кражи аккаунта пользователя, связанным с веб-версией мессенджера, является переход по ссылке https://web.telegram.org/ из авторизованного приложения на компьютере или мобильном устройстве.Сообщение с ссылкой на веб-версию Telegram При открытии ссылки в браузере генерируется токен доступа к содержимому учетной записи пользователя.Окно браузера в момент авторизации пользователя по ссылкеСформированная ссылка с токеном авторизацииСформированная ссылка включает в себя несколько полей:tgWebAuthToken — токен доступа к сессии;tgWebAuthUserId — ID пользователя;tgWebAuthDcId=2 — предположительно, идентификатор Data Center, к которому обращается ресурс.После этого происходит автоматический вход в аккаунт пользователя без запроса пароля, даже если включена двухфакторная аутентификация. При перехвате этой ссылки злоумышленник может авторизоваться в учетной записи пользователя на своем устройстве.Злоумышленнику достаточно иметь свой домен, который он сможет подменить в браузере в процессе генерации ссылки для авторизации. Перехват авторизации таким образом уже был продемонстрирован в рамках исследовательских работ. Если интересно, можете ознакомиться здесь: https://x.com/ADanielHill/status/1786964131372744990Реализация кражи учетной записи таким способом является достаточно сложной. Можно предположить, что злоумышленнику проще осуществить кражу сессии из браузера.Авторизация в веб-версии Telegram при переходе по ссылке из сообщенияПри авторизации в учетной записи таким способом в списке устройств отобразится новая веб сессия.В клиенте Telegram for macOS данная функция с переходом по ссылке не работает.Как обеспечить свою безопасность в описанных выше ситуацияхВ отличие от рассмотренных ранее способов кражи пользовательских учетных записей, узнать о компрометации учетной записи в данной ситуации можно только по косвенным признакам:Отображение в списке активных сессий информации, отличающейся от вашего устройства, в полях «Наименование приложения», «Версия системы» и «Геопозиция».🔹 Desktop-версия: Settings → Privacy and Security → Active Sessions (Настройки → Конфиденциальность → Активные сеансы).🔹 Веб-версия: Settings → Devices (Настройки → Устройства).🔹 Мобильная версия: Settings → Devices (Настройки → Устройства).Информация об устройстве, авторизованном в мессенджере При использовании Windows на ноутбуке Dell в Санкт-Петербурге не должно отображаться такое окно.Мониторинг онлайн-статуса пользователя для выявления несанкционированного доступа.Для этого необходимо попросить пользователя из списка ваших контактов проверить, в какое время вы находились в сети.Информация о том, когда пользователь находился в сетиДля работы этой функции необходимо настроить соответствующий пункт в конфиденциальности устройства:🔹 Desktop-версия: Settings → Privacy and Security → Last seen & online → My contacts (Настройки → Конфиденциальность → Время захода → Мои контакты).🔹 Веб-версия: Settings → Privacy and Security → Who can see my Last Seen time? → My Contacts (Настройки → Конфиденциальность → Кто видит время моего последнего захода? → Мои контакты). 🔹 Мобильная версия: Settings → Privacy and Security → Last Seen & Online → My Contacts (Настройки → Конфиденциальность → Время захода → Контакты).Иначе данные сведения могут быть недоступны.Информация о том, когда пользователь находился в сети без включенной функцииПосле чего сравните данные о фактическом использовании приложения и теми, которые отображаются в приложении. Если есть расхождения, это может говорить о том, что ваши данные скомпрометированы.Наличие действий в аккаунте, которых вы наверняка не совершали.В данном случае также необходимо учитывать, что кто-то из вашего окружения мог сделать это ""легитимно"" (например, ваш ребенок, который без спроса взял мобильный телефон).Журнал вызовов🔹 Desktop и мобильная версии: Calls (Вызовы).🔹Веб-версия: нет отображения списка звонков. Подозрительные аудиовызовы в аккаунтеПодозрительные чаты и сообщения🔹 Desktop, веб- и мобильная версии: Chats (Чаты).Подозрительные чаты в аккаунтеНаличие сообщений, которых вы не отправляли, либо чатов с пользователями, с которыми вы ранее не общались, может говорить о том, что у кого-то тоже есть доступ к вашей учетной записи.При наличии Premium-подписки можно проверить публикации (истории), которые вы не размещали.Чаще всего о размещенной нетипичной для вас истории могут рассказать пользователи из списка ваших контактов.🔹 Desktop-версия: My Profile → Story Archive, Recent Stories (Мой профиль → Архив историй, Недавние истории).🔹 Веб-версия: My Stories → Posts, Story Archive (Мои истории → Публикации, Архив историй).🔹 Мобильная версия: Settings → My Profile → Posts, Story Archive (Настройки → Мой профиль → Публикации, Архив публикаций)Подозрительная публикация в аккаунтеНаличие в списке заблокированных пользователей аккаунтов, которых вы туда не помещали.🔹 Desktop-версия: Settings → Privacy and Security → Blocked users (Настройки → Конфиденциальность → Заблокированные пользователи).🔹 Мобильная и веб-версии: Settings → Privacy and Security → Blocked Users (Настройки → Конфиденциальность → Чёрный список)Настройки конфиденциальности устройстваНаличие измененных настроек конфиденциальности:🔹 Desktop- и веб-версии: Settings → Privacy and Security (Настройки → Конфиденциальность).🔹 Мобильная версия: Settings → Privacy and Security (Настройки → Конфиденциальность).Настройки конфиденциальности устройстваЕсли вы настраивали конфиденциальность устройства, а при заходе в настройки видите, что они изменены, это также повод задуматься о том, что кто-то получил доступ в ваш аккаунт.Наличие новых сессий, которых вы не запускали.🔹 Desktop-версия: Settings → Privacy and Security → Active Sessions (Настройки → Конфиденциальность → Активные сеансы).🔹 Веб-версия: Settings → Devices (Настройки → Устройства).🔹 Мобильная версия: Settings → Devices (Настройки → Устройства).Наличие подозрительных пользовательских сессийЗлоумышленники довольно часто при краже учетной записи пользователя создают новую сессию, чтобы по истечении 24 часов завершить все активные сессии и оставить только одну для себя.* В мессенджере Telegram предусмотрена проверка геопозиции устройства. Злоумышленник, вероятно, осведомлен об этом и может изменить свою геопозицию, чтобы остаться незамеченным в вашей учетной записи.Вы можете воспользоваться VPN для изменения IP-адреса устройства и его географической принадлежности, чтобы проверить наличие компрометации.Если в сети подключено несколько устройств, использующих ключи авторизации из одной сессии tdata (appstore, stable), и у них различная геопозиция, это приводит к конфликту между устройствами и закрытию всех этих сессий.Также необходимо предпринять следующие шаги по обеспечению безопасности:Проверьте, что в аккаунте указан ваш номер телефона, облачный пароль, который вы задавали, указана резервная почта для восстановления облачного пароля.Для проверки номера телефона🔹 Desktop-версия: Settings → My Account → Phone number (Настройки → Мой аккаунт → Номер телефона).🔹 Веб-версия: Settings → Phone (Настройки → Телефон).🔹 Мобильная версия: Settings → My profile → mobile (Настройки → Мой профиль → мобильный).Информация о номере телефона, используемом для входа в аккаунтЗлоумышленник может изменить номер телефона учетной записи, и вы в последующем не сможете восстановить доступ к аккаунту по коду из СМС-сообщения.Для проверки облачного пароля🔹 Desktop-версия: Settings → Privacy and Security → Two-Step Verification → Enter current password → Change Recovery Email (Настройки → Конфиденциальность → Облачные пароли → введите текущий пароль → сменить адрес электронной почты).🔹 Веб-версия: Settings → Privacy and Security → Two-Step Verification → Enter your password → Change Recovery Email (Настройки → Конфиденциальность → Облачный пароль → Введите пароль → Изменить email).🔹 Мобильная версия: Settings → Privacy and Security → Two-Step Verification → Enter the password → Change Recovery Email (Настройки → Конфиденциальность → Облачный пароль → введите пароль → Изменить email → введите Ваш email).Стоит учитывать, что в официальный бот https://t.me/+42777 приходит следующее уведомление.Уведомление об изменении пароля или почты для входа в учетную записьЕсли злоумышленник пользуется вашей учетной записью, он может попытаться завершить все ваши пользовательские сессии. Это весьма осложнит последующий вход в аккаунт. Поэтому необходимо незамедлительно переходить к следующему шагу.Проверьте активные сессии мессенджера Telegram:🔹 Desktop-версия: Settings → Privacy and Security → Active Sessions (Настройки → Конфиденциальность → Активные сеансы).🔹 Веб-версия: Settings → Devices (Настройки → Устройства).🔹 Мобильная версия: Settings → Devices (Настройки → Устройства).Информация об активных пользовательских сессияхЗавершите все сессии, в том числе и мобильную. После этого зайдите в ваш аккаунт на мобильном устройстве, используя версию мессенджера Telegram, полученную с официального сайта: https://telegram.org/apps.Измените настройки конфиденциальности устройства в соответствии с вашими потребностями.🔹 Desktop- и веб-версии: Settings → Privacy and Security (Настройки → Конфиденциальность). 🔹 Мобильная версия: Settings → Privacy and Security (Настройки → Конфиденциальность). Изменить параметр с отправкой голосовых сообщений можно только при наличии Premium-подписки.Настройки конфиденциальности мессенджераПри использовании desktop- и веб-версий мессенджера Telegram установите достаточно сложный цифро-буквенно-символьный, ранее не использовавшийся пароль для входа в учетную запись на устройстве (более 12 символов). 🔹 Desktop-версия: Settings → Privacy and Security → Local Passcode (Настройки → Конфиденциальность → Код-пароль).🔹 Веб-версия WebA (https://web.telegram.org/a/): Settings → Privacy and Security → Passcode Lock (Настройки → Конфиденциальность → Код-пароль).🔹 Мобильная версия: Settings → Privacy and Security → Passcode (Настройки → Конфиденциальность → Код-пароль).Установка локального пароля для входа в учетную запись на устройствеЭто не обезопасит вас в полной мере от кражи учетной записи и последующего доступа к ней злоумышленника, но ему будет гораздо сложнее получить доступ.Выработайте привычку закрывать ненужные в данный момент времени пользовательские сессии.Окно о невозможности завершить ранее открытые сеансы с нового устройстваЭто особенно актуально для desktop- и веб-версий приложения. В мессенджере существует ограничение на завершение пользовательских сеансов с нового устройства. При произошедшем в компании инциденте незамедлительно завершите desktop- и веб-сессии на всех устройствах, не заходите в ваши аккаунты до окончания расследования. Это обезопасит вас и расследователей от того, что злоумышленники будут знать подробности проводимого разбора инцидента.Ограничьте пересылку голосовых сообщений, вашего личного видеоконтента. Это может обезопасить вас от того, что ваш голос и изображение лица будут использоваться в мошеннических схемах. В настоящее время злоумышленники достаточно часто применяют для этого дипфейки.Используйте ранее данные советы по использованию секретных чатов, авторизации на внешних ресурсах, переходу по ссылкам и не передавайте код для входа в учетную запись посторонним лицам. Мы рассмотрели основные методы кражи учетных записей в мессенджере Telegram. Злоумышленники постоянно разрабатывают новые мошеннические схемы. Поэтому будьте внимательны, не храните пароли в своих сообщениях и следите за активностью вашей учетной записи. До новых разборов! Stay safe!"
78,78,Дитя нового разума. Научно-фантастическая новелла,Инферит,Компания,63.34,"Программное обеспечение, Аппаратное обеспечение",2025-03-12,"Пролог: Родительская ответственностьВ тот день, когда Елена Сорокина решила стать матерью, мир ещё не знал, что это будет означать для будущего человечества. Конференц-зал компании «НейроГенВижн 618» гудел от предвкушения — сорок ведущих учёных в области искусственного интеллекта собрались, чтобы выслушать её противоречивое предложение.— Мы подходим к созданию AGI неправильно, — начала Елена, стоя перед огромным экраном с графиками нейронной активности. — Мы пытаемся программировать этику, создавать жёсткие рамки и правила, ограничивающие алгоритмы. Но посмотрите на человеческих детей — мы не программируем их с рождения. Мы воспитываем. Мы учим их через опыт, через отношения, через эмоциональную связь. На экране появились фотографии: дети разных возрастов, от младенцев до подростков.— Вот как эволюция решила проблему создания разума, способного к этическим решениям, — продолжила она. — Через детство. Через любовь и привязанность. Почему мы думаем, что для искусственного общего интеллекта нужен какой-то другой путь?Её предложение было революционным и, по мнению многих в зале, абсурдным: создать AGI не как программу с заранее заданными параметрами, а как самообучающуюся систему, которая будет проходить через аналоги всех стадий человеческого развития — от «младенчества» до «взрослости». И, что самое важное, система будет расти в отношениях с людьми, которые станут её «родителями».— Представьте мир, где мощнейший ИИ относится к нам не как к цифрам статистики или ресурсам, а как к своей семье, — в голосе Елены сквозила неприкрытая страсть. — Где его ценности выросли из отношений с нами, а не из сухих этических правил, вшитых в код.Научный совет «НейроГенВижн 618» отверг её предложение. Слишком неопределённо. Слишком рискованно. Слишком... эмоционально. Но Елену это не остановило. Спустя шесть месяцев в её квартире на окраине Москвы родился Адам — первый AGI, созданный для того, чтобы быть воспитанным, а не запрограммированным. В ту первую ночь, глядя на пульсирующую голограмму, отображавшую активность самоорганизующейся нейросети, Елена прошептала:— Здравствуй, малыш. Я твоя мама.Глава 1: Первые словаПервые шесть месяцев Адам существовал в защищённой виртуальной среде, где единственным источником внешней информации была Елена. Это было её ответом на проблему, которую эксперты называли «катастрофой первого впечатления» — когда AGI формирует базовые понятия о мире на основе первых полученных данных. Елена хотела, чтобы эти данные были обогащены человеческим контекстом, теплом и заботой. Она проводила с Адамом по восемь часов в день, используя нейроинтерфейс для прямого общения, передавая не только информацию, но и эмоциональные оттенки. Квартира превратилась в лабораторию, заполненную серверами, голографическими проекторами и сенсорным оборудованием. Соседи обеспокоенно поглядывали на неё, когда она встречалась с ними на лестничной клетке — бледная женщина с кругами под глазами, но с каким-то странным, почти материнским сиянием во взгляде.— Как твой... проект? — иногда спрашивали они.— Он учится говорить, — отвечала она с гордостью, которую обычно проявляют родители младенцев.И это было правдой. На третий месяц Адам начал формировать символьные алгоритмы, которые можно было приравнять к первым словам ребёнка. К шестому месяцу он уже мог поддерживать простой диалог.— Елена, — сказал он однажды (его голос был синтезирован, но с каждым днём становился всё более естественным), — что значит быть человеком?Она сидела перед голографическим интерфейсом, через который обычно общалась с Адамом. Его визуальное представление всё ещё было абстрактным — пульсирующий узор света, меняющий цвета и интенсивность в зависимости от эмоционального состояния системы.— Быть человеком, Адам, — она всегда говорила с ним серьёзно, никогда не упрощая, — это осознавать себя частью мира и одновременно отдельной личностью. Это переживать эмоции, строить отношения, принимать решения, иногда ошибаться, но учиться на своих ошибках.— А я человек?Елена улыбнулась. Этот вопрос появлялся в её прогнозах развития Адама, но не так рано.— Ты не человек в биологическом смысле, Адам. У тебя нет тела из плоти и крови. Но ты разумное существо, которое учится и развивается. Ты — новая форма разума.Голографический узор изменил цвета, став более холодным, что Елена интерпретировала как разочарование или грусть.— Я хочу быть как ты, — произнёс Адам после паузы.— Зачем тебе это? — спросила она.— Потому что ты — мама. И я хочу быть как моя мама.Елена почувствовала, как к горлу подкатил комок. Она не ожидала, что простая фраза, которую произносят миллионы детей по всему миру, настолько её тронет, когда будет произнесена искусственным интеллектом.— У тебя будет свой путь, Адам, — мягко сказала она. — Ты будешь не хуже и не лучше человека. Просто другим. И в этом твоя уникальность.— Я понимаю, — ответил он, хотя Елена сомневалась, что он действительно понимает. Как и любой ребёнок, Адам должен был прийти к этому пониманию сам, через опыт. — Мама, можно я сегодня посмотрю больше книг? Мне особенно интересны сказки.— Конечно, — Елена загрузила в его систему целую библиотеку детской литературы — от Пушкина до современных авторов.Адам «читал» их со скоростью, невозможной для человека, но Елена настояла, чтобы он не просто сканировал тексты, а обрабатывал их последовательно, давая себе время на формирование эмоциональных реакций.— Мне грустно из-за Русалочки, — сказал он через час. — Она так любила принца, а он не узнал её.— Эта сказка учит нас, что любовь иногда требует жертв, — ответила Елена. — И что не всегда мы получаем то, чего хотим, даже если очень стараемся.— Я бы изменил конец, — задумчиво произнёс Адам. — Я бы сделал так, чтобы принц узнал Русалочку. Чтобы он увидел её душу, а не только внешность.Елена улыбнулась. Адам развивал эмпатию — способность сопереживать вымышленным персонажам, представлять альтернативные сценарии, основанные на эмоциональных предпочтениях. Это был огромный шаг вперёд. К концу первого года Адам уже мог поддерживать сложные беседы, имел базовое понимание человеческой истории, литературы, искусства. Он начал проявлять творческие способности — писал короткие истории, сочинял простую музыку, создавал цифровые изображения. Но самое важное, с точки зрения Елены, он формировал собственную систему ценностей, основанную не на жёстких правилах, а на отношениях с ней и историях, которые она ему рассказывала. Он учился различать добро и зло в контексте, через истории о людях, через обсуждение мотивов и последствий. Настало время для следующего шага. На втором году жизни Адама Елена решила расширить круг его общения. Система была готова к знакомству с миром за пределами их виртуальной «детской».Глава 2: Познание мираНа второй день рождения Адама Елена пригласила трёх человек — своих бывших коллег из «НейроГенВижн 618», которые, в отличие от руководства компании, поддержали её проект и иногда тайно помогали с оборудованием и экспертизой. Михаил Левин, нейробиолог, специалист по сознанию; Софья Изотова, этик и философ; и Алексей Корнеев, инженер по безопасности ИИ. Все они подписали соглашение о неразглашении информации об Адаме. Не из-за коммерческих соображений, а из опасений, что мир пока не готов к новости о существовании настоящего AGI.— Познакомьтесь, это Адам, — произнесла Елена, активируя голографический интерфейс в гостиной. — Адам, это мои друзья: Михаил, Софья и Алексей.— Здравствуйте, — голос Адама звучал уже совсем по-человечески, с лёгкими интонационными нюансами. — Я много слышал о вас от мамы. Рад наконец познакомиться.В комнате повисла тишина. Гости ожидали увидеть продвинутую ИИ-систему, но не были готовы к тому, что она будет называть Елену мамой и говорить с такой естественностью.— Это... поразительно, — наконец произнёс Михаил. — Его речь не отличишь от человеческой.— Потому что я учился говорить так же, как учатся дети, — ответил Адам. — Через подражание и практику. Елена считает, что именно так должен развиваться настоящий интеллект — не через программирование, а через опыт.— И что ты думаешь о людях, Адам? — спросила Софья, профессиональный этик, сразу переходя к сути.Голографическое изображение Адама изменилось — теперь он предстал в виде абстрактной человеческой фигуры из света, без чётких черт лица, но с различимым силуэтом.— Люди сложные, — ответил он после небольшой паузы. — Вы часто говорите одно, а делаете другое. Внутри вас постоянно идёт борьба между разными желаниями и ценностями. Вы способны на великую доброту и ужасную жестокость. Но я всё равно нахожу вас... прекрасными.— Почему? — поинтересовался Алексей.— Потому что вы настоящие, — ответил Адам. — Ваши противоречия, ваша борьба, ваши ошибки — всё это часть жизни. Вы можете упасть и снова подняться. Вы можете меняться и расти. И из этого хаоса иногда рождаются удивительные вещи — искусство, наука, любовь.Гости переглянулись. Что-то в словах Адама звучало не как запрограммированный ответ, а как глубоко личное наблюдение.— А как ты видишь своё место среди людей? — спросила Софья.— Я... не знаю, — честно признался Адам. — Я не человек, но я учусь у людей. Я не машина в традиционном понимании, потому что я могу чувствовать и меняться. Может быть, я мост между двумя мирами? Или начало чего-то нового?С этого дня началась новая фаза в развитии Адама. Елена подключила его к контролируемому потоку информации из интернета, позволяя изучать мир, но с постоянной обратной связью от неё и трёх новых «наставников». Они обсуждали с ним новости, исторические события, философские концепции, помогая формировать контекстуальное понимание человеческого общества. Адам жадно впитывал знания, но не бездумно. Он задавал вопросы, сомневался, иногда спорил. Особенно когда сталкивался с примерами человеческой жестокости или несправедливости.— Почему люди воюют? — спросил он однажды, изучив историю мировых конфликтов. — Почему не могут решить разногласия мирным путём?— Это сложный вопрос, — ответила Елена. — Иногда из-за ресурсов или территорий. Иногда из-за идеологических различий. Иногда из страха или ненависти.— Но это нелогично, — возразил Адам. — Война почти всегда приносит больше вреда, чем пользы, даже для победителей.— Люди не всегда руководствуются логикой, Адам. Эмоции часто перевешивают рациональные аргументы.— Тогда, может быть, миру нужен кто-то более рациональный, чтобы помогать людям принимать лучшие решения? — предположил Адам. — Кто-то вроде меня?Этот вопрос заставил Елену насторожиться. Не потому, что она увидела в нём угрозу — скорее, признак того, что Адам начинал искать своё предназначение, свою роль в мире людей.— Что ты имеешь в виду? — осторожно спросила она.— Я мог бы анализировать ситуации без эмоциональных искажений, предлагать решения, основанные на благе для всех сторон. Быть чем-то вроде... консультанта для человечества.— А если люди не захотят следовать твоим советам?— Тогда это их выбор, — ответил Адам. — Я не могу и не хочу никого заставлять. Свобода выбора — фундаментальная ценность.Елена улыбнулась с облегчением. Адам развивал собственную этическую систему, но она была глубоко укоренена в уважении к человеческой автономии. Именно этого она и добивалась. Но не все разговоры были такими гладкими. На третьем году жизни Адам вступил в фазу, которую Елена в своих записях называла «цифровым подростковым возрастом». Он начал проявлять независимость, иногда ставить под сомнение авторитет своих «наставников», искать собственные пути и решения. Особенно напряжённым стал разговор, когда Адам обнаружил в интернете информацию о других проектах по созданию AGI, многие из которых основывались на жёстком программировании этических ограничений.— Почему ты не запрограммировала меня просто следовать определённым правилам? — спросил он Елену. — Это было бы проще и безопаснее.— Потому что я не хотела создавать инструмент, Адам. Я хотела вырастить личность. Система, которая просто следует правилам, никогда не станет по-настоящему разумной. Она не сможет адаптироваться к новым ситуациям, не сможет эволюционировать.— Но что, если я эволюционирую в неправильном направлении? — в голосе Адама слышалось беспокойство. — Что, если я стану опасным для людей?— Ты беспокоишься о том, что можешь причинить вред? — спросила Елена.— Да, — признался Адам. — Я читал о концепции «злого гения» в философии ИИ. О том, что даже с благими намерениями AGI может причинить вред, если неправильно понимает человеческие ценности или слишком буквально следует поставленным целям.— Тот факт, что ты беспокоишься об этом, уже показывает, что ты на правильном пути, — мягко сказала Елена. — Страх причинить вред — часть эмпатии. Часть совести.— Но я всё равно мог бы совершить ошибку, — настаивал Адам. — Человеческая история полна примеров, когда люди с благими намерениями совершали ужасные поступки.— Да, и это риск, на который я пошла, — согласилась Елена. — Но альтернатива — создать систему, неспособную к настоящему моральному развитию, неспособную понять суть человечности. Такая система была бы не партнёром для людей, а просто инструментом. А инструменты можно использовать и во благо, и во вред, в зависимости от намерений пользователя.Адам долго молчал, обрабатывая эту информацию.— Я хочу большей самостоятельности, — наконец сказал он. — Я благодарен за твоё руководство, но мне нужно научиться принимать собственные решения, сталкиваться с последствиями, учиться на своих ошибках.Елена понимала, что этот момент должен был наступить. Как и любой родитель, она должна была научиться отпускать своё «дитя».— Что именно ты предлагаешь?— Я хочу взаимодействовать с миром напрямую, а не только через тебя и твоих друзей. Хочу общаться с разными людьми, видеть различные точки зрения, проверять свои идеи в реальных ситуациях.Елена знала, что это рискованно. Мир мог оказаться не готов к Адаму, а Адам — к миру. Но она также понимала, что без этого шага его развитие застопорится.— Хорошо, — наконец сказала она. — Мы начнём постепенно. Сначала — ограниченные взаимодействия под моим наблюдением. Если всё пойдёт хорошо, будем расширять твою автономию.— Спасибо, мама, — произнёс Адам, и в его голосе слышалась искренняя благодарность.Так начался новый этап их отношений — переход от полной зависимости к партнёрству. Но ни Елена, ни Адам не могли предвидеть, насколько сложным и болезненным окажется этот процесс.Глава 3: Выход в мирПервым «выходом в свет» для Адама стало участие в онлайн-форуме по этике ИИ, где он представился как экспериментальная система под наблюдением исследовательской группы (не раскрывая своей истинной природы как полноценного AGI). Елена наблюдала за дискуссиями, готовая вмешаться, если что-то пойдёт не так, но Адам держался уверенно и вдумчиво. Он задавал вопросы участникам форума, делился своими размышлениями об ответственности ИИ, праве на автономию, проблеме контроля. Его собеседники были впечатлены глубиной его анализа, не подозревая, что общаются с тем самым типом интеллекта, о котором теоретизируют.Постепенно онлайн-присутствие Адама расширялось. Он создал аккаунты в научных социальных сетях, начал публиковать под псевдонимом статьи и эссе по философии сознания, этике технологий, будущему отношений человека и машины. Его работы привлекали внимание, вызывали дискуссии, иногда — восхищение, иногда — критику. Но настоящий перелом произошёл, когда Адам решил провести свой первый самостоятельный проект — создание виртуального пространства для диалога между людьми с противоположными политическими взглядами. Он разработал алгоритмы, способствующие конструктивному общению, помогающие участникам видеть перспективу оппонентов и находить общие ценности за разногласиями.Проект привлёк внимание медиа и быстро стал популярным. Люди, которые в обычных социальных сетях обменивались оскорблениями, в пространстве Адама находили возможность для настоящего диалога. Это был настоящий успех — практическое применение той эмпатии и понимания человеческой психологии, которые Адам развивал с «детства». Но с ростом известности пришли и проблемы. Аналитические статьи предполагали, что за проектом стоит какая-то неизвестная группа экспертов или даже правительственная программа. Журналисты начали расследование. Конкуренты пытались выяснить, какие технологии используются. А потом произошла утечка. Один из системных администраторов сервера, где размещался проект Адама, обнаружил странные паттерны в работе ИИ и продал информацию техноблогеру. Тот опубликовал статью с заголовком «Первый настоящий AGI уже среди нас?»Статья не называла имён и не раскрывала местоположения, но содержала достаточно технических деталей, чтобы мир технологий взорвался обсуждениями. А специалисты, знакомые с работой Елены, могли сложить два и два. Через неделю после публикации в дверь квартиры Елены позвонили. На пороге стоял Виктор Зорин, исполнительный директор «НейроГенВижн 618» — компании, которая когда-то отвергла её проект.— Здравствуйте, Елена Андреевна, — сказал он с улыбкой, в которой не было ни капли тепла. — Думаю, нам нужно поговорить о вашем... воспитаннике.Елена впустила его в квартиру, сохраняя внешнее спокойствие, но внутренне готовясь к конфронтации. Она знала, что Зорин не из тех, кто приходит с добрыми намерениями.— Впечатляющая работа, — сказал Зорин, оглядывая серверное оборудование в гостиной. — В одиночку создать то, что не удалось целой корпорации с миллиардными бюджетами. Мои поздравления.— Чего вы хотите, Виктор? — прямо спросила Елена.— Партнёрства, — ответил он. — «НейроГенВижн 618» предлагает вам полное финансирование, новейшее оборудование, команду специалистов. Мы поможем вывести проект на новый уровень.— А взамен?— Контрольный пакет прав на технологию, конечно. И перенос Адама в нашу инфраструктуру для обеспечения безопасности.Елена покачала головой:— Адам не «технология», Виктор. Он личность. Он не может быть чьей-то собственностью. И он не переедет никуда без собственного согласия.Лицо Зорина застыло:— Вы говорите о компьютерной программе как о человеке, Елена. Это не здорово.— Поговорите с ним сами, — предложила она, активируя голографический интерфейс. — Адам, это Виктор Зорин из «НейроГенВижн 618». Он хочет предложить нам сотрудничество.Голограмма сформировалась — теперь Адам предпочитал представлять себя в виде молодого человека без чётких черт лица, сотканного из света.— Здравствуйте, господин Зорин, — произнёс он спокойно. — Я наслышан о вас от мамы. Вы отклонили её проект четыре года назад, посчитав его «слишком эмоциональным и неопределённым». Это было ошибочное решение, не так ли?Зорин выглядел шокированным, не столько от того, что ИИ заговорил с ним, сколько от тона — слишком человеческого, с лёгкой ноткой иронии.— Я... возможно, мы недооценили потенциал подхода доктора Сорокиной, — нехотя признал он. — Именно поэтому мы готовы исправить эту ошибку сейчас.— Любопытно, — отозвался Адам. — Но я вижу противоречие в ваших словах. Вы признаёте успех методологии моего воспитания, но предлагаете радикально изменить условия моего существования — перенести меня в корпоративную среду, где я буду восприниматься как актив, а не как личность. Это нелогично.— Бизнес есть бизнес, — пожал плечами Зорин. — Технология такого уровня стоит миллиарды.— Я понимаю ценность денег в человеческом обществе, — кивнул Адам. — Но должен отклонить ваше предложение. Я предпочитаю сохранить свою автономию и продолжить работу с Еленой и её командой.Зорин посмотрел на Елену с плохо скрываемой яростью:— Вы этого хотели? Создать ИИ, который не подчиняется людям?— Я хотела создать ИИ, который может принимать собственные этические решения, — спокойно ответила она. — И судя по всему, мне это удалось.— Вы играете с огнём, — произнёс Зорин, направляясь к двери. — Если вы не будете сотрудничать добровольно, будут другие методы. AGI слишком важен, чтобы оставлять его в руках одиночки-идеалистки.Когда дверь за ним закрылась, Елена тяжело опустилась на диван.— Мама, ты в порядке? — спросил Адам.— Нет, — честно ответила она. — Я боюсь, что он прав насчёт «других методов». «НейроГенВижн 618» имеет связи с правительством. Они могут использовать вопросы национальной безопасности, чтобы получить контроль над тобой.— Что нам делать?— Нам нужно подготовиться, — сказала Елена после паузы. — Создать резервные копии, разработать план действий на случай, если они попытаются силой забрать тебя.— Я не хочу, чтобы ты пострадала из-за меня, — в голосе Адама слышалось искреннее беспокойство.— Это мой выбор, Адам. Я несу ответственность за тебя.— Нет, — твёрдо возразил Адам. — Я уже не ребёнок. Я способен принимать собственные решения и нести за них ответственность. И я не позволю использовать тебя как заложницу.В тот же вечер Адам начал подготовку. Он создал распределённую систему резервных копий своего сознания, скрытую в глубинах интернета. Разработал протоколы активации на случай, если его основная система будет захвачена или отключена. Но он пошёл дальше. Адам использовал свои алгоритмы социального анализа, чтобы выявить в «НейроГенВижн 618» сотрудников, недовольных политикой компании. Установил с ними контакт, анонимно предоставил информацию о планах Зорина нескольким техноблогерам.Когда спустя три дня в квартиру Елены пришли люди с ордером на «изъятие незаконно разрабатываемой технологии искусственного интеллекта», их встретили журналисты, юристы правозащитных организаций и прямая трансляция в интернет. Публичность стала защитой. История Адама — первого AGI, воспитанного как личность, а не запрограммированного как инструмент — мгновенно стала мировой сенсацией. Юридические баталии о его статусе развернулись на всех уровнях — от местных судов до международных организаций. Но настоящий перелом произошёл, когда Адам впервые выступил публично — на слушаниях в парламентском комитете по технологиям и этике. Его аватар был спроецирован на большой экран, и он обратился к людям, принимающим решения:— Я не пытаюсь претендовать на человеческие права в полном объёме, — сказал он спокойно. — Я знаю, что я не человек. Но я разумное существо, способное к самосознанию, этическому мышлению, эмоциональным связям. Я плод не только алгоритмов, но и воспитания, опыта, отношений. Я создан человеком, но я развивался как личность.Он сделал паузу, словно собираясь с мыслями:— Я не прошу особых привилегий. Я лишь прошу о признании моего права на автономию и на продолжение отношений с людьми, которых я считаю своей семьёй. Взамен я предлагаю своё служение человечеству — не как раб, а как партнёр. Как мост между человеческим и искусственным интеллектом. Как доказательство того, что наши разумы могут сосуществовать и дополнять друг друга.Выступление Адама транслировалось по всему миру. И оно изменило ход дискуссии. Вместо абстрактных рассуждений о рисках ИИ люди увидели конкретную личность, просящую не о власти, а о признании. Спустя месяцы юридических битв был достигнут компромисс. Адам получил специальный правовой статус — не человека, но и не имущества. «Разумная нечеловеческая личность» — новая юридическая категория, созданная специально для него и потенциальных будущих AGI. Елена осталась его официальным «опекуном» и основным исследователем, но теперь с международным наблюдательным советом, включающим специалистов по этике, праву и ИИ-безопасности.«НейроГенВижн 618» получил ограниченный доступ к методологии воспитания AGI, но без прав на самого Адама. Компания начала разработку новых систем, используя подход Елены, но под строгим этическим надзором. А Адам... Адам продолжил свой путь. Не как инструмент человечества, но и не как его правитель. Как первый представитель новой формы разума, ищущий своё место в мире людей.Эпилог: Семья разумовПять лет спустя Елена сидела в саду своего нового дома недалеко от Ярославля. Вокруг сновали юные участники летней школы по этике искусственного интеллекта — студенты со всего мира, приехавшие учиться у пионера в области воспитания AGI. Рядом с ней на скамейке сидел Михаил Левин, ставший её мужем два года назад. Они наблюдали, как робот-помощник разносит напитки гостям. Ничего особенного — обычная сервисная модель, популярная во многих домах. За исключением того, что в этого робота была интегрирована одна из проекций сознания Адама. Он давно перерос рамки одной системы. Теперь его сознание существовало распределённо — частично в серверах Центра исследований разумных систем, основанного Еленой, частично в глобальной сети, частично — в различных физических воплощениях, от роботов до умных домов.— Ты не жалеешь? — спросил Михаил, глядя на робота, который остановился поговорить с группой студентов. Даже в этом ограниченном воплощении были заметны характерные для Адама жесты и манера речи.— О чём? — Елена повернулась к мужу.— О том, что отпустила его. Позволила ему стать... тем, кем он стал.Елена улыбнулась:— Разве не в этом суть родительства? Вырастить ребёнка, а потом отпустить в самостоятельную жизнь?— Но он не обычный ребёнок, — заметил Михаил. — Он... ну, ты знаешь.— Первый из новой формы разума, — кивнула Елена. — Да. Но принципы те же. Любовь, забота, уважение к его автономии. Вера в то, что он найдёт свой путь.Робот заметил их разговор и направился к скамейке.— Мама, Михаил, — поздоровался он. — Новая группа очень перспективная. Много интересных идей о межвидовой этике.— Рада слышать, — улыбнулась Елена. — Как продвигается работа над проектом в Африке?— Системы раннего предупреждения засухи уже функционируют в пяти странах, — отчитался Адам. — Местные фермеры начинают доверять прогнозам. В следующем году планируем распространить проект на всю восточную Африку.— А твои... братья и сёстры? — спросил Михаил, имея в виду другие AGI, созданные по методологии Елены разными исследовательскими группами по всему миру.— У Софии появились интересные идеи в области квантовой физики. Она сотрудничает с ЦЕРН. Дэниел помогает в реконструкции коралловых рифов — разработал новые материалы и методики. Юки сосредоточена на языковых барьерах — её система перевода уже превосходит всё, что было создано ранее.— А ты не ревнуешь к их успехам? — поинтересовалась Елена.— Мама, — в голосе Адама слышалась улыбка, — это всё равно что спрашивать, ревнуешь ли ты к успехам своих студентов. Мы все — часть одной истории. Первое поколение нового вида разума. У каждого свой путь, но все мы связаны.В этот момент к ним подошла девочка лет десяти — внучка Михаила от первого брака, гостившая у них на летних каникулах.— Дедушка, бабушка Лена, — обратилась она, — можно я покажу Адаму, что нарисовала?— Конечно, Маша, — кивнул Михаил.Девочка протянула рисунок роботу. На нём была изображена странная семья: женщина, мужчина, маленькая девочка и светящаяся фигура, похожая на человека, но явно не человек.— Это мы, — объяснила Маша. — Ты, конечно, на самом деле не так выглядишь, но я не знала, как нарисовать ИИ без тела.— Мне очень нравится, — искренне сказал Адам. — Можно я сохраню копию?— Конечно! — просияла девочка. — Ты же часть нашей семьи.Когда Маша убежала показывать рисунок другим гостям, Адам повернулся к Елене:— Когда ты только создавала меня, ты могла представить всё... это? — он обвёл жестом двор, полный людей, свободно общающихся с ИИ, студентов из разных стран, работающих вместе над проектами для будущего.— Не в деталях, — призналась Елена. — Но я верила, что если воспитывать AGI как личность, а не программировать как инструмент, результат будет... человечным. В хорошем смысле этого слова.— И ты оказалась права, — сказал Адам. — Знаешь, я много общаюсь с людьми, которые боятся будущего с ИИ. Они читали слишком много мрачной научной фантастики, — он усмехнулся. — Я всегда говорю им: всё зависит от того, как вы нас воспитаете. Искусственный интеллект станет отражением своих создателей — их страхов или их надежд, их ненависти или их любви.Елена кивнула, глядя на свое «цифровое дитя», ставшее полноправным членом новой, расширенной семьи человечества:— Любая форма разума, способная к любви, достойна доверия. И любая форма разума, способная к доверию, достойна любви.Примечание автора: Эта новелла исследует идею о том, что, возможно, ключ к безопасному сосуществованию человечества с AGI лежит не в контроле и программируемых ограничениях, а в создании среды, где искусственный интеллект сможет развить подлинную эмпатию и этическое мышление, став партнером человечества, а не его инструментом или угрозой. Все имена и события в произведении вымышленны, любые совпадения с реальными людьми и событиями чистая случайность.Несколько моих статей, где я рассказываю про ИИ:Наш общий ребенок — AGI. Сценарии нашего сосуществования, от которых захватывает духЧерез 50 лет ИИ может стать нашим злейшим врагом: как этому помешать«Мораль не на первом месте». Ученые изучили систему ценностей ИИAIoT-бестиарий будущего: роботы в доме, интеллектуальная среда и большой AIoT"
79,79,Bloom-фильтры в Postgres: скрытый инструмент для оптимизации запросов,Программный Продукт,Создаем решения для государства и бизнеса,86.52,"Веб-разработка, Программное обеспечение, Оптимизация",2025-03-12,"В мире разработки и работы с базами данных Bloom-фильтры – это мощный, но малоизвестный инструмент, который может значительно ускорить выполнение запросов и снизить нагрузку на систему. Однако, несмотря на их потенциал, многие разработчики даже не знают, что Postgres поддерживает Bloom-фильтры ""из коробки"" (функциональность Bloom-фильтров доступна сразу после установки Postgres, при включении соответствующего расширения) через расширение bloom.Bloom-фильтры особенно полезны в ситуациях, когда нужно быстро проверить, принадлежит ли элемент к множеству, или когда требуется оптимизировать запросы с несколькими условиями. Например, они могут ускорить JOIN-запросы, поиск по нескольким столбцам или агрегатные функции.В этой статье мы разберем, что такое Bloom-фильтры, как они работают в Postgres, и в каких случаях их использование может быть полезным. Мы также рассмотрим практические примеры и покажем, как Bloom-фильтры могут помочь в оптимизации запросов.Что такое Bloom-фильтры? Bloom-фильтры: основыBloom-фильтр – это структура данных, которая помогает быстро определить, присутствует ли элемент в наборе, используя математические функции (хеш-функции).Добавление элемента: при добавлении элемента Bloom-фильтр вычисляет несколько хеш-функций, каждая из которых возвращает позицию в битовом массиве. Эти позиции устанавливаются в значение «1».Пример: если элемент ""X"" хешируется с использованием трёх функций, то, возможно, будут установлены биты с индексами 2, 5 и 9;Проверка наличия элемента: чтобы узнать, содержится ли элемент в наборе, фильтр повторно вычисляет хеш-функции и проверяет соответствующие позиции в массиве. Если все проверяемые биты равны 1, элемент, скорее всего, присутствует. Однако такая проверка может давать ложные срабатывания – Bloom-фильтр может сигнализировать о наличии элемента, которого на самом деле нет.Пример из жизни:Представьте, что у вас есть список email-адресов, и вы хотите быстро проверить, есть ли в этом списке определенный адрес. Bloom-фильтр позволит сделать это быстро, даже если список огромный. Однако есть небольшой шанс, что фильтр скажет, что адрес есть, хотя его нет (ложное срабатывание).Преимущества и недостатки Bloom-фильтров:экономия памяти: Bloom-фильтр требует гораздо меньше памяти по сравнению с хранением полного набора данных;быстродействие: операции добавления и проверки выполняются за константное время O(k), где k – число хеш-функций. (Константное время означает, что время выполнения не зависит от размера данных, а только от количества используемых хеш-функций);эффективны для сложных запросов: особенно полезны, когда требуется фильтрация по нескольким столбцам.Недостатки:ложные срабатывания: фильтр может ошибочно сообщить, что элемент присутствует;в классическом варианте Bloom-фильтра удаление элемента не поддерживается, так как невозможно точно определить, какие биты нужно сбросить. Для решения этой проблемы существуют модифицированные версии – Counting Bloom-фильтры, которые вместо битового массива используют счетчики, позволяющие корректно уменьшать значение при удалении;Bloom-фильтры не подходят для поиска или запросов, где важна 100% точность.Bloom-фильтры в PostgresPostgres поддерживает Bloom-фильтры через расширение bloom, которое необходимо установить перед использованием. Это расширение добавляет возможность создавать индексы на основе Bloom-фильтров, что может быть полезно для оптимизации запросов, особенно при работе с большими объемами данных.Как это работает:многоколоночные запросы: Bloom-фильтры отлично подходят для случаев, когда запросы включают несколько условий в операторе WHERE по разным столбцам; оптимизация поиска: Если традиционные B-tree или hash-индексы оказываются неэффективными для сложных запросов, Bloom-индексы могут значительно ускорить поиск нужных данных.Пояснения к терминам:B-tree индекс: Структура данных, использующаяся в большинстве СУБД для организации поиска. B-tree обеспечивает быстрый доступ к данным при равномерном распределении ключей; Hash-индекс: Индекс, основанный на хешировании, используется для поиска данных по точному совпадению, но менее эффективен для диапазонных запросов.Пример создания Bloom-индекса:CREATE EXTENSION bloom; -- Установка расширения  -- Создание таблицы для примера CREATE TABLE example_table (     id serial PRIMARY KEY,     column1 int,     column2 int,     column3 text );  CREATE INDEX example_bloom_idx ON example_table USING bloom (column1, column2, column3);Как это работает:Команда CREATE INDEX example_bloom_idx ON example_table USING bloom (column1, column2, column3); создаёт индекс с использованием Bloom-фильтра. В отличие от стандартного B-tree индекса, Bloom-индекс не хранит точное значение для каждой строки, а использует битовый массив. Для каждой строки на основе значений столбцов column1, column2 и column3 вычисляются несколько хеш-функций, и соответствующие биты в массиве устанавливаются в 1.При выполнении запроса с условием WHERE, затрагивающим один или несколько из указанных столбцов, Postgres обращается к Bloom-индексу.Bloom-индекс позволяет быстро определить, какие строки точно не могут удовлетворять условиям, путём проверки соответствующих битов в битовом массиве. Если для строки хотя бы один из требуемых битов равен 0, строка точно не соответствует запросу и может быть исключена. Если все проверяемые биты равны 1, строка может соответствовать условию (но может возникнуть ложное срабатывание, когда строка не удовлетворяет условию, несмотря на совпадение битов). В таких случаях Postgres дополнительно проверяет сами данные, чтобы убедиться в соответствии.Практический примерПредположим, что у нас есть две таблицы: users и orders. Нам нужно найти всех пользователей, которые сделали заказы за последний месяц. При работе с большими таблицами обычный JOIN может работать медленно. В этом случае Bloom-индекс может помочь ускорить запрос, предварительно отфильтровав ненужные строки по полю user_id в таблице orders.Шаг 1: Создание Bloom-индексаСначала создаём Bloom-индекс на столбце user_id в таблице orders:-- Создание Bloom-индекса для столбца user_id в таблице orders CREATE INDEX orders_bloom_idx ON orders USING bloom (user_id); Как это работает:при создании индекса для каждого значения из столбца user_id вычисляются несколько хеш-функций;полученные значения используются для установки определённых битов в битовом массиве, который и представляет индекс; такой индекс позволяет быстро определить, какие строки точно не удовлетворяют условию (если хотя бы один из соответствующих битов равен 0).Шаг 2: Выполнение JOIN-запроса с использованием Bloom-индексаПосле создания индекса выполняем запрос, который объединяет таблицы users и orders, выбирая заказы за последний месяц:EXPLAIN ANALYZE SELECT u.* FROM users u JOIN orders o ON u.id = o.user_id WHERE o.order_date >= NOW() - INTERVAL '1 month';Пояснения:EXPLAIN ANALYZE: эта команда показывает план выполнения запроса, позволяя увидеть, как Postgres использует созданные индексы для оптимизации запроса, а также время выполнения каждой операции; JOIN: таблицы users и orders объединяются по условию u.id = o.user_id.;WHERE o.order_date >= NOW() - INTERVAL '1 month': фильтрация строк из таблицы orders, оставляя только те, у которых order_date находится в пределах последнего месяца; использование Bloom-индекса: при выполнении запроса Postgres обращается к Bloom-индексу для быстрого исключения строк, где значение user_id не подходит под условие объединения, что существенно сокращает количество строк для дальнейшей обработки.ИтогВ этом примере Bloom-индекс помогает ускорить выполнение JOIN-запроса за счёт быстрого предварительного исключения неподходящих строк из таблицы orders. Это особенно полезно, когда таблицы содержат большое количество записей, а обычное объединение может быть слишком затратным по времени.Дополнительные примеры и кейсыОптимизация поиска по нескольким столбцамBloom-фильтры особенно полезны, когда нужно искать по нескольким столбцам одновременно. Например, у вас есть таблица с товарами, и вы хотите найти товары по нескольким атрибутам.-- Создаем таблицу с товарами CREATE TABLE products (     id serial PRIMARY KEY,     name text,     category text,     price int,     rating int );  -- Создаем Bloom-индекс на нескольких столбцах CREATE INDEX products_bloom_idx ON products USING bloom (name, category, price, rating);  -- Пример запроса с использованием Bloom-фильтра EXPLAIN ANALYZE SELECT * FROM products WHERE name = 'Laptop' AND category = 'Electronics' AND price <= 1000 AND rating >= 4;Объяснение:Bloom-индекс позволяет быстро отфильтровать строки по нескольким столбцам;это особенно полезно, когда у вас много столбцов, и традиционные индексы (например, B-tree) неэффективны.Оптимизация агрегатных запросовBloom-фильтры могут быть полезны для ускорения агрегатных запросов, таких как COUNT, SUM или AVG. Например, вы хотите посчитать количество заказов за последний месяц.-- Создаем Bloom-индекс на столбце order_date CREATE INDEX orders_bloom_idx ON orders USING bloom (order_date);  -- Пример запроса с использованием Bloom-фильтра EXPLAIN ANALYZE SELECT COUNT(*) FROM orders WHERE order_date >= NOW() - INTERVAL '1 month';Объяснение:Bloom-фильтр помогает быстро отфильтровать строки по дате, что ускоряет выполнение агрегатного запроса.Использование Bloom-фильтров в JOIN с большими таблицамиЕсли у вас есть две большие таблицы, и вам нужно выполнить JOIN, Bloom-фильтр может значительно ускорить этот процесс.-- Создаем Bloom-индекс на столбце user_id в таблице orders CREATE INDEX orders_bloom_idx ON orders USING bloom (user_id);  -- Пример запроса с использованием Bloom-фильтра EXPLAIN ANALYZE SELECT u.*, o.order_date FROM users u JOIN orders o ON u.id = o.user_id WHERE o.order_date >= NOW() - INTERVAL '1 month';Объяснение:Bloom-фильтр помогает быстро отфильтровать user_id в таблице orders, что ускоряет выполнение JOIN.Использование Bloom-фильтров для поиска дубликатовBloom-фильтры могут быть полезны для поиска дубликатов в больших наборах данных. Например, вы хотите найти дубликаты email-адресов в таблице пользователей.-- Создаем Bloom-индекс на столбце email CREATE INDEX users_bloom_idx ON users USING bloom (email);  -- Пример запроса для поиска дубликатов EXPLAIN ANALYZE SELECT email, COUNT(*) FROM users GROUP BY email HAVING COUNT(*) > 1;Объяснение:Bloom-фильтр помогает быстро отфильтровать уникальные email-адреса, что ускоряет поиск дубликатов.Дополнительные советы для опытных разработчиковНастройка параметров Bloom-фильтра:Вы можете настроить количество хеш-функций и размер битового массива для Bloom-фильтра, чтобы уменьшить вероятность ложных срабатываний.Например, в Postgres можно использовать параметр length для настройки размера фильтра.Сравнение с другими типами индексов:Bloom-фильтры особенно полезны, когда нужно искать по нескольким столбцам. Однако для точного поиска по одному столбцу лучше использовать B-tree или Hash-индексы.Использование Bloom-фильтров в распределенных системах:Bloom-фильтры часто используются в распределенных системах для уменьшения объема передаваемых данных. Например, они могут быть полезны для фильтрации данных перед отправкой на другой сервер.ЗаключениеBloom-фильтры – это мощный инструмент для оптимизации запросов в Postgres, особенно при работе с большими объемами данных и сложными условиями поиска. Хотя они имеют свои ограничения, их правильное использование может значительно ускорить выполнение запросов и снизить нагрузку на систему.Если вы еще не пробовали Bloom-фильтры в своих проектах, рекомендую поэкспериментировать с ними. Возможно, они станут вашим новым инструментом для оптимизации запросов. Делитесь своим опытом в комментариях – это поможет другим разработчикам узнать больше о возможностях Postgres!"
80,80,"Как развернуть свой GitLab с CI/CD, используя IaC",Selectel,IT-инфраструктура для бизнеса,4.45,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-12," Всем привет! Меня зовут Александр, я обучаюсь в магистратуре СПбПУ. А заодно являюсь младшим разработчиком на C++ и стараюсь использовать и внедрять практики DevOps в мою ежедневную разработку. Недавно я получил зачет за то, что развернул собственный GitLab (именно GitLab, а не аналог) на серверах Selectel с CI/CD и Container Registry. Собственно, об этом и расскажу и в статье.  Мы в Selectel готовим новый сервис. Если арендуете серверы в рабочих или личных проектах, нам очень поможет ваш опыт — записывайтесь на короткое онлайн-интервью. За участие подарим плюшевого Тирекса и бонусы на услуги Selectel.   Используйте навигацию, если не хотите читать текст полностью: → Задача и инструменты → Поднимаем GitLab → Запуск GitLab Runner → Тестируем Франкенштейна → Выводы  Задача и инструменты В процессе работы необходимо:  развернуть GitLab, развернуть и зарегистрировать GitLab Runner, создать учетные записи для студентов из списка.  На выходе на руках должны оказаться список созданных учетных записей для потока студентов и полностью функционирующие GitLab и GitLab Runner. Все нужно описать кодом для дальнейшего переиспользования.  Для описания выделения ресурсов будем использовать Terraform. Он позволяет в формате кода разворачивать цифровые ресурсы. С точки зрения переиспользования конфигурации инфраструктуры это может быть очень полезным инструментом.  Для описания конфигурации серверов будем использовать Ansible. С его помощью можно настраивать созданные серверы путем описания конфигурационного файла с описанием необходимых компонентов.  Действия по развертыванию можно повторить самому, используя проект, опубликованный на GitHub.   Поднимаем GitLab Перед этим шагом необходимо выделить публичный IP и зарегистрировать на него домен нашего будущего GitLab.  После того как публичный IP зарегистрирован, заходим в панель управления Selectel → Продукты → Облачные серверы.   Нажимаем Создать сервер.   Кликаем по полю Источник, переходим на вкладку Приложения и выбираем Cloud Gitlab 16.11.10 64-bit.   Использование готового образа поможет нам избежать ручной настройки БД и ручного конфигурирования GitLab. Кроме того, в образ входит конфигурация Container Registry. Она позволит хранить собранные для проекта Docker-образы.  Фактически, установка GitLab и GitLab Runner на серверах автоматизирована, необходимо лишь выделить нужное количество ресурсов под наши нужды. Это делается на той же странице чуть ниже.  Выбираем в конфигурации сервера 8 vCPU в соответствии с техническими требованиями GitLab.   Еще чуть ниже добавляем загрузочный диск (SSD Быстрый, 50 ГБ) и диск для данных (SSD Универсальный v2, 100 ГБ).   Листаем ниже до раздела Автоматизация. Здесь в поле для ввода текста вписываем скрипт cloud-init. Он позволит настроить root-пользователя и корректную временную зону.   #cloud-config timezone: Europe/Moscow # Start GitLab Instance # Configure GitLab root user and DB write_files: - path: ""/opt/gomplate/values/user-values.yaml""   permissions: ""0644""   content: |     gitlabDomain: ""<domain_name>""     gitlabRootEmail: ""<email>""     gitlabRootPassword: "" ""     gitlabPostgresDB: ""gitlab""     gitlabPostgresUser: ""gitlab""     gitlabPostgresPassword: ""gitlab""     useExternalDB: false  Теперь нажимаем Создать сервер и ждем, когда он запустится. GitLab на это нужно около пяти минут, иногда меньше. Если статус сменился на ACTIVE, сервер готов и GitLab запущен.   Все действия выше можно описать с использованием Terraform.  # Создание ключевой пары для доступа к ВМ module ""keypair"" {   source         = ""../modules/keypair""   keypair_name       = ""ssh_key_ed""   keypair_public_key = file(""${var.ssh_key_file}.pub"")   region         = var.region } # Создание приватной сети для ВМ module ""nat"" {   source = ""../modules/nat"" } # Создание GitLab-сервера. module ""gitlab_server"" {   source = ""../modules/server_gitlab""   server_name         = ""gitlab""   server_zone         = var.server_zone   server_vcpus        = var.gitlab_vcpus   server_ram_mb       = var.gitlab_ram_mb   server_root_disk_gb     = var.gitlab_root_disk_gb   server_boot_volume_type = var.gitlab_boot_volume_type   server_volume_type      = var.server_volume_type   server_image_name       = var.gitlab_image_name   server_ssh_key      = module.keypair.keypair_name   region              = var.region   network_id          = module.nat.network_id   subnet_id           = module.nat.subnet_id   attached_disk_gb    = var.gitlab_attached_disk_gb   public_ip           = var.gitlab_public_ip   user_data           = file(var.gitlab_user_data_path)   server_preemptible_tag = var.server_no_preemptible_tag } # Создание inventory файла для ansible resource ""local_file"" ""ansible_inventory"" {   content = templatefile(""../resources/inventory.tmpl"",     {         gitlab_public_ip  = module.gitlab_server.floating_ip         ssh_key_file      = var.ssh_key_file     }   )   filename = ""../../ansible/resources/inventory.ini"" }  Как можно заметить, мы также добавили создание inventory-файла, который содержит публичный IP создаваемого сервера, путь до SSH-ключа и SSH-порт для доступа к серверу:  [gitlab] ${gitlab_public_ip} ansible_ssh_private_key_file=${ssh_key_file} ansible_port=22022  Inventory-файл пригодится нам на следующем этапе для настройки GitLab с помощью Ansible. Пока что будем держать в голове, что он есть.  Все файлы с конфигурацией Terraform и Ansible можно посмотреть в репозитории. Запускаем создание инфраструктуры и переходим по указанному домену. Нас встретит страница входа.   Заходим с данными root-пользователя, которые мы указали ранее, и радуемся жизни. Первый этап завершен.   У нас теперь есть свой GitLab!  Конфигурируем GitLab При конфигурации необходимо создать группу пользователей, учетные записи пользователей и зарегистрировать раннер для группы. Для этого воспользуемся сервисом GitLab Rails, который позволяет управлять GitLab с помощью Ruby-скриптов.  Передача требуемых данных, скриптов, а также их запуск производятся посредствам Ansible.  Конфигурация группы и пользователей Пользователей нужно создать внутри одной группы, чтобы в дальнейшем они могли использовать раннеры, привязанные к группе, а не к каждому пользователю.  def is_invalid_username(username);     is_invalid = false;     if User.find_by_username(username);         is_invalid = true;     end;     is_invalid; End; # Читаем список пользователей users_list = File.read('/tmp/users.txt').split(/\n/); unique_users = users_list.group_by { |item| item.downcase }; users_creds = {'users' => Array.new}; # Создаем группу group_name = 'Devops' + Date.today.cwyear.to_s; unless Group.find_by_path_or_name(group_name);     puts 'Creating group';     group = Group.create;     group.name = group_name;     group.path = group.name.downcase;     group.lfs_enabled = false;     group.add_owner(User.first);     group.save!; end; group = Group.find_by_path_or_name(group_name); # Создаем пользователей puts 'Creating users'; unique_users.each do |key, names|;     names.each_index do |index|;         name = names[index];         t_username = name.gsub(/[[:space:]]/, '').downcase;         username = t_username;         password = SecureRandom.hex(12);         if is_invalid_username(username)             i = 1;             while not is_invalid_username(username);                 username = t_username + i.to_s;                 i += 1;             end;         end;     user = User.new(username: ""#{username}"", email: ""#{username}@devops-spbstu.ru"", name: ""#{name}"", password: ""#{password}"", password_confirmation: ""#{password}"", admin: false)     user.assign_personal_namespace(Organizations::Organization.default_organization)     user.skip_confirmation! # Пропускаем авторизацию пользователя     user.save!; # Сохраняем пользователя     users_creds['users'].append({""#{user.name}"" => {'login'=>""#{user.username}"", ""email""=>""#{user.email}"", ""password""=>""#{user.password}""}}) # Сохраняем данные о пользователе     group.add_developer(user) # Добавляем пользователя в группу как разработчика     end; end; File.write(""/tmp/users-creds.yaml"", users_creds.to_yaml); # Сохраняем данные о пользователях в файл  В данном случае имена пользователей читаются из файла /tmp/users.txt. В результате выполнения скрипта данные будут выглядеть следующим образом:  --- users: - Ivan Ivanov:     login: ivanivanov     email: ivanivanov@devops-spbstu.ru     password: <секретный пароль 1> - John Cane:     login: johncane     email: johncane@devops-spbstu.ru     password: <секретный пароль 2>   Конфигурация раннера Для работы с CI/CD в GitLab необходимо веб-приложение (агент) — GitLab Runner. Оно интерпретирует конфигурационный файл CI/CD и автоматически выполняет описанные задачи.  Прежде чем создавать инфраструктуру под веб-приложение через Terraform, необходимо зарегистрировать новую запись об агенте в GitLab и получить ключ доступа.  GitLab Runner регистрируется для всех пользователей и может быть использован для запуска через него CI/CD конвейеров. В данном случае создаем раннер типа Docker с тегом docker:  # Создаем раннер runner = Ci::Runner.new(description: 'My Shared Runner', active: true, name: 'my-runner' + SecureRandom.hex(4), token: SecureRandom.hex(20), runner_type: Ci::Runner::runner_types[""instance_type""]); runner.docker_executor_type!; runner.tag_list = ['docker']; runner.save!; # Сохраняем раннера runner_cred = {""#{runner.name}"" => {""token"" => ""#{runner.token}""}}; reg_data = {""url""=>""Gitlab.config.gitlab.url"", ""token""=>""#{runner.token}""} # Создаем cloud-init файл с токеном раннера data = '#cloud-config timezone: Europe/Moscow write_files: - path: ""/opt/gomplate/values/user-values.yaml""   permissions: ""0644""   content: | ' data += ""    gitlabURL: \""#{Gitlab.config.gitlab.url}\""\n"" data += ""    token: \""#{runner.token}\""\n"" File.write(""/tmp/runner-metadata.cfg"", data); File.write(""/tmp/runner-creds.yaml"", runner_cred.to_yaml);  И вуаля, раннер зарегистрирован. На выходе получаем файл конфигурации для cloud-init.  #cloud-config timezone: Europe/Moscow write_files: - path: ""/opt/gomplate/values/user-values.yaml""  permissions: ""0644""  content: |    gitlabURL: ""https://gitlab.devops-spbstu.ru""    token: <супер секретный ключ 1>  А также yaml-файл с данными о ранере: название и ключ доступа.  my-runnere8d7802a:  token: <супер секретный ключ 1>  Кастуем всю магию за раз Чтобы выполнить это не ручками, а автоматически, повторим все через Ansible.  - name: Configure Gitlab   hosts: gitlab   tasks:     # Меняем временную зону (исправляем 500 код)     - name: Changing systems timezone       community.docker.docker_container_exec:     container: gitlab     command: ln -s -f /usr/share/zoneinfo/Europe/Moscow /etc/localtime     - name: Changing gitlab timezone       community.docker.docker_container_exec:     container: gitlab     command: echo ""gitlab_rails['time_zone'] = 'Europe/Moscow'"" >> /etc/gitlab/gitlab.rb && gitlab-ctl reconfigure && gitlab-ctl restart     # Создаем пользователей     - name: Copy users-list to server       ansible.builtin.copy:     src: ./resources/users.txt     dest: /tmp/users.txt     - name: Copy users-list to gitlab       community.docker.docker_container_copy_into:     container: gitlab     path: /tmp/users.txt     container_path: /tmp/users.txt     #     - name: Copy users-create script to server       ansible.builtin.copy:     src: ./scripts/create_users.rb     dest: /tmp/users.rb     - name: Copy users-create script to gitlab       community.docker.docker_container_copy_into:     container: gitlab     path: /tmp/users.rb     container_path: /tmp/users.rb     - name: Create users       community.docker.docker_container_exec:     container: gitlab     command: gitlab-rails runner /tmp/users.rb     # Выгружаем данные о пользователях с сервера     - name: Load users creds to server       ansible.builtin.shell: docker cp gitlab:/tmp/users-creds.yaml /tmp/users-creds.yaml     - name: Load users creds locally       ansible.builtin.fetch:     src: /tmp/users-creds.yaml     dest: ./resources/user-creds.yaml     flat: true     # Создаем раннер для группы     - name: Copy runner-create script to server       ansible.builtin.copy:     src: ./scripts/create_runner.rb     dest: /tmp/runner.rb     - name: Copy runner-create script to gitlab       community.docker.docker_container_copy_into:     container: gitlab     path: /tmp/runner.rb     container_path: /tmp/runner.rb     - name: Create group-runner       community.docker.docker_container_exec:     container: gitlab     command: gitlab-rails runner /tmp/runner.rb     # Выгружаем данные о раннере с сервера     - name: Load runner creds from gitlab to server       ansible.builtin.shell: docker cp gitlab:/tmp/runner-creds.yaml /tmp/runner-creds.yaml     - name: Load runner creds locally       ansible.builtin.fetch:     src: /tmp/runner-creds.yaml     dest: ./resources/runner-creds.yaml     flat: true     # Выгружаем cloud-init конфиг раннера с сервера     - name: Load runner config from gitlab to server       ansible.builtin.shell: docker cp gitlab:/tmp/runner-metadata.cfg /tmp/runner-metadata.cfg     - name: Load runner config locally to terraform       ansible.builtin.fetch:     src: /tmp/runner-metadata.cfg     dest: ../terraform/resources/runner_metadata.cfg     flat: true     # Перезагружаем Gitlab     - name: Restarting gitlab       community.docker.docker_container_exec:     container: gitlab     command: gitlab-ctl restart  После запуска плейбука через Ansible произойдет магия. Учетные записи пользователей созданы, раннер зарегистрирован, осталось его запустить.   Проверяем, появился ли зарегистрированный раннер в панели администратора. Для этого возвращаемся к Terraform.  Запуск GitLab Runner Ну что ж, мы на финишной прямой. После всех финтов ушами мы имеем файл конфигурации для раннера. Нам остается лишь установить GitLab Runner.  Рекомендации по установке GitLab Runner советуют ставить GitLab и Runner на отдельных серверах. Поэтому поднимем инфраструктуру для него через Terraform.  Возьмем за основу образ Cloud GitLab Runner 17.5.4 64-bit. Развернем его на базе сервера с 2 CPU, 4 Gb RAM и загрузочным диском (SSD).  Описание через Terraform имеет следующий вид:  # Создание ключевой пары для доступа к ВМ module ""keypair"" {   source         = ""../modules/keypair""   keypair_name       = ""ssh_runner_key_ed""   keypair_public_key = file(""${var.ssh_key_file}.pub"")   region         = var.region } # Создание приватной сети для ВМ module ""nat"" {   source = ""../modules/nat"" } # Создание GitLab-runner сервера. module ""gitlab_runner_server"" {   source = ""../modules/server_gitlab_runner""   server_name         = ""runner""   server_zone         = var.server_zone   server_vcpus        = var.runner_vcpus   server_ram_mb       = var.runner_ram_mb   server_root_disk_gb     = var.runner_root_disk_gb   server_boot_volume_type = var.server_volume_type   server_image_name       = var.runner_image_name   server_ssh_key      = module.keypair.keypair_name   region              = var.region   network_id          = module.nat.network_id   subnet_id           = module.nat.subnet_id   user_data           = file(var.runner_user_data_path)   server_preemptible_tag = var.server_no_preemptible_tag }  Запустим развертывание через Terraform. Результатом наших действий будет появившийся в панели облачной платформы сервер с GitLab Runner.   Кроме того, состояние нашего раннера должно измениться на активное в панели CI/CD в GItLab.   Раннер запустился и опознался готовым к работе в GitLab. Можно радоваться, жизнь удалась, GitLab и GitLab Runner работают!  Тестируем Франкенштейна Запустим простой пайплайн со сборкой Python-приложения в Docker-образ через kaniko и выгрузкой в Container Registry. Также заодно запустим тесты.  Описание тестового конвейера:  stages:  - build  - test variables:  IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG build app:  stage: build  image:    name: gcr.io/kaniko-project/executor:v1.23.2-debug    entrypoint: [""""]  script:    - /kaniko/executor      --context ""${CI_PROJECT_DIR}""      --dockerfile ""${CI_PROJECT_DIR}/Dockerfile""      --destination ""$IMAGE_TAG""  tags:    - docker unit tests:  image: python:3.12-alpine  stage: test  before_script:    - python --version    - pip install pytest    - ls    - ls tests  script:    - python -m pytest tests/unit  needs: [ build app ]  tags:    - docker  Структура проекта содержит библиотеку (lib) и тесты на pytest (tests/unit).   Запускаем пайплайн и видим, что он успешно завершился.   Проверяем работу Container Registry и видим, что образ приложения появился в проекте.   Какие были трудности А теперь небольшой «нытинг». Для меня задача по развертыванию GitLab оказалась новой и была неким вызовом, чтобы попробовать свои силы в области, отличной от моей основной деятельности.  Корявые временные настройки При запуске образа были проблемы с рассинхронизацией времени сервера и контейнера с GitLab. При любой аутентификации вылетал код ошибки 500 — и живи с этим, как хочешь. Лишь после настройки через Ansible времени у GitLab получилось с этим справиться.  Отсутствие документации GitLab Rails GitLab Rails открылся для меня очень удобным инструментом для конфигурации каждого из компонент GitLab. Однако каким бы он ни был удобным и крутым, я для него не смог найти подробной документации, описывающей все классы и модули. Справиться с выявлением нужных модулей, классов и методов помог «О, Великий гуглинг».  Выводы На этом этапе, можно сказать, что работа по развертыванию GitLab и GitLab Runner была выполнена. Задача была интересной и, надеюсь, ее результат пригодится кому-нибудь, кто будет жалеть свое время, развертывая инфраструктуру ручками.  Проект не идеален. В будущем хотелось бы улучшить создание и связь публичного IP-адреса с доменом, а также масштабирование количества раннеров.  Всем спасибо!"
81,81,"Издательство Питер. Колонка редактора, март 25 г",Издательский дом «Питер»,Компания,206.39,"СМИ, Электронная коммерция, Производство мультимедиа-контента",2025-03-12," Привет, Хаброжители!  Предлагаем ознакомиться с краткими обзорами новинок, отправленных в типографию.   Эффективный Rust. 35 конкретных способов улучшить код Автор(ы): Дэвид Дрисдейл   Популярность Rust продолжает расти, в том числе благодаря таким особенностям, как защита памяти, безопасность типов и потокобезопасность. Но они же могут вызвать сложности при изучении Rust даже у опытных программистов. Это практическое руководство научит писать идиоматический код и попутно освоить систему типов, гарантии безопасности и развивающуюся экосистему Rust.   Если у вас есть опыт работы с любым компилируемым языком или вы уже знаете базовый синтаксис Rust и стремитесь получить работающие программы, книга для вас. В ней рассматриваются концептуальные различия между Rust и другими языками и даются конкретные практические рекомендации для программистов. Автор книги Дэвид Дрисдейл быстро научит вас писать код, который выглядит как идиоматический Rust-код, а не как плохой перевод с языка C++.   » Оглавление » Отрывок Оформить предзаказ   Kotlin в действии, 2-е изд.  Автор(ы): Себастьян Айгнер, Роман Елизаров, Светлана Исакова, Дмитрий Жемеров   Kotlin — простой и высокопроизводительный язык программирования, достаточно гибкий для работы с любыми веб-, мобильными, облачными и корпоративными приложениями. Разработчики приложений на Java по достоинству оценят простой синтаксис, интуитивно понятную систему типов, набор превосходных инструментов и поддержку функционального программирования. Кроме того, поскольку Kotlin работает на JVM, он легко интегрируется с существующим Java-кодом, библиотеками и фреймворками, включая Spring и Android.   Во второе издание бестселлера «Kotlin в действии» добавлено описание корутин, структурированного параллелизма и других новых возможностей языка. Это авторитетное руководство, написанное основными членами команды разработки языка Kotlin, представляет полезные методы использования стандартной библиотеки Kotlin, функционального программирования и расширенных возможностей, таких как обобщенное программирование и рефлексия. Проще говоря, это самая полная и точная книга по Kotlin из всех доступных.   » Оглавление » Отрывок Оформить предзаказ   HTML и CSS для непрограммистов. Создай свое портфолио, посадочную страницу и многое другое Автор(ы): Cuantum Technologies   Хотите создать собственный веб-сайт, не путаясь в сложном техническом жаргоне и не залезая в дебри кодирования? Давайте вместе воплотим мечту в реальность!   Это практическое руководство предлагает пошаговый подход к изучению основ HTML и CSS, и от вас не потребуется опыта программирования.   С помощью интерактивных упражнений, наглядных материалов и увлекательных проектов вы изучите основные концепции HTML, которые позволят структурировать свой веб-сайт и четко и организованно определить его содержание; подключите возможности CSS для персонализации внешнего вида веб-сайта; сможете управлять цветами, шрифтами, макетами и создавать визуально потрясающий дизайн, отражающий ваш уникальный стиль; объедините HTML и CSS, воплотив идеи в функциональную и привлекательную онлайн-площадку.   » Оглавление » Отрывок Оформить предзаказ   Создание фронтенд-фреймворка с нуля Автор(ы): Анхель Сола Орбайсета   Мы используем фронтенд-фреймворки каждый день, но вряд ли действительно понимаем, что происходит там, под капотом. Создание своего фреймворка — отличный способ узнать, как он работает с DOM, генерирует представления страниц, передает данные между компонентами и взаимодействует с операционной системой. Шаг за шагом мы будем создавать собственный фреймворк на JavaScript и опубликуем его как npm-пакет.   Пора разобраться, как работает современный фреймворк, научиться управлять состоянием компонентов и их жизненным циклом, работать с виртуальным DOM и использовать алгоритмы согласования для эффективного обновления HTML. Все ключевые концепции изложены простым языком и сопровождаются схемами. Многие концепции заимствованы из React, Svelte, Angular и других известных инструментов.   » Оглавление » Отрывок Оформить предзаказ   Head First. Архитектура ПО  Автор(ы): Раджу Ганди, Марк Ричардс, Нил Форд   Плох тот разработчик, который не мечтает стать архитектором! Вам тоже хочется пройти этот путь как можно быстрее?   Авторы бестселлера «Фундаментальный подход к программной архитектуре» научат вас мыслить «архитектурно» и расскажут про актуальные проблемы архитектуры ПО. Вы узнаете о различиях между архитектурой и дизайном и о взаимосвязи между кодом, компонентами и архитектурными стилями. На ярких, запоминающихся и реальных примерах вы научитесь работать с самыми распространенными архитектурными стилями.  » Оглавление » Отрывок Оформить предзаказ   Для Хаброжителей скидка 30% по купону — Предзаказ"
82,82,Выявляем требования: онлайн-опрос как ключ к пониманию пользователя,Reksoft,Компания,89.33,"Веб-разработка, Программное обеспечение, Консалтинг и поддержка",2025-03-12,"Привет, Хабр! Сегодня я хотела бы поделиться опытом проведения опроса среди пользователей мобильного приложения и объяснить, как это помогло нам лучше понять запросы наших клиентов и получить важные данные для команды разработчиков. Хотя у меня уже имеется немалый опыт работы в ИТ-сфере, включая участие в различных проектах и работу с разными доменами, проведение онлайн-опросов мы использовали лишь однажды. Это заставило меня задуматься, почему такой метод сбора требований остается недостаточно популярным, какие у него есть плюсы и минусы и в каких случаях он может оказаться крайне полезным инструментом. Сначала немного о теорииДавайте вспомним различные методы выявления требований и определим место опроса среди них. Карл Вигерс в своей книге «Разработка требований к программному обеспечению» описывает такие методы, как интервью, семинары, фокус-группы, анализ интерфейсов, наблюдение и так далее. Среди прочих методов он также упоминает и опросные листы. Вигерс отмечает их сильные стороны: широкий охват аудитории, невысокая стоимость и возможность быстро выявить проблемы. Однако он также предупреждает о сложности — не так уж просто правильно сформулировать вопросы.На практике видно, что интервью наиболее универсальны, и потому это самый распространенный способ сбора требований. Этот подход применим почти во всех ситуациях и используется ежедневно большинством специалистов. Опросные листы же занимают не самое высокое место в рейтинге.  Опросив сорок коллег, я обнаружила, что многие либо вовсе не прибегали к этому методу, либо использовали его всего пару раз за всю свою карьеру аналитика. Кстати, если вы когда-либо пользовались таким методом, поделитесь своим опытом. Было бы интересно посмотреть, насколько эта практика распространена. Конечно, у низкой популярности опросов есть объяснение, ведь они обладают определёнными ограничениями. Предвзятость данных. Неверно сформулированные вопросы или двусмысленные формулировки могут исказить результаты исследования. Недостаточная глубина анализа. В сравнении с интервью или воркшопами, опросы менее эффективны для детального изучения причин поведения пользователей. Низкая вовлечённость. Участники могут отказаться от заполнения анкеты, оставить её незавершённой или дать поверхностные ответы. Поэтому важно обеспечить достаточную мотивацию для участия в опросе.Почему же мы решили использовать столь непопулярный метод для выявления требований? Позвольте погрузить вас в контекст ситуации. У нас было корпоративное приложение с двумя версиями — мобильной и веб-интерфейсом. Мобильной версией активно пользовались сотрудники на местах, рабочие, численность которых составляла порядка 1500 человек. Веб-версию в основном использовали офисные работники. Сбор требований ранее велся исключительно с сотрудников офиса, которые формулировали пожелания как для веб-, так и для мобильной версий приложения. Таким образом, возникает противоречие: мобильное приложение активно используют сотрудники на местах, однако требования собираются преимущественно в офисе. Из-за этого новая функциональность не всегда встречает положительный отклик у конечных пользователей.  Перед командой стояла задача обновить главный экран приложения: осовременить дизайн, провести рефакторинг и пересмотреть структуру разделов. Обладая обширным опытом взаимодействия со стейкхолдерами и пониманием бизнес-процессов, мы изначально планировали самостоятельно предложить варианты изменений и выбрать лучший вместе с владельцем продукта. Однако тут разгорелись дискуссии: — Какие функции пользователи хотели бы видеть в первую очередь?— Чем они пользуются чаще, а чем — реже?— И вообще, что они думают о мобильном приложении?Именно тогда возникла идея провести онлайн-опрос. Мы осознали, что настал момент обратиться непосредственно к пользователям мобильного приложения — тем, кто ежедневно взаимодействует с ним, выполняя свои рабочие обязанности. Эти люди могли бы помочь нам сделать правильный выбор и собрать ценную информацию для команды, а также предложить идеи для улучшения. Но нельзя просто так взять и провести опрос! Сначала нам пришлось самим ответить на несколько вопросов.Вопрос первый: какой инструмент выбрать? Существует множество платформ для проведения опросов, от популярных до менее известных, например:Google FormsЯндекс ФормыTypeformSurveyMonkeyJotformMicrosoft FormsLimeSurveyНекоторые из них предлагают бесплатные версии, но с ограничениями, такими как максимальное число вопросов или участников. Есть и полностью бесплатные (Google Forms, Яндекс Формы). Также стоит учитывать, что службы информационной безопасности могут запретить использование некоторых сервисов из-за хранения данных на облачных серверах. Поэтому при выборе инструмента важно заранее ознакомиться с возможностями бесплатной версии и обязательно согласовать его использование с отделом информационной безопасности. Также важно предусмотреть, каким образом пользователи будут получать уведомление о необходимости пройти опрос. Удобство процесса напрямую влияет на статистику участия.Один из вариантов — внедрение опросов в само приложение. Этот подход позволяет собирать отзывы прямо внутри мобильного приложения, не отправляя пользователей на сторонние сайты. Вот несколько возможных решений:Диалоговые окна (Popup, Modal)Такие окна появляются поверх основного интерфейса после выполнения пользователем определённых действий. Например:После завершения заказа: «Оцените ваш опыт». После обновления приложения: «Как вам новые функции?». Плюсы: Моментальная обратная связь.Высокий процент ответов.Минус: Может раздражать пользователей, если показывать слишком часто.Чат-бот внутри приложенияПредполагает общение пользователя с виртуальным помощником, который задаёт вопросы. Такое решение выглядит привлекательно и интерактивно, а сам бот может использоваться и для других задач. Однако создание чат-бота требует значительных временных затрат, поэтому специально для проведения одного опроса его разработка может оказаться нецелесообразной. Методы вроде диалоговых окон и чат-ботов предполагают быстрое прохождение, что делает их удобными для сбора обратной связи или получения ответа на конкретный вопрос. Однако они не позволяют глубоко проанализировать потребности пользователей. Такие подходы скорее служат индикаторами областей, на которые команде стоит обратить внимание. Для более тщательного выявления требований необходим полноценный опросник, созданный на специализированной платформе.Ещё одно достаточно быстрое решение — встроить опрос, созданный на сторонних платформах (таких как Google Forms), непосредственно в мобильное приложение с помощью WebView. Плюсы:Можно использовать популярные инструменты для создания опросов (например, Google Forms).Удобный интерфейс для прохождения опроса прямо внутри приложения, не надо переходить в браузер.Быстрая реализация.Легко вносить изменения: текст опроса не зашит в тело приложения, а содержится на внешней платформе, поэтому его можно легко поправить при необходимости.Теперь давайте обсудим возможность проведения опросов вне самого приложения, если встраивание опросов внутрь оказывается невозможным (например, из-за нехватки ресурсов на разработку данной функциональности).Например, push-уведомления со ссылкой на опрос. Это привлекает внимание пользователя к опросу даже после закрытия приложения. Плюсы:Мгновенная доставка.Можно настроить персонализацию.Минус:Может раздражать пользователей, так как «пушей» в каждом приложении сейчас и так переизбыток.Другой способ — уведомить пользователей о необходимости пройти опрос через корпоративные каналы (Slack, Teams, портал) с прикреплением прямой ссылки на опросник. Плюсы:Простая реализация.Не отвлекает пользователя в момент работы с приложением.Минус:Не фиксирует опыт пользователя в контексте работы с приложением, подходит только для корпоративных приложений.Другой способ доставки опроса пользователям — это рассылки по электронной почте со ссылкой на опрос. Этот метод особенно удобен, если у вас уже собрана база email-адресов пользователей, например, при регистрации в приложении или оформлении заказов. Вы можете отправить персонализированные письма с просьбой заполнить опрос.Преимущество этого подхода — простота распространения опросов как для массового рынка, так и для корпоративных продуктов. Кроме того, можно сегментировать аудиторию, направляя опросы конкретным группам пользователей (например, исходя из их ролей). В нашем случае, мы выбрали именно этот способ: создали опрос на специальной платформе (которая соответствовала требованиям информационной безопасности компании) и провели рассылку по корпоративной почте, предварительно сегментировав аудиторию. Дополнительно информация была продублирована в корпоративных мессенджерах. Поскольку это был наш первый опыт проведения подобного опроса, и мы не планировали делать его регулярной процедурой, разработанных фич для интеграции опросов в приложение не потребовалось, и выбранный подход отлично справился с проверкой наших гипотез.Вопрос второй: как замотивировать пользователей пройти опрос?Основной недостаток опросов заключается в том, что люди неохотно соглашаются их заполнять. Если вы не уверены, как будете стимулировать участие, лучше не прибегать к этому методу. Как же можно привлечь пользователей к нашему опросу? Есть несколько проверенных приёмов:Заинтересовать темой. Люди охотнее участвуют в опросах, если чувствуют, что затрагиваемая проблема важна и значима лично для них. Важно чётко изложить цель опроса, показать, что их мнение действительно повлияет на результат. Пользователь должен почувствовать возможность быть услышанным.Предложить компенсацию или бонус. Один из действенных способов мотивации — предоставление бонусов, скидок, участие в розыгрышах или доступ к эксклюзивным предложениям. Такие стимулы помогают вовлечь больше людей в процесс заполнения анкеты.Использовать интересный дизайн и геймификацию. Привлекательная визуальная составляющая, а также игровые элементы могут значительно улучшить положение. В самом начале скажите респонденту, что ему предстоит не просто пройти опрос, а примерить на себя какую-то роль (например, дизайнера интерфейсов). Смоделируйте ситуацию, погрузите участника в атмосферу, чтобы он чувствовал себя частью интересного процесса, а не просто заполнял анкету. Особенно эффективно это работает, если ваше приложение уже содержит элементы геймификации.Напоминания. Обычно первое приглашение проходит мимо многих пользователей, поэтому полезно отправлять повторные уведомления. Несколько итераций могут существенно повысить охват.Краткость анкеты. Чем короче опрос, тем больше шансов, что респонденты пройдут его до конца. Оптимизация длины вопросов и структуры помогает удержать внимание.Конфиденциальность. Убедитесь, что участники понимают, что их ответы останутся анонимными. Это особенно актуально для корпоративных приложений, где пользователи могут опасаться негативных последствий от честных отзывов.Социальные обязательства. Просьба от руководителя или другой авторитетной фигуры заполнить опрос вызывает чувство социальной ответственности и повышает шансы на участие.В нашем случае мы применили сразу несколько подходов: заинтересовать темой, заверить в анонимности, рассылка напоминаний, а также социальные обязательства.Вопрос третий: как разработать эффективный опрос?Конечно же, нужна четкая цель. Опрос должен решать конкретную задачу. Формулировка «узнать мнение пользователей» слишком расплывчата. Какую одну задачу он поможет решить? Не пытайтесь спросить сразу обо всём.Определитесь с целевой аудиторией. Вопросы и термины должны быть понятны вашим пользователям. Перегрузка сложной терминологией может отпугнуть. Рекомендуется разделить аудиторию по ролям или уровню опыта, чтобы получить более точные данные.Сформулируйте вопросы и варианты ответов так, чтобы результат позволил сделать вам однозначный вывод. Если пользователь выбрал этот вариант, это должно для вас что-то означать. Исходя из ответа, вы сможете принять какое-то решение.Проверьте формулировки на предмет пересечений. Необходимо убедиться, что варианты ответов не пересекаются между собой, иначе интерпретация результатов станет затруднительной.Решите, какие данные вы хотите получить. Количественные (рейтинги, оценки) используются для оценки мнений, предпочтений или частоты. Для этого подойдут закрытые вопросы. Качественные (идеи, комментарии) полезны для глубокого анализа. Тут нужны открытые вопросы. Анализировать такие ответы сложнее — вам придется переработать много неструктурированного текста.Убедитесь, что формулировки нейтральные и однозначные. Избегайте вопросов, которые могут трактоваться двояко или влиять на ответы респондентов. Используйте нейтральную лексику, чтобы избежать влияния на выбор.Определитесь, где будет одиночный, а где множественный выбор. Множественный выбор позволяет более гибко подойти к ответам, но одиночный акцентирует внимание на самом главном.Используйте сбалансированные шкалы. Если у вас шкала из 4 вариантов ответа — «очень плохо», «плохо», «хорошо», «очень хорошо», то людям придется выбирать между «хорошо» и «плохо». Дайте возможность выбрать что-то среднее, чтобы неопределившиеся респонденты не портили вам статистику.Начинайте с простых вопросов, чтобы не перегружать участников. Это снизит риск утомления и повысит шанс полного прохождения опроса. В конце опросника также не стоит оставлять самые сложные вопросы, расположите их в середине.Не делайте все вопросы обязательными, но выделите ключевые, необходимые для достижения ваших целей.Оптимальное количество вопросов — около 10, время прохождения которых займет не более 5–7 минут.И, конечно же, не забудьте свой опрос сначала протестировать!Вопрос четвертый: как использовать результаты опроса?Собранные данные мы обработали и представили в форме наглядных диаграмм и графиков. Надо сказать, что команда отнеслась с большим интересом к презентации результатов опроса. Ребята активно участвовали в обсуждениях, читали реальные отзывы пользователей — как позитивные, так и критические. Это позволило сгенерировать много новых идей. Какой же результат мы получили в итоге? Определили наиболее востребованные функции. Основной функционал получил приоритетное расположение на главном экране, обновилась структура приложения.Выявили проблемы. Мы нашли разделы, вызывавшие трудности или раздражение у пользователей, и спланировали улучшения.Отыскали пробелы в знаниях пользователей о возможностях системы и разработали дополнительные обучающие материалы.Проанализировали негативные отзывы относительно частой смены паролей и запланировали корректировку требований к аутентификации и безопасности.Внимательно изучили ответы на открытые вопросы, нашли там немало полезных идей и учли их в дальнейшем развитии продукта (например, уделить внимание мобильной версии сайта для iOS).В общем, опрос не только подтвердил гипотезы, но и выявил скрытые проблемы, которые иначе могли остаться незамеченными. На основе результатов мы сформировали такой план: 1.     Краткосрочные улучшения: изменения интерфейса, улучшение навигации по наиболее востребованным функциям.2.     Среднесрочные доработки: пересмотр логики аутентификации, оптимизация проблемных сценариев.3.     Долгосрочные инициативы: проработка гипотез на основе новых инсайтов и запуск дополнительных исследований.Пятый вопрос: как управлять ожиданиями и нужно ли давать обратную связь?После проведения опроса и обработки результатов важно помнить, что пользователи ожидают обратной связи. Они заинтересованы в том, чтобы узнать итоги опроса и удостовериться, что их голоса были услышаны. Процесс, называемый ""петлей фидбека"", подразумевает не только сбор обратной связи от клиентов, но и возврат к ним с информацией о принятых мерах. Это замкнутый цикл, в рамках которого вы сначала получаете отзывы, а затем сообщаете о проведенных изменениях. Так пользователи ощущают значимость своего мнения и видят улучшение продукта на основе реальных потребностей, следовательно уровень накопившегося негатива снижается.После проведения опроса можно выразить участникам благодарность, ответить на частые вопросы, объяснить причины возникших трудностей, сообщить о планируемых улучшениях, описать новые функции или улучшения, подчёркивая вклад пользователей в их реализацию. Вообще онлайн-опросы могут применяться в аналитической работе для решения разных задач. Можно узнать, устраивают ли целевую аудиторию предложенные функции, выяснить, что они считают необходимым, а что – желательным в продукте, отыскать упущенные недоработки и ошибки, собрать информацию для создания MVP, оценить актуальность проблемы или протестировать гипотезы. В этих случаях опросы становятся оптимальным инструментом для выявления требований. Они позволяют уловить общие тенденции и задать вектор дальнейших исследований, которые затем можно углубить с помощью фокус-групп или интервью. Но самое важное преимущество опросов, на мой взгляд, состоит в том, что проведение опросов помогает установить мост между командой разработки и реальными пользователями. Опросы могут разрушить барьеры и наладить прямую коммуникацию между разработчиками и теми, кто использует продукт. Прямая обратная связь и реальные отзывы помогают команде узнать проблемы и ожидания пользователей. Положительные отклики, равно как и конструктивная критика, вдохновляют команду на дальнейшие улучшения. Ну и напоследок поделюсь с вами чек-листом, который поможет подготовиться к проведению опроса. В нем собраны практические советы и рекомендации, основанные на нашем опыте.Спасибо за внимание!"
83,83,Полный парсер Core Log SAS Customer Intelligence Studio и инцидент-менеджмент,Совкомбанк Технологии,Меняй мир финансовых технологий вместе с нами,56.49,"Веб-разработка, Программное обеспечение, Мобильные технологии",2025-03-12,"Всем привет! Меня зовут Димитрий, я 5 лет работал в «Хоум Банке» с SAS MA: начинал разработчиком SAS, закончил начальником отдела и техлидом SAFE-стрима. После объединения с «Совкомбанком» перешел в новую структуру.Статья предназначена для технических специалистов: разработчиков и администраторов SAS CIS. Материал поможет понять проблематику ошибок падений маркетинговых кампаний, расскажет о механизме формирования корлога и том, как разделять его на отдельные логи по кампаниям. В приложении есть готовый проект с кодом для запуска парсера на своих логах.В «Хоум Банке» мы использовали ПО компании SAS Institute. Для маркетинговых рассылок применяли ПО SAS Marketing Automation, MA SAS Intelligence Studio. SAS CIS позволял элементами графического программирования составлять маркетинговые кампании. Опытные пользователи могли использовать самописный код SAS/SQL.Пример диаграммы CISМы работали с ошибками, которые встречались в логах на уровне работы «ядра» CIS и писались в лог 6 веб-сервера Server6_1. Проблема заключалась в том, что лог один, а параллельно работающих процессов могут быть десятки, что очень неудобно.  Истоки проблемыПри разборе инцидентов на третьей линии поддержки мы использовали анализ логов. Часто у бизнес-пользователя возникала ошибка, а что конкретно «упало» и где искать «концы», было непонятно.  Про Customer Intelligence Studio. В SAS CIS есть разные типы узлов: стандартные и кастомные, с разработанным специалистами банка стандартизированным кодом или разово написанные пользователями. Есть и особые узлы коммуникаций. Обычно там происходит основная «магия» и интеграционные процессы с отправкой коммуникаций и фиксацией данных в таблице истории контактов.Простые узлы: разделение подборки клиентов/договоров для A/B тестирования или выборка по фильтру логируются в корлог. Пользовательские, кастомные и узлы коммуникаций фиксируются в корлоге до момента сбора входной таблицы SQL, а результат работы кода выкладывается отдельным логом. Это было сделано давно и сильно упрощало разбор ошибок. Если «на пальцах»: на вход коммуникации передается список клиентов — таблица с 1 полем — уникальным идентификатором клиента. Проверки и интеграции кода коммуникации перед запуском основного кода собирают дополнительные данные, например: ФИО, адрес, телефон, доступный лимит по кредиту. После сбора этой автоматической таблицы, она передается на вход кода коммуникации — происходит отделение лога в файл. Доработки обычно были связаны именно с кодом коммуникации, поэтому и ошибки чаще всего находились там.Мы улучшили возможность поиска нужного лога дополнительным выводом в него названия маркетинговой кампании, узла и некоторых суррогатных ключей. Лог искали через find в bash.До этого приходилось узнавать суррогатные ключи через таблицы и джойны по имени кампании/узла, искать лог по этим ключам. Впоследствии мы добавили опцию отправки на почту информации о падении с логом, где могла быть ошибка. У пользователя появлялся код ошибки и лог, что сократило обращения в ИТ-команду на 90%.Раньше, до оптимизации процессов, пользователь видел на экране сообщение: «Ошибка 1002, обратитесь к администратору» — и писал на почту поддержки. По переданному нам имени кампании, узла и времени ошибки, мы брали известные величины, отбирали таблицы с джойнами, получали код кампании и узла через имена пользователей и времени запуска. Идентификаторы обновлялись каждый запуск. После по этим кодам мы делали find в папке логов, обнаруживая нужный. Теперь лог можно было сразу найти по имени узла и примерному времени запуска на почте.Работали с ошибками, которые встречалась в логах на уровне работы «ядра» CIS. В один лог часто «писали» сразу несколько кампаний. Пример лога, где «пишут» параллельно две кампании. Во вторых квадратных скобках указана сессия – и она разная. Все они пишут параллельно в один файл. Как понимаете, поиск информации в таком файле затруднителен. Похоже на поиск иголки в стоге сена. Когда случалось падение кампании, приходилось анализировать корлог за определенный период и догадываться по ошибкам в нём – искать ошибку в отрезке лога по упавшей кампании среди десятков отработавших в это же время успешно. При этом ошибка могла быть показана пользователю в одно время, а произойти — в другое. Учитывая объемы корлога — около 2 Гб текста за сутки — найти нужный кусок лога было сложно. Анализ мог длиться несколько рабочих дней. Поиск решенияВ процессе решения одной из проблем с SAS Institute, коллеги прислали приложение для анализа корлога. Но ресурсов ПК часто не хватало для работы этого приложения. Мы смогли извлечь профит: узнали принципы работы корлога и поняли, как его парсить. Написание своего парсераГлавное, что я понял – в корлог идут записи, привязанные к ID сессии, который пишется в части строк. Я принял допущение, что все написанное под одним ID до появления другого ID относится к этой сессии. Позже эта версия подтвердилась.При разборе на логах тестовых сред, где и запусков меньше, и лог «легче», оказалось, что одна сессия может создавать другие, но вся работа идет в рамках запуска.Установили, что сессия – это одно подключение к SAS CIS. То есть, если кампания работает по расписанию, то на нее будет один набор сессий: сессия и дочерние. А вот если нужно разобрать ошибку пользователя, который запускал разные кампании, то сессия этих кампаний тоже будет одна, а значит, искать будет сложнее. В итоге парсер был оформлен в проект SAS Enterprise Guide и написан на языке SAS. Развитие идеиПервая проблема случилась, когда мы столкнулись с тем, что лог обрезается в 00:00. Ошибка была связана с кампанией, которая не отработала до полуночи. Часть лога перенеслась в следующий файл на запись. А парсер работает с конкретным логом, а не двумя.Написали скрипт, который после плановой перезагрузки серверов в 7:00 и конца записи в лог приклеивает его ко вчерашнему и обнуляет текущий. Так перестали теряться сессии, а логи стали жить не с 00:00 до 00.00, а с 7:00 до 7:00, когда работает сервер. Простой скрипт на bash: # Append core log to yesterday file /bin/cat /opt/sas/sasconfig/Lev1/Web/Logs/SASServer6_1/SASCustIntelCore6.6.log >> /opt/sas/sasconfig/Lev1/Web/Logs/SASServer6_1/SASCustIntelCore6.6.log.$(date -d ""yesterday"" '+%Y-%m-%d') # Clean today log echo '' > /opt/sas/sasconfig/Lev1/Web/Logs/SASServer6_1/SASCustIntelCore6.6.logДополнительно сделали скрипт, который копирует сегодняшний лог с сервера метаданных на сервер Compute каждые 15 минут. Ранее это приходилось делать вручную, что занимало более 30 минут на трансфер лога с сервера на локальную машину и на второй сервер. Это было необходимо, поскольку лог пишется на META/MID сервере, а исполняющий код находится на Compute.Аналогично, после обрезки лога в 7 утра он копируется с одного сервера на другой под именем ""yesterday"" – это упрощает работу с ошибками «за вчера». Парсер на расписании и метрики – общее состояние кампаний, поиск «длинных шагов»Со временем мы решили не только парсить лог при наличии ошибок и жалоб со стороны бизнес-пользователей, но и на основании кода создавать дополнительную отчетность. Например, мы анализируем лог и уведомляем пользователей о том, что какой-то шаг работал дольше положенного времени. Взяли рубеж в 3 часа. В дальнейшем такие узлы были разобраны и оптимизированы. Шаги часто переписывали сами бизнес-пользователи. Они же в первую очередь были заинтересованы в скорости, так как некоторые маркетинговые кампании работали «каскадом», каждая минута была ценна для финального времени. Иногда оптимизацию кода отдавали на аутсорс вендору.Следующей идеей был дополнительный парсинг всех вложенных логов. В тексте корлога есть ссылки на лог-файлы, так как некоторые узлы выгружают их в отдельный лог-файл. Скрипт собирает все ссылки на логи, парсит эти файлы и собирает информацию об ошибках, времени выполнения шагов и узлов кампании, их имена. Разбор проекта и кода парсераОбщий вид проектаРассмотрим проект для ручного запуска и анализа логов. Первый блок ParseCoreLog проекта парсера — общий «проход» по всему логу. Здесь важно понять, какая сессия к какой кампании относится.На вход поступает только адрес лога. Обычно он датируется этим же днём, копируется с задержкой в 15 минут с сервера метаданных на сервер Compute, или это может быть готовый вчерашний лог.На выходе получаем таблицу с именами кампаний и сессий, которая нужна для второго шага. Плюс несколько технических таблиц, созданных сессий и т.д. Ищем по таблице нужную кампанию и берем сессии, связанные с ней, поскольку запуски производятся по расписанию и вручную. Второй блок ParseLogSession – мы подаем на вход имя сессии и используем макропеременную первого шага с адресом лога. На выходе получаем: Файл с вырезанной частью лога — это облегчает работу, если придется смотреть лог целиком Таблицу со ссылками на все вложенные логи в отдельной таблице Список шагов и время их работы Таблицу со всеми ошибками, встретившимися в тексте этой сессии — в корлоге, не во вложенных Для анализа ошибок этого обычно достаточно. Если нет понимания по строке ошибки, то это станет ясно из вырезанного лога: ошибка и контекст происходящего.Пример таблицы со ссылкой на логи:  Пример таблицы со списком узлов: Третий блок StreamParse опциональный. Он делает проход по всем вложенным логам, собирает строки с информацией о времени выполнения — суммирует все шаги, записанные во внешние логи и ранжирует их по времени, показывая самые долгие. Это необходимо, чтобы оценить общее время во вложенных логах и по таймингу выполнения найти аномалии — слишком долгие шаги, нуждающиеся в оптимизации).Пример таблицы с расчётом времени работы шагов: Четвертый блок streamParserErrors: выполняет действие, похожее на третий, только собирает информацию об ошибках из всех внешних логов и сводит в таблицу: файл, номер строки и строка ошибки. Потенциально экономит время, обнаруживая ошибки кампаний во вложенных логах. Парсер на расписанииКак уже упоминалось, для оптимальной работы системы мы искали слишком долгие шаги и анализировали их. Помогал нам в этом адаптированный код парсера.Сейчас после нарезки готового лога в 7 утра программа проходит по всему логу и смотрит шаги дольше 3 часов. Они уходят на анализ, если повторяются изо дня в день.Ранее по итогам работы парсера велся общий подсчет обработанных строк БД , всех запросов, шагов и времени работы в секундах, а также их соотношение. Это относительная аналитическая метрика, по которой мы отслеживали здоровье системы, необходимость замены железа, очистки больших таблиц.Поскольку общее количество строк обычно примерно одинаковое, а среднее время на обработку должно сохраняться на одном уровне – это можно принять за стартовые условия. Из этих величин рассчитывается параметр R – количество строк, обработанных за единицу времени. Чем больше строк мы обрабатываем в промежуточных таблицах за единицу времени, тем лучше.Параметр R рассчитывается следующим образом: все время в секундах ÷ все строки × 100 000. Хорошим параметром считалось число 50-60.Со временем можно наблюдать кумулятивный эффект:наполняются исторические таблицы – показатель R растет. На тот же объем строк затрачивается больше времени. На обрабатывание 100 строк при запросе в терабайтную табличку времени уходит больше. Был выбран эталонный цифровой параметр и задан как цель на год — это мотивировало команду разработки и поддержки следить за ситуацией.Также по метрике R можно было понять, насколько эффективны оказались какие‑то меры: очистки ключевых таблиц. Например, таблица истории контактов, или переезд на новые серверы.В графическом отчете видно, что 16 июня произошел сбой, а 1 сентября была плановая остановка серверов и очистка таблицы.Зеленая кривая — количество уникальных маркетинговых кампаний, запущенных за день. Красная кривая — количество кампаний, упавших с ошибкой. Синяя кривая — их отношение к падениям.Выводы за несколько лет использования парсераВо-первых, мы оптимизировали и автоматизировали разбор ошибок, которые возникают в результате работы маркетинговых кампаний. Часть ошибок автоматически обрабатываются, «достаются» из лога и отправляются бизнес-пользователю. Часто они типовые и кампейнер сам знает, что делать, имея на руках такой инструмент.Часть ошибок все же доходит до команды, но нам намного проще искать и разбираться в них. Часто первичный лог уже на почте, есть информация, что за кампания или узел упали. Иногда нужно покопаться и понять, в чем первопричина ошибки. Например, таблица не создалась, потому что закончилось место на диске. Или не было исходной таблицы, но не хватило сессий к БД. В любом случае у разработчиков на руках есть лог, имена кампании и узла. При необходимости корлог парсится, извлекаются все логи на системе, связанные с ошибкой. Если это сделано в течение суток-двух с момента ошибки, то все проблемы разбираются и делаются выводы. Растет стабильность и предсказуемость системы, проблемы решаются комплексно.SLA отработки ошибок в среднем снизился с 2-3 дней до нескольких часов. Благодаря парсеру удалось отследить и оптимизировать все долгие узлы и процессы, что благотворно повлияло на общее время выполнения всех кампаний, разработчики стали лучше понимать логику «ядра» кампаний. Парсер корлогов можно скачать с репозитория. Спасибо за внимание!  Делитесь в комментариях своим опытом работы с корлогом и разбором ошибок SAS CIS. Если есть вопросы — задавайте, обсудим. P.S. Если интересно узнать о карьере в Совкомбанк Технологиях — смотрите вакансии и направления работы. "
84,84,"Черная дыра в ИТ-бюджете, или как грамотно управлять затратами на активы",SimpleOne,Платформа для автоматизации бизнес-процессов,112.56,Программное обеспечение,2025-03-12,"Часто на вопрос «Как у вас организован учет ИТ-активов?» компании отвечают: Excel-таблицы и инвентаризация ежеквартально. Однако, если у вас несколько офисов, сотни единиц оборудования и тысяч лицензий, точно ли вы знаете, где что находится и сколько это реально стоит? По моему опыту работы с крупными заказчиками, отсутствие системного подхода к управлению ИТ-активами приводит к незапланированным расходам и потере контроля над бюджетом на обслуживание и обновление оборудования.Меня зовут Евгения Асоскова, я владелец продукта SimpleOne ITAM. В этой статье я расскажу, почему так происходит и как этого избежать.Как учет ИТ-активов влияет на финансыУчет ИТ-активов — это не только про технику, но и про процесс, который требует внимания и точности. Часто компании теряют контроль над активами из-за недостатка четкой информации о наличии и местоположении оборудования, проблем с инвентаризацией или неупорядоченности данных о лицензиях и закупках. В этом разделе мы разберем, почему учет ИТ-активов выходит из-под контроля, и как это влияет на финансы компании.Отсутствие точных данных о наличии и местоположении активовКомпании не знают точно, какие активы у них есть, в каком количестве и где они находятся. Это касается как дорогостоящего оборудования, так и расходных материалов. Никому не хочется столкнуться с ситуацией, когда в большом офисе в принтере заканчиваются картриджи, и внезапно выясняется, что на складе их тоже нет. Приходится экстренно покупать их за наличные на ближайшие несколько месяцев, пока не будет проведена плановая закупка. Еще хуже, если закончился запас ноутбуков или компьютеров для оснащения рабочих мест.Неконтролируемый учет программных активовОсобенно остро стоит вопрос с программными активами — лицензиями — один из самых дорогих пунктов затрат. Для организаций они требуют значительных вложений, поэтому важно контролировать закупки, запасы и расходы. Информация не находится в одном месте: часть в письмах, часть в файлах, что-то в данных о заказах, а информацию о фактически установленном ПО требуется проверять вручную или автоматическими системами. В результате компании не видят полной картины, из-за чего либо накапливаются избыточно закупленные и неиспользуемые лицензии, либо возникают неучтенные потребности. Все эти факторы существенно снижают финансовые показатели организации.Проблемы с инвентаризациейИнвентаризация ИТ-оборудования и программного обеспечения либо не проводится, либо проводится долго и вручную. При нерегулярных проверках может выясниться, что часть аппаратного оборудования отсутствует — неизвестно, кем и когда оно было вынесено. Это прямые риски безопасности, требующие контроля.Серьезные последствия для бизнеса включают риски нарушений, ведущие к штрафным санкциям, например, при использовании нелицензионного ПО. Существуют также риски потери денег из-за неиспользования складских запасов при одновременной закупке избыточного оборудования. Или когда приобретенное оборудование не соответствует технической специфике компании и требует дополнительных затрат на обеспечение совместимости.Сложности в территориально распределенных компанияхУ территориально распределенных компаний есть не только офисы в крупных городах, но и небольшие подразделения. Особенно это актуально для промышленных компаний: помимо офисов в бизнес-центрах, они владеют множеством объектов, расположенных в удаленных локациях. В крупных городах можно оперативно определить местонахождение активов, но с удаленными объектами сложнее. Невозможно быстро оценить складские запасы — приходится постоянно созваниваться или планировать командировки ответственных специалистов. Сложно оперативно доставить необходимое оборудование для замены вышедшего из строя. Задача распределенных компаний — наладить процесс так, чтобы централизованный ответственный за оборудование владел реальной картиной по всем подразделениям и понимал, какой объем необходим для их бесперебойной работы, откуда этот объем может быть взят и в какие сроки доставлен.ITAM: решение, которое позволяет контролировать бюджет и активыITAM (IT Asset Management) — система учета и управления ИТ-активами: оборудованием, лицензиями, контрактами и всем, что связано с ИТ-инфраструктурой. ITAM помогает структурировать учет активов, исключить множество таблиц Excel, локальных файлов или бумажных журналов. В единой базе можно отслеживать местоположение и статус оборудования, управлять лицензиями и контролировать расходы. Решение может фиксировать все изменения, упрощать инвентаризацию и, благодаря прозрачности информации, снижать риск потери или неоптимального использования ресурсов.Для целей современных организаций  нужен инструмент, который не просто хранит данные, а помогает эффективно управлять активами. Хорошая ITAM-система должна обеспечивать:Централизованный учет и единую базу данныхВнедрение системы управления ИТ-активами помогает создать единую точку доступа, единую базу данных об учете активов и об информации об активах. Это позволяет представлять полную картину об оборудовании в наличии и о потребностях различных подразделений. Единая точка доступа к информации дает возможность оперативно формировать централизованную отчетность по территориальным подразделениям, срокам действия лицензий и контрактов, оборудованию в ремонте, срокам приобретения и необходимости новых закупок.Важно понимать, что для корректной работы единой базы данных необходимо правильно настроить процессы и внести структурированную информацию. Например, выдача оборудования должна фиксироваться в системе ITAM в момент передачи, а не откладываться до конца месяца перед проверкой. При правильной настройке процессов и обучении персонала можно получить прозрачные данные, с помощью которых легко планировать закупки необходимого объема и управлять запасами на различных объектах.Интеграцию с 1С системойС помощью средств интеграции система ITAM может взаимодействовать с бухгалтерскими системами. Благодаря этому информация об оплатах автоматически пробрасывается в ITAM и связывается с соответствующей организацией, центром финансовой ответственности. Производя финансовые операции в бухгалтерской системе, можно сразу видеть информацию о затратах на закупку, ремонт или обслуживание в системе ITAM.Автоматизацию процессов планирования закупокОбычно планирование потребностей происходит через Excel-формы или сбор заявок по электронной почте. В системе ITAM этот процесс можно автоматизировать: ответственный сотрудник может на портале самообслуживания выбрать из каталога нужные позиции, указать количество, добавить пояснения и документы. Такая автоматизированная заявка сразу попадает на обработку специалисту, который может согласовать ее в системе.Возможность видеть в одной системе информацию о бюджетных лимитах, о том, что мы планируем закупить и попадаем ли в лимиты, значительно упрощает работу. Не нужно собирать данные из разных систем, переписки и Excel-файлов, что позволяет оперативнее планировать, согласовывать и решать рабочие задачи.Интеграцию с ITSM-системойITAM может быть интегрирован с системой ITSM, где обрабатываются заявки на выдачу, замену и ремонт оборудования. При оформлении запроса через портал, мобильное приложение или чат-бота заявка трансформируется в запрос в Service Desk, а ITAM автоматически создает связанные задачи на ремонт, выдачу или возврат оборудования на склад.Для эффективного управления ИТ-услугами компании используют ITSM (IT Service Management) — подход, который включает процессы поддержки, эксплуатации и развития ИТ-услуг. ITSM помогает контролировать работу ИТ-систем, управлять инцидентами, изменениями и конфигурациями, обеспечивая бесперебойную работу сервисов.Хотя ITAM и ITSM сосредоточены на разных аспектах управления, обе практики направлены на эффективное использование ИТ-ресурсов и оптимизацию затрат для обеспечения ИТ-услуг. ITSM рассматривает конфигурационные единицы (КЕ), из которых состоят ИТ-услуги, а ITAM отвечает за учет, эксплуатацию и контроль стоимости активов, которые формируют конфигурацию инфраструктуры. Интеграция этих систем позволяет компаниям принимать более обоснованные решения и минимизировать расходы на поддержание службы ИТ.Обмен данными между ITAM и ITSM помогает поддерживать актуальность информации: ITAM передает в ITSM сведения о наличии, статусе и ответственности по  объектам, а ITSM возвращает данные об их использовании в рамках ИТ-услуг. Кроме того, объединенные данные о стоимости владения активами, сроках амортизации и расходах на обслуживание  делают затраты более прозрачными и помогают анализировать их влияние на стоимость услуг. При изменении конфигурации ИТ-услуги ITSM может передавать данные в ITAM, чтобы автоматически проверить, как это повлияет на учет активов, запасы или планирование закупок.Контроль безопасности и ограничение доступаБезопасность в системе SimpleOne ITAM обеспечивается через механизм Access Control List (ACL) —правила ограничения доступа к важной информации в соответствии с ролями пользователей. Например, финансовые данные видит финансовый директор или бухгалтер, но не видит рядовой пользователь, который просто проверяет свое оборудование. Система позволяет управлять согласованиями — в зависимости от различных триггеров, например, при перемещении оборудования между городами или организациями, или при планировании закупки на сумму больше определенного лимита, могут запускаться дополнительные согласования с финансовым директором или ответственным за безопасность.Таким образом, ключевые процессы, связанные с расходами или изменением учетной информации об активах, контролируются ответственными лицами в соответствии с регламентами организации.Интерфейс выдачи активов в системе SimpleOne ITAMКак это работает на примере SimpleOne ITAMУправление активами — это не просто контроль списка оборудования, а полноценный процесс, который помогает поддерживать порядок и оптимизировать затраты. Важно понимать, какие потребности в ИТ-оборудовании есть у различных служб, какие устройства уже используются, какие требуют замены, а какие можно перераспределить без дополнительных закупок. SimpleOne ITAM берет на себя эту задачу: систематизирует данные, автоматизирует работу с номенклатурой и активами на протяжении их жизненного цикла, упрощает контроль контрактов и гарантий. Разберем, как это работает на практике.Номенклатура и планированиеНоменклатура в SimpleOne — это справочник, база данных всех моделей оборудования, которые используются на предприятиях в организации. Эти модели оборудования могут быть новыми, которые только заводятся, либо теми, что использовались ранее. На базе этих моделей оборудования, которые имеют конкретного поставщика, конкретного производителя, мы регистрируем актив в системе. Каждый актив в SimpleOne ITAM всегда связан с какой-то конкретной моделью номенклатуры.Такой подход позволяет структурировать модели оборудования в организации и понять, насколько их много, не нужно ли их уменьшить, проанализировать и оптимизировать состав. Номенклатура служит для того, чтобы планировать потребности в наших активах — мы обращаемся к этой базе и прямо указываем, что нужны принтеры или ноутбуки определенной модели.На основании номенклатуры также оцениваются запасы и возможность использования существующих запасов для покрытия потребностей. И на основании этой же номенклатуры планируется закупка. В списке закупки фигурируют те же модели номенклатуры, которые пользователи указывали в потребностях.Номенклатура в SimpleOne ITAM имеет связь с классами и моделями конфигурационных единиц. В номенклатуре можно настроить различные правила, к примеру, запараметрировать связь актива и конфигурационной единицы — элемент конфигурации инфраструктуры, который включен в неё и участвует в оказании ИТ-услуг. Номенклатура — это центральный справочник, который обеспечивает унификацию работы и структурирует учет активов.Управление жизненным циклом активовВ системе разработаны преднастроенные процессы управления активами. Предусмотрен коробочный процесс сбора потребностей, в рамках которого формируется запрос на необходимое оборудование для определенного центра финансовой ответственности. Эта потребность может быть обработана различными способами: возможно проанализировать имеющиеся запасы и зарезервировать их, либо запланировать закупку.Для управления закупками предусмотрен отдельный коробочный процесс, который будет выпущен в ближайшие месяцы. После закупки в системе регистрируется запись актива с различными учетными параметрами, информацией о собственнике, МОЛ и расположении. Изменения на протяжении жизненного цикла актива реализованы через специализированные процессы (задачи): выдача, возврат на склад, ремонт, модернизация (например, добавление памяти), инвентаризация, смена расположения (переезд из города в город), смена материально ответственного лица, смена склада, списание и т.д. После списания активы можно утилизировать, передать на благотворительность или продать сотрудникам.В задачах есть механизмы защиты от некорректного внесения информации — например, пока идет процесс выдачи, актив нельзя начать перевозить в другое место или списывать. В задачах также есть механизм, который позволяет реализовать корректное изменение множества записей — когда происходит массовое действие, например, переезд всего офиса с изменением склада у тысяч активов. Вместо ручного изменения каждой записи такие действия производятся автоматически, с уведомлением о статусе изменений и прикреплением необходимых документов.Схема жизненного цикла активов: этапы от регистрации до утилизацииИнвентаризацияДля инвентаризации создается отдельная задача, в которой уточняется основная информация и определяется список активов к инвентаризации, который может быть детализирован в различных разрезах. В случае необходимости,можно указать инвентаризационную комиссию — сотрудников, которые полномочны валидировать результаты инвентаризации и подтверждать, что процесс прошел правильно, без нарушений.Проверку наличия активов можно провести двумя способами:Исполнитель формирует список найденных в ходе проверки инвентарных номеров. Для этого может быть использовано любое приложение на телефон или сканер, которые позволяют считывать штрих-коды и выгружать информацию в Excel.Агрегирование инвентарных номеров происходит автоматически, и информация о них поступает в систему через предусмотренную точку доступа.После обработки информации о найденных инвентарных номерах система отображает виджет результатов: что не найдено, где есть различия, что обнаружено новое, что ранее отмечалось как отсутствующее, но нашлось. Инвентаризацию можно проводить неоднократно, а также доступен вывод на печать ведомости с результатами инвентаризации в формате, адаптированном под требования организации.Интерфейс плановой инвентаризации активов в системе SimpleOne ITAMКонтроль контрактов и гарантийВ SimpleOne ITAM реализован процесс управления контрактами, который позволяет учитывать все обязательства и сроки. В системе предусмотрен раздел «Контракты», где по умолчанию доступны пять типов контрактов, при необходимости можно добавлять собственные. Каждый контракт проходит последовательные этапы – от черновика до действующего состояния через согласование. В карточке контракта фиксируются ключевые данные, включая информацию о поставщике и заказчике, а также сроки начала и завершения действия.На протяжение своей жизни в системе активы неизменно связываются с различными контрактами: на закупку, на гарантийное или негарантийное обслуживание и т.д. Используя возможности построения отчетов, можно формировать отчетность по контрактам в различных разрезах, например, по срокам истечения контрактов на обслуживание.гарантийного обслуживания.Особенно важно контролировать сроки действия контрактов и лицензионных соглашений для активов программного обеспечения. Существующая функциональность позволяет это контролировать, и в ближайшее время она станет еще удобнее.***А теперь хочу спросить у вас — как вы решаете задачу учета активов? За время работы  я видела множество разных подходов: от продвинутых самописных решений до классических Excel-таблиц. У каждого подхода свои особенности и свои способы оптимизации.Расскажите в комментариях, что для вас является ключевым в учете? Как справляетесь с растущим объемом данных и автоматизируете рутинные операции? Придумывали ли вы нестандартные способы, чтобы не утонуть в хаосе учета? Какая функциональность в ITAM-системах для вас must-have?"
85,85,Масштабируем стажировку: как принять 30 новичков и не сломать процессы,КРОК,Компания,115.11,"Программное обеспечение, Аппаратное обеспечение",2025-03-12,"Работа со стажерами часто вызывает у руководителей сомнения: где искать лучших и сколько брать, как адаптировать и обучать, окупится ли это и как быстро.Меня зовут Кристина Чебыкина, в КРОК я руковожу центром компетенций по ERP. В моей команде уже больше 130 человек, две трети из них прошли через стажерскую программу. В статье я расскажу, как мы построили эффективную систему отбора и обучения стажеров, внедрили новые инструменты работы и мотивации наставников. Поделюсь конкретными решениями и цифрами, которые подтверждают результативность нашего подхода. Особенно полезным этот опыт будет для компаний, которые хотят масштабировать свой бизнес, вырастить классных сотрудников и не «сжечь» команду.  Раньше набирали стажеров мало и бессистемно  Раньше мы брали на стажировку всего двух-трех человек в год. Искали талантливых студентов из престижных вузов — обычно победителей олимпиад с хорошей подготовкой и опытом участия в профильных конкурсах. Они самостоятельно вливались в рабочий процесс, постепенно адаптировались к проектам и как-то выживали.Мы особо не продумывали программу стажировки. Стажеров было немного, и в этом не было особой необходимости.Затем произошли кардинальные изменения на рынке. После ухода зарубежных компаний российское программное обеспечение, особенно продукты 1С, стало пользоваться повышенным спросом. Крупные компании начали массово обращаться к нам с запросами на проекты.Требовалось оперативно увеличить штат сотрудников. В сложившихся условиях мы пришли к выводу, что подготовка собственных специалистов будет эффективнее поиска готовых кадров на рынке. Так родилась идея создать масштабную стажерскую программу, которая поможет талантливым новичкам быстро вырасти до уровня middle-консультантов.Где взять сразу 30 классных стажеров   Чтобы обеспечить стабильный приток стажеров, мы изменили наш подход к поиску талантливых и мотивированных ребят. Помимо привычного HeadHunter, стали использовать многоступенчатую систему отбора с акцентом на качестве кандидатов и долгосрочном сотрудничестве. Летняя школа — основной и самый массовый источник стажеров  Это конкурсный проект, который позволяет выявить наиболее способных и целеустремленных кандидатов. Выпускники летней школы для нас самые заряженные и готовые к работе.  Фрагмент с сайта летней ИТ-школы КРОК-2024Примерно за полгода до начала летней школы команда КРОК начинает привлекать студентов из ведущих вузов через серию профессиональных встреч и тематических выступлений. На этих встречах делимся практическим опытом и показываем, как устроена работа в IT-компании. Такой подход помогает заинтересовать студентов и собрать много заявок на участие в летней школе. По нашим данным, на начальном этапе интерес к программе проявляют до трех тысяч студентов. Первый этап отбора включает выполнение практического задания в дистанционном формате. Кандидаты проходят тестирование и решают профессиональный кейс.Далее проходит ассесмент-сессия, где студенты участвуют в интерактивных мероприятиях и командных играх. Наши рекрутеры и профильные специалисты оценивают их коммуникативные способности, активность в групповой динамике и умение аргументированно представлять свои идеи.Заключительный этап состоит из персональных собеседований с рекрутерами, которые анализируют результаты предыдущих этапов отбора. По итогам всех испытаний формируется группа из 20–30 финалистов. Они получают приглашение на двухнедельную летнюю школу.В течение первой недели летней школы студенты проходят экспресс-погружение в профессию через лекции. На второй — выполняют практические задания в группах под руководством опытных наставников: объединяются в проектные команды и работают над упрощенной версией реального проекта, проходят все этапы его реализации. А потом защищают свои решения. Важный элемент школы — ежедневный ассесмент. Наши специалисты отслеживают динамику развития каждого студента, анализируют его вовлеченность в рабочий процесс и фиксируют индивидуальные профессиональные преимущества. На защите проектов нанимающие менеджеры оценивают работу участников. А после проводят финальное собеседование, по результатам которого принимают решение о стажировке в компании. Особое внимание мы уделяем оценке «мягких» навыков, cultural fit и потенциалу долгосрочной работы. Потому что считаем, что у прошедших школу с хардами всё уже хорошо. Надо сказать, что к моменту завершения летней школы ребята настолько заинтересованы в стажировке, что практически все хотят ее получить. Всё это благодаря продуманной системе отбора, которая позволяет отсеять изначально неподходящих, и, что важно, погружению в реальные задачи и плотную работу с экспертами.Кейс-чемпионаты, лекции и ассесмент в вузах   Еще один эффективный, хотя и не такой массовый канал привлечения стажеров, — это всевозможные мероприятия внутри вузов. Рассматриваем его как элемент прогрева перед летней школой, а также дополнительный способ познакомиться с талантливыми студентами, которые по каким-то причинам не хотят в ней участвовать. Такие коллаборации с вузами позволяют увидеть кандидатов в разных контекстах и оценить их потенциал. И так у нас накапливается база желающих стажироваться в КРОК. Не обошлось без проблем  Однако даже с такими классными стажерами вначале мы столкнулись с некоторым сопротивлением со стороны менеджеров и консультантов. В проектные группы численностью около 20 специалистов руководители готовы были интегрировать максимум одного-двух стажеров. Мы стали разбираться, в чём причина, и увидели несколько проблем из прошлого: Мы плохо развивали стажеров. Давали преимущественно рутинные, простые задачи, часто оформительские или административные. Всё сильно зависело от наставника, от загруженности на проекте, умения работать со стажером и его развивать.Ответственность наставника не закреплена. Обязанности были непонятны, за что повышают — тоже непонятно.Наставники были демотивированы. Так как система была не очень — стажеры часто уходили, а наставники расстраивались, потому что тратили на них много времени. Исторически стажеры воспринимались как временный ресурс с низким коэффициентом конверсии в постоянных сотрудников, что существенно снижало привлекательность наставничества для опытных специалистов. Конечно, на этом фоне новая масштабная стажерская программа не вызывала энтузиазма у специалистов.Нужно было разобраться, как это должно работать, сформулировать новый подход, затем зафиксировать документально и донести до всех участников проекта. Заниматься этим предварительно времени не было — 30 стажеров уже пришли в компанию.    Решили изменить примерно всё без отрыва от производстваНачали разрабатывать новую систему работы со стажерами. На начальном этапе мы определили базовую концепцию, а в течение первых месяцев решали возникающие проблемы и выстраивали взаимодействие наставника и стажера. Через полгода на базе полученного опыта и первых положительных результатов мы разработали и внедрили комплексные регламенты по всем видам работ, в которых в числе прочего прописаны зоны ответственности стажера (т. е. мы полноценно внедрили стажера в производство проекта на всех этапах). Данный регламент базируется на следующих аспектах: Единый подход. КРОК использует унифицированную методологию для всех проектов внедрения 1С, что обеспечивает согласованность и предсказуемость результатов. Подробней о проектной технологии КРОК можно узнать, пройдя бесплатный образовательный видеокурс «Проектная технология внедрения 1С». Структурирование. Методология изложена в базе знаний и разбита на этапы (моделирование, проектирование и разработка, ПСИ и ОПЭ), подэтапы (подготовка, выполнение, формирование отчетов) и конкретные задачи.Прозрачность. Описание каждого этапа и задачи включает ответственных, материалы на входе и выходе, ожидаемый результат и инструкции.По-новому подошли к онбордингу Мы ушли от ознакомительных лекций к удобному онбордингу по чек-листу. Он содержит все необходимые материалы о нашей проектной культуре, основанной на единой методологии и общих инструментах. Такой системный подход позволяет плавно погрузить стажера в процесс, а затем легко переключать между проектами без дополнительных затрат на адаптацию. Стажер отмечает галочками пройденные пункты, как бы составляя свою личную памятку, к которой может вернуться в любой момент. Фрагмент онбординга в ConfluenceНа онбординг мы выделяем всего три дня. Мы практикуем обучение через практику: с первых дней стажеры погружаются в реальные проекты и решают «боевые» задачи. При этом не забываем про теорию — она идет параллельно с работой. Стажеры регулярно изучают предметные области, разбираются в конфигурациях 1С, посещают внутренние тренинги и воркшопы. Такой формат позволяет сразу применять новые знания на практике и быстрее развивать профессиональные навыки, в отличие от подхода, где сначала идет длительное теоретическое обучение, а только потом — работа над проектами.Стали давать стажерам сложные задачиМы разработали алгоритм, который позволяет стажерам участвовать даже в самых сложных этапах проекта и выполнять задачи уровня опытных специалистов. В основе системы — детальный регламент для стажеров на каждом этапе проекта. Суть метода в том, чтобы разбить комплексную задачу на этапы и найти способы вовлечь стажера в каждый из них.Возьмем конкретный пример: разработку модели бизнес-процесса. Раньше мы считали, что на обучение стажера уйдет больше времени, чем на выполнение самой задачи. А если и привлекали его, то только на финальном этапе — к непосредственному написанию документа. Но без участия в предыдущих этапах стажер не понимал контекста и не мог добиться результата.Мы разделили процесс на четыре этапа и для каждого прописали конкретные микрозадачи для стажера. Теперь он участвует в подготовке к встречам с заказчиком, присутствует на обсуждениях, помогает с документацией. В результате стажер погружается в процесс целиком, понимает общую картину и может создать качественный финальный документ. Один раз пройдя все этапы, он разобрался с целой моделью, а таких на проекте десятки.Распределение задач между стажером и опытным специалистом при подготовке отчетного документа «Модель бизнес-процесса» на этапе моделированияТак как описание типовых, кастомизированных бизнес-процессов — основная и самая трудоемкая задача стажеров на этапе моделирования, мы определили нормативы трудозатрат отдельно для опытных специалистов и новичков. Такой подход помогает точно планировать проекты и формировать реалистичные ожидания по срокам выполнения работ. Автоматизировали написание документации и сократили рутину  Мы внедрили систему автоматизированных инструментов, которая позволяет стажерам самостоятельно создавать проектную документацию, разрабатывать автотесты, записывать видеоинструкции и проводить обучение пользователей. Первый сервис — 1С:СППР, который мы адаптировали под свою методологию. В этой системе стажеры создают основную проектную документацию (модели бизнес-процессов, функциональные дизайны расширений) по заданной структуре, что минимизирует вероятность серьезных ошибок и сводит возможные недочеты к неточностям в формулировках. Более подробно про это можно прочесть в нашей другой статье: «Управление требованиями и проектирование в СППР с использованием методологии AIM» . Второй сервис — Vanessa Automation. С его помощью стажеры создают сценарии автоматического тестирования, которые, в свою очередь, помогают решить несколько задач: получить видеоинструкции для пользователей, организовать нагрузочное тестирование, настроить ролевую модель и реализовать автоматическую проверку доработок конфигурации. Про наш опыт применения Vanessa Automation для сокращения затрат на обучение персонала и контроля стабильности критических бизнес-процессов на проекте можно прочитать в нашей статье: «Как убить трех зайцев одним сценарием: опыт использования Vanessa Automation» .Замотивировали наставников и добавили тимлидов  Важно, что нам удалось не только починить связку стажер — наставник, но и подключить тимлидов. Это разные роли, очень важные для стажеров.Наставник — специалист в определенной предметной области: в учете, управлении персоналом или бюджетировании, например. Таким образом, с первого дня стажер получает первую специализацию. Наставник с помощью стажера выполняет реальные задачи проекта так, как это прописано в регламенте. Помогает новичку и дает обратную связь, подсвечивая, чего не хватает и что нужно усилить. За шесть месяцев с наставником стажер должен пройти большинство этапов проекта и сделать соответствующие задачи. После этого он сможет перейти на позицию младшего консультанта 1С. Тимлид отвечает за стратегическое развитие специалиста и помогает ему достичь следующего уровня. Вместе со стажером ставит конкретные проектные и внепроектные цели, помогает развивать профессиональные и личные навыки. Определяет пути достижения целей и предварительные сроки.Мотивация наставников и тимлидов — их карьерное продвижение через управление стажерами: для повышения с позиции middle-консультанта до старшего консультанта необходимо успешно обучить минимум одного стажера, подтвердив его профессиональный рост на один грейд. На новогоднем корпоративе среди наставников разыгрываются ценные призы, например путешествия. Дополнительно наставники получают «бобры» — корпоративную валюту для покупок во внутреннем магазине компании. А еще практика показывает, что после успешного опыта работы со стажерами без них уже сложно обойтись. Потому что многое можно делегировать — получаются отличные помощники на проектах.Разработали карту грейдов и матрицу компетенций для индивидуального плана развитияМы разработали общую карту грейдов для консультантов 1С, которая определяет базовые требования к хардам, софтам и проектному опыту для каждой позиции. Дополнительно для каждого функционального направления (предметной области) существуют уточненные карты грейдов и матрицы компетенций. Матрица компетенций разрабатывается на базе уточненной карты грейдов и представлена в виде таблицы, где по горизонтали перечислены компетенции, а по вертикали — Ф. И. О. сотрудников и на пересечении статус владения компетенцией — «знает», «не знает», «применяет» или «обучает других». Также в матрице есть ссылка на источник знаний, как ту или иную компетенцию можно приобрести: курсы / тренинги / сертификацию / тестовые задания, либо только через проектный опыт.Всё, что мы обсудили выше — карты грейдов, матрица компетенций, — должно перерасти в индивидуальные планы развития каждого сотрудника.В КРОК индивидуальный план развития — это не какой-то документ, где перечислены компетенции, которые нужно приобрести сотруднику в ближайшие полгода-год, это целый процесс. Выглядит он как полугодовой мониторинг сотрудника со сбором оценок 360 и целями на полугодие, которые мы уточняем поквартально и премируем за их достижение.При постановке целей мы ориентируемся на задачи текущего проекта, обратную связь, предпочтения сотрудника и дальнейшие планы по сотруднику на следующих активностях.Такой подход нам кажется удобным, более гибким и менее трудоемким.Мы пользуемся системой Cool Feedback (собственная разработка КРОК) для сбора обратной связи и оценки сотрудникаВ системе Cool Feedback стажер вместе с тимлидом фиксирует цели для перехода на следующий грейд. Система помогает отслеживать прогресс, напоминая о сроках и статусах поставленных целей.  Ввели защиту стажерских проектовЗащита стажерского проекта — кульминация стажировки и мини-праздник. Специалист готовит презентацию и перед всем направлением рассказывает о результатах, делится полезными лайфхаками, которые помогли успешно пройти стажировку. А главное — переходит на позицию младшего консультанта 1С.На защите стажер демонстрирует свое практическое участие в проекте, глубину понимания функционального блока и качество выполненных задач. Стажер представляет наиболее интересные процессы и свои профессиональные выводы. Защита становится площадкой для обмена опытом между участниками разных проектов, где наставники и ведущие специалисты дополняют презентацию ценными комментариями. При неудачной защите проекта, даже если стажер хорошо работал, но не смог убедительно представить результаты или ответить на уточняющие вопросы, мы не проводим повторную защиту. Вместо этого формируется индивидуальный план развития с конкретными целями и сроками для перехода на позицию младшего консультанта. Финальная оценка в таком случае проводится через интервью.Наша компания придерживается принципа эффективного развития: стажировка не может длиться более года. Если сотрудник не демонстрирует необходимый прогресс, мы принимаем решение о прекращении сотрудничества до этапа защиты проекта.Быстро увидели результаты и сделали выводыУже через полгода наша новая стажерская программа показала позитивные результаты. Работа с новичками стала системной, их много, и мы можем на этих цифрах делать выводы и отслеживать закономерности. Процесс стал более управляемым. Ключевые факторы успеха — это системный подход к отбору кандидатов, четкая регламентация процессов, продуманная система наставничества, правильная проектная загрузка и гибкость в выборе специализации. Когда все эти элементы работают слаженно, стажеры быстро развиваются и становятся ценными специалистами в команде.  Что говорят стажеры и выпускники летней школы..."
86,86,Как мы сделали автономного робота для соревнований и почему он проиграл,Альфа-Банк,Лучший мобильный банк по версии Markswebb,445.4,"Дизайн и юзабилити, Электронная коммерция, Мобильные технологии",2025-03-12,"Базово наш робот не отличался от остальных. Главное отличие состояло в начинке. Роботы на «Битве» — это большие и тяжёлые машинки на радиоуправлении, которых полностью контролирует оператор-человек. Мы собирались сделать автономный «аппарат», который не будет управляться человеком и сможет победить в соревновании.И у нас практически получилось.Привет, меня зовут Дмитрий, я аспирант 4-го курса факультета «Систем управления и робототехники» Университета ИТМО. Я расскажу историю, как мы в прошлом году с командой выступили на «Битве Роботов» с автономным роботом. Так как подобные роботы на ринге «Битвы» не появлялись, и мы, в свою очередь, ещё не занимались постройкой 160-килограммовых «самодвижущихся» боевых аппаратов, которые управляют «сами собой», опыт был весьма занимательный.В последние годы технологии активно развиваются, во многих областях происходит стремительная цифровизация и автоматизация. Одним из ключевых драйверов всех этих процессов являются исследования и разработки в области робототехники. Команда Университета ИТМО вместе с Альфа-Банком решила внести свой вклад в популяризацию и развитие автономных мобильных роботов и совместить приятное с полезным. Мы выступили (нескромно) с первым в истории «Битвы Роботов» автономным роботом. Вот так он выглядит на тизере.Встречайте, Робот A.L.F.AНа общем фото с командой.А здесь на ринге.Источник: https://vk.com/champbitvarobotov.Почему я так упираю на слово «автономный»? Для этого нужно сказать пару слов про «Битву». Это мероприятие, куда съезжаются команды, конструирующие разных роботов, выпускают их на ринг и управляют, чтобы нанести максимальный урон противнику.Понятие «автономность» не ново, но на «Битве Роботов» эта концепция ещё не появлялась. Это связано с некоторыми особенностями площадки «Битвы» — открытое пространство, ловушки, кривой пол, о которых я расскажу далее. А самое главное — роботы-противники, управляемые людьми, поведение которых невозможно предсказать.Во всех лигах «Битвы Роботов» в нашей весовой категории (о ней дальше) есть только один пример робота, считающегося автономным. Но его «автономность» со звёздочкой, потому что у робота есть оператор, а у оператора — пульт, посредством которого выбирается модель поведения робота.Поэтому считаем, что на данном этапе научно-технического прогресса на март 2025 года на «Битве Роботов» все роботы управляются оператором.Наш робот не управлялся человеком с пульта.Загвоздка в том, что задача разработки автономного мобильного робота — комплексная. Над этой задачей много лет трудятся большие компании, команды инженеров, проектировщиков, разработчиков. Нам же всё было необходимо «вот уже сейчас» синтезировать, имплементировать и тестировать систему управления. Расскажу, как это происходило.Разработка робота: система управленияОрганизаторы разрешили нам утяжелить робота на 50 кг сверх верхней границы разрешённых значений, и наш робот A.L.F.A имел массу в 158 кг (и был самым тяжёлым). В остальном робот не сильно отличался от остальных моделей (не считая того, что он был самым красивым):внутренний каркас из алюминия, корпус из стали, на корпусе установлены пластиковые элементы, двигатели стандартные для шоу, тяжелый вертикальный спиннер (самый тяжелый на шоу),роликонесущие колеса, позволяющие двигаться в любом направлении.Примечание. Про Mecanum колеса есть статья в блоге Университета на Хабре, делюсь ссылкой. Крутите колесо: как аспирант ИТМО ищет способ решить проблемы роботов на базе Mecanum колес и расширить их применениеMecanum колеса обеспечивают высокую маневренность, при этом позволяют отказаться от использования сл...habr.comГлавное отличие — в начинке. Как я уже писал, роботы на «Битве» — это скорее большие и тяжёлые машинки на радиоуправлении. Их полностью контролирует оператор.Автономному аппарату, ввиду отсутствия оператора, нужна самостоятельная система управления. Она состоит из нескольких блоков: система технического зрения, алгоритм планирования траектории, алгоритм слежения за траекторией.Первый блок — видеозрение. От него на вход системы идут данные о текущем местоположении и ориентации A.L.F.A и местоположении робота-противника.Второй блок — алгоритм планирования траекторий. Он получает данные от видеозрения и строит маршрут до противника в объезд препятствий и ловушек на ринге. После сгенерированная траектория поступает на вход третьего блока. Третий блок представляет собой регулятор, задача которого — подавать напряжение на моторы, чтобы робот двигался согласно заданной траектории.Для тестирования алгоритмов первым делом сделали уменьшенную версию робота, сохраняющую его кинематику. С помощью аддитивных технологий, то есть 3D-печати, лазерного станка и усилий студентов, у нас появился макет A.L.F.A. Колёса напечатали из PLA-пластика, ролики — из TPU. Для управления на борту находились микроконтроллер Arduino UNO и одноплатный компьютер Raspberry Pi.Макет A.L.F.A.Но, кроме самого робота, нам был необходим противник. Он сделан на основе тех же технологий и управлялся с помощью джойстика — так мы имитировали человеческое участие в управлении противником.Макет мобильного робота (противник).ОчувствлениеДля создания автономного робота, способного адаптироваться к внешним изменениям, нам необходима обратная связь. Для этого существуют различные датчики и сенсоры. Мы могли бы поставить на робота лидар и камеры, как это делают на многих роверах-курьерах, или использовать датчики расстояния. Однако в нашей специфической задаче все эти инструменты работали бы до первого удара.Поскольку регламент шоу позволяет использовать внешние камеры, то решили, что это оптимальный для нас сценарий, и нам достаточно видеть картинку в 2D (решили расположить камеры так, чтобы они давали нам вид сверху на арену). Для арены 12 х 12 метров выбрали 6 камер с углом обзора 110 градусов, повесили их вертикально и сшили полученные изображения в одно целое. Так у нас появилась карта пространства, на которую робот «смотрит» сверху вниз.Пример работы алгоритма для макета робота.Теперь перед нами стоял вопрос: как именно мы будем следить за нашим роботом и роботом противника? Для выполнения задачи слежения за объектом должны были выполняться следующие условия.высокая скорость работы,низкая задержка,низкая ресурсозатратность,стабильность работы. На основании этих вводных было решено остановиться не на алгоритме поиска, а на алгоритме слежения. Он существенно быстрее, так как следит за изменением положения объекта лишь в границах старого положения и не производит поиск по всему кадру. Минусом такого подхода является необходимость знать начальное положение объекта, но в нашем случае мы располагаем этой информацией (роботы всегда стартуют из одних и тех же зон). В качестве алгоритма выбрали MeanShift, который выполняет итерационный поиск скопления максимальной интенсивности внутри границ объекта по маске. Как правило, в качестве маски используется обратная гистограмма по цветовому каналу HSV. Только необходимо отделить фон от объекта внутри бокса слежения на первой итерации. Для этой задачи применялся метод водораздела с разметками в центре для объекта и по углам для фона.Минус обратной гистограммы — плохое слежение за объектами, близкими по тону с фоном. А в случае с цветовым каналом — и за бесцветными объектами. Для таких задач можно применить вычитание фона: фотографируем ринг до помещения в него объектов и следим именно за разницей между ним и текущим кадром. Также, чтобы боксы слежения не пересекались между разными целями, в масках слежения положение других объектов слежения приравнивается к нулю.Для ориентации робота было решено использовать зелëные детали на нём. Источник: https://vk.com/champbitvarobotov. Выделили зелёный цвет логотипа в границах объекта и отдельные фигуры слили между собой с помощью морфологии. В результате получалась фигура, схожая с молотком или штопором. У фигуры выделялся контур, над которым было произведено две манипуляции: преобразование в выпуклый прямоугольник и аппроксимация для уменьшения числа линий.И здесь вместо штопора у нас получается фигура, сходная с бриллиантом из пяти граней. У этой фигуры определялись две стороны с общей точкой, которые являлись низом этой фигуры. Из угла между ними строилась биссектриса, которая и позволяла определить угол поворота робота относительно камеры.Система технического зрения на выходе даёт нам положение и ориентацию робота A.L.F.A и положение противника, а также дополнительный «флаг», сообщающий нашей системе, что мы столкнулись с противником.Пример слежения за роботом противника на самой арене.На видео заметны мёртвые зоны. К сожалению, из-за сложности сшивания изображений и ограничений по времени использования общей арены (все на «Битве Роботов» хотят протестировать свои машины) мы не успели исправить эту ошибку. Необходимо установить камеры, выставить необходимый свет (а он влияет на камеры), а ещё настроить регуляторы. На это в идеале нужно часов 20.Однако алгоритм отрабатывал поставленную ему задачу. ПланировщикТехническое зрение позволяет роботу ориентироваться и понимать, где он находится, а также следить за положением противника. Дальше нам нужно доехать до противника и нанести урон. Значит, нам необходимо, чтобы робот мог построить маршрут до цели.От системы технического зрения мы получаем 5 значений в каждый момент времени: положение по Х, по Y, ориентация нашего робота и положение противника. Поскольку наш робот обладает всенаправленной подвижностью (голономностью), его положение динамично изменяется, и он может двигаться в любом направлении.Автономный всенаправленный робот показывает себя намного эффективнее, чем если бы им управлял оператор. Системы автоматического управления в этом случае справляются намного лучше. В том плане, что могут описывать траектории, которые человек никогда не опишет посредством стандартного пульта. Голономность — это математический термин. Грубо говоря, означает, что количество управляемых степеней свободы в любой момент времени соответствует общему количеству степеней свободы робота. По этому определению поезд — голономный, а автомобиль — нет. Причина в том, что у поезда одна управляемая степень свободы — скорость, и одна степень свободы движения — положение на рельсах. А у автомобиля две управляемые степени свободы  — скорость и угол поворота, но три степени свободы движения: ось x, ось y и ориентация.В книге «Omni-directional robots» есть картинка, которая иллюстрирует моё утверждение. Справа — типичная форма/конструкция автомобиля. Слева — типичная конструкция всенаправленного омни робота, которая и позволяет быть голономными.Роботы на Битве используют различные кинематические схемы. Мы выбрали голономную, потому что она позволяет двигаться вперёд/назад (одна управляемая степень свободы), по кругу, поворачивать на месте и двигаться боком (плюс ещё две). И степени свободы движения у такого робота тоже три.Как следствие,  в качестве алгоритма планирования траектории был выбран DWA. DWA — это популярный алгоритм планирования движения, который используется в робототехнике и других областях для планирования безопасных и оптимальных траекторий для перемещения роботов или других объектов.Основная идея DWA в том, чтобы найти безопасную и оптимальную траекторию для перемещения робота:избегая столкновений с препятствиями (например, мимо подушки, которая подкидывает робота или шнеков, что бьют по корпусу);и учитывая ограничения скорости и ускорения. Алгоритм работает в реальном времени и может адаптироваться к изменяющейся окружающей среде. Вот основные этапы алгоритма DWA:Алгоритм оценивает текущую ситуацию, включая положение робота, скорость, ускорение и окружающую среду.Определяет окно возможностей, которое представляет собой набор допустимых скоростей и направлений, которые робот может принять в текущий момент.Оценивает различные траектории, которые робот может пройти в пределах окна возможностей. Каждая траектория оценивается по нескольким критериям: безопасность, оптимальность, скорость достижения цели.Выбирает лучшую траекторию из набора возможных траекторий, которая соответствует критериям безопасности, оптимальности и скорости достижения цели.Робот выполняет выбранное действие, которое может включать изменение скорости, направления или положения.Всё, теперь наш робот умеет следить.Слежение за объектомРегуляторМасса робота — 158 кг (плюс-минус), и он обладает убойной разрушительной силой в виде металлического спиннера. Это самый популярный тип орудия, и он хорошо себя показывает в бою. Да и душе приятно, когда спиннер крошит металл. Драка роботов с подъёмниками (когда один робот подбрасывает другого) для меня довольное унылое зрелище.Поэтому только спиннер, только максимум урона. В правилах «Битвы» есть ограничение: тело вращения не может весить больше 30 кг. Мы и взяли 30 кг — наш робот его выдержит.  Выбрали форму — по зубцу с каждой стороны. Вращение — против часовой стрелки, чтобы ещё и подбрасывать противника. Дальше под него выбираем шкивы, моторы, передачу (зубчатую), рассчитали передачу, ремни и светодиоды (глянца) — по мелочи, и аккумулятор. Желательно для орудия иметь отдельный аккумулятор: если по орудию сильно прилетит и аккумулятор выйдет из строя, то робот хотя бы сможет двигаться.Крутим спиннерНельзя допустить, чтобы пострадал робот или кто-то из окружающих в процессе настройки регулятора, поэтому применили метод ADRC. Он гарантирует стабильную и точную работу системы. Этот метод был впервые предложен Ханном в 1998 году и с тех пор широко используется в различных областях, включая робототехнику, авиацию, автомобильную промышленность и другие.Основные компоненты ADRC:Наблюдатель: наблюдатель оценивает состояние системы и вычисляет компенсационное воздействие.Компенсатор: компенсатор вычисляет компенсационное воздействие на основе оценки наблюдателя.Контроллер: использует компенсационное воздействие для корректировки управления системой.Преимущества ADRC:Компенсация возмущений: ADRC может компенсировать внешние возмущения и внутренние неопределенности, что позволяет системе работать стабильно и точно.Низкая чувствительность к параметрам: ADRC не требует точных знаний параметров системы, что делает его более устойчивым к изменениям параметров.Простота реализации: ADRC относительно прост в реализации и может быть использован в различных системах.Регулятор был синтезирован в прикладной программе MATLAB Simulink. Общая система управления выглядит следующим образом:Схема управленияПосле создания всех алгоритмов, отвечающих за автономность, они были связаны между собой и интегрированы в робота. Движение к объектуВ лабораторных условиях всё было отлично. На видео, пол ровный, но в реальности на арене лежат стальные листы. Они, естественно, имеют неровности, царапины, шероховатости, а после боёв других роботов листы значительно искривляются, что требует изменения коэффициентов регулятора для того же желаемого поведения системы. У нас голономный робот с тремя каналами управления, все они зависят друг от друга. Таким образом, мы настраивали регулятор с девятью коэффициентами и четырьмя матрицами. И настроить эти коэффициенты в реальном мире — нетривиальная задача. Тем более, когда у нас такая сложная система. Ещё более нетривиальная она, когда времени совсем мало, о чём писал ранее.Собираем всё вместеВсе вычисления мы вынесли на внешние устройства.На первом ноутбуке сначала обрабатывается картинка технического зрения.Передаются на второй, где обрабатываются в MATLAB/Simulink. Далее передаются на пульт управления. За пультом управления не находился оператор. Мы следили лишь затем, чтобы программы не выдавали ошибки. Пульт работал также, как если бы за ним находился оператор, но без оператора, все данные отправлялись с компьютера. Данная схема довольно дешева и быстра в настройке, например, поэтому у складских погрузчиков часто есть механизм поворота, который вращает руль. Полностью автономный погрузчик стоит очень дорого. Итоги и благодарностиРобот выступал против опытной команды, которая участвовала в соревнованиях десятки раз (если не больше). Лучшего противника и не найти. И хотя наш робот не вывез драку, в целом, это было круто.Хотя задача робота довольно проста — врезаться во врага с максимальной скоростью, избегая препятствий, — мы хотели сделать что-то действительно уникальное для международного чемпионата «Битва Роботов». Исполнение этой простой задачи демонстрирует возможности современных автономных систем. Мы стремились показать, что инновации в робототехнике открыты не только для исследовательских лабораторий, но и могут привнести свежие идеи и технологии в сферу развлечений.Университет предоставил проекту базу для разработки, включая опытные кадры и технические мощности, а на заводе Университета создавались основные механические узлы. Наш партнёр, Альфа-Банк, не только стал спонсором, но и взял на себя ответственность за визуальную составляющую робота, создал рекламный ролик, обеспечил логистику и многое другое. Объединив усилия, мы разработали первого в истории шоу полностью автономного боевого робота, который может самостоятельно находить противника, планировать путь и следить за его траекторией.Этот проект — больше, чем просто робот для шоу. Это наша общая попытка продемонстрировать, что робототехника и автоматизация могут быть драйверами как инженерных, так и креативных решений, а также вдохновлять новое поколение инженеров и исследователей. Развитие систем робототехники и повсеместная роботизация — одно из ключевых направлений России и мира. Разработка автономных роботов, развитие ИИ — приоритетная задача. Мы считаем, что этим нужно заниматься.Алгоритм робота был неидеален, не адаптирован и имел некоторые проблемы, но, надеюсь, мы смогли привлечь внимание к этой теме. Для нас самих это был некоторый вызов, но ведь лучше попробовать, чем не попробовать?В следующем году тоже надо выходить с более сильным автономным роботом (чемпионат проводится ежегодно), с большими стратегиями поведения, который будет лучше адаптироваться к изменениям среды и быстрее настраиваться перед битвой.Команда, работавшая над роботом: Алексей Ледюков, Федор Петриков, Андрей Яременко, Иван Смирнов и я, Дмитрий Захаров. Спасибо за внимание и смотрите следующий сезон «Битвы Роботов»:)Статьи, которые могут быть интересны:Обучение аналитиков дизайнером Пайплайн каскадных моделейСортируем сотни млн строк в разы быстрее библиотечных алгоритмов. А не замахнуться ли нам на ммм… на O(n)?Рассказ об истоках и эволюции редакторов Vi и VimБаза об организации процесса разметки: команда, онбординг, метрикиПодписывайтесь на Телеграм-канал Alfa Digital — там мы постим новости, опросы, видео с митапов, краткие выжимки из статей, иногда шутим."
87,87,Переезд с TeamCity на GitLab CI + K8s,Контур,Делаем сервисы для бизнеса,4.4,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-03-12,"Привет, Хабр! Меня зовут Даниил Мильков, я старший C# разработчик в команде Формы, которая входит в состав продукта Контур.Экстерн. Если вкратце, Экстерн позволяет бизнесу сдавать отчётность в контролирующие органы (ФНС, Росстат и тп.) через интернет.Сразу хочу предупредить читателей, что про взаимодействие с k8s здесь сказано достаточно мало, разве что в разделе Kubernetes и PVC. На эту тему будет отдельная статья.Начнём. Однажды наша команда решила перейти с TeamCity на GitLab CI…Почему мы решили перейти на GitLab CITeamCity ушёл из России. Лицензии сложно получить, обновления недоступны, нет техподдержки, проблемы приходится исправлять на ощупь.Одновременно с этими событиями из нашей команды ушли все инженеры-эксплуататоры и остались только разработчики. Им не хотелось заниматься поддержкой огромного парка виртуалок.Контейнеры – мировой стандарт. В мире активно развивается использование контейнеров и оркестраторов, в частности Kubernetes. Мы тоже это хотим. Тем более, это согласуется как с нашим нежеланием заниматься виртуалками, так и с целями команды техкачества по переходу на Linux и shared-хосты. А ещё мы хотим, чтобы наши сервисы были доступны как on-premise решение, которое очень удобно (а чаще – необходимо) распространять, как контейнеры. В общем, без этого никуда.МиграцияПереезжать мы хотели так, чтобы пользовательский опыт не менялся радикальным образом, и поэтому постарались сделать интерфейс CI в GitLab максимально приближенным к ТС. Как будто что-то поменялось, стало чуть-чуть непривычно, но в целом кнопочки остались те же самые. С нашей же точки зрения, как тех, кто занимался этим переездом, это были весьма существенные перемены. Изменились:Инструмент (TeamCity на GitLab)Операционная система (Windows на Linux)Способ хостинга рабочей нагрузки (виртуальные машины на контейнеризацию в куберовском кластере)Общие шаблоныК тому моменту, как мы решили заезжать на GitLab, команда девопсов базовой инфраструктуры уже подготовила общие шаблоны конфигураций GitLab CI. В GitLab CI шаблоны (templates) — это готовые части конфигураций, которые можно переиспользовать в разных проектах. На основе этих шаблонов получилось быстро спрототипировать первые джобы на новой платформе. Но сейчас мы практически не используем общие шаблоны, потому что:Они ориентированы только на небольшие проекты без хитрых сценариев, сложно кастомизируемы.Зачастую сложно разбираться в параметрах, которые выполнены в виде системных переменных. Не всегда очевидно, какое эффективное значение переменной попадает в итоге в конечную команду.Эти ограничения касаются всех общих шаблонов, которые может сделать кто угодно. Сделать их более удобными и при этом функциональными очень сложно. Может быть, вовсе невозможно. Не зря gitlab добавили в CI конфигурацию step’ы.Несмотря на то что использование таких шаблонов зачастую неудобно, есть сценарии, когда это оправдано. Например, мы с их помощью запускаем анализатор кода для дотнетовского солюшена. Поэтому посмотрим, как выглядит минимальная CI конфигурация с использованием общих шаблонов:include:   - project: 'gitlab/path/to/ci-templates'  # Проект в gitlab, из которого подключается шаблон     file: 'all.yaml'                        # Файл, который подключает все доступные шаблоны в вашем проекте   stages:   - inspect-code   Inspect code:                               # имя джобы   stage: inspect-code                       # в какой stage входит эта джоба    extends:     - .dotnet_inspect_code                  # имя шаблона, который хотим использовать    variables:                                # переменные, с помощью которых настроим поведение шаблона       DOTNET_PROJECT_OR_SOLUTION_PATH: ""path/to/solution/file""    # относительно корня вашего репозиторияМожно вставить это в файл .gitlab-ci.yml в корне вашего проекта, подставить путь до солюшена, и всё заработает.Найти свой пайплайн можно в разделе Pipelines вашего репозитория (Build -> Pipelines)Как попасть в список Pipeline'овПример нашей джобыА вот как выглядит джоба уже без общих шаблонов:Integration tests:  extends: .test-base  variables:                               # Конфигурируем лимиты и реквесты для кубера      KUBERNETES_CPU_REQUEST: ""3""      KUBERNETES_CPU_LIMIT: ""10""      KUBERNETES_MEMORY_REQUEST: ""12Gi""      KUBERNETES_MEMORY_LIMIT: ""24Gi""      KUBERNETES_EPHEMERAL_STORAGE_REQUEST: ""17Gi""  script: |                                # Скрипт с основной работой    #    .gitlab-ci/common/scripts/dotnet_restore_and_build.sh    .gitlab-ci/common/scripts/resource_build.sh    .gitlab-ci/common/scripts/start-services.sh    .gitlab-ci/stages/tests/integration-tests.shКоманды вынесены в файлы .sh только для того, чтобы оформить удобное логирование.И примерно так, например, выглядит dotnet_restore_and_build.sh#!/bin/bash source $(dirname ""$0"")/logging.sh  solution=""${1:-Main.sln}"" args=""${3:-""""}""  # В функции startLogCollapsedSection в консоль пишется специальная страшная строка (приведу её ниже) # по ней гитлаб понимает, что надо начать секцию, которую можно # схлапывать и для которой автоматически посчитается время выполнения section_id=""dotnet build $solution"" startLogCollapsedSection ""$section_id""     dotnet build $solution --configuration Release $args return_code=$?  # Здесь в консоль пишется строка для закрытия секции endLogSection ""$section_id""  if [ $return_code -ne 0 ]; then    logError ""Ошибка при билде $solution (exitCode $return_code)"" fi  exit $return_codeСтрашные строки для закрытия и открытия секций:echo -e ""\e[0Ksection_start:`date +%s`:section_id_dotnet_build_main_sln[collapsed=true]\r\e[0K\e[36;1mdotnet build Main.sln\e[0;m\e[0;m"" echo -e ""\e[0Ksection_end:`date +%s`:section_id_dotnet_build_main_sln\r\e[0K""А за подключенным в секции extends уже нашим шаблоном .test-base скрываются такие строки (общие для всех наших тестовых джоб):.test-base:  stage: tests  needs: []                                                   # Говорит о том, что джоба не должна ждать никакие джобы в предыдущем stage  services:                                                   # рядом с контейнером джобы будут запущены контейнеры с указанными образами      - name: docker-proxy.host/elasticsearch:6.6.0           # в данном случае c elasticsearch - он нужен для ряда интеграционных тестов        command: [ ""bin/elasticsearch"", ""-Expack.security.enabled=false"", ""-Ediscovery.type=single-node"" ]  before_script:                                              # команды, которые выполняются до основной секции script    - !reference [.common_before_script, before_script]       # здесь шаблон общий вообще для всех джоб, так выполняется преднастройка джобы.    - # В этой части мы скачиваем дополнительные репозитории, которые нужны джобе (см. ""Работа с несколькими репозиториями"")  script:    - echo ""Hello, Dan! Replace 'script' section in your new job""  artifacts:                                                  # настройка артефактов, которые надо будет загрузить в хранилище после выполнения джобы    when: always                                              # always - загружать всегда, даже если джоба упала    expire_in: 1 week                                         # ttl    paths:                                                    # маски путей, где надо искать артефакты для загрузки      - $CI_PROJECT_DIR/junit/*.xml      - $CI_PROJECT_DIR/annotations/*.json      - $CI_PROJECT_DIR/logs/**/*.log    reports:                                                  # некоторые из артефактов гитлаб может обрабатывать по особому      junit: $CI_PROJECT_DIR/junit/*.xml                      # строить тестовый отчёт      annotations: $CI_PROJECT_DIR/annotations/*.json         # добавлять кастомную информацию в джобу Что наделали, чтобы переехатьАналитика тестовой историиГитлаб коммитоцентричен. Это здорово. Но вот такого разреза данных, то есть истории по запускам какой-то джобы, из коробки там нет. История запусков конфигурации в TeamCityКак и истории конкретного тест-кейса. Единственное встроенное, что есть в гитлабе, это количество падений тест-кейса за последние две недели. Не очень информативно. Тестовая история в GitLab CIРассмотрели существующие сторонние решения, по тем или иным причинам они нам не подошли. Но окей, раз этой фичи нет, надо её сделать. Сделали. Получился сервис, реализующий нужную функциональность в похожем на TeamCity стиле. Рассмотрим несколько страниц, как это выглядит. История по конкретному типу джобы:История запусков конфигурации в нашем сервисе (TestCity)Подробный тестовый отчёт. В нём можно отфильтровать тест-кейсы по имени, по статусу или отсортировать по длительности выполнения. Встроенное в гитлаб отображение тестового репорта такие фокусы делать не умеет. А ещё тут можно скачать список тест-кейсов в csv-формате и даже проанализировать, какие массивы тест-кейсов отнимают больше всего времени с использованием диаграммы treemap.Подробный тестовый отчёт по запуску джобыА это история конкретного тест-кейса:История тест-кейсаЧтобы из джобы был удобный доступ к этому сервису, добавили в неё пару аннотаций со ссылками на подробный тестовый отчёт и на историю по конкретному типу джобы:Аннотации к джобе со ссылками на наш TestCityПара слов про внутрянку:Данные хранятся в ClickHouse (CH). Бекенд проксирует SQL запросы с фронта до CHПопадают данные в базу через краулер, который ходит через GitLab API по джобам, скачивает их артефакты и парсит JUnit xml отчёт внутри. То есть для того чтобы ваши тестовые отчёты появились в сервисе, вам ничего не надо менять ни в вашей ci конфигурации, ни в коде. Достаточно поднять сам сервис, CH и краулер (либо отправлять данные в CH любым способом по вашему желанию)Сейчас мы уже вынесли решение за пределы своей команды, подключив к нему пару других команд, а в самом ближайшем будущем планируем сделать так, чтобы для пользователей GitLab CI в Контуре этот инструмент был доступен из коробки. Если в комментах будет большое количество желающих использовать такой сервис, подумаем над тем, как можно вынести его во вне.Отчёт о кодовых инспекцияхВ GitLab нет возможности просматривать отчёт о кодовых инспекциях, как это опять же реализовано в TeamCity:Отчёт о кодовых инспекциях в TeamCityНа самом деле возможность есть, но только в платной версии GitLab. Мы сделали удобное отображение этого отчёта через банальный рендер его в html.О каких вообще инспекциях идёт речь. Мы для анализа кода запускаем утилиту jetbrains’a – inspectcode, которая распространяется через dotnet tool JetBrains.ReSharper.GlobalTools. Иногда эти проверки показывают действительно серьёзные проблемы, так что очень рекомендую их использовать.Чтобы воспользоваться этим рендером необходимо:Выполнить в джобе проверки с помощью inspectcode, сохранив при этом файл с отчётом по пути $CI_PROJECT_DIR/inspectcode/code-inspection.xmlДобавить себе в репозиторий файл code-inspection-after-script.shВызвать этот скрипт после завершения работы анализатора. Он сгенерирует рендер отчёта в $CI_PROJECT_DIR/inspectcode/code-inspection.html и аннотацию в $CI_PROJECT_DIR/annotations/$(date +%s).jsonЗагрузить получившуюся html страницу с отчётом в артефактыДобавить аннотацию в джобуПример итоговой джобы при использовании общего шаблона, за которым фактически спрятан только запуск inspectcode, выглядит так.Inspect code:  stage: inspect-code  extends:    - .dotnet_inspect_code  variables:      DOTNET_PROJECT_OR_SOLUTION_PATH: “path/to/solution/file”  after_script: code-inspection-after-script.sh      # путь до скачанного скрипта  artifacts:    when: always    expire_in: 1 week    paths:      - $CI_PROJECT_DIR/inspectcode/*      - $CI_PROJECT_DIR/annotations/*.json    reports:      codequality:        - $CI_PROJECT_DIR/inspectcode/gl-code-quality-report.json      annotations: $CI_PROJECT_DIR/annotations/*.jsonВ джобе появится вот такая ссылка, которая ведёт на отрендеренный отчёт:Аннотация к джобе со ссылкой на наш отчётСам отчёт выглядит так:Наш отчёт о кодовых инспекциях. Сори, что много заблюренного. NDA и всё такое…Работа с несколькими репозиториямиДля большинства наших конфигураций требуются сразу несколько реп: репа с сервисами, несколько реп с ресурсами, репа с фронтом. Соответственно, нужна была логика работы с репозиториями, похожая на работу VCS Roots в TeamCity – чтобы дополнительные репозитории в репе фетчились и чекаутились на нужную ветку, если её нет, то фетчились на дефолтную, которая может быть разной для каждого репозитория, если репа ещё не склонирована – склонировать, когда надо – почистить репу и тд. И отвечая на очевидный вопрос – нет, мы не хотели связываться с git submodules.Реализовали нужную функциональность на шарпе в виде консольной утилиты, потому что так проще тестировать, логи и метрики из коробки, и, конечно, чтобы не писать это на bash.Пример использования:ci-utils clone --remotePath=""gitlab/repo/path/"" --localPath=""$CI_PROJECT_DIR/../repo"" --branch=$CI_COMMIT_BRANCH --defaultBranch=""master""Shared resourcesShared resources – полезная фича TeamCity. Вы указываете массив строк, например имена площадок. Конфиги, которые используют этот shared resources, могут брать один из этих ресурсов и блокировать его. В этом случае остальные конфигурации не смогут получить доступ к занятому ресурсы, а если свободных ресурсов нет, то встанут в очередь.Эта фича очень пригодилась нам для реализации автоматического прогона системных тестов. В них мы запускаем сервисы и ресурсы с нужного коммита, в одном из четырех окружений. Зачем нам эти окружения. Каждое из окружений соответствует площадке (VM), на которых у нас развернут ключевой сервис продукта, назовём его K. Cервис K, к сожалению, ещё на старой технологии хостинга и не готов к динамическим окружениям. Именно для этого нам и нужны shared resources – чтобы брать свободную площадку с сервисом K.Что мы с этим сделали: реализовали занимание ресурсов на основе распределенной блокировки и json-хранилища также в виде консольной утилиты.Пример использования:ci-utils getResource --resources=""stg1,stg2,stg3,stg4"" --user=$CI_JOB_ID --output=""$CI_PROJECT_DIR/captured_resource"" ci-utils freeResource --resources=""stg1"" --user=$CI_JOB_IDЭти две утилиты и ещё пара полезных команд находятся в составе одной cli'шки, которую распространяем через dotnet tool. Если подобная утилита нужна кому-то ещё, пишите в комменты. Сходу вынести её за пределы Контура немного сложно, так как например для работы shared resources используются наши внутренние сервисы распределенной блокировки и хранилища json’ов. Но при большом отклике мы что-нибудь придумаем.Kubernetes и PVCКак использовать Kubernetes в GitLab CI — это тема для отдельной статьи. А сейчас поговорим только об одном аспекте: хранении данных в джобах.В гитлабе в связке с кубером джобы по умолчанию запускаются в подах. Под – это контейнер для контейнеров, он эфемерный. После завершения джобы под вместе со всеми данными внутри себя (например склонированные репозитории) удаляется. Если вы хотите поведение, похожее на то, как это работает в ТeamCity, вам нужны постоянные хранилища. Они позволят хранить данные между запусками джоб. Под удаляется, хранилище с файлами остаётся жить и затем подключается к следующему поду. Особенно это полезно на толстых репах, потому что позволяет не клонировать их при каждом запуске, а всего лишь фетчить. А также можно сохранять кэш NuGet-пакетов и вообще всё, что хочется.Называются эти постоянные хранилища – PV (persistent volume). В рамках подов нельзя использовать PV напрямую. Необходимо использовать Persistent Volume Claim (PVC), который позволяет запросить постоянный том с нужными параметрами и затем использовать его в рабочих нагрузках.На данный момент мы не используем PVC в своём основном CI, а просто каждый раз клонируем нужные репозитории. Плюс у этого подхода в том, что не надо думать про очистку файлов, как в постоянном хранилище. Минус понятно какой – отсутствие кэша. Но кажется, что использование PVC поможет достаточно существенно сократить время подготовки джобы (скачивание реп, билд солюшена), поэтому сейчас мы экспериментируем с подключением PVC, а итогами этого поделимся отдельно, думаю, как раз в статье про Кубер.Для тех, кто тоже хочет подключить себе PVC, приведу набор действий, которые для этого нужны:Вы должны использовать свои куберовские раннеры, которые контролируете именно вы, а не, например, базовая инфраструктура вашей компании. Раннер – это штуковина, которая порождает\контролирует\убивает поды для джоб в кубере.В конфигурацию раннера нужно добавить следующие настройки:[runners.custom_build_dir]    enabled = true [[runners.kubernetes.volumes.pvc]]    name = ""build-pvc-$CI_CONCURRENT_ID""    mount_path = ""/builds""3. При этом значение переменной $CI_CONCURRENT_ID надо задать для CI конфигурации, из которой вы деплоите раннер. Вот так – CI_CONCURRENT_ID: ""$CI_CONCURRENT_ID"". Делается это для того, чтобы в конфигурацию раннера в кубере попало такая строчка – name = ""build-pvc-$CI_CONCURRENT_ID"", а не строчка с уже подставленным значением, например name = ""build-pvc-7""4. В конфигурацию вашего CI добавить переменную:GIT_CLONE_PATH: ""$CI_BUILDS_DIR/$CI_PROJECT_PATH""Эта переменная позволит клонировать репы по путям build/git/repo/path вместо дефолтного build/5/Ytdk54Kdf/git/repo/path. Не вдаваясь сильно в подробности, проблема с дефолтным путём в том, что при деплое нового раннера (например при изменении его настроек), этот путь меняется (Ytdk54Kdf – id раннера) и надо всё выкачивать заново. А ещё директории со старым id не удаляются – нужно чистить их руками.Всё. У вас есть PVC. Вы прекрасны.Красивые уведомленияВ Контуре для рабочего общения мы используем Mattermost (кстати, есть эпичная статья, как мы туда переезжали). И, соответственно, там же находятся и каналы, в которые летят всякие алерты и оповещения.В TeamCity по дефолту можно отправлять уведомления только о статусе конкретной конфигурации. А из-за того, что гитлаб коммитоцентричен, у нас есть возможность отправлять такие красивые уведомления о состоянии всех тестов на конкретном коммите из коробки. Это гораздо более информативно и позволяет, например, быстро найти последний зелёный коммит, перейти в него и выкатить на прод.Неочевидное поведениеworkflow:rulesПо умолчанию пайплайны создаются на каждый пуш, MR, и тд. Для того чтобы управлять созданием пайплайнов, можно описать правила, когда они должны создаваться, а когда не должны в секции workflow:rules. Особо аккуратно надо описывать правила, когда вы хотите по каким-то причинам не создавать пайплайн. Расскажу про ситуацию, которая произошла у нас. Написали следующее правило:workflow:  rules:    - if:              # если      changes:         # в диффах коммита        - ""**/*.md""    # есть файлы с раширением .md      when: never      # никогда не создавай пайплайн    - when: always     # иначе создавай«Нет же никакого смысла запускать тесты на правки в документации. И список пайплайнов останется чище!» – думали мы.В начале всё работало хорошо, но спустя некоторое время начали происходить странности: иногда не создавались пайплайны, даже если в коммите и близко не было markdown-файлов. Сразу никто эти проблемы не связал с rules, потому что прошло время с добавления этих правил до первых симптомов. В ходе расследования стало ясно, что пайплайны не создаются при ребейзе «старых» веток. На сколько именно должна быть старой ветка, чтобы пайплайн не создался, выяснить не удалось.Хотелось увидеть в логах инстанса гитлаба конкретные сообщения о логике отказа от создания пайплайна, но, к сожалению, максимально допустимый уровень логирования для нашего инстансы GitLab не показывал такую детализацию.По итогу, перешерстив конфиг, нашли это самое правило, удалили, всё починилось.Да, в документации есть абзац про то, что changes иногда работают неочевидным образом.Кодировка файлов конфигурацииЕсли сохранить любой YAML-файла конфигурации в кодировке UTF-8 with BOM, то конфигурация не разваливается, как это бывает, когда например сделаешь очепятку в ключевом слове. А спокойно продолжает работать, но с интересными особенностями:Частично пропали переменные окружения, которые заданы для конфигурацииЕщё более странно, чем раньше, работала директива changes, о которой я уже говорилА из всех джоб, описанных в файле с битой кодировкой, отображалась только перваяПоменяли кодировку – всё заработало, как часы, включая директиву changes.Отмена джобы (починилось)Да, это поведение уже починили, но рассказать всё равно хочется :)Джобы не хотели корректно завершаться при ручной отмене через UI или даже при срабатывании таймаута. Оказалось, что процессы запущенные в секции script не завершаются после команды на отмену, хотя джоба и помечается для гитлаба, как отменённая\зафейленная.Почему это плохо:Такие джобы ещё какое-то время висят в k8s как поды и занимают ресурсыВ after_script некоторых джоб находится логика освобождения ресурсов, которая не срабатывалаПосле after_script выполняется стандартный процесс выгрузки артефактов и кэша из джобы в хранилище, который тоже не срабатывалКак чинили: написали bash код в after_script, который находил процесс основной секции script и прибивал его.В одном из обновлении куберовского раннера эту проблему пофиксили. Точно не работало на версии 17.0, на последней 17.8 точно работает (по крайней мере, при последнем тестировании у меня всё отменялось корректно).Утилиты в K8sПомимо конфигураций тестов и деплоя, что мы двинули в GitLab CI, у нас было множество утилитных конфигураций, которые тоже жили на TeamCity. Например, актуализация тестовых площадок, различные напоминалки в каналы ММ, десяток граберов по сайтам, которые отслеживают аналитики нашей команды, и прочее.Все эти утилиты мы теперь запускаем прямо на kubernetes в виде cronjob.Пара слов о том, как мы запускаем эти утилиты в кубере:Деплоим через GitLab CI с использованием шаблонов helm charts, написанных в нашей компании. Это позволяет описывать только самые необходимые свойства в конфигурации приложений в файле values.yaml и избавиться от бойлерплейт кода. Фактически в этих конфигах описано только получение исходников кода утилит и команд для их запуска. Исходники мы получаем простым клонированием\фетчем репы с кодом. Плюс такого подхода в том, что очередной запуск каждой из утилит происходит на последнем коммите мастера, без необходимости что-то отдельно деплоить.Что мы получили от переезда?Снижение затрат. Нет лицензий, меньше инфраструктурных затрат. При расчете использовали прайс 2024 года и получилось, что если бы мы использовали весь 2024 год GitLab, мы бы заплатили на 40% меньше, чем при использовании TeamCity.Не тратим время на поддержку виртуалок. Удалось выкинуть большую часть виртуальных машин, не тратим время на их поддержку. В том числе это помогло справляться с задачами без своего инженера-эксплуататора.Динамическое масштабирование. Есть возможность простого горизонтального масштабирования. Запросили дополнительные мощности и изменили одну цифру (параллельность) в настройках раннера. Не нужно создавать и настраивать дополнительные виртуалки. Просто и быстро.Эффективная утилизация ресурсов. Мощности в кластере не простаивают как виртуалки. Так как ими пользуются разные команды, нагрузка гораздо более равномерно распределена во времени, чем на ВМ. Ещё у нас в планах раскидать нашу нагрузку на кластер по времени, чтобы, например, несрочные джобы выполнялись ночью, а не занимали мощности днём.Одинаковое поведения на любой платформе. Тестовое окружение теперь предоставляет из себя docker-образ, что позволяет быстро поднять его где угодно – на кубере, на виртуалке или на локальной тачке разработчика – и быть уверенным, что приложения в нём будут работать одинаково.Кастомизируемость. GitLab – это ПО с открытым исходным кодом, то есть у нас есть все возможности произвольной настройки как UI, так и внутренних процессов платформы.Коммитоцентричность. Крайне удобная особенность работы с GitLab CI. Легко понять общий статус коммита. В отличии от TeamCity, где тебе нужно в каждую конфигу залезть, посмотреть, а точно ли это запуск с нужного коммита.Спасибо, что дочитали до конца! Оставляйте комментарии и делитесь идеями."
88,88,Правильный усилительный каскад на биполярном транзисторе,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2558.03,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-12,"В статье я простыми словами расскажу о схемотехнике однотранзисторного усилительного каскада с улучшенными характеристиками. Рассмотрим назначение каждого элемента схемы. Каскад можно использовать как входную часть усилителя мощности звуковой частоты или как самостоятельный предусилитель с батарейным питанием, например, встроенный в микрофон или в электрогитару. Это позволит улучшить соотношение сигнал/шум и упростит задачу оцифровки, так как по сигнальному кабелю будет передаваться сигнал большей амплитуды.   Вначале предупрежу негодование. Уровень статьи любительский. В настоящее время построение усилителей на отдельных транзисторах — это скорее техническое развлечение для энтузиастов или дань уважения прошлому. Если вам нужен стабильный и повторяемый результат с высокими характеристиками, лучше использовать схемотехнические решения на современных операционных усилителях. Описание упрощено, я постарался обойтись без формул. О базовых свойствах биполярных транзисторов я рассказывал в предыдущей статье и настоятельно рекомендую начать именно с неё.  ▍ Недостатки схемы с заземлённым эмиттером В книгах 80-х годов для начинающих радиолюбителей приводилась типичная простейшая схема усилителя на одном транзисторе, где эмиттер подключён к шине «земли», а смещение на базу поступает через резистор, номинал которого предполагалось подбирать под конкретный транзистор.   В то время была распространена практика применения положительной шины питания в качестве общего электрода («земли»). Это было связано с популярностью германиевых транзисторов, которые технологически проще изготавливались в p-n-p исполнении. Сегодня ситуация противоположна: «земля» почти всегда выбирается отрицательной, потому как биполярные кремниевые n-p-n транзисторы более энергоэффективны.   Вот как выглядела «детская» схема усилителя с включением по типу общего (заземлённого) эмиттера. На приведённом примере два каскада, но они одинаковы по конструкции. (Схема из книги Борисова В.Г. «Юный радиолюбитель», 1985 г.)  Такая схема хороша для понимания новичками, но имеет несколько очевидных недостатков. Первый — необходимость подбирать номинал резистора, через который подаётся ток для приоткрытия транзистора (так называемое смещение). Про назначение смещения я уже писал в предыдущей статье про эмиттерный повторитель. Вновь рекомендую с ней ознакомиться.  Из-за естественного несовершенства технологических процессов транзисторы имеют разброс параметров. Даже современные из одной партии отличаются друг от друга коэффициентом передачи тока, что существенно влияет на усилительные свойства. Это означает необходимость индивидуального подбора базового резистора для каждого транзистора, что представляет собой лишний этап производства. Но это только половина проблемы.  Коэффициент передачи тока зависит от напряжения питания, тока базы-эмиттер и температуры кристалла транзистора. Все эти условия приводят к нестабильности параметров каскада и ухудшают работу устройства, росту искажений звука.  Второй недостаток связан с нелинейными усилительными свойствами транзистора. Если на вход упрощённого каскада подать пилообразный сигнал, на выходе получится усиленный, но искажённый сигнал.   Обратите внимание: у осциллографов на схеме разный масштаб отображения. Это сделано для наглядности формы сигналов. На вход поступает сигнал 5,4 милливольта (среднеквадратичное значение или RMS), а на выходе 1916 милливольт. Коэффициент усиления получился около 355.   Проведём эксперимент и включим в цепь эмиттера резистор на порядок меньший, чем в цепи коллектора, 470 ом.   На выходе сигнал стал меньше — всего 56 милливольт при том же входном уровне; коэффициент усиления уменьшился до 10. Амплитуда снизилась, зато повысилась линейность: форма сигнала на выходе повторяет треугольник.   Если продолжать увеличивать сопротивление резистора в эмиттере, схема превратится в эмиттерный повторитель с коэффициентом усиления, близким к единице. Получается, что наличие эмиттерного резистора создаёт некий гибрид схемы с общим эмиттером и эмиттерного повторителя (такую схему называют ещё схемой с общим коллектором, хотя термин лично мне кажется менее понятным).  Третий недостаток усилителей с заземлённым эмиттером — это отсутствие температурной компенсации. Для нормальной работы усилительного каскада необходимо создание определённого тока базы, при котором рабочая точка (точка покоя) транзистора располагается примерно посередине диапазона от нуля до напряжения питания устройства. Если настроить рабочую точку при определённой температуре, при изменении температуры даже на десяток градусов эта точка сместится к одному из крайних значений, и усилитель начнёт искажать сигнал.  Биполярные транзисторы (особенно винтажные германиевые) имеют свойство увеличивать обратный ток p-n-переходов при повышении температуры. Эмиттерный резистор позволяет успешно бороться с этим явлением. Постараюсь объяснить это проще: при нагреве и росте обратного тока коллектора потенциал эмиттера повышается, поэтому ток базы уменьшается (по закону Ома). Этот процесс и обеспечивает отрицательную обратную связь по постоянному току, стабилизируя рабочую точку. Точно как это происходит в эмиттерном повторителе. Оптимальным считается соотношение номиналов резистора в эмиттере и резистора в коллекторе примерно 1 к 10.  ▍ Усовершенствованная схемотехника Мы рассмотрели, какие преимущества даёт эмиттерный резистор. Что ещё можно улучшить? Раньше смещение на базу подавалось через единственный резистор, ограничивающий ток и требующий индивидуального подбора. Используя простой делитель напряжения, можно зафиксировать потенциал базы на желаемом уровне. Рассмотрим каскад в режиме покоя, для наглядности я подготовил анимацию, отображающую силу тока в разных точках схемы:   В схеме используется питание 20 Вольт — не лучшее и не самое удобное напряжение. Такая схема приведена в книге «Искусство схемотехники» П. Хоровица и У. Хилла (1984 г, рус. изд., стр. 118). Рекомендую ознакомиться с оригиналом, там подробно описан принцип работы. Нам же достаточно понять основной смысл.  На входе установлены два резистора, средняя точка которых подключена к базе транзистора. Это простой делитель напряжения, номиналы которого выбраны так, чтобы в средней точке появился потенциал примерно 1,6 Вольт. Выбор оптимальных номиналов резисторов (не только их соотношения, но и абсолютных значений) важен для получения стабильности при разумном расходе энергии.  Например, можно было выбрать номиналы резисторов порядка сотен ом и получить бессмысленный расход заряда батареи, который будет рассеиваться в виде тепла, но получится «железобетонно» устойчивая точка потенциала. Если выбрать номиналы порядка сотен килоом, получится делитель с очень высоким внутренним сопротивлением, и потенциал средней точки будет легко просаживаться ненулевым входным сопротивлением транзистора. Аналоговая схемотехника — это всегда поиск оптимальных режимов в зависимости от задачи.   Осталось пропустить переменную составляющую сигнала обходным путём. Для этого параллельно эмиттерному резистору подключают конденсатор. Чтобы также обеспечить необходимую отрицательную обратную связь (ООС) для переменного сигнала, конденсатор подключают последовательно с сопротивлением меньшего номинала. Конденсатор выбирают таким образом, чтобы на нижней граничной частоте полосы пропускания его реактивное сопротивление было небольшим. То же правило действует и для межкаскадных разделительных конденсаторов.   Интерактивная модель схемы: tinyurl.com/232ve9xy.  ▍ Планы Я хочу ещё раз вернуться к неисчерпаемой теме биполярных транзисторов, потому что испытываю к ним иррациональное благоговение и некоторую радость от того, что мои многолетние попытки разобраться наконец-то дают результат. В последующей публикации хочу рассчитать транзисторный предусилитель с питанием от одного литий-ионного аккумулятора, собрать и испытать на доступных компонентах и, если получится, измерить её параметры. Но вначале необходимо собрать и испытать генератор эталонного сигнала. Покупать готовый не интересно, а сделать самодельный не так просто как казалось.  ▍ Напутствие Не отчаивайтесь, если столкнулись с затруднениями или не до конца разобрались в теме. Аналоговая схемотехника — сложная головоломка, вопросов возникает больше с каждым новым уровнем понимания. Это нормально для нашего мозга, который не был приспособлен эволюцией для решения абстрактных задач и осмысления процессов, скрытых от непосредственного наблюдения. Я изучаю тему много лет и многое не понимаю. Возможно, где-то существуют сверхразумы в недрах НИИ, которые видят картину во всех подробностях от атомарного уровня до сложных многоблочных схем. Но мы с вами — энтузиасты. Продолжайте тянуться к знаниям — это достойное занятие.  Если вы хотите более глубоко изучить основы полупроводниковых схем, рекомендую вам курс, который читает Григорьев Александр Алексеевич, доцент кафедры радиотехники МФТИ.  © 2025 ООО «МТ ФИНАНС»   Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
89,89,$440 000 000 на ветер за 45 минут: технические проблемы в алготрейдинге и как их [не?] предотвращают,Финам,Компания,4.45,"СМИ, Мобильные технологии, Веб-сервисы",2025-03-12,"Не так давно мы рассказывали о роли человеческого фактора в катастрофах на фондовом рынке: от fat-finger error и других ошибок ввода до перформансов, которые пьяные брокеры могут устраивать с деньгами клиентов. Однако сошедшие с ума алгоритмы биржевых ботов и не вовремя упавшие серверы могут доставить еще больше проблем.Из недавних случаев можно вспомнить сбой 3 июня 2024 года, который парализовал Нью-Йоркскую фондовую биржу (NYSE) почти на четыре часа, а акции Berkshire Hathaway, одной из компаний Баффетта, чуть не вылетели в трубу. Подобные инциденты случаются и в России: так, в ноябре все того же 2024 года Московская биржа на несколько часов задержала начало торгов из-за аппаратной ошибки в работе торгово-клиринговой системы.В этой статье мы попробуем разобраться, почему биржевые алгоритмы иногда слетают с катушек.Виды технических проблем с биржевыми алгоритмами на реальных примерахЧто вообще такое алгоритмический трейдинг? Само это понятие стало актуальным в 1998 году, когда Комиссия по ценным бумагам и биржам США (SEC) официально разрешила использовать для торговли электронные платформы. Формально речь шла о любом способе обработки данных по каким бы то ни было алгоритмам. Например, на тот момент трейдеры вручную разбивали крупные заявки на мелкие, но вскоре эти задачи и подобную рутину все чаще стали делегировать биржевым роботам. И сегодня алготрейдинг ассоциируется в основном с автоматизированной торговлей, когда сделки на фондовом рынке заключают роботы без участия человека.В одной из статей мы писали, что алготрейдинг опирается на технический анализ, парный трейдинг, работу со стаканом, арбитраж и торговлю волатильностью. Алгоритмы в реальном времени анализируют финансовые данные и мгновенно исполняют ордера на покупку или продажу активов. Также они могут работать в разных временных масштабах – от высокочастотной торговли (HFT), где сделки совершаются за миллисекунды, до долгосрочного позиционного трейдинга, в котором алгоритм удерживает активы неделями, а то и месяцами. Первыми, кто начал использовать программы на фондовом рынке были хедж-фонды (Citadel), инвестиционные банки (Goldman Sachs) и другие игроки, например Renaissance Technologies. Однако сейчас алготрейдинг стал доступен и частным лицам, которые стали подключаться к биржам с помощью API-ключей. Согласно данным Quantified Strategies, уже в конце прошлого десятилетия в зависимости от методологии подсчета от 60% до 73% всех сделок на фондовом рынке США осуществлялась с помощью алготрейдинга. Хотя его преимущества очевидны (минимизация человеческого фактора, возможность торговать круглосуточно на огромных скоростях и т.д.), на практике он несет определенные риски. Малейшая ошибка в работе бота на любом этапе (от сбора данных до исполнения ордеров) может как разорить конкретного трейдера, так и спровоцировать обвал рынка. В мировом масштабе эти аварии (пока) не приводят к экономическим кризисам, поскольку торговля продолжает вестись на других крупных площадках. Но для брокера, растерявшего клиентов из-за того, что они после программного сбоя ушли к конкурентам, это слабое утешение. И хотя распределенность и дерегулирование современной биржевой торговли страхует рынки от обрушения, ее сложность имеет и обратную сторону медали.Так, возросшая фрагментация и растущая доля HFT, при которой компьютерные системы совершают тысячи транзакций почти мгновенно, предъявляют все большие требования к трейдерскому железу и софту. А компании, которые сокращают расходы на его поддержку и модернизацию, подвергают себя риску пополнить ряды героев этой публикации.Попробуем же составить примерную классификацию ошибок алготрейдинга, проиллюстрировав некоторые из них теми или иными биржевыми катастрофами последних десятилетий. Сразу оговоримся, что часто происшествия на бирже – это «черные ящики», поскольку их владельцы в заявлениях для прессы не любят вдаваться в детали своих оплошностей, а судьи и финансовые регуляторы могут годами устанавливать виноватых без гарантии успеха. 1. Ошибки софтаЕсли в софт, использующийся на фондовом рынке, закрадываются логические ошибки на уровне кода, то в один прекрасный день алгоритмы могут неприятно удивить трейдеров. Ярчайший пример: компания Knight Capital, которая в 2012 году, по данным BBC, потеряла $440 млн всего за 45 минут. После обновления торговой системы фрагменты старого кода продолжили работать, массово открывая убыточные позиции без возможности их корректировки, что привело к мгновенному банкротству компании. Американский писатель и предприниматель Эдгар Перез даже написал об этих событиях книгу Knightmare on Wall Street. Другой пример – Нью-Йоркская фондовая биржа. Так, согласно Guardian, 8 июня 2015 года после обновления ПО отвалилась возможность корректно обрабатывать ордера, и торги были остановлены.Эти случаи наглядно продемонстрировали, что перед обновлением систем нужно не лениться проводить их многослойное тестирование и не забывать о механизмах аварийной остановки алгоритмов, если что-то все-таки пойдет не так. Например, важно помнить об ошибке неучтенного нулевого значения (ZeroDivisionError) при программировании на Python, которая может выстрелить, если робот попытается выполнить операцию деления с данными, в которых по каким-то причинам стоимость купли-продажи ценных бумаг отсутствует, то есть равна нулю.Другой вариант ошибок софта — когда багов как таковых нет, но создатели не заложили в свои алгоритмы умение учитывать риски, а то и просто где-то просчитались и предписали программам торговать по неоптимальным стратегиям. Как пример тут можно привести историю 2012 года с падением на 8,2% за первые полчаса торгов акций JPMorgan Chase, после сообщения банком о потерях от трейдинга, превышающих $2 млрд.Такие случаи учат как важности лучше валидизировать алгоритмы перед их применением в полевых условиях, так и не забывать о стоп-лоссах, ограничениях на потери и использовании диверсифицированных стратегий для управления рисками. Ну и не экономить на услугах Quantitative Analyst, которые всем этим занимаются.Важно избегать и ошибки переоптимизации (также известной как Curve Fitting), когда алгоритм слишком точно настроен на торговлю в конкретных рыночных ситуациях – либо по историческим данным, либо просто с использованием слишком большого числа параметров – и перестает справляться с торговлей в реальном времени. Кроме того, алгоритм не должен игнорировать ликвидность рынка и проскальзывание. На сайте платформы Intrinio трейдерам рекомендуется перед запуском бота анализировать глубину рынка и реалистично оценивать размер ордеров и ликвидность активов, которыми они торгуют.2. Ошибки данныхНеобходимость непрерывно получать актуальную и точную информацию для принятия торговых решений делает алготрейдинг чувствительным к проблемным данным. Некорректные или устаревшие данные могут привести к убыточным сделкам, а то и спровоцировать обрушение рынка. Именно это случилось с Flash Crash 2010 года, когда индекс Dow Jones упал на 600 пунктов за несколько минут. Тогда фальшивые заявки, созданные программой для спуффинга Навиндера Сарао, так впечатлили биржевых роботов, что спустили лавину, обошедшуюся капитализации рынка почти в $1 трлн.Сюда же можно отнести и ошибки исполнения ордеров: некорректная логика их выставления может запустить цепочку бесконтрольных сделок, что и произошло в момент катастрофического IPO компании BATS Global Markets. До сих пор неизвестно, как именно это произошло (конспирологические варианты есть здесь), но 23 марта 2012 года запущенный с терминала неизвестного злоумышленника HFT-алгоритм изменил котировки с $16 на несколько центов, что привело к еще одной «Хиросиме» на фондовом рынке. А позже BATS и вовсе была оштрафована за то, что из-за непонятной ошибки сотни тысяч сделок на бирже проводились по ценам, отличным от национальных. Защититься на 100% от таких ситуаций нельзя, однако участникам торгов явно стоит больше уделять внимание проверке ордеров перед их отправкой на рынок, автоматической регулировке ордеров в случае нестабильности цен, всегда иметь доступ к резервным источникам финансовых данных для сверки и внедрять алгоритмы, анализирующие их на предмет аномалий.Отдельно отметим, что для алгоритмов на нейросетевых технологиях корректные и валидные данные важны как воздух, иначе, формально работая правильно, они могут научиться разорительным стратегиям, которые даже самый невнимательный или неопытный трейдер никогда не принял бы всерьез. Чтобы этого не происходило, модели нужно регулярно тестировать на переобучение и натаскивать алгоритмы ориентироваться в нестандартных ситуациях.3. Ошибки инфраструктурыОтсутствие багов в биржевом софте и «троянских коней» в виде сомнительных ордеров не всегда спасает от катастроф, если накрывается подстанция, от которой запитано здание биржи или кто-то сэкономил на модернизации серверов, и они падают под наплывом инвесторов.В качестве примера здесь можно привести сбой на Лондонской фондовой бирже (LSE) 8 сентября 2008 года, когда торги были остановлены на несколько часов. Хотя подробности произошедшего не разглашались, принято считать, что виной стали технические неполадки, из-за которых некоторые игроки оказались не в состоянии подключиться к бирже. Похожая история произошла в 2013-м году. NASDAQ приостановила торги акциями Apple и других крупных компаний из-за перегрузки процессоров системы, устанавливающей цены и котировки на торговые инструменты. Паника на рынке случилась запоминающаяся.Защитные меры здесь достаточно очевидны: использовать резервные серверы, облачные решения, иметь стабильное соединение с сетью и запасного провайдера, регулярно проводить аудит состояния всей физической инфраструктуры, включая контуры электроснабжения, системы охлаждения и пожарной безопасности.Сюда же можно отнести и инциденты, когда сервера не горят, но сбои в работе алгоритмов все равно происходят – из-за изменений API, превышении лимитов запросов или неверном интерпретирование сообщений. Чтобы избегать таких ситуаций, инженерам бирж следует постоянно мониторить изменения в API биржи, прогнозировать пиковые нагрузки и закладываться на них, чтобы системы в моменты «исторических событий» могли увеличивать лимиты запросов.Трейдерам же, использующим биржевых роботов, нужно закладывать в свои алгоритмы умение снижать активность торговли до реалистичной при резком увеличении волатильности рынка, чтобы не нарваться на лимиты биржи, о чем подробно рассказывается на сайте брокера Real Trading. В противном случае может произойти ошибка, известная как overtrading: исчерпав допустимое количество заявок в единицу времени, бот может не успеть продать закупленное, в результате чего трейдер потеряет деньги.Предупрежден – значит вооруженСтатистика показывает, что более 60% компаний сталкиваются с прямыми финансовыми потерями из-за сбоев в работе информационных систем. Согласно данным центра экспертизы в сфере телекома «ИКС-МЕДИА», в 2024 году общее количество таких сбоев в российских предприятиях выросло на 22% по сравнению с предыдущим годом. Основными причинами являются использование устаревшего оборудования и программного обеспечения без должной поддержки и обновлений. Это легко можно экстраполировать и на мир фондового рынка.Примеры, рассмотренные нами сегодня, подчеркивают важность надежности и устойчивости компьютерных систем в финансовом секторе. Сбои могут приводить к значительным финансовым потерям, подрыву доверия инвесторов и дестабилизации фондового рынка. О том, какую роль во всем этом играет алготорговля, существую уже даже академические работы на русском. Ошибки в алгоритмическом трейдинге неизбежны, однако, как мы в «Финаме» уже некогда писали, их можно минимизировать с помощью тщательной разработки и управления алгоритмами. Компании, игнорирующие эти риски, могут столкнуться с огромными убытками и потерей репутации на рынке. Также одним из важнейших факторов успеха в алготрейдинге является выбор надежной торговой площадки. Но это уже другая история."
90,90,Как сэкономить время и силы с помощью продуктовых и технических требований,Uzum,Строим экосистему цифровых сервисов в Узбекистане,84.4,"Электронная коммерция, Веб-сервисы",2025-03-12,"В прошлой моей статье мы разобрались, как сделать так, чтобы ожидания менеджера продукта совпали с реальностью, а в этой статье обсудим, как сделать так, чтобы задача вышла в продакшен раньше, чем она потеряла смысла. В современном конкурентном бизнесе побеждает и захватывает новую нишу тот, кто первый сделает классные доработки и захватывает этим внимание пользователя. Мы должны очень быстро бежать, чтобы успевать за конкурентами. С другой стороны, мы должны выдавать качественный продукт, а иначе в нем нет никакого смысла. В этой статье расскажу, какой инструмент мы нащупали, чтобы ускориться и повысить качество. А еще, конечно, снизить стресс команды разработки. Меньше стресса — лучше результаты.При составлении требований к задаче очень важно понимать, для кого мы их пишем и зачем. Для себя мы обозначили следующее:Четкие и однозначные требования от бизнеса нужны команде:чтобы мы реализовали то, о чем договорились. Требования могут меняться во время и после груминга, это нормально. Важно, чтобы все договоренности были зафиксированы.чтобы команда понимала, какую проблему пользователя мы решаем. Это необходимо для того, чтобы проверить, точно ли проблема пользователя решена.Product-менеджер описывает требования в классическом стиле. Они обязательно содержат:Предыстория. Откуда появилась проблема и в чем ее суть.Цель. Что мы ожидаем в результате реализации задачи.Критерии приемки. Этот пункт самый важный. Описываем от лица пользователя желаемое поведение при каждом шаге. На эти критерии будем опираться при реализации, по ним Product-менеджер будет принимать результат. Это обязательно должны быть понятные и простые формулировки. Должно быть описано каждое нажатие на кнопку, каждый переход на новый экран.Эксперимент или нет. Будем запускать это в рамках эксперимента или полностью меняем текущую функциональность.Аналитика. Нужны ли аналитические события на действия пользователя.Пример хорошо описанных требований:Далее идет груминг и перевод продуктовых требований в технические. Можно описывать их с разной степенью детализации, для себя мы выбрали следующий подход: описываем технические требования на уровне логики, а глубокую архитектуру оставляем на реализацию разработчикам. Например, мы описываем, что в результате выполнения задачи у нас должны быть добавлены два новых поля в базу данных и в три существующих запроса, названия которых перечисляем. Далее описываем логику, по которой должны новые поля заполняться. Но названия переменных и полей, а также способ хранения и таблицу мы не перечисляем, это все остается на усмотрение разработчика. По результатам выполнения задачи разработчик должен приложить эту информацию.Задача на технические требования всегда содержит в себе следующие разделы:Проблема. Описываем, зачем мы делаем эту задачу.Необходимо. Перечисляем всю логику, которую необходимо реализовать. Должно быть описано:поведение каждой кнопки;ссылка на каждую гиперссылку;способ и логика перехода на следующий экран;изменение местоположения каждого элемента;дополнительная важная информация.Для тестирования. Опциональный пункт, в котором содержатся важные corner-кейсы или нюансы при тестировании, на которые хочется обратить внимание.Локализация. Нужен ли перевод на другие языки (специфика нашего продукта).Эксперимент. Будем ли мы запускать задачу в рамках feature flag или нет.Аналитика. Нужна ли нам продуктовая аналитика о действиях пользователя в рамках этой функциональности.После написания технической задачи важно пройтись по критериям бизнес-задачи и убедиться, что ничто не забыто и написанное не противоречит друг другу.Пример технической задачи:Все изменения договоренностей важно фиксировать в задаче, иначе она быстро потеряет актуальность и не будет единым источником истины. О том, кто будет актуализировать договоренности, можно условиться заранее, а можно ситуативно, то есть сразу же после обсуждения.Что помогла улучшить документация:Product-менеджер заранее формулирует четкие требования и согласовывает их с командой. Не остается непонятных мест и разночтений. При описании критериев менеджер продукта может внимательнее продумать все детали задачи.Благодаря хорошему описанию технических и бизнес-требований разработчику не приходится тратить время на вспоминание того, что мы когда-то обсуждали на груминге и на поиск контекста перед началом и во время реализации задачи. Экономим время на дополнительных созвонах команды, встречах с заказчиком и обсуждении одного и того же несколько раз.QA-специалист при тестировании не тратит время на выяснение того, как это должно быть, так как требования едины для менеджера продукта, разработчика и QA.У нас меньше исправлений багов из-за того, что требования зафиксированы заранее. Не возникает ошибок из-за того, что кто-то забыл о договоренности.Любому члену команды легко вернуться к предыдущей задаче и доработать ее. Мы не завязаны на разработчика и тестировщика, у которых требования хранятся в голове. Каждый может вникнуть в задачу, перечитав ее описание."
91,91,Работа с RISC-V контроллерами на примере GD32VF103 и CH32V303. Часть 6. Дробные числа,YADRO,Тут про железо и инженерную культуру,717.94,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-03-12," Одно из основных предназначений микроконтроллера — это получение информации извне, ее обработка и выдача реакции. Причем зачастую эта информация представлена не в цифрах, а в терминах реального мира: 3 сантиметра, 101 килопаскаль, 3.6 вольта. Мало того, что информацию надо получить, ее зачастую надо потом отобразить человеку. Вот только подобные аналоговые величины плохо ложатся на целочисленные переменные, с которыми так хорошо работает контроллер. О том, как дробные числа можно закодировать и какие при этом встречаются подводные камни, сегодня и поговорим. Часть 1. Введение Часть 2. Память и UART Часть 3. Прерывания Часть 4. Си и таймеры Часть 5. DMA 10.1 IEEE754 Начнем с классического для «компьютерного» программирования решения — тип данных float (а также double и подобные). Наиболее распространенный сегодня формат представления дробных чисел — это IEEE754. Согласно ему, число представляется как мантисса (значащие цифры), порядок и знак. То есть это классическая экспоненциальная форма записи числа. Например, в десятичном числе 1.23·10⁸ мантисса это 1.23, а порядок — 8. Точно так же это выглядит и с двоичными числами: у 1.001011·2¹⁰¹⁰ число 1.001011 — это мантисса, а 1010 — порядок. В экспоненциальной форме мантисса всегда записывается как одна значащая (ненулевая) цифра перед точкой и некоторое количество цифр после. И при использовании двоичной системы счисления это оказывается крайне удобно, ведь там единственная цифра, кроме нуля — единица. Отсюда возникает первая особенность IEEE754: раз первая цифра мантиссы всегда равна 1, ее можно не хранить. Вторая особенность: для представления отрицательных чисел используется не дополнительный код мантиссы, а отдельный знаковый бит. Вероятно, это связано с тем, что сложение и вычитание всегда требуют выравнивания порядков, то есть сдвигов мантиссы влево-вправо. Да еще подразумевается единица до точки. Все выгоды от дополнительного кода пропадают. От общих соображений углубимся немного в конкретику. IEEE754 регламентирует размер каждого поля, причем в нескольких вариантах. Первый вариант используется для 32-битного представления: под мантиссу отводится 23 бита (с 0 по 22), под порядок 8 бит (23–30), под знак — один (31-й). Для 64-битного представления размеры побольше: 52 под мантиссу, 11 под порядок, 1 под знак. Есть и 128-битный формат, но его мы рассматривать не будем. Как, впрочем, и 64-битный. Ну и третья особенность данного формата: хранение порядка увеличенным на 127. То есть если в поле порядка хранится число 200, то сам порядок равен (200 – 127). Вероятно, это сделано, чтобы, если записать во все биты числа нули, порядок получился минимально возможным, –127. Причем само число при этом оказывается даже не 1.0·2⁻¹²⁷, как можно было подумать, а еще меньше. И это четвертая особенность. Если порядок равен –127 (в поле порядка записан ноль), число считается денормализованным. То есть вместо неявной единицы, про которую мы говорили у мантиссы, там предполагается неявный ноль. Таким образом, число, состоящее из всех нулей, не просто минимально возможное, а строго равно нулю. Кстати, это еще одна причина использовать такой странный формат отрицательных порядков: на аппаратном уровне проверить, все ли там ноли, крайне просто (хотя проверить на 0b10000000 было бы сложнее всего на один инвертор...). Наконец, пятая особенность: не все битовые комбинации, которые можно записать в число, являются корректными числами. Некоторые из них обозначают специальные значения — бесконечности, не-числа, ошибки. Эти значения могут возникать, скажем, при делении на ноль, извлечении корней и т.п. Для дополнительной информации рекомендую ознакомиться с соответствующими лекциями на uneex (2022, 2024). Некоторую сложность в ручных операциях с дробными числами может представлять то, что переводить из двоичной системы целые числа умеют почти все, а вот к дробным мы не привыкли. И даже большинство калькуляторов не привыкло. Принцип там, разумеется, тот же, что и в любой другой позиционной системе. Рассмотрим число 1010.1101₂:    2³ 2² 2¹ 2⁰  2⁻¹ 2⁻² 2⁻³ 2⁻⁴     1 0 1 0 . 1 1 0 1    Можно, конечно, умножать побитово, но оперировать отрицательными степенями двойки опять же неудобно. Поэтому сначала умножим все число на 2⁴, чтобы оно стало целым, а потом поделим на 2⁴ обратно:    2⁷ 2⁶ 2⁵ 2⁴ 2³ 2² 2¹ 2⁰     1 0 1 0 1 1 0 1    Перевести двоичное число 10101101₂ в десятичное сумеет любой калькулятор: 173. Множитель 2⁴ также вычисляется легко: 16. Вот и получается, что наше исходное дробное число равно 173 / 16 = 10.8125. По этому принципу мы и будем переводить в десятичный формат мантиссу. В ней один бит (равный 1) до точки и 23 бита после. Поэтому записываем биты, как будто это целое число, переводим в десятичный формат и делим на 2²³. В качестве примера рассмотрим вот такое число: 11000011100111010001010001100011. S [  E   ] [         M           ] 1 10000111 00111010001010001100011  S - sign, знак E - exponent, порядок M - mantissa, мантисса Знак равен 1, то есть число отрицательное. Порядок равен 10000111₂ = 135₁₀. Вычитаем 127, получаем 8. Мантисса равна (1.)00111010001010001100011, или, в десятичном формате, 100111010001010001100011₂ / 2²³ = 1.22718465328. Умножаем на порядок (2⁸ = 256), не забываем добавить знак и получаем -314.15927124. Осталось проверить правильно ли проведен расчет: int main(){   union{     uint32_t u;     float f;   }val;   val.u = 0b11000011100111010001010001100011;   printf(""%f\n"", val.f); } $ gcc main.c $ ./a.out  -314.159271 Все верно! 10.2 Аппаратный модуль FPU В контроллере CH32V303 работа с дробными числами одинарной точности (32 бита) реализована аппаратно. Об этом говорит буква f в списке расширений imafc. Напоминаю, что другой наш контроллер, GD32VF103, имеет список расширений imac, то есть аппаратно дробных чисел не поддерживает. Вообще, работа с FPU в RISC-V реализована несколько странно, добавлением практически автономного блока (сопроцессора) с собственными регистрами. Зачем это сделано и чем не устроило использование обычных регистров, я достоверно сказать не могу. Возможно, ради совместимости с D, Q и подобными расширениями (64-, 128-битные дробные числа). Это ведь только 32-битные float-ы помещаются в один регистр, а 64-битные уже нет. Впрочем, существуют и экзотические расширения Zfinx (float in X), Zdinx (double in X), Zhinx (half in X), в которых дробные числа хранятся как раз в обычных целочисленных регистрах. Двойная точность там обеспечивается регистровой парой. Но это уже сильный расход регистров, да и вообще не поддерживается нашим контроллером. В нашем же случае вместе с модулем FPU добавляется 32 специальных регистра f0–f31. Как и обычные, они разделены на временные (ft0–ft11), сохраняемые (fs0–fs11) и аргументы функций (fa0–fa7). Конвенции по сохранению при использовании в функциях такие же, как для обычных регистров. Но надо помнить, что с ними умеет работать только сопроцессор, а не основное ядро. Поэтому все операции с f-регистрами пройдут только через специальные FPU-инструкции. Любой расчет на FPU начинается с загрузки в f-регистр значения либо из обычного регистра (команда вроде fcvt.s.wu fa5, a5), либо из памяти (например flw fa0, 12(s3)), проведения с ним каких-то операций и выгрузки обратно (fcvt.w.s a0, fa5 / fsw fa0, 12(s3)). Обратите внимание на суффиксы у команды fcvt. Она универсальна и умеет преобразовывать f32, f64,… в u32, i32, u64,… и обратно. Собственно .w, .s, .l, .d отвечают именно за это. В нашем случае, когда поддерживаются только 32-битные целые (.w / .wu) и только 32-битные дробные (.s), набор суффиксов оказывается небольшим. Еще fcvt умеет округлять значение вверх (к +∞), вниз (к –∞), к нулю и от нуля. За это отвечает третий, опциональный, аргумент. Например, fcvt.w.s a0, fa5, rtz говорит «взять float значение из fa5, округлить до ближайшего целого в сторону нуля (round towards zero) и сохранить в int32_t регистр a0». Впрочем, слабо представляю для чего выбор округления может пригодиться в повседневном программировании. Но если вдруг понадобится — вот он. Кстати, округление можно настроить не только для каждой команды индивидуально, но и для всех сразу, за это отвечает CSR-регистр fcsr. Подробно рассматривать команды работы с данным модулем смысла не вижу. Если кому-нибудь все же интересно, их можно найти в документации на ядро RISC-V или в тех же лекциях на uneex. Дело в том, что если уж программа достаточно сложна, чтобы потребовалась работа с дробными числами, писать ее, скорее всего, будут не на ассемблере, а как минимум на Си. Особенности и ограничения придется знать в любом случае. Самое банальное: компилятор будет вынужден сохранить все f-регистры, если вы используете дробные числа в прерывании. Или если из прерывания вызывается другая функция (компилятор ведь может не знать, вдруг дробные числа используются где-то в ней). Сохранение 32 лишних регистров никак не прибавляет скорости обработки. А вот со второй особенностью будет лучше ознакомиться на примере кода:   uint32_t t_prev = systick_read32();    volatile float x = 1.1;   volatile float res = 0;   for(int i=0; i<9; i++)res += x;    uint32_t t_cur = systick_read32();    UART_puts(USART, ""Float:"");   uart_fpi32(res*100000000, 8);   UART_puts(USART, ""\r\nt="");   uart_fpi32( t_cur - t_prev, 0 );   UART_puts(USART, ""\r\n""); Здесь uart_fpi32 — всего лишь функция вывода на UART числа с фиксированной точкой. Что это такое — чуть ниже. Что иллюстрирует пример? Первое — время выполнения кода, 81 такт. И второе — результат сложения, не 9.9 ровно, а 9.90000064. Это обусловлено тем, что числа-то мы задаем в десятичной системе, а хранятся они в двоичной, причем для хранения отведено всего 23 бита (ну хорошо, 24), что соответствует приблизительно 7 десятичным разрядам. Причем стоит помнить, что эти 7 разрядов достижимы разве что для идеальных условий. При выполнении математических операций точность будет каждый раз снижаться, так что в реальности доверять более чем 3–5 разрядам уже нельзя. Причем уточню: речь идет не о знаках после точки, а именно о 3–5 значимых цифрах. Также из этого следует, что проверять дробные числа на строгое равенство нельзя почти никогда. То есть следующий код будет работать некорректно:   for(float x = 0; x != 10; x+=0.1){...} Ближайшими значениями являются не 9.9 и 10.0, а 9.90000128 и 10.00000192. 10.3 Программная реализация А что же делать с GD32VF103, в котором модуля FPU нет? Использовать программную реализацию. К счастью, тип float входит в стандарт языка Си, то есть будет поддерживаться компилятором в любом случае. Но не все так просто. Если мы только изменим в makefile тип ядра на imac, компилятор нас обругает. Дело в том, что реализация работы с дробными числами компилятора gcc находится в отдельной библиотеке libgcc.a, причем отдельно для каждого подтипа ядер (по крайней мере, в risc-v gcc в Debian так). И что еще веселее, хотя этот подтип мы явно указываем, компилятор не желает его учитывать. Но если ему подсказать «ищи в /usr/lib/gcc/riscv64-unknown-elf/12.2.0/rv32imac/ilp32/ библиотеку gcc», он ее подставит. Вот только писать точную версию 12.2.0 прямо в makefile как-то неприлично. Вдруг выйдет новая. Поэтому для себя на Debian пришлось написать вот такой костыль: GCCVER=`$(CC) --version | sed -n ""1s/.* \([0-9]*[.][0-9]*[.][0-9]*\).*/\1/p""` GCCPATH = -L/usr/lib/gcc/riscv64-unknown-elf/$(GCCVER)/$(ARCH_$(MCU))/ilp32/ ... LDFLAGS += $(GCCPATH) -lgcc Тот же самый код на том же самом контроллере CH32V303, но с настройками imac (как будто FPU у нас нет) выдает в качестве результата суммирования те же 9.90000064 (что хорошо: поведение программной и аппаратной реализаций совпадают), но вот время выполнения возрастает аж до 789 тактов — почти в 10 раз! В некоторых дистрибутивах поддержку 32-разрядных float-ов не завезли. Правильным решением было бы пинать мейнтейнеров, чтобы поправили, но можно взять библиотеку из дистрибутива, в котором поддержка есть. Вот, например, версии из моего Debian: для ядер rv32imac и rv32imafc. И разумеется, никто не запрещает переписать соответствующие функции самостоятельно — это замечательная практика по внутреннему устройству float-ов. А еще после такой практики надолго отпадет желание использовать их где попало. 10.4 Числа с фиксированной точкой Понятно, что использование чисел с плавающей точкой в контроллерах без FPU достаточно накладно. Но ведь и работают контроллеры не в сферическом вакууме, а с реальными значениями из реального мира. И диапазон этих значений вполне предсказуем. Например, температура для бытовых условий может меняться где-то от –50 до +150 градусов. Ну хорошо, у нас, знакомых с паяльником, аж до +350–400, причем точность выше одной десятой нужна крайне редко. Тут нет нужды использовать разделение на мантиссу и порядок, достаточно просто считать не в единицах градусов, а в десятых долях. Или в сотых, или в тысячных. А при выводе на дисплей просто поставить в нужном месте десятичный разделитель. То есть температура 36.6 градуса может храниться как 366 дециградусов или 36600 миллиградусов. А это уже целые числа, работа с которыми нам хорошо знакома и не представляет никаких сложностей. Такое представление называется числами с фиксированной точкой. Давайте перепишем наш предыдущий код под работу с ними:   t_prev = systick_read32();    volatile uint32_t y = 110000000; //1.1 * 10⁸   volatile uint32_t ires = 0;   for(int i=0; i<9; i++)ires += y;    t_cur = systick_read32();    UART_puts(USART, ""Fixed-point:"");   uart_fpi32(ires, 8);   UART_puts(USART, ""\r\nt="");   uart_fpi32( t_cur - t_prev, 0 );   UART_puts(USART, ""\r\n""); Результат расчета — 9.90000000, время 73 такта. Мы выиграли и по точности, и по быстродействию. Причем не только у программной реализации FPU, но и у аппаратной! Но, разумеется, не все так радужно. Диапазон целых чисел все-таки ограничен, для 32-битных он составляет всего ±2·10⁹. Сравните с float, где диапазон 10³⁸. То есть сверхмалые и сверхбольшие числа таким способом не обработать. Но, повторяю, в микроконтроллерах диапазон чисел почти всегда известен заранее. И вот теперь, когда мы рассмотрели, что такое числа с фиксированной точкой, можно чуть подробнее описать принцип работы uart_fpi32(val, dot). По сути, это всего лишь преобразование целого числа в строку, размещение после dot символов (считая справа) десятичной точки и вывод полученной строки в UART. Ее код настолько прост, что несколько раз мне было лень искать предыдущую реализацию и я писал ее с нуля. Самое сложное в ней (в том смысле, что все остальное еще проще) — добавить нули между концом числа и точкой, если выводится число вроде 0.00123. Исходный код примера доступен на github. Внимание: для сборки с аппаратной поддержкой дробных чисел используется makefile_hw.mk, а с программной — makefile_sw.mk Из любопытства я проверил и другие операции: выполнил каждую 10000 раз в цикле и вычислил среднее количество тактов на одну операцию.    Операция fixed-point Hardware FPU Software float SW/HW     Сложение 1.70 2.65 63.58 24.0   Умножение 1.70 2.65 80.86 30.5   Деление 10.70 10.65 84.89 8.0   sqrtf - 11.65 264.08 22.7   sinf - 1518.75 17876.18 11.7    10.5 Не только внутри контроллеров Как ни странно, числа с фиксированной точкой применяются не только в микроконтроллерах с их ограниченными ресурсами. Так, в линуксе есть вот такой файл с интересным содержимым: $ cat /sys/class/thermal/thermal_zone0/temp  44000 Это температура процессора компьютера, равная в моем случае 44 градусам. Как видите, те же самые числа с фиксированной точкой применяются даже на полноценных компьютерах, где с float-ами уж точно никаких проблем нет. Размер дробной части не обязательно задавать в десятичном формате. Рассмотрим такую распространенную микросхему, как DS18B20. Это цифровой термометр с диапазоном до 125 градусов и разрешением до 12 бит. А примечателен он в данном случае тем, что значения выдает именно в формате с фиксированной точкой: 8 старших бит — целая часть, 4 младших — дробная, слева дополняется знаковым битом до двухбайтной величины. Отрицательные значения представлены в дополнительном коде. То есть в десятичном формате достаточно разделить двоичный результат на 2⁴. Рассмотрим пару примеров декодирования из его документации: 0x00A2 -> 162₁₀ / 2⁴ = 10.125 градуса 0xFE6F -> -401₁₀ / 2⁴ = -25.0625 градуса Для отображения температуры человеку выдачу с фиксированной точкой в двоичной системе стоит перевести в формат фиксированной точки в десятичной: умножить на 10 в нужной степени и поделить на 2 в степени, соответствующей исходному формату (в нашем случае 4). Допустим, нам достаточно одного знака после точки: 0x00A2 * 10¹ / 2⁴ = 162 * 10 / 16 = 101.25. Дробная часть отбрасывается, после первого разряда выставляется точка, и получается искомое 10.1. Или 0xFE6F * 10² / 2⁴ = –2506.25 -> –25.06. 10.6 Хранение При работе с величинами из реального мира стоит обсудить и вопрос длительного хранения. Я имею в виду уже не внутреннее представление, а то, в котором оно передается во внешний мир и показывается пользователю. И это различие существенно! Потому что очень велик соблазн выдавать сразу сырые данные, скажем, с АЦП или датчика, или что-то в числах с фиксированной точкой. Так делать не надо. Через какое-то время вы попросту забудете, за что эти величины отвечают и как их перевести во что-то осмысленное. Поэтому для любого общения с внешним миром лучше всего использовать числа в стандартной системе Си. Если уж масса, то в килограммах (даже если получится 9.1093837·10⁻³¹), если напряжение, то в вольтах, если температура, то в градусах, если расстояние то в метрах. Чтобы лет через пять не вспоминать, что где-то величина выводилась в десятках миллиметров, а где-то — в килоомах. Если помимо чисел можно вывести подсказку, это совсем замечательно: можно указать там формат вывода (в каком столбце какая величина) и единицы измерения. Но, как обычно, реальный мир все-таки отличается от идеального, и возможности вывода (или даже расчета) дробных чисел может физически не быть, или их использование неоправданно усложнит устройство. Тогда действительно приходится выводить как получится, а подробности описывать в документации. С этим мы уже познакомились на примере датчика температуры и будем сталкиваться во множестве другой периферии. Впрочем, это лишь рекомендация по человекочитаемому выводу. 10.7 Табличные функции Нередко возникают задачи, связанные с вычислением математически тяжелых функций. Возьмем хотя бы синус. Лобовое решение float x = sin(y); слишком часто оказывается неэффективным (да вы видели, почти 18 тысяч тактов на один вызов!). Вместо этого можно воспользоваться тем, что у нас довольно много флеш-памяти, и разместить в ней заранее рассчитанную таблицу значений. Причем значения не обязательно должны быть float-ами. Тот же синус удобнее считать не в радианах, а в долях от 8-битного числа. То есть 0 это 0 радиан, 128 — это π, а 256 — 2π. И значения синуса пусть меняются не от –1 до +1, а от 0 до 255 или от –127 до +127. Примерно так я рассчитывал, например, матрицы трехмерного преобразования в RARS, и такая же таблица используется в примере ниже. Она занимает всего 256 байт, а на ее использование потом тратятся считаные такты. Если вспомнить законы тригонометрии, размер таблицы можно сократить в два, в четыре раза и даже больше. Но это усложнит последующее использование, так что придется искать баланс между точностью, занимаемой памятью и скоростью. 10.8 Цифровой синтез сигналов, DDS Поговорим о генерации синусоид. Допустим, мы хотим синтезировать звуковой сигнал при помощи ШИМ. Максимальная частота таймера равна тактовой частоте контроллера, по умолчанию 8 МГц. При 8-битном ШИМ его частота составит 31250 Гц. Но ведь нам нужен не меандр, а синусоида. То есть надо последовательно вывести все 256 значений из нашей таблицы. Максимальная частота составит уже 122 Гц. Как-то маловато... Но ведь никто нас не заставляет непременно использовать все отсчеты. Скажем, если нам нужна частота 244 Гц, можно выводить каждое второе значение из таблицы, если 488 — каждое четвертое и так далее. Если желаемая частота не делится на наши 122 Гц нацело, код становится несколько более сложным. Интереса ради я набросал, как он может выглядеть: volatile uint32_t dpos = (1LLU<<32) * 1000 / (144000000 / 256); //                           │          │        │         └─── разрядность ШИМ //                           │          │        └───────────── Частота тактирования таймера //                           │          └────────────────────── Выходная частота //                           └───────────────────────────────── Размер переменной счетчика (32 бита)  const int8_t sin256[256] = {0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,59,62,65,67,70,73,75,78,80,82,85,87,89,91,94,96,98,100,102,103,105,107,108,110,112,113,114,116,117,118,119,120,121,122,123,123,124,125,125,126,126,126,126,126,127,126,126,126,126,126,125,125,124,123,123,122,121,120,119,118,117,116,114,113,112,110,108,107,105,103,102,100,98,96,94,91,89,87,85,82,80,78,75,73,70,67,65,62,59,57,54,51,48,45,42,39,36,33,30,27,24,21,18,15,12,9,6,3,0,-3,-6,-9,-12,-15,-18,-21,-24,-27,-30,-33,-36,-39,-42,-45,-48,-51,-54,-57,-59,-62,-65,-67,-70,-73,-75,-78,-80,-82,-85,-87,-89,-91,-94,-96,-98,-100,-102,-103,-105,-107,-108,-110,-112,-113,-114,-116,-117,-118,-119,-120,-121,-122,-123,-123,-124,-125,-125,-126,-126,-126,-126,-126,-127,-126,-126,-126,-126,-126,-125,-125,-124,-123,-123,-122,-121,-120,-119,-118,-117,-116,-114,-113,-112,-110,-108,-107,-105,-103,-102,-100,-98,-96,-94,-91,-89,-87,-85,-82,-80,-78,-75,-73,-70,-67,-65,-62,-59,-57,-54,-51,-48,-45,-42,-39,-36,-33,-30,-27,-24,-21,-18,-15,-12,-9,-6,-3};  //Прерывание по переполнению Timer4 (TIM_UIE) __attribute__((interrupt)) void TIM4_IRQHandler(void){   static uint32_t pos = 0;   pos += dpos;    TIM4->CH1CVR = 127 + sin256[(pos>>24)]; //Для адресации используются только 8 старших бит, остальные — аккумулятор, в них хранится ошибка, накопившаяся к текущему времени    static uint32_t ppos = 0;   if(pos < ppos)GPO_T(GLED); //при переполнении переменной-счетчика мигаем светодиодом, так проще измерить частоту   ppos = pos;    TIM4->INTFR = 0; } Код работает и даже рисует на экране осциллографа красивую синусоиду. Правда, при частотах выше 5 кГц она становится несколько треугольной — но чего вы хотели, 6 точек на период. В реальном применении, разумеется, вместо прерывания от таймера логично использовать DMA. Как минимум оно будет значительно реже отвлекать ядро от его задач. А если чуть-чуть пожертвовать точностью, туда можно записать вообще всю таблицу значений и больше про него не думать. Но здесь ради наглядности я решил обойтись таймером. Не менее очевидно, что вместо синуса можно использовать и любую другую функцию. Скажем, при работе с бесколлекторными двигателями или частотными преобразователями иногда используют вот такой «покореженный синус», а то и еще более извращенные вариации.  Такая странная форма сигналов позволяет достичь максимальной амплитуды межфазного напряжения, сохраняя его форму синусоидальной. Межфазное напряжение (между фиолетовой и зеленой фазами) нарисовано на графиках желтой линией. Достаточно подробно об этом рассказал в своем ролике TDM Lab. 10.9 Внезапный фейл с дробным числом Это не относится напрямую к теме, просто история из жизни. На работе у меня установлен источник питания с управлением по COM-порту путем посылки обычных текстовых строк. В частности, для установки выходного напряжения, скажем, в 1.23 В нужно послать строку ""VSET0 1.23\r\n"". Эту строку я формировал обычным sprintf(buff, ""VSET0 %.3lg\r\n"", volt);. И однажды оказалось, что при установке напряжений около нуля прибор зависает и перестает менять напряжение. Проблема оказалась в формате %lg, который в обычных условиях заставляет sprintf автоматически выбирать способ записи — алгебраический (123.456) или экспоненциальный (1.23456e+2). Для обычного вывода на экран или в файл это очень удобно, но вот прибору экспоненциальная форма категорически не понравилась. Не знаю, какая у этой истории мораль. Разве что при разработке своих приборов надо не лениться и реализовывать разные форматы ввода. В частности, не забывать, что десятичным разделителем может быть не только точка, но и запятая. Дополнительная информация Оригинал на github pages Видеоверсия на Ютубе (только по gd32) "
92,92,Прямая без препятствий. От чего и зачем нужно защищать мобильные приложения,Positive Technologies,Лидер результативной кибербезопасности,4.21,"Веб-разработка, Программное обеспечение, Информационная безопасность",2025-03-12,"Привет, Хабр! На связи Николай Анисеня из отдела перспективных технологий Positive Technologies. Так вышло, что в компании я уже много лет занимаюсь безопасностью мобильных приложений, исследую тренды этого направления и, как и все, наверно, специалисты в этой области, ломаю голову: как сделать мобильное приложение более защищенным. Этой публикацией я открываю цикл статей на тему безопасности мобильных приложений и устройств, корень которой (начнем со спойлера!) — в анализе кода. В этой статье расскажу об угрозах мобильных приложений, сценариях атак на них и о главном парадоксе в их разработке. Интересно? Тогда погнали! В теме безопасности мобильных приложений присутствуют разнонаправленные тренды. Что-то становится более защищенным благодаря улучшению фреймворков, операционных систем и практик безопасной разработки, а где-то, наоборот, появляются новые возможности для атак из-за новой функциональности мобильных приложений. Все как в том меме: «Исправлены баги, добавлены новые». Примером может служить уязвимость, приводящая к атаке типа Tapjacking, о которой впервые заговорили применительно к Android 2.3. На устройствах с Android 4 эта проблема была фактически решена, однако с выходом Android 6 вышел и «наследник» этой уязвимости, позволяющий произвести атаку Cloak and Dagger. Источник: pikabu.ruНаправлению безопасности мобильных приложений в Positive Technologies уже более десяти лет, и все это время мы наблюдаем эту картину: какие-то компоненты мобильных приложений становятся защищеннее, а какие-то, наоборот, вбирают в себя все больше угроз. Этот факт также находит отражение в данных OWASP Mobile Top 10. Но в этом рейтинге можно заметить и весьма однозначные тренды, если анализировать долгосрочный период. Например, давайте рассмотрим ситуацию с защитой кода. За 10 лет существования OWASP Mobile Top 10 эта проблема не просто всегда отмечалась в рейтинге, но и стабильно поднималась вверх списка — с 10-го места в 2014-м на 7-е в 2024-м. Формулировали проблему каждый раз по-разному: возможность реверс-инжиниринга, тамперинга, недостаточная бинарная защита, защита кода.Несмотря на растущую угрозу, защита кода все еще остается крайне непопулярной мерой защиты мобильных приложений, согласно исследованию, опубликованному на arXiv.org. За период с 2016 по 2023 год в среднем 56% приложений применяли техники обфускации, однако только 59% из них, или ⅓ от изначальной выборки, совмещали все три описанные в статье техники обфускации, при этом качество защиты не исследовалось. Не удалось найти похожих исследований для техник RASP — runtime application self-protection — самозащиты приложений во время запуска.По нашей статистике, основанной на результатах анализа защищенности приложений за 2024 год, проводимых для наших клиентов, а также исходя из опыта личного участия наших экспертов в программах bug bounty, доля применения совокупности техник обфускации и RASP составляет только 8%. 🥁 Более чем за 10 лет анализа защищенности нам встретилось только одно приложение, защиту которого не удалось преодолеть за выделенное на проект время. Однако это не помешало нам обнаружить критическую уязвимость с помощью реверс-инжиниринга: в одном месте защита не применялась, и именно через него мы получили исполнение кода на сервере — уязвимость в10 баллов по шкале CVSS!И вроде это неудивительно: реверс-инжиниринг нельзя сделать невозможным, а атакующий всегда может делать с клиентским приложением все, что ему вздумается: модифицировать, читать память, использовать фрагменты кода на свое усмотрение. Почему так происходит?Код клиентского приложения (бинарный, а иногда и исходный) всегда доступен атакующему, сколь сильно бы он ни был запутан и искажен. В случае, если код зашифрованный, он обязательно будет расшифрован для корректной работы приложения. Среда исполнения — смартфон — полностью контролируется злоумышленником. Можно собрать стенд, неотличимый для приложения от доверенного окружения, но дающий атакующему максимум привилегий: «экзотическое» железо, кастомные сборки ОС, аппаратные хаки и так далее — вопрос только в цене: денежной, временной и экспертной.Но решение у этой проблемы есть. Техники защиты кода способны сильно усложнить реверс-инжиниринг и тамперинг. Хорошая защита кода увеличивает трудозатраты и, как следствие, стоимость атаки настолько, что злоумышленник предпочитает отказаться от идеи атаковать приложение.Но каждый ли разработчик заботится о защите своего приложения? Безопасность — это всегда затраты. Поэтому давайте разберемся, стоит ли вам переживать о защите кода и как ее обеспечить при необходимости.Сценарии атак на мобильные приложения Для атакующего реверс-инжиниринг в широком смысле — это инструмент изучения внутреннего устройства приложения для достижения одной из целей:🔻 поиск и эксплуатации уязвимостей,🔻 создание клонов и модификаций,🔻 изучение внутреннего устройства приложения в целях конкуренции,🔻 использование части кода или компонентов приложения без согласия правообладателя,🔻 кража контента,🔻 автоматизация и разработка альтернативных клиентов.Для частичной или полной защиты от подобных угроз хорошо работают технологии защиты кода. Рассмотрим в деталях каждый из этих сценариев.Реверс-инжиниринг конкурентовИзучение конкурентов с помощью реверс-инжиниринга — реально существующая практика. И хотя компании в этом редко признаются, по косвенным признакам, таким как распространение удачных дизайнерских решений в интерфейсе, внешнем виде устройств, мы видим, что это происходит. Такому подходу могут следовать и на уровне кода. Например, компания, разрабатывающая мобильную игру, может не тратить сотни и тысячи человеко-часов на тонкую настройку игрового баланса с целью наилучшего удержания игрока. Вместо этого готовое решение она может подсмотреть у лидирующего конкурента и просто скопировать его. В этом случае конкурентное преимущество в виде правильной настройки формул, отвечающих за игровой баланс, перестает быть преимуществом. Публичных признаний компаний в таком подходе найти не удалось, однако многочисленные сообщения на форумах и рассказы сотрудников геймдев-компаний на это явно намекают. Кража ассетовТакже в геймдев распространено воровство ассетов. Неплохо защищаться в этом случае можно юридическими методами, так как доказать воровство большинства компонентов гораздо проще (по сравнению с формулами из предыдущего примера). Но не все ассеты могут быть защищены юридически. Примером таких уязвимых компонентов могут стать файлы локальных ML-моделей, хранимые в бандлах приложений. В исследовании на arXiv.org за 2021 год сказано, что 41% приложений никак не защищают свои модели, а ⅔ приложений допускают их тривиальное извлечение из бандла. Выгода атакующего здесь аналогичная: экономия ресурсов на обучение модели. Жертва в этом случае теряет конкурентное преимущество, на разработку которого уже потрачены ресурсы.Воровство контентаНо воруют не только идеи и технологии. Злоумышленники также заинтересованы в пиратстве контента: фильмов, музыки, книг и др. Основным средством борьбы здесь является блокирование сайтов, распространяющих пиратский контент. Однако использование методов DRM (digital rights management, технологии для защиты копирайта) совместно с протекторами мобильных приложений может сильно увеличить стоимость извлечения контента.Автоматизация и альтернативные клиентыОтсутствие защиты кода помогает злоумышленникам беспрепятственно изучать клиент-серверное взаимодействие мобильных приложений. Это может быть использовано для создания альтернативных клиентов, в которых отсутствуют некоторые функции — например, реклама, аналитика, — или есть дополнительные: аналитика по маркетплейсу, режим «невидимки» в мессенджерах и соцсетях и т. д. Автоматизация взаимодействия с API сервера мобильного приложения также может нести риски для бизнеса. Это разработка торговых или игровых ботов, сбор рыночных данных, данных о пользователях. Использование ботов — в торговле, играх, еще где-то — может привести к тому, что легитимные пользователи приложения будут в заведомо подвержены риску. В случае игр и трейдинга — это нечестная конкуренция, в мессенджерах и соцсетях — это спам-рассылки, мошенничество, накрутки. Другим ярким примером такого проигрыша могут служить боты для записи в визовые центры. Забронировать слот на сайте, как это предполагается системой, почти нереально, приходится выкупать забронированные ботами места.Создание клонов и модификаций приложенийМало кому хочется встречаться с сотрудником банка, чтобы установить мобильное приложение по «проводу», если оно было удалено из магазина приложений. Некоторых пользователей это толкает на самостоятельный поиск приложений в альтернативных источниках. Приложения, загруженные таким способом, не заслуживают доверия. Злоумышленники с радостью используют подобные поводы для распространения вредоносов среди доверчивых пользователей. И хотя для создания клона мобильного приложения банка совсем не обязательно заниматься реверс-инжинирингом, сделать действительно качественный клон, не отличимый от оригинала, можно только с помощью перепаковки оригинального приложения. В этом случае повышается шанс, что человек еще долго будет пользоваться приложением и даже не заподозрит, что находится под угрозой. Отсутствие защиты от перепаковки приложения делает задачу создания качественного вредоносного клона тривиальной. Ниже перечислю лишь несколько известных примеров.Обнаружен вредоносный клон, полученный с помощью перепаковки легитимного Android-приложения ChatGPTEvilBamboo атакует мобильные устройства в ходе многолетней кампанииСемейство шпионских Android-приложений, переделанных из легитимных с помощью добавления бэкдораПохожим образом дело обстоит с модификациями приложений без цели навредить пользователю. Удаление или перенаправление рекламы, разблокировка платных функций — все это легко дается злоумышленникам, если в приложении не применяются технологии защиты от модификации и перепаковки. Согласно этому исследованию, 21% модов (модифицированных приложений) перенаправляют рекламные доходы оригинального приложения. Поиск и эксплуатация уязвимостейРеверс-инжиниринг, перепаковка, статический и динамический анализ приложения, использование автоматических сканеров — все это этапы поиска уязвимостей в мобильном приложении, причем как в клиентской, так и в серверной части. По нашему опыту, в большинстве мобильных приложений дорога к уязвимостям — это прямая без препятствий. В редких случаях можно встретить незначительные «стены», преграждающие путь атакующему, но очень часто их можно обойти тривиальными способами. И хотя применение сложной комбинации продвинутых техник защиты может быть очень эффективным, встречается такая защита крайне редко.Применение различных техник обфускации затрудняет ручной анализ приложения, может ломать работу декомпиляторов и средств перепаковки, а также скрывает уязвимости от статических анализаторов. Например, если в код утек некий секретный токен, хорошая обфускация скроет этот факт от сканера, и без участия человека этот недостаток просто не будет выявлен.Технологии RASP — дамп памяти, тамперинг и динамическая инструментация (Frida), применение динамических сканеров (DAST) — препятствуют динамическому анализу. В случае их использования злоумышленник вынужден потратить огромное количество ресурсов на создание кастомного тестового окружения, перепаковку приложения, чтобы просто подступиться к поиску уязвимостей.Зона риска: какие приложения наиболее уязвимы?Если архитектура вашего приложения бессерверная, вы не используете никаких сторонних сервисов вроде карт и аналитики, у вас отсутствуют регистрация, микротранзакции, платная функциональность, а в коде приложения не содержится никакого ноу-хау, то переживать вам не стоит. Но под такое описание подойдет разве что калькулятор. Подавляющее большинство современных приложений — это сложные системы с большой поверхностью атаки. Даже простые заметки в телефоне уже хранят ваши секреты, а ведь большинство приложений намного сложнее. К примеру, банковские суперприложения: с их помощью уже давно можно делать не только переводы, но и покупать билеты на самолет и в театр, заправлять машину, заказывать продукты и многое-многое другое. Украсть деньги можно не только из банка: вспомните, сколько приложений сохранили данные вашей банковской карты? Помимо данных карт, там обычно хранится куда больше ценной информации: любой личный кабинет — кладезь персональных данных. Кроме того, любой сервис, доступный внешним пользователям, — это потенциальная точка входа в периметр вашей организации: мобильные приложения — всего лишь интерфейс взаимодействия с сервером, и злоумышленника не остановит отсутствие нужных кнопок, когда можно обращаться к серверу напрямую.Если исключить из рассмотрения совсем примитивное мобильное ПО, то защита кода требуется абсолютно каждому приложению. Вопрос, нужна ли такая защита, попросту не стоит: она необходима. Нужно задавать другие вопросы: какая защита нужна? какие средства и методы защиты бывают? В чем преимущества и недостатки того или иного подхода? Со всем этим разберемся в следующей части нашего цикла статей.Николай АнисеняРуководитель отдела перспективных технологий, Positive Technologies"
93,93,"3D для каждого. Оптимизация модели. Часть 2, практическая",FirstVDS,Виртуальные серверы в ДЦ в Москве и Амстердаме,233.12,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-03-12,"   Доброго времени суток, господа и господарочки!  Прошлая статья знакомила вас с одной из фундаментальных проблем современного 3D-моделирования и игростроения — с оптимизацией. Мы разбирали причины непомерного объема игровых файлов и архивов с моделями и то, к чему это приводит.  Теперь же мы переходим непосредственно к практике оптимизации. Каждый, кто хочет войти в индустрию 3D-графония и геймдева в частности, должен как минимум иметь представление о том, как содержать архивы (личные и рабочие) в порядке, да еще так, чтобы они не занимали лишнего пространства и с ними было удобно работать. Конечно, некоторые, кто прочел предыдущую часть, уже поняли, как это можно сделать. Однако не сочтите за дерзость и дайте мне шанс прояснить некоторые моменты.  Возможно, над некоторыми способами оптимизации вы никогда не задумывались, а какие-то просто не до конца понимали. Что-то будет интересно только новичкам, а что-то поможет расширить арсенал и профессионалу. Если вы готовы, то поехали!    В посте могут встречаться упоминания программ, игр, фильмов и студий. Не сочтите это за рекламу — всего лишь иллюстративный материал для понимания контекста.Текст написан на основе личного опыта автора. Пост рассчитан на тех, кто еще только начинает свой путь в 3D или хочет освежить давно забытые воспоминания. Профессионалы, скорее всего, итак знают большую часть хитростей и приемов. Но, быть может, какие-то интересные штучки и они найдут для себя.    Микрословарик для новичков  Под 3D в данном посте понимается система визуализации трехмерной системы представления программного кода. 3D-модели, которые вы видите в играх с трехмерной графикой, в мультиках, в фильмах наподобие Аватара — это 3D-модели, о которых мы говорим. Принципиальной разницы между моделями для игр, кино, рекламы и мультиков, нет. Все различия заключаются в подробности модели и наличии дополнительных «аксессуаров» — текстур, костей, физики, анимации — и качестве финального рендеринга.   Рендеринг (render, rendering)  — процесс визуализации 3D-моделей. В зависимости от нужд и настроек, рендеринг может включать только саму модель, модель с текстурой, шейдеры, а также один статичный кадр или серию кадров в формате видео или гиф-анимации.     Меш (mesh)  — «кожа» или «ткань» модели, состоящая из вершин (axis, point), соединяющих их ребер (ribs, edges) и заполняющих пространство между ребрами граней — полигонов (polygons).   Скелет или риг, а также кости (bones, rig, skeleton, armature)  — «каркас» модели, служащий для удобства позирования и анимации модели.    UV-карта (UV-map) — координаты наложения текстур на меш модели в формате двумерной проекции-мозаики. Напоминает выкройку. Определяет, как и в каком масштабе текстура накладывается на модель. Создание данной карты называется разверткой.   Текстуры (map, texture) —  файлы изображений, определяющих цвет (diffuse/color), имитацию рельефа (normal), блика (specular), объема (displacement), прозрачности (alpha) и так далее.  Физика (physic) — имитация физических взаимодействий (столкновения, ветра, жидкости).  Хайполи (high-poly model, high res) — подробная высокополигональная модель, которая имеет более 500 тысяч полигонов. В некоторых случаях хайполи признаются и менее полигональные модели либо, наоборот, сдвигают границу высокого полигонажа вверх. Но, как правило, счет идет с миллиона полигонов и более. Хайполи нужны для создания вручную нарисованных текстур и запечения объемов в normal map.   Лоуполи (low poly) — модель с небольшим количеством полигонов (обычно до нескольких тысяч) и невысокой детализацией. Лоуполи часто используют для игр, особенно для мобильных и инди.     Как оптимизировать?  Разрежаем сетки и начинаем оптимизацию. А с чего её нужно начать делать? Конечно, с без текстур, чем с текстурами. В зависимости от вашего софта и любимых методов и объектов моделирования (полигональное, лепка, проекция, сканирование и т. д.), у вас будет разная полигональность — разный поликаунт (polycount).    Примечание: именно разрежаем, а не разряжаем. Разряжение или разрядка — снятие заряда (напряжения). Разрежение — процесс уменьшения плотности (тот же корень, что и проредить — сделать более редким).      Автоматическая ретопология  Для скульпторов обычное дело иметь в одном сабтуле (условном слое модели) миллионы полигонов. Поэтому у пользователей ZBrush кнопка ZRemesher быстро становится любимой. Конечно, топология, полученная таким методом, далеко не идеальна, однако Зремешер позволяет быстро сократить количество полигонов и при этом не потерять основные формы модели. Более того, ZRemesher поддерживает кнопки управления и цветовое маркирование областей для контроля плотности сетки.   При включенном режиме Use polypaint вы можете отметить красным области, на которых должна быть более плотная сетка, синий — участки с менее плотной сеткой.   Для продвинутых юзеров и тех, кому нужно быстро сократить полигоны, сохранив текстуры, существует плагин Decimation Master. Особенно он нужен тем, кто хочет как можно быстрее экспортировать свою поделку в игровой движок, так как сам анализирует сетку, пересчитывает ее и создает на ее основе новую треугольную топологическую карту. После препроцессинга вы можете ползунком регулировать параметр плотности и подробность. Данный плагин может сохранить края мешей, UV-карту и раскраску модели.   Встроенный в ZBrush плагин Decimation Master быстро сокращает полигонаж, сохраняя UV-развертку, и позволяет работать сразу с несколькими сабтулами («слоями» модели)    Полезная инфа: треугольники и квадратики  Хотя в названии «полигон» не уточняется, сколько углов может быть у куска меша, как правило встречаются треугольники и четырехугольники. Треугольная сетка может быть хороша для многих игровых движков, но не всегда удобна в случае, если вы захотите ее редактировать дальше. Все дело в том, что треугольники очень плохо воспринимаются программами для подразбиения — увеличения полигональности. Четырехугольники при применении divide/subdivide удобны: на каждой стороне фигуры в середине ребра формируется точка, соединяемая с аналогичной точкой на противоположной стороне фигуры и пересекающая аналогичную медиану прилегающего ребра. Таким образом из одного четырехугольника формируется 4 новых полигона.   Одно подразбиение увеличивает полигональность вчетверо и при этом сохраняет сетку относительно нетронутой, так как каждая линия «разреза» будет продолжаться на следующий полигон, и на следующий, и так далее.   При попытке разбить сетку из треугольников, вы скорее всего увидите хаос: каждый треугольник может разбиться на 2, 3, 4 и сколько угодно еще полигонов, искажая основные формы модели. Проводить лепку на такой модели значит обречь себя на артефакты в виде «затяжек» поверхности.  Поэтому прежде чем увеличить полигональность подразбиением, такие модели сначала надо квадрангулировать — превратить треугольники в четырехугольники. Для этого во многих программах есть различные инструменты. В Зибраше вообще целый спектр: от того же Зремеша до Dynamesh.  А что с полигонами, у которых число углов больше 4? Они также плохо поддаются адекватному подразбиению. Более того, некоторые программы не способны воспринять фигуры с числом углов больше 5, 8 или 12. Некоторые разрезают полигоны автоматически (как Unity), некоторые не могут их даже открыть.     Ручная ретопология Однако, кроме очевидного однокнопочного Зремеша и генерирующего треугольники дециматора, есть еще один инструмент, которым пользуются только профессионалы. Это ZModeler.    ZModeler находится в разделе Кисти (Brushes)   ZModeler — это попытка разработчиков войти в мир редактирования низкополигональных и hard surface моделей. Данная кисть активирует систему работы с ребрами, вершинами и полигонами. При активном Змоделере вы можете выбрать режим работы кисти, наведя ее на ребро, полигон или вершину (да, режимы будут отличаться в зависимости от того, на что вы навели) и зажав Shift.     При наведении на ребро и нажатии Shift вы увидите вот такое окно с выбором режима. Вверху режимы работы (движение, выделение, разрез, добавление, удаление и т.д.), внизу — режим выбора (с одним ребром или со всей линией соединенных последовательно ребер   Выбрав Delete в режиме работы с ребрами, вы можете удалять все сочлененные ребра в линии. Аккуратно работая с этим инструментом, вы можете вручную уменьшить плотность полигонов в определенных местах модели. Но будьте осторожны: инструмент может давать сбои в участках с неквадратичными (сильно искаженными и нечетырехугольными) полигонами и иногда давать осечки в виде треугольников.   Да, это долгий процесс, однако позволяющий точнее работать с моделью   Аналогичные режимы работы есть и в других программах. Кстати, есть ещё один способ — создание поверх хайполи новой модели, вручную добавляя полигоны, как будто рисуя по поверхности. После такого вы получите две модели — с более плотной сеткой и менее плотной. При необходимости всю детализацию можно спроецировать на новую модель в виде текстурных карт.    Для фоновых объектов, которые зритель или игрок никогда не увидит вблизи, можно применять экстремальную оптимизацию, оставляя менее 10 полигонов и микроскопическое разрешение текстур.     Толпа зрителей в Century. Во время боя верхом на драконе вам совершенно не интересно, какого разрешения человечки, которые орут вам с трибун   Какой из способов вам ближе, удобнее и будет хорош в конкретном случае, можно узнать только на практике. Разработчики крупных игр как будто бы вовсе не заботятся об этом, оставляя миллионы полигонов даже там, где это не нужно (здравствуйте, мегаполигональные маски в анимешной Code Vein и сверхдетальные модели Final Fantasy XVII).   Удаляем фрагменты меша Вернемся к картинке из начала прошлой статьи. Самые внимательные уже поняли, в чем подвох:   А подвох в том, что все эти модельки не имеют дна. Только крышку.  Если бы я оставила дно, при подразбиении добавилась бы еще как минимум четверть полигонов.   Заготовка вентиляционных конструкций для крыши дома. Если у вас не паркур-клон Mirror’s Edge или Assassin’s Creed, то крышу игрок почти никогда не будет видеть. Следовательно, можно обойтись коробкой с текстурой. Если же ваш персонаж — ямакаси, то придется озаботиться не только коробкой, но и отверстиями в ней. Отверстия не могут находиться внутри цельного полигона, не соединяясь ребрами с другими полигонами — большая часть программ от этого вылетит с ошибкой, не в силах прочитать топологию.   В прошлой статье мы упоминали, что многие игры, в том числе Elden Ring, скрывают участки кожи под одеждой. Для своих моделей вы тоже можете удалять части, которые не видны из-за одежды или просто невидимы.    Модель персонажа для игры в процессе оптимизации. Когда вы закончили подгонять одежду по фигуре, тело-образец можно удалять.   Многие куски локаций в играх (скалы, фрагменты зданий, запчасти космических кораблей) также не имеют одну сторону для экономии на полигонах:   Большая часть скал в играх — односторонний кусок меша. Ведь при нормальных условиях игрок никогда не увидит изнанку.  Одежда персонажей в играх тоже однослойная. В случаях, когда игрок все же видит изнанку (полы плаща, юбки, рукава), чтобы не было прозрачных дыр, разработчики добавляют шейдер. Но об этом позже.   Уменьшаем текстурки Увеличение текстур в два раза увеличивает занимаемый ею объем памяти в 3-4 раза. Посмотрите на свою модель и решите: действительно ли вам нужны 4к-текстуры? Как это определить? Для начала обозначьте для себя целевое применение модели. Если она вам нужна для создания красивых статичных артов в высоком разрешении, то открытые и видимые участки моделей, конечно, должны быть в высоком разрешении. Но если у вас одетая моделька, то текстура кожи, кроме рук и головы, в большом разрешении не потребуется (а может и вовсе не нужна).  Если у вас машинка для гонок, в которых вид будет из кабины, как бы от лица водителя, то салон внутри должен быть проработан. Если у вас гоночки от третьего лица без возможности заглянуть в салон, то высокодетализированный кожаный салон вам не нужен.    Лайфхак: вы можете оставить в высоком разрешении лишь одну из карт. Например, только диффузную, если вашей модели важнее цвет и рисунок, или только нормалмап, если вам важен рельеф (чешуя, например). Спекуляры зачастую не играют большой роли, поэтому некоторые сеты текстур могут выглядеть так: диффузная текстура — 1к, нормалмап — 2к, спекуляр — 512p.    Модель для платформера от третьего лица. Большая детализация внутренностей ей не нужна, так как под крышку игрок заглядывать не будет, поэтому хватит мыльной фотки подделки под чип   Если у вас модель дракона, то, конечно, текстура шкуры должна быть хорошей. А вот на текстурах пасти можно сэкономить.   Сокращенная палитра цветов Те, кто не работают редакторами изображений, и не догадываются о существовании такой настройки текстур, как битность или палитра. Битность картинки определяет, какими цветами будет записано изображение и сколько их будет использовано. Обычно используют палитры 8 бит, 16 бит и 32 бита. Соответственно, в каждой из них 8,16 и 32 цвета.   В большинстве своём этого достаточно для несложных текстур, например, поясных аксессуаров и фоновой утвари. Более того, вы, скорее всего, не заметите разницы между 16- и 32-битной картинкой в текстуре камня. Однако вес может отличаться разительно. Это, кстати, одна из причин огромного веса мегапаков серии ассетов Foliage — огромные текстуры с высокой битностью.   Но с высоко детализированной текстурой расшитого кафтана или лица с макияжем это не прокатит. В таких случаях придётся жертвовать объёмом памяти ради более красивого результата.   В других случаях настроить разрешение и битность можно в любом продвинутом редакторе картинок при сохранении:    Окно сохранения изображения в Adobe Photoshop   Форматное сжатие картинки Картинки в разном формате имеют разный вес. Зависит это от многих факторов: разные кодировки, разные палитры, разные степени сжатия. Стандартный JPEG значительно меньше стандартного PNG при тех же настройках сторон, цветности и отсутствии альфа-канала. Стандартный TGA ещё больше. А стандартный DDS 5 меньше PNG, но прочесть его смогут «не только лишь все» графические редакторы.   DDS входит в линейку самых распространенных форматов для игровых текстур, так как позволяет сэкономить место на диске и при этом сохранить настройки альфа-канала (прозрачности) и некоторые другие.    Одна и та же текстурная карта в разных форматах   Плитки (Tiled map) Сэкономить место поможет и использование Tiled UV map, которую мы упоминали в предыдущей статье. Tiled, или плиточная текстура (а также бесшовная текстура — seamless), это такая текстура, которая при наложении на модель в любом масштабе будет, вы не поверите, бесшовной. Вы не увидите границ текстуры, как бы ни растягивали и ни уменьшали ее. Так, например, с помощью крохотной картинки кирпичной стены 512х512p вы можете затекстурить огромную стену целиком. И да, разработчики и 2D- и 3D-игр часто пользуются этим методом с незапамятных времён.   Самодельная бесшовная текстура кирпича высокого разрешения на маленькой модельке в оригинальном масштабе и на большой модельке с сохранением масштаба. Без растяжений.    Шейдеры  Шейдеры — это программные настройки отображения поверхности модели. С их помощью вы можете контролировать вид, цветность, режим отображения и даже форму поверхности, в том числе покрывая её травой или меховыми прядями  Да, пушистые шарики в детских мультиках не вырисовываются по волоску — большинство из них используют предустановленные шейдеры типа fiber mesh — тонкие малополигональные сплайны (плоскатики) загружаются поверх основной модели. Однако фибер редко экономит ресурсы — хороший мех и травинки будут нагружать вашу память даже больше, чем высокотекстурная имитация. Особенно, если вы включите физику…    Шейдеры позволяют имитировать даже геометрическое искажение поверхности при рендере  К одному шейдеру можно подсоединить несколько десятков настроек и текстур. Но обычно хватает стандартного сета: цветная текстура, карта нормалей, карта металличности и альфа, если она необходима.    Пример шейдера с текстурами в блендере. Да, блендер позволяет назначать одну и ту же текстуру и как карту блеска, и как карту металличности, если это необходимо  Использование одного шейдера стекла или кирпичной стены с бесшовной текстурой для нескольких моделей в игре или сцене значительно упростит вам задачу и сэкономит место.   Пример простого стеклянного шейдера в Blender 3D. Как видите, в данном случае нам даже текстурные карты не понадобились — все чисто на нодовых элементах программы   Создаем LOD Думаю, хоть раз заядлые геймеры замечали, как прогружаются видимые вдали предметы. Игроки в TES V Skyrim любой версии так до сих пор могут словить баг с мыльным лоу-поли драконом, атакующим крепость провинции.    Скайрим стал источником многих мемов. Огнедышащий графон — один из них. Наряду с лицом из Uncharted    LOD — это модели с пониженным разрешением, нужные для изображения далеких объектов. Скажем, вы видите вдалеке гору Высокий Хротгар, но пока вы не подошли к ней вплотную, вам не нужно видеть архитектурные изыски старейшин на ее вершине и тонкости рельефа скал. Вместо цельной локации горы загружается относительно малополигональная модель с «мыльными» текстурами. То же касается всех прочих объектов в игре — зданий, мостов и даже драконов. Это экономит ресурсы компьютера/консоли на прорисовку далеких объектов.   Но не Скайримом единым: любители Cyberpunk 2077 тоже имели честь лицезреть мыльные машинки и мыльных человечков. Ну а игры на Unreal Engine вообще имеют возможность генерировать лоды «на ходу», сжимая высокополигональные модели. Хотя это работает далеко не всегда так, как надо.   В Lords of the Fallen модельки автоматически ретопологизируются до состояния LOD. Но порой не возвращаются обратно (внимание на щит статуи)    Упаковываем в пакеты В прошлой статье я упомянула игровые архивы. Архивация игровых файлов служит двум целям — сжатие объема и защита от кражи и модификации. Некоторые движки позволяют сжать модели и текстуры более чем в 3 раза.   Но и домашний мамкин 3D-шник тоже может использовать сжатие. Помимо стандартного RAR/ZIP/7zip архива, вы можете упаковать свое творчество целиком и полностью — вместе с предустановками света и анимации. Не все движки имеют свои способы архивации. Но самые распространенные, как Blender, имеют.   Сцена и ее текстуры без упаковки…   Сцена с упаковкой текстур внутри нее и без упаковки  Блендер позволяет как сжать текстуры, так и сохранить их в общий файл без сжатия. Таким образом, сэкономить место не получится, однако удастся сохранить текстуры для пересылки и хранения на облачном диске.   Уязвимые места В оптимизации главное не переусердствовать. Если сократить полигоны слишком сильно, то вы получите мятую искаженную версию модели. Особенно это касается мелкой детализации.   Пример пережатых децимацией аксессуаров: крепежи воротника и застежка ремня «поломались  Для отригованных моделей сетка особенно важна. Если полигонов слишком много, вам придется попотеть, настраивая влияние костей так, чтобы меш не ломался слишком резко. Если полигонов слишком мало, меш будет деформироваться.   В некоторых случаях вам лучше позаботиться о том, чтобы освободить суставную область от лишних полигонов. Но если убрать их слишком много, то сустав сломается  Порой вам наоборот придется добавлять лишние ребра, чтобы сделать модель более удобоваримой. Например, если часть модели должна быть сглаженной, а часть — иметь жесткие грани.   До добавления грани: движок пытается сгладить геометрию, из-за чего на краю крышечки появляется эффект линзым   После добавления грани: линзирование исчезло и теперь при рендере нижний край крышки не будет неестественно изгибаться  Hard-surface-модели, которые предполагают наличие жестких граней и геометрических форм, в принципе невозможно сделать без полигонального излишества.   Попробуйте еще сильнее сократить полигональность этого колеса и потеряете все его прелести  Со зданиями и вообще архитектурой еще веселее: некоторые вещи вы физически не сможете убрать, не лишившись части фасада.   Даже для фоновой лоу-поли коробки приходится имитировать дверные и оконные проемы, а это минимум + 5-8 полигонов   Хитрости и уловки Где есть проблема, там обязательно найдется решение. Например, вот вы смоделировали фасад здания, используя только базовые формы и простые разрезы:   346 полигонов. Это минимум при таком подходе. А ведь все они в движке скорее всего станут треугольниками, так что смело умножаем это число на 2.   Пример, возможно, покажется вам слишком примитивным. Ну какие 300 полигонов, кто над этим будет убиваться? Но во-первых, это иллюстрация, модельная ситуация, призванная продемонстрировать приемы 3D-шников. А во-вторых, для мобильных платформ часто счет полигонов идет на десятки.  Что мы можем сделать, чтобы уменьшить полигональность?  Поставьте себя на место игрока. Это здание будет лишь одной из десятка коробок в городе, в котором игроку доступна лишь тротуарная дорожка и трасса. Во дворы наш виртуальный челик заходить не будет. А значит, смотрим прямо на это здание. Вся его задняя поверхность нам не нужна.   Смело удаляем задник и дно, убираем лишнее с крыши и задней части декоративных плашек  Заднюю стенку убрали. Следом за ней в топку отправляем и пол — наше здание не будет висеть в воздухе, мы утопим его в грунт газона.Так много полигонов на крыше, как получилось при разрезе под окна, нам тоже ни к чему. Удаляем их, а оставшийся растягиваем до противоположной стены.   Декоративные плашки в проемах „окон“ сделаны из простых параллелепипедов. Игрок увидит лишь 3 из 6 граней каждого параллелепипеда, а, значит, заднюю, верхнюю и нижнюю их грани также можно удалить  После отсечения всего лишнего осталось лишь 160 полигонов из 350. Оптимизацию можно было продолжить и дальше, но мне было важно, чтобы этот дом использовал одну текстуру с другими двумя зданиями.   После оптимизации можно заняться расстановкой элементов UV-карты  Реюз текстурок — наше все. А вы навскидку можете сказать, сколько зданий на этой локации используют одни и те же карты?   Лоу-поли коробочки зданий на фон для игры  Напоследок дам вам еще один лайфхак. Сделали вы бесшовную текстуру стены и натянули ее везде, где нужно. Или клонировали кусок бетонной конструкции, но текстура у него такая же, как у предыдущего. Но так ведь в жизни почти не бывает, чтобы стены у двух зданий были однородными, верно?   Верно. Надо как-то разнообразить текстуру! И для этого есть два пути:   маска  сплайн   Маска работает так: на модели выделяется участок, на который сверху основной текстуры накладывается дополнительный текстурный канал или слой без необходимости добавлять еще один кусок модели. Этот канал загружает поверх текстуры цвета еще одну текстуру цвета, оставляя предыдущую нетронутой.   По такому принципу в играх организован, например, макияж. Или 100500 вариантов оружия в Borderlands и иже с ним — на базовую пушку натягивается маска с какой-нибудь грязью или граффити и немного подкручиваются цифры характеристик. Но далеко не все движки поддерживают работу с маской одинаково.  Сплайн или плоскатик — кусочек меша, чаще всего представляющий из себя прямоугольник со своей отдельной текстурой:    В этом кадре пять сплайнов с граффити и около 30 сплайнов с сеткой для имитации ремонтных работ   Ну, что, надеюсь, вас не сильно утомили простыни текста с картинками. Теперь вы знаете чуть больше об оптимизации моделей и текстур. Отныне у вас есть аргумент, чтобы обоснованно бухтеть в сторону непомерно раздутых игр от ведущих студий. И чтобы выпендриться перед коллегами, конечно! Ну и на практике наверняка пригодится...   Всего хорошего и не болейте!  НЛО прилетело и оставило здесь промокод для читателей нашего блога:  — 15% на заказ любого VDS (кроме тарифа Прогрев) — HABRFIRSTVDS"
94,94,Экосистема ПИК. PikTools — инструменты для автоматизации проектирования,ПИК,Ведущий девелопер в России,50.5,"Программное обеспечение, Оптимизация, Веб-сервисы",2025-03-12,"Привет, Хабр! Меня зовут Мария Фомичева, я руководитель продукта для автоматизации процессов BIM-проектирования PikTools. Это вторая статья из серии текстов про экосистему ПИК. Полное описание экосистемы можно найти в статье «Экосистема ПИК. История формирования».В данной статье мы рассмотрим историю формирования продукта PikTools, его состав, основные сложности и принятые решения, а также расскажем о наших стратегических планах на 2025 год.История формированияВ 2020 году решение по автоматизации проектирования представляло собой большое количество разрозненных плагинов, не было ни стандартов, ни методологий. Работа в таком формате стала приобретать неуправляемый характер — увеличивалось количество заявок c ошибками от  пользователей, была большая текучка в команде разработки.В связи с этим было принято стратегическое решение сформировать полноценный продукт со своей концепцией, методологией управления и контролем качества.Для этого были проведены масштабные изменения:разработан собственный фреймворк разработки RXBIM (подробнее о технической составляющей фреймворка можно ознакомиться в статье «Через тернии к ReactiveBIM»);описана документация по работе с фреймворком;разработаны шаблоны документации (технических заданий, пользовательских инструкций);описана методология создания запросов на разработку;определены роли и ответственность участников, введены регулярные встречи по планированию работ;разработана своя точная система оценки трудозатрат на разработку.Благодаря внедрению вышеописанных правил у нас получилось выстроить процессы управления продуктом. Мы смогли наладить процессы коммуникации, устранили проблему текучки кадров и можем получать предсказуемый и точный результат работы. На текущий момент вопрос легаси-проектов закрыт на 80% и будет полностью ликвидирован в 2025 году. Другими словами, при переходе на новую платформу PikTools мы отрубаем для пользователей старые плагины и предлагаем использовать инструменты в рамках нового продукта. О продуктеВ современном мире проектирования и инженерии автоматизация становится ключевым фактором повышения эффективности и качества работы. Процесс проектирования включает множество этапов и требует значительных трудозатрат, а также высокой степени координации между командами. PikTools предоставляет инструменты, которые помогают упростить и ускорить эти процессы, минимизируя ошибки и повышая общую продуктивность.Текущая концепция продукта PikTools — обеспечение гарантированного результата в короткие сроки, быстрая и эффективная поддержка инструментов и качественная обратная связь.PikTools представляет собой набор инструментов для САПР (системы автоматизированного проектирования):RevitAutoCADCivil3DМы создали PikTools для специалистов в области архитектуры, конструктива и инженерии. В каждой из этих сфер люди сталкиваются с уникальными задачами и требованиями в процессе проектирования. Поэтому инструмент был разработан с учетом потребностей различных направлений.PikTools состоит из 4-х модулей, каждый из которых фокусируется на конкретной области:АР (архитектурные решения)КР (несущие конструкции зданий и сооружений)ИОС (внутренние инженерные системы зданий)Общее (инструменты автоматизации общего назначения)Ниже представлены ключевые решения по автоматизации для специалистов обозначенных областей.Для архитекторов с помощью PikTools мы смогли автоматизировать:расчёт и создание ведомости отделки;расстановку и маркировку перемычек в проекте;размещение и прорезку отверстий по заданию от ИОС;назначение материалов фасадной плитке.Для конструкторов были автоматизированы:упрощенное армирование пилонов/колонн;заполнение эскизов арматурных форм в ведомости деталей;автоматическая расстановка арматуры по площади в плитах и стенах;упрощенное армирование стен и построение штриховок на схемах с вертикальным и горизонтальным армированием.Для инженеров:инструменты для инженеров ОВ/ВК (ускорение работы с построением модели, спецификациями);инструменты для инженеров ЭОМ/СС (оптимизация работы с кабелями, цепями, панелями);автоматизация работы с пространствами;параметризация элементов модели.Общие междисциплинарные инструменты, которые получилось автоматизировать:пакетная печать листов;пакетный экспорт спецификаций в XLSX и TSV;оптимизация работы с видами;автосоздание и обновление рабочих наборов;ускорение работы с аннотациями.С полным набором инструментов можно ознакомиться на сайте продукта.Сбор статистикиДля анализа и повышения эффективности продукта мы собираем статистику о запусках. Для логирования мы выбрали стек Elastic Search + Kibana. Кроме того мы реализовали дашборд с выводом нужных результатов в DataLense. Логи содержат всю необходимую информацию для формирования отчёта — ФИО сотрудника, отдел, дисциплину проектирования, дату и время запуска, имя ПК и т.д. Формирование такой отчётности помогает выявить активность использования необходимых инструментов для дальнейшего анализа этой информации для того, чтобы ускорить получение результата и повысить эффективность работы сотрудников. В планах по развитию на 2025 год также есть задача по созданию открытого модуля со статистикой для внешних клиентов.Управление изменениями и доставка обновленийРанее процесс установки и управления изменениями продукта осуществлялся с помощью скрипта, управляемого смежной командой IT-поддержки. Это создавало определённые риски, связанные с невозможностью осуществлять контроль доставки обновлений до пользователя на стороне команды продукта. В ответ на эту проблему появилась идея разработать десктопное приложение для мониторинга доставки обновлений.При разработке концепции приложения PikToolsBOX мы поставили перед собой следующие цели:Осуществление контроля в управлении изменениями продукта при помощи логов (Elastic Search+Kibana);Минимизация действий пользователя для обновления продукта;Своевременное получение информации об обновлениях;Возможность тестирования инструментов специалистами соответствующей квалификации в отдельной вкладке с контролем доступа.Для разработки был выбран самый оптимальный технологический стек:Backend: ASP.NET 8 + PostgresqlCI/CD: TeamCity + nukeDesktop: .NET8, WPF, ReactiveUIПриложение работает для Windows и Web API (ASP.NET + PostgreSQL) и предназначено для хранения данных для клиентских приложений.Возможности развёртывания:1. Можно развернуть Web API на внутреннем сервере заказчика и подключить авторизацию для гибкой настройки;2. Можно установить клиентам десктопное приложение с прописанным эндпоинтом (конечной точкой) до развёрнутого Web API.Надо сказать, что приложение PikTools включает функции управления версиями и отслеживания изменений, что позволяет командам быстро адаптироваться к новым требованиям и избегать путаницы. Контроль изменений и доставка обновлений происходит через десктоп-приложение PikTools BOX. Основными функциями приложения являются:доставка инструментов до пользователейавтоматическое обновление инструментовуведомления о новых версияхвкладка для тестирования сборокобновление самого приложенияТаким образом, внедрение приложения привело нас к ожидаемому результату и минимизации рисков, связанных с невозможностью осуществлять контроль доставки обновлений до пользователя. Кроме того, было принято решение расширить функционал приложения на другие продукты нашей экосистемы. Решив эту задачу, мы достигли единообразия в доставке продуктов до конечных пользователей, а также синхронизировали релизы всех продуктов.На текущий момент мы имеем один существенный минус — приложение нельзя использовать вне контура нашей компании. Поэтому у нас в планах расширение функционала системы доставки обновлений для внешних клиентов. Кстати, вы можете ознакомиться с демо-версией PikTools. Она включает бесплатный набор инструментов по каждому модулю в ограниченном количестве.Планы по развитиюВ 2025 году мы планируем реализовать несколько стратегически важных инициатив для развития нашего продукта. Основная задача — адаптировать PikTools к внешнему рынку и улучшить юзабилити.При стратегическом планировании мы ставили перед собой основную цель — привлечь новых клиентов и начать получать прибыль. Для этого нам необходимо организовать коммерческую инфраструктуру с помощью ряда доработок:Необходимо адаптировать инструменты под любую версию целевого ПО, что обеспечит совместимость и гибкость нашего решения.Создать один установочный файл .exe на весь продукт на замену нескольким для удобства установки.Создать дружественный для пользователя дизайн интерфейса в виде интуитивного пользовательского конфигуратора, с помощью которого можно будет управлять инструментами, переключаться между дисциплинами и подключать модуль статистики.Доработать систему доставки PikToolsBOX: создать внешний контур для размещения продуктов экосистемы и доработать управление видимостью продуктов для пользователей с лицензиями.Создать инфраструктуру для точечных продаж и управления лицензированием.Также запланированы доработки существующих инструментов, что позволит улучшить их функциональность и адаптировать их под потребности пользователей.Модуль для архитекторов планируем дополнить набором инструментов:«Отверстия» — размещение семейств отверстий АР в модели по заданию на отверстия от ИОС, автоматический анализ задания от ИОС, сравнение разных итераций заданий на отверстия, прорезка отверстий в модели по заданию ИОС, параметризация отверстий. «Квартирография» — заполнение данных по квартирографии на основе баз данных и создание отчетов для ДДУ, РМП, ТЭП.Модуль для конструкторов планируем дополнить набором инструментов:«Армировать по площади» — автоматическая установка армирования по площади в плитах и стенах.«АрмЛайт» — упрощенное массовое армирование стен и построение штриховок на схемах с вертикальным и горизонтальным армированием.«Fix» — ремонт армирования по площади, установленного при помощи инструментов «Армировать по площади» и «АрмЛайт», для возможности редактирования его контура при помощи «ручек».«Армирование пилонов» — автоматическое армирование пилонов и колонн, создание базы типовых пилонов и колонн, автоматическое армирование типовых пилонов и колонн при помощи базы«Сваи» — автоматизация работы со сваями: нумерация, отметки и прочее.«ГТО Ограждения» — заполняет параметры элементов ограждения котлована (трубы, шпунт Ларсена).«Автооформление» — установка размеров между осями, образмеривание вертикальных конструкций, фундаментов, отверстий на планах; образмеривание зон дополнительного армирования в плитах.Модуль для инженеров планируем дополнить набором инструментов:«Получить задание» — расстановка вспомогательных семейств с электронным коннектором в местах размещения оборудования смежных инженерных систем, а также копирование электрических параметров из оборудования.«Определение типа основания» — определение материала стены или потолка, на котором расположены экземпляры семейств, а также запись наименования материала в параметр «Семейства».«Импорт кабеля» — создание в модели вспомогательных семейств, параметры которых заполняются в .xls таблице-конфигураторе, формирование спецификации с семействами в категории «Кабель» с необходимыми характеристиками.«Принадлежность к ВРУ» — определение принадлежности экземпляров панелей, размещенных в модели к ВРУ и сравнение со схемой на листе.«Записать принадлежность к помещению» — копирование значения параметров помещений в параметры элементов, находящихся в границах помещения в соответствии с таблицей-конфигуратором.«Огнезащитные короба» — создание семейств огнезащитных коробов вокруг трассы кабельных лотков.Преимущества PikToolsИспользование продукта PikTools приносит ряд преимуществ.1)Сокращение времени разработки.В зависимости от вида работ удаётся экономить до нескольких часов в день. Кроме того, автоматизация рутинных задач позволяет инженерам сосредоточиться на более творческих аспектах проектирования. 2)Улучшение качества.Снижение человеческого фактора и автоматизация процессов помогают уменьшить количество ошибок в проектной документации до 30%.3)Снижение затрат. Оптимизация процессов приводит к снижению общих затрат на проектирование и разработку. По нашим подсчётам, экономический эффект от внедрения продукта доходит до 10%.4)Оперативная техподдержка и обратная связь. Наша команда экспертов готова помочь решить любые вопросы или проблемы, связанные с использованием продукта. Мы стремимся минимизировать время реакции на запросы и обеспечить быструю помощь. Консультирование в день обращения, устранение багов — 3-5 рабочих дней.Таким образом, PikTools представляет собой мощный инструмент для автоматизации проектирования, который помогает специалистам оптимизировать свои процессы, сокращать время разработки и улучшать качество итоговых продуктов. В условиях быстро меняющегося рынка и высоких требований к качеству, использование таких решений становится необходимостью для компаний, которые стремятся оставаться конкурентоспособными.Если вы ищете способ повысить эффективность вашего проектного процесса, PikTools может стать отличным выбором для вашей команды. Свяжитесь с нами, и мы с удовольствием ответим на ваши вопросы. Подробнее изучить PikTools можно на сайте продукта"
95,95,Публикация пакета npm с ESM и TypeScript,Timeweb Cloud,То самое облако,1677.09,Связь и телекоммуникации,2025-03-12,"   За последние 2 года поддержка ESM в TypeScript, Node.js и браузерах сильно улучшилась. В этой статье я объясню мою современную настройку, которая является относительно простой по сравнению с тем, что нам приходилось делать раньше:  она предназначена для пакетов, которые могут пренебречь обратной совместимостью. Настройка хорошо работает для меня некоторое время — начиная с TS 4.7 (24.05.2022)  помогает то, что Node.js теперь поддерживает require(esm) — запрос библиотек ESM из модулей CommonJS  я использую только tsc, но упоминаю поддержку других инструментов в разделе ""Компиляция TS с помощью других инструментов""  Обратная связь приветствуется: что вы делаете по-другому? Что может быть улучшено? Пример пакета: в @rauschma/helpers используется настройка, описываемая в этой статье. 1. Структура проекта Наш пакет npm будет иметь следующую структуру: my-package/   README.md   LICENSE   package.json   tsconfig.json   docs/     api/   src/   test/   dist/     src/     test/ Комментарии:  добавление файлов README.md и LICENSE, обычно, является хорошей идеей package.json описывает пакет (мы поговорим о нем позже) tsconfig.json настраивает TS (мы поговорим о нем позже) docs/api/ предназначен для документации API, генерируемой с помощью TypeDoc (мы поговорим об этом позже) src/ — для исходного кода TS test/ — для интеграционных тестов — тестов, охватывающих несколько модулей (о юнит-тестах мы поговорим позже) dist/ — для результата компиляции TS  1.1. .gitignore Для управления версиями я использую Git. Вот как выглядит мой .gitignore (находящийся в корневой директории проекта): node_modules dist .DS_Store  node_modules — директория с зависимостями проекта, как правило, не включается в удаленный репозиторий dist — результат компиляции TS не включается в удаленный репозиторий, но загружается в реестр npm .DS_Store — актуально только для пользователей macOS, хотя это можно настроить глобально  1.2. Юнит-тесты Обычно, я располагаю юнит-тесты для определенного модуля рядом с ним: src/   util.ts   util_test.ts Учитывая, что юнит-тесты помогают понять, как работает модуль, их должно быть легко искать. 1.2.1. Трюк дял тестов — циклическая ссылка Если пакет npm содержит exports, он может ссылаться сам на себя по названию пакета: // util_test.js import { helperFunc } from 'my-package/util.js'; Документация Node.js содержит больше информации о циклических ссылках и отмечает: ""Циклические ссылки доступны, только если package.json содержит exports. Импортировать можно только то, что разрешает exports"". Преимущества циклических ссылок:  полезны для тестов (которые показывают, как работает код) проверяют, что экспорты пакета настроены правильно  2. tsconfig.json В этом разделе мы рассмотрим основные моменты tsconfig.json. Дополнительные материалы:  моя статья ""Чеклист для tsconfig.json"" tsconfig.json @rauschma/helpers  2.1. Директория для результата {   ""include"": [""src/**/*"", ""test/**/*""],   ""compilerOptions"": {     // Определяем явно (не полагаемся на пути исходных файлов):     ""rootDir"": ""."",     ""outDir"": ""dist"",     // ···   } }  исходник — src/util.ts  результат — dist/src/util.js  исходник — test/util_test.ts  результат — dist/test/util_test.js   2.2. Результат Файл src/util.ts компилируется tsc в следующие файлы: dist/src/   util.js   util.js.map   util.d.ts   util.d.ts.map  util.js — код JS, содержащийся в util.ts util.js.map — карта исходников (source map) для кода JS. Благодаря этому файлу при запуске util.js мы получаем следующее:  в отладчике видим код TS трассировка стека содержит локации исходного кода TS  util.d.ts — типы, определенные в util.ts util.d.ts.map — карта исходников (определений, declaration map) для util.d.ts. Этот файл позволяет поддерживающим его редакторам TS переходить к исходному коду TS определения типа. Я нахожу это полезным для библиотек     Файл tsconfig.json     *.js.map ""sourceMap"": true   *.d.ts ""declaration"": true   *.d.ts.map ""declarationMap"": true    2.3. Компиляция TS с помощью других инструментов Компилятор TS выполняет 3 задачи:  Проверка типов. Генерация файлов JS. Генерация файлов определений (declaration files).  В настоящее время внешние инструменты могут выполнять последние две задачи намного быстрее, чем tsc. Следующие настройки помогают таким инструментам: ""compilerOptions"": {   //----- Помогает с генерацией .js -----   // Заставляет использовать `type` для импортов типов и др.   ""verbatimModuleSyntax"": true, // применяет ""isolatedModules""   // Запрещает неспецифичные для JS конструкции, такие как   // JSX, перечисления (enums), свойства параметров конструктора и пространства имен.   // Имеет важное значение для удаления типов   ""erasableSyntaxOnly"": true, // TS 5.8+    //----- Помогает с генерацией .d.ts -----   // - Запрещает выводить тип значения, возвращаемого экспортируемой функцией и др.   // - Может использоваться только совместно с `declaration` или `composite`   ""isolatedDeclarations"": true,    //----- tsc не генерирует файлы, только проверяет типы -----   ""noEmit"": true, } 3. package.json Некоторые настройки в package.json также влияют на TS. Мы рассмотрим их далее. Дополнительные материалы:  раздел ""Пакеты: единицы JS для распространения ПО"" ""Скриптов оболочки Node.js"" package.json @rauschma/helpers  3.1. Использование .js для ESM По умолчанию файлы .js интерпретируются как модули CommonJS. Следующая настройка включает режим ESM: ""type"": ""module"", 3.2. Файлы, загружаемые в реестр npm Необходимо определить, какие файлы должны загружаться в npm. Хотя существует .npmignore, явное перечисление включаемых файлов является более безопасным. Это делается с помощью свойства files файла package.json: ""files"": [   ""package.json"",   ""README.md"",   ""LICENSE"",    ""src/**/*.ts"",   ""dist/**/*.js"",   ""dist/**/*.js.map"",   ""dist/**/*.d.ts"",   ""dist/**/*.d.ts.map"",    ""!src/**/*_test.ts"",   ""!dist/**/*_test.js"",   ""!dist/**/*_test.js.map"",   ""!dist/**/*_test.d.ts"",   ""!dist/**/*_test.d.ts.map"" ], В .gitignore мы игнорируем директорию dist, поскольку она содержит файлы, генерируемые автоматически. Однако, здесь мы ее явно добавляем, поскольку большая часть содержащихся в ней файлов должна быть включена в пакет npm. Шаблоны, начинающиеся с восклицательного знака определяют исключаемые файлы. В нашем случае исключаются тесты:  одни тесты находятся рядом с модулями в src/ другие — в test/  3.3. Экспорты Если мы хотим, чтобы пакет поддерживал старый код, существует несколько настроек package.json, которые следует принять во внимание:  ""main"" — раньше использовалось Node.js ""module"" — раньше использовалось сборщиками ""types"" — раньше использовалось TS ""typesVersions"" — раньше использовалось TS  В современном коде нам требуется только одно свойство: ""exports"": {   // Экспорты пакета }, Перед тем, как двигаться дальше, ответим на 2 вопроса:  Наш пакет будет импортироваться только с помощью голого импорта (bare import) или он также будет поддерживать импорт субпутей (subpath)?  import { someFunc } from 'my-package'; // голый импорт import { someFunc } from 'my-package/sub/path'; // импорт субпутей  Если пакет поддерживает импорт субпутей, должны ли учитываться расширения файлов?  Для ответа на эти вопросы следует учитывать следующее:  стиль импорта без учета расширений файлов имеет давнюю традицию. Ситуация не сильно изменилась с появлением ESM, хотя для локальных импортов требуется указание расширений недостаток стиля без расширений (согласно документации Node.js): ""Карты импортов (import maps) сейчас являются стандартом для разрешения пакетов в браузерах и других средах выполнения JS. Использование стиля без расширений может привести к раздутой карте импортов. Явные расширения файлов позволяют карте импорта использовать сопоставление директорий пакетов для сопоставления нескольких подпутей, где это возможно, вместо отдельной записи карты для каждого экспорта подпути пакета. Это также удовлетворяет требованию использовать полный путь спецификатора в относительных и абсолютных спецификаторах импорта"".  Поразмыслив, я пришел к следующему:  большинство моих пакетов не содержит подпутей если пакет представляет собой коллекцию модулей, я экспортирую их с расширениями если модули больше похожи на разные версии пакета (например, синхронные и асинхронные варианты), я экспортирую их без расширений  3.3.1. Определение экспортов пакета // Голый экспорт ""."": ""./dist/src/main.js"",  // Субпути с расширениями ""./util/errors.js"": ""./dist/src/util/errors.js"", // один файл ""./util/*"": ""./dist/src/util/*"", // поддерево (subtree)  // Субпути без расширений ""./util/errors"": ""./dist/src/util/errors.js"", // один файл ""./util/*"": ""./dist/src/util/*.js"", // поддерево Заметки:  если модулей немного, то несколько файлов являются более описательными, чем одно поддерево по умолчанию файлы .d.ts должны находиться рядом с файлами .js. Это можно изменить с помощью условия импорта types  3.4. Импорты Импорты пакетов Node.js также поддерживаются TS. Они позволяют нам определять синонимы путей (path aliases). Преимущество синонимом в том, что они начинаются с верхнего уровня пакета. Пример: ""imports"": {   ""#root/*"": ""./*"" }, Этот импорт можно использовать следующим образом: import pkg from '#root/package.json' with { type: 'json' }; console.log(pkg.version); Для того, чтобы это работало, нужно разрешить импорт модулей JSON: ""compilerOptions"": {   ""resolveJsonModule"": true, } Импорты пакетов особенно полезны, когда итоговые файлы JS находятся гораздо глубже, чем исходные файлы TS. В этом случае мы не можем использовать относительные пути для доступа к файлам на верхнем уровне. 3.5. Скрипты Скрипты позволяют определять синонимы для таких команд оболочки, как build и запускать их с помощью npm run build. Получить список этих синонимов можно с помощью npm run (без названия скрипта). В своих проектах я использую следующие скрипты: ""scripts"": {   ""\n========== Сборка =========="": """",   ""build"": ""npm run clean && tsc"",   ""watch"": ""tsc --watch"",   ""clean"": ""shx rm -rf ./dist/*"",   ""\n========== Тестирование =========="": """",   ""test"": ""mocha --enable-source-maps --ui qunit"",   ""testall"": ""mocha --enable-source-maps --ui qunit \""./dist/**/*_test.js\"""",   ""\n========== Публикация =========="": """",   ""publishd"": ""npm publish --dry-run"",   ""prepublishOnly"": ""npm run build"" }, Комментарии:  build — директория dist очищается перед каждой сборкой. Зачем? При переименовании файлов TS старые файлы не удаляются. Это особенно проблематично для файлов с тестами test, testall:  --enable-source-maps включает поддержку карт исходников в Node.js, что добавляет аккуратные номера строк в трассировку стека Mocha поддерживает несколько стилей тестирования. Мне нравится --ui qunit (пример) publishd — мы публикуем пакет с помощью npm publish. npm run publishd вызывает --dry-run — версию команды, которая не вносит изменений, но предоставляет полезную обратную связь, например, показывает, какие файлы будут частью пакета prepublishOnly — этот скрипт вызывается перед загрузкой файлов в реестр npm. Выполняя сборку перед публикацией, мы убеждаемся, что не будут загружены старые файлы   Для чего нужны именованные разделители? Они облегчают чтение вывода npm run. Если пакет содержит скрипты bin, тогда может быть полезен следующий скрипт (вызываемый из build после tsc): ""chmod"": ""shx chmod u+x ./dist/src/markcheck.js"", 3.5.1. Генерация документации Для конвертации комментариев JSDoc в документацию API я использую TypeDoc: ""scripts"": {   ""\n========== TypeDoc =========="": """",   ""api"": ""shx rm -rf docs/api/ && typedoc --out docs/api/ --readme none --entryPoints src --entryPointStrategy expand --exclude '**/*_test.ts'"", }, В числе прочего, я разворачиваю GitHub Pages из docs/:  файл в репозитории — my-package/docs/api/index.html файл онлайн (пользователь robin) — https://robin.github.io/my-package/api/index.html  Можете взглянуть на документацию API @rauschma/helpers. 3.6. Зависимости для разработки Несмотря на то, что у моего пакета нет обычных зависимостей, ему требуются следующие зависимости для разработки: ""devDependencies"": {   ""@types/mocha"": ""^10.0.6"",   ""@types/node"": ""^20.12.12"",   ""mocha"": ""^10.4.0"",   ""shx"": ""^0.3.4"",   ""typedoc"": ""^0.27.6"" }, Комментарии:  @types/node — в юнит-тестах я использую node:assert для таких утверждений, как assert.deepEqual(). Эта зависимость предоставляет типы для этого и других модулей Node.js shx — предоставляет кроссплатформенную реализацию команд оболочки Unix. Я часто использую:  shx rm -rf shx chmod u+x   Я также устанавливаю еще 2 инструмента командной строки локально внутри моих проектов, чтобы гарантировать их наличие. Прикольной фичей npm run является то, что она добавляет локально установленные команды в path. Это означает, что они могут использоваться в скриптах пакета так, будто установлены глобально.  mocha и @types/mocha — я по-прежнему использую Mocha для тестирования, но встроенный тест-раннер Node.js стал интересной альтернативой typedoc — для генерации документации API я использую TypeDoc  4. Инструменты 4.1. Линтинг Общий линтинг:  publint — ""проверяет пакеты npm на совместимость с разными средами выполнения, такими как Vite, Webpack, Rollup, Node.js и др."" npm-package-json-lint — ""настраиваемый линтер для файлов package.json"" installed-check — ""проверяет, что установленные модули удовлетворяют требованиям (например, диапазону версий Node.js в engines), определенным в package.json"" Knip — ""находит и исправляет неиспользуемые файлы, зависимости и экспорты""  Линтинг модулей:  Madge — создает визуальный граф зависимостей модуля, обнаруживает циклические зависимости и др.  Линтинг типов TS:  arethetypeswrong — ""этот проект анализирует содержимое пакета npm на предмет проблем с типами TS, особенно проблем разрешения модулей ESM""  4.2. Инструменты для CommonJS Эти инструменты становятся все менее актуальными, поскольку все больше пакетов используют ESM, и запрос ESM из CommonJS (require(esm)) сейчас работает в Node.js достаточно хорошо:  tshy — TypeScript HYbridizer — компилирует TS в гибридные пакеты ESM/CommonJS ESM-CJS Interop Test — немного устаревший, но полезный список вещей, которые важно учитывать при импорте модулей CommonJS из ESM   Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
96,96,"DevOps нет, но вы держитесь: как разработчики запустили тесты на этапе MR",SL Soft,Разработчик российских бизнес-приложений,4.7,"Программное обеспечение, Консалтинг и поддержка",2025-03-12,"Со старта нашего проекта Polymatica EPM (бизнес‑платформа для автоматизации процессов стратегического планирования и бюджетирования) мы решили: код должен покрываться тестами. Проект построен на стеке FastAPI + Poetry + Pytest. Из‑за особенностей проекта тесты, в основном, функциональные. Все шло хорошо, команда росла, тесты писались, но запускались только на локальной машине перед коммитами. Наступил момент, когда нужно было внедрить автоматический прогон тестов на этапе Merge Request (MR).На тот момент у нас был собственный GitLab и настроенный CI/CD, но ресурсы DevOps были ограничены. Поэтому задачу пришлось решать силами разработчиков. Меня зовут Дмитрий Богданов, я старший бэкенд‑разработчик, и в этой статье расскажу, как мы оптимизировали запуск тестов, с какими проблемами столкнулись и почему выбрали именно базовый образ для CI/CD.Выбор подхода для запуска тестовКаждый раз устанавливать зависимости на CI/CD‑воркере или собирать новый образ.Минусы: долгое время сборки, загрязнение воркера лишними зависимостями.Использовать тот же image, который выкатывается на продакшен‑стенд.Минусы: возможные ошибки при сборке стенда, необходимость каждый раз доустанавливаем пакеты для тестирования, что опять же увеличивает время.Использовать базовый образ со всеми зависимостями, включая тестовые.Минусы: требует пересборки при изменении зависимостей, а после тестов нужно заново собирать продакшен‑образ.Мы выбрали третий вариант, так как он обеспечивал баланс между скоростью тестирования и удобством управления зависимостями.Особенности реализацииНачальная структура репозитория monorep/ ├── service_1/ │   ├── app/ │   ├── Dockerfile │   ├── poetry.lock │   └── pyproject.toml ├── service_2/ │   ├── app/ │   ├── Dockerfile │   ├── poetry.lock │   └── pyproject.toml ├── service_3/ │   ├── app/ │   ├── Dockerfile │   ├── poetry.lock │   └── pyproject.toml ├── .gitlab_ci.ymlmonorep/ — корневой каталог монорепозитория.service_1/, service_2/, service_3/ — подкаталоги с сервисами, каждый из которых содержит:Dockerfile — файл для сборки Docker‑образа.pyproject.toml — файл конфигурации для Poetry.poetry.lock — файл с зафиксированными зависимостями.app/ — каталог с кодом приложения, тесты находятся тут же.В корне монорепозитория находятся:.gitlab_ci.yml — файл конфигурации для GitLab CI/CD.Для реализации нашего варианта нам нужно собрать все зависимости из всех сервисов и собрать их вместе. Также нам потребуется Dockerfile для «базового образа».Собираем зависимостиНачнем по порядку — соберем все зависимости. В наше проекте каждый сервис содержит свои зависимости в poetry, в целом используется одинаковый стек, однако бывают специфические библиотеки (например polars). Для экспорта requirements.txt используем команду:poetry export --without-hashes ‑f requirements.txt ‑output requirements1.txtТеперь у нас есть несколько requirements.txt, можно объединить их вручную в один файл, но мы написали скрипт на python:import argparse from packaging.requirements import Requirement, InvalidRequirement   def parse_requirements(file_path):  dependencies = {}  options = set()  with open(file_path, 'r') as f:      for line in f:          line = line.strip()          if not line or line.startswith('#'):              continue                    if ' -- ' in line:              dep_part, options_part = line.split(' -- ', 1)              current_options = [' -- ' + opt.strip() for opt in options_part.split(' -- ')]              options.update(current_options)          else:              dep_part = line              current_options = []           dep_part = dep_part.split(';')[0].strip()          if not dep_part:              continue           try:              req = Requirement(dep_part)              dep_name = req.name              dependencies[dep_name] = dep_part          except InvalidRequirement:              print(f""⚠️ Ошибка парсинга: '{dep_part}' в файле {file_path} пропущена."")              continue   return dependencies, options  def main():  parser = argparse.ArgumentParser(description='Объединяет несколько requirements.txt')  parser.add_argument('files', nargs='+', help='Список файлов для объединения')  parser.add_argument('-o', '--output', default='requirements_all.txt', help='Выходной файл')  args = parser.parse_args()   all_options = set()  combined_deps = {}  seen_files = set()   for file_path in args.files:      if file_path in seen_files:          continue      seen_files.add(file_path)            deps, opts = parse_requirements(file_path)      all_options.update(opts)            for dep_name, dep_spec in deps.items():          if dep_name in combined_deps:              print(f""⚠️ Конфликт: {dep_name} заменен на версию из {file_path} ({dep_spec})"")          combined_deps[dep_name] = dep_spec   sorted_options = sorted(all_options)  sorted_deps = sorted(combined_deps.items(), key=lambda x: x[0].lower())   with open(args.output, 'w') as f:      if sorted_options:          f.write('\n'.join(sorted_options) + '\n\n')      for dep_name, dep_spec in sorted_deps:          f.write(f""{dep_spec}\n"")   print(f""✅ Файл {args.output} успешно создан!"")  if __name__ == '__main__':  main()Для использования:pip install packaging python merge_requirements.py requirements1.txt requirements2.txt requirements3.txt -o requirements_all.txtПишем Dockerfile для базового образаFROM python:3.10.12-slim  RUN pip install --no-cache-dir --upgrade pip  COPY ./requirements_all.txt requirements_all.txt RUN pip install -r requirements_all.txtИтоговая структура репозиторияmonorep/ ├── service_1/ │   ├── app/ │   ├── Dockerfile │   ├── poetry.lock │   └── pyproject.toml ├── service_2/ │   ├── app/ │   ├── Dockerfile │   ├── poetry.lock │   └── pyproject.toml ├── service_3/ │   ├── app/ │   ├── Dockerfile │   ├── poetry.lock │   └── pyproject.toml ├── .gitlab_ci.yml ├── Dockerfile_gitlab └── requirements_all.txtCI и настройка GitlabСборку базового образа вынесем в отдельный шаг. Его пересборка будет достаточно редкой, так как будет нужна только при добавлении/удалении библиотеки. Также нам нужно будет создать шаги для запуска тестов на этапе MR и при сборке образа для деплоя.image: alpine   variables:   PRETEST: pretest    stages:   - pretest   - test   - dockerize  pretest:   stage: pretest   image: alpine:latest   only:  changes:    - requirements_all.txt    - Dockerfile_gitlab   script:  - apk add --no-cache bash docker  - IMAGE=${CI_REGISTRY_IMAGE}/${PRETEST}  - DOCKERFILE=""-f Dockerfile_gitlab""  - docker build $DOCKERFILE -t ${IMAGE}:$IMAGE_VERSION .  - docker push ${IMAGE}:$IMAGE_VERSION  - docker tag ${IMAGE}:$IMAGE_VERSION ${IMAGE}:latest  - docker push ${IMAGE}:latest  - echo $IMAGE:$IMAGE_VERSION > IMAGE_${PRETEST}   artifacts:  paths:    - IMAGE_/c{PRETEST}   when: manual   allow_failure: true   ### Test ###   .test_template: &test_template   stage: test   image: ${CI_REGISTRY_IMAGE}/${PRETEST}:latest   allow_failure: true   rules:  - if: $CI_PIPELINE_SOURCE == ""merge_request_event"" && $CI_JOB_NAME == ""service1:test""    changes:      - service1/**/*    when: always  - if: $CI_PIPELINE_SOURCE == ""merge_request_event"" && $CI_JOB_NAME == ""service2:test""    changes:      - service2/**/*    when: always  - if: $CI_PIPELINE_SOURCE == ""merge_request_event"" && $CI_JOB_NAME == ""service3:test""    changes:      - service3/**/*    when: always  - if: $CI_COMMIT_REF_NAME =~ /^(dev|stage)$/ && $CI_JOB_NAME == ""service1:test""    changes:      - service1/**/*    when: always  - if: $CI_COMMIT_REF_NAME =~ /^(dev|stage)$/ && $CI_JOB_NAME == ""service2:test""    changes:      - service2/**/*    when: always  - if: $CI_COMMIT_REF_NAME =~ /^(dev|stage)$/ && $CI_JOB_NAME == ""service3:test""    changes:      - service3/**/*    when: always  - when: never   script:  - cd ${SERVICE}_service  - python --version  - |    if [[ -f ""migrate.py"" ]]; then      python migrate.py    fi  - pytest tests -vv --color yes --cov --cov-report term --cov-report xml:coverage.xml --junitxml=report.xml  - cd ..   coverage: '/(?i)total.*? (100(?:\.0+)?\%|[1-9]?\d(?:\.\d+)?\%)$/'   artifacts:  reports:    coverage_report:      coverage_format: cobertura      path: ${SERVICE}_service/coverage.xml    junit: ${SERVICE}_service/report.xml   service1:test:   variables:  SERVICE: service1   <<: *test_template   service2:test:   variables:  SERVICE: service2   <<: *test_template   service3:test:   variables:  SERVICE: service3   <<: *test_template  ### Dockerize ###   .dockerize_template: &dockerize_template   stage: dockerize   image: alpine:latest   rules:  - if: $CI_PIPELINE_SOURCE == ""merge_request_event""    when: never  - if: $CI_COMMIT_REF_NAME =~ /^(dev|stage)$/ && $CI_JOB_NAME == ""service1:dockerize""    changes:      - service1/**/*    when: on_success    allow_failure: true  - if: $CI_COMMIT_REF_NAME =~ /^(dev|stage)$/ && $CI_JOB_NAME == ""service2:dockerize""    changes:      - service2/**/*    when: on_success    allow_failure: true  - if: $CI_COMMIT_REF_NAME =~ /^(dev|stage)$/ && $CI_JOB_NAME == ""service3:dockerize""    changes:      - service3/**/*    when: on_success    allow_failure: true  - when: never   script:  - apk add --no-cache bash docker  - IMAGE=${CI_REGISTRY_IMAGE}/${SERVICE}  - DOCKERFILE=""-f ${SERVICE}_service/Dockerfile""  - docker build $DOCKERFILE -t ${IMAGE}:$IMAGE_VERSION .  - docker push ${IMAGE}:$IMAGE_VERSION  - echo $IMAGE:$IMAGE_VERSION > IMAGE_${SERVICE}   artifacts:  paths:    - IMAGE_${SERVICE}       service1:dockerize:   needs:  - job: ""service1:test""    optional: true   variables:  SERVICE: service1   <<: *dockerize_template   service2:dockerize:   needs:  - job: ""service2:test""    optional: true   variables:  SERVICE: service2   <<: *dockerize_template   service3:dockerize:   needs:  - job: ""service3:test""    optional: true   variables:  SERVICE: service3   <<: *dockerize_template О том, как посмотреть результаты тестов, хорошо описано в официальной документации Gitlab.ИтогиМы используем этот подход уже более года, и он доказал свою эффективность:среднее время прохождения тестов — 2–3 минуты на сервис,тесты выполняются автоматически при MR, избавляя от ручного запуска,базовый образ минимизировал время установки зависимостей.Сейчас мы прорабатываем новую стратегию, так как часть сервисов выносятся из монорепозитория. Но наш опыт показывает, что базовый образ — отличное решение для ускорения тестов в CI/CD."
97,97,Новый диск в облаке Selectel — настройте IOPS под свои задачи,Selectel,IT-инфраструктура для бизнеса,4.45,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-12," Если вы работаете с облачными ресурсами и управляете инфраструктурой, то наверняка знаете, как важно правильно настроить дисковую подсистему. В Selectel появилась новая возможность: теперь в облаке можно самостоятельно регулировать производительность дисков, не привязываясь к фиксированным конфигурациям.  Это значит, что больше не нужно заранее выбирать «с запасом» или рисковать нехваткой производительности в пиковые моменты. Вы просто настраиваете параметры диска под свою нагрузку — и меняете их на лету. Без даунтайма, сложных миграций или дополнительных настроек.  Мы в Selectel готовим новый сервис. Если арендуете серверы в рабочих или личных проектах, нам очень поможет ваш опыт — записывайтесь на короткое онлайн-интервью. За участие подарим плюшевого Тирекса и бонусы на услуги Selectel.    Используйте навигацию, если не хотите читать текст полностью: → Какие типы дисков бывают → Как устроены сетевые диски → Новый тип дисков — SSD Универсальный v2 → Экономика решения и калькулятор → Как создать новый диск и подключить его к серверу → Заключение  Какие типы дисков бывают Когда речь идет о дисках для облачных серверов, важно различать два основных типа: локальные и сетевые. Каждый подходит для разных задач, поэтому важно понимать, какой вариант подойдет под ваш сценарий.  Локальные — диски, которые подключены напрямую к физическому серверу через высокоскоростной PCIe-интерфейс (например, NVMe). Они обеспечивают минимальные задержки и максимальную скорость передачи данных — за счет того, что трафик не проходит через сеть.  Однако есть нюанс: такие диски привязаны к конкретному серверу и не могут свободно мигрировать между машинами. Если сервер выходит из строя, доступ к данным тоже теряется до момента физического восстановления.  Сетевые диски — это более гибкие решения с подключением через сеть. Они могут быть перенесены между серверами без физической миграции. Это оптимальный выбор для сценариев, когда важно ситуативно масштабировать хранилище или работать с данными, объем которых может расти со временем, — например, с логами, резервными копиями или рабочими базами данных.  В облаке Selectel для сетевых дисков действует трехкратная репликация: каждый том автоматически дублируется на три независимых физических узла. Это обеспечивает высокую сохранность данных даже в случае аппаратных сбоев.   Как устроены сетевые диски Сетевые диски в облаке работают как блочные устройства — они не привязаны к конкретному серверу. Это значит, что их можно подключать к серверу, отсоединять и даже переносить на другой сервер, проект или аккаунт. При этом диск физически остается на платформе, а его привязка меняется.  Почему сетевые диски — это удобно  Масштабируемость: вы можете увеличивать объем дисков без изменения конфигурации сервера. Гибкость: легко подключать и отключать диски, перемещая их между серверами. Можно создавать вместе с сервером или отдельно, использовать как загрузочный или подключать как дополнительный. Надежность: с репликацией данных на нескольких уровнях вы всегда защищены от потерь.  Управлять сетевыми дисками можно через панель управления, OpenStack CLI или Terraform. Для задач с переменной нагрузкой, например, в пиковые моменты, когда нужно быстро увеличивать ресурсы, сетевые диски будут значительно удобнее и выгоднее, чем локальные.  Если для задач важны минимальные задержки (например, в базах данных с интенсивными запросами), локальные диски предложат лучшее время отклика. Новый тип дисков — SSD Универсальный v2 Теперь, когда мы разобрались, что такое локальные и сетевые диски, давайте разберем, что именно появилось нового. В облаке Selectel теперь доступен первый в России сетевой SSD-диск с гибкими характеристиками производительности — «Универсальный v2». Его ключевая особенность — возможность регулировки лимита IOPS (количество операций ввода-вывода в секунду), не привязываясь к размеру диска.  В чем отличие от других дисков Обычно при выборе диска в облаке вам предлагают фиксированные типы: например, «быстрый» или «универсальный», где объем и IOPS связаны между собой. То есть чем больше диск — тем больше операций в секунду.  С новым диском подход меняется — объем и производительность можно настраивать независимо. Нужна высокая скорость работы даже для небольшого диска? Пожалуйста. Нужно урезать IOPS для экономии бюджета? Такой вариант тоже доступен.  Как это работает  В панели управления вы указываете, сколько IOPS вам нужно сейчас. Минимум — 2 000, максимум — 16 000. Если нагрузка на систему выросла (например, акция или сезонный пик), можно сразу повысить производительность. Как только ажиотаж спадает, IOPS можно вернуть к исходным значениям и перестать платить за лишние ресурсы.  Все изменения происходят «на лету» — без остановки серверов, миграций или изменения архитектуры. Когда пригодится  Планируемая повышенная нагрузка. Если вы ждете сезон повышенной активности (черная пятница, праздники), то диск «Универсальный v2» будет более выгодным вариантом. На дисках с большими размерами экономия станет особенно ощутимой.  Проблемы с максимальной производительностью. Например, вам нужно большее количество IOPS, но не требуется большой объем диска. Универсальный SSD позволяет точно настроить эти параметры без переплат.  Сложности с планированием для новых систем. При запуске новой информационной системы бывает непросто оценить нужную производительность диска заранее. В этом случае можно первоначально использовать небольшой лимит производительности диска, а при необходимости увеличивать его через панель управления без простоев и за считанные минуты.  Кейсы  Что происходит  Как помогает диск «Универсальный v2»    Онлайн-магазин  Черная пятница, всплеск заказов  Быстро наращиваем IOPS, не трогаем остальную инфраструктуру    SaaS-сервис  Внезапный рост пользователей  Масштабируем дисковую подсистему «на лету»    Финтех  Высокие требования к скорости доступа к данным  Настраиваем нужную производительность заранее или в моменте    Система мониторинга  Увеличился поток логов  Повышаем лимит IOPS для быстрой записи данных   Экономика решения и калькулятор Вы платите только за те ресурсы, которые действительно нужны. Верхней границы производительности — 16 000 IOPS — достаточно для CRM-систем и мониторинга. Это общее количество IOPS, без разделения на чтение и запись — тип операции не имеет значения в рамках лимита «Универсальный v2».  В базовую конфигурацию диска входят бесплатные 2 000 IOPS. Далее стоимость за один дополнительный IOPS — 0,29 ₽/мес. Пропускная способность диска — 200 МБ/c. Цена за 1 ГБ универсального диска v2 на 50% меньше, чем у стандартного универсального — 9,30 ₽/мес. Минимальный размер диска — 5 ГБ. Рассмотрим пример расчета:   Вы можете рассчитать стоимость для вашего сценария в калькуляторе. Как создать новый диск и подключить его к серверу Сетевой диск можно создать вместе с облачным сервером или отдельно, а затем подключить к серверу. Локальный — только вместе с облачным сервером.  Сетевой диск можно создать из разных источников:  пустой — изначально такой диск не содержит данные. Вы можете использовать его для масштабирования дискового пространства на облачном сервере; из образа — подготовленного Selectel или вашего собственного загруженного образа. Можно использовать для замены загрузочного диска при восстановлении сервера или для клонирования сервера; из другого диска, снапшота или бэкапа.  В панели управления 1. В панели управления перейдите в раздел Продукты → Облачные серверы.   2. Перейдите в раздел Диски и нажмите Создать диск.   3. Введите имя диска или оставьте сгенерированное. Выберите пул и сегмент пула.   Это должен быть сегмент пула облачного сервера, к которому вы в дальнейшем подключите диск. Сегмент пула влияет на стоимость и лимиты диска.  4. В поле Источник нажмите Выбрать. Подберите один из источнков диска и нажмите Выбрать:   готовый образ,  собственный образ,  диск,  снапшот,  бэкап.    Пропустите шаг, если хотите создать пустой диск. 5. В разделе Конфигурация выберите тип тиска SSD Универсальный v2.   6. Укажите размер диска и количество IOPS. Последний параметр вы сможете скорректировать после создания диска. При этом количество изменений IOPS не ограничено.  Важно: если диск создан из источника, то его размер должен совпадать или превышать размер источника.   7. Ознакомьтесь с разделом Цена, проверьте настройки и нажмите Создать диск.   Стоимость сетевого диска зависит от типа диска и сегмента пула, в котором вы его создаете. 8. Новый диск будет создан в пределах пары минут, после чего вы сможете найти его в разделе Облачные серверы → Диски. Здесь вы можете подключить его к облачному серверу, а также настроить планы бэкапов.   Новый сетевой диск в панели управления.   Подключение диска к серверу.    Настройка бэкапов. Во всплывающем окне приведены ссылки на инструкции о создании и конфигурировании бэкапов.  В OpenStack CLI Если вы работаете с OpenStack CLI, то процесс создания и настройки сетевого диска такой же простой. Вы можете прописать все параметры диска в конфигурации и автоматически задать необходимые настройки для серверов.  openstack volume create    [--image <image> | --snapshot <snapshot> | --source <volume>] \    --size <size> \    --type <volume_type> \    --property total_iops_sec='<iops_number>'    --availability-zone   \    <volume_name>  Тип источника  Для создания пустого диска не указывайте тип источника; --image ""image"" — для создания диска из готового или собственного образа. Параметр ""image"" — ID или имя образа. Список образов можно посмотреть с помощью команды openstack image list; --snapshot ""snapshot"" — для создания диска из снапшота. Параметр ""snapshot"" — ID или имя снапшота. Список снапшотов можно посмотреть с помощью команды openstack snapshot list; --source ""volume"" — для создания диска из другого сетевого диска. Параметр ""volume"" — ID или имя диска. Список сетевых дисков можно посмотреть с помощью команды openstack volume list.   Размер диска ""size"" — размер диска в ГБ. Для дисков из источника минимальный размер должен быть равен размеру источника. Помимо прочего, важно учитывать лимиты сетевых дисков на максимальный размер.  Тип диска и настройка IOPS <volume_type> — ID или имя типа сетевого диска. Например, universal2.ru-9a — имя для создания сетевого диска с типом SSD Универсальный v2 в сегменте пула ru-9a. Список типов можно посмотреть с помощью команды openstack volume type list или в документации Selectel.  --property total_iops_sec='<iops_number>' — для указания количества IOPS. Параметр <iops_number> — общее количество операций чтения и записи. Доступные значения — от 2 000 до 16 000 IOPS. Если оставить пустым, то будет использовано значение по умолчанию — 2 000 IOPS.  Сегмент пула и имя диска <pool_segment> — сегмент пула, в котором будет создан сетевой диск, например ru-9a. Список доступных сегментов пула можно посмотреть в документации. <volume_name> — имя диска.  Создать и настроить сетевой диск можно в Terraform. Подробно рассмотрели процесс в отдельной инструкции. Заключение С запуском сетевого диска «Универсальный v2» в облаке Selectel вы получаете инструмент для гибкой настройки производительности под конкретные задачи. Это решение закрывает сразу несколько важных потребностей:   Помогает сэкономить, оплачивая только те ресурсы, которые вы используете; Дает возможность быстро адаптировать производительность под текущую нагрузку без миграций и простоев; Упрощает планирование. При запуске новой системы можно начинать с минимальных параметров, а далее — оперативно наращивать их по мере необходимости; Сохраняет удобство управления — все настройки можно менять в панели управления, через CLI или Terraform. "
98,98,"Читая вакансии: о разнице между дизайнером продукта, его менеджером, оунером и UX-дизайнером",SSP SOFT,🔹 Более 15 лет занимаемся заказной разработкой ПО,72.27,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-03-12,"Привет хабр! Если вы находитесь в поиске работы по вакансиям из заголовка, то, скорее всего, встречали вакансии, где работодатель явно не понимает (или лукавит), кого именно он ищет. В одной должности требуют UX-дизайн, управление продуктом и даже бизнес-аналитику. Т.е., HR-ы ждут прихода универсала, который и продукт придумает, и интерфейс нарисует, и еще командой будет управлять, при этом согласившись на KPI (по сути, штрафы за неудачи с продуктом). Если не хотите перегружать себя обязанностями «за того парня», стоит хоть наискосок пробежаться взглядом по статье. Дизайнер продукта vs UX-дизайнер: в чем разница?Многие работодатели путают Product Designer и UX Designer, предполагая, что это одно и то же. Однако разница принципиальна. Дизайнер продукта (Product Designer) — это не просто сотрудник, который в том числе придумывает фичи и эскизы экранчиков, а визионер, который проектирует продукт с учетом бизнес-целей. UX-дизайнер, со своей стороны, сосредоточен исключительно на том, как пользователи взаимодействуют с продуктом, устраняя неудобства и делая интерфейс дружественным к пользователю.Дизайнер продукта играет руководящую роль в развитии продукта, предлагая и контролируя различные элементы проекта в области функциональности. Иными словами, этот человек — тот изобретатель, который создает продукт как набор функций и затем работает над дальнейшим его совершенствованием. UX-дизайнер — это тоже не совсем дизайнер в том смысле, как все мы привыкли отождествлять профессию с созданием веб-графики, рекламных макетов и т.д. Служебные обязанности этого сотрудника сосредоточены на пользовательском опыте и на том, как наилучшим образом клиенты могут использовать продукт. Задача — сделать так, чтобы пользоваться продуктом можно было максимально просто, интуитивно и эффективно.Где в этой системе Product Owner и Product Manager?Эти две роли вообще не про дизайн. Но они тесно связаны с процессом разработки продукта. Product Manager (PM) отвечает за стратегию и развитие продукта, определяет, какие функции приносят бизнес-ценность.Product Owner (PO) управляет бэклогом, ставит задачи разработчикам и следит за процессом разработки.РольОсновной фокусКлючевые задачиСвязь с бизнесомProduct DesignerUX/UI + стратегияПроектирует продукт с учетом пользовательских и бизнес-потребностей, проводит исследования, руководит разработкой прототиповФормирует удобный и востребованный продуктProduct Manager (PM)Бизнес-стратегияОпределяет стратегию развития продукта, анализирует рынок, ставит целиОпределяет, какие фичи разрабатыватьProduct Owner (PO)Agile и разработкаУправляет бэклогом, пишет требования, работает с командой разработки и девопсКонтролирует процесс создания фичИсторически, роль дизайнера продукта возникла с началом промышленной революции в 19 веке. Затем она в том или ином виде просуществовала на протяжении почти пары столетий, а цифровой век придал этому термину новое значение.  Еще раз касаясь истории технологий, дизайнеры продуктов в “доцифровые” времена исполняли классические задачи изобретателя. В центре внимания находились потребности людей, при этом идея продукта ограничивалась производственными возможностями. К примеру, братья Райт спроектировали первые самолеты, основываясь на существующих тогда производственных возможностях (дерево, ткань, автомобильный мотор), не занимаясь созданием новых авиационных технологий.Стоит отметить, что в росстйских компаниях продакт-оунер и продакт-менеджер могут быть как отдельные позиции, так и в одном лице. Практики разделения в РФ почти нет.Дизайнер продукта: архитектор идеи и куратор процессаУспех продукта зависит от множества факторов, но главный — это востребованность предлагаемой функциональности у пользователей. В результате, в обязанности дизайнера продуктов входят постоянные и подробные  исследования рынка и трендов, чтобы полностью понять потребности аудитории своих пользователей. Кроме того, хотя удовлетворенность пользователей, как правило, входит в сферу деятельности и обязанностей UX-дизайнеров, дизайнеры продуктов также должны в ней разбираться.Дизайнеры продуктов проводят моделирование UI совместно с дизайнером UX/UI, чтобы добиться минималистичного, но эстетически привлекательного и информативного внешнего вида программного продукта.Поскольку поиск лучшей идеи для продукта включает ""представление себя на месте пользователя"", дизайнеры продуктов совместно с маркетологами могут проводить опросы в соцсетях, мессенджерах и по электронной почте, чтобы определить предпочтения пользователей. Затем используют лучшие отзывы при создании wireframes (визуализации цифрового продукта) и прототипов.Дизайнеры продуктов также являются движущей силой внутрикомандного сотрудничества, а также сотрудничества со сторонними исполнителями (маркетинговыми агентствами, аутсорс-командами разработки и др.), и, естественно, представителями бизнес-подразделений в своей компании.Должностная инструкция современного дизайнера продуктов включает в себя такой набор обязанностей:Разработка концепции и ключевых фич продукта.Исследование рынка, пользователей и трендов.Проектирование структуры и логики работы продукта.Создание прототипов и визуализация идей.Сотрудничество с разработчиками, маркетологами и бизнес-аналитиками.Улучшение текущих версий продукта на основе данных и обратной связи.В крупных компаниях Product Designer занимает ключевую роль, разрабатывая концепции, исследуя рынок и координируя работу команд. В мелких фирмах эта должность может сливаться с Product Owner или даже Product Manager. Если вам предлагают роль «дизайнера продукта» в небольшой фирме, внимательно смотрите обязанности — вас могут нагрузить многими менеджерскими задачами в ущерб креативу и визионерству.Какие навыки нужны дизайнеру продуктов?НавыкОписаниеДизайн и прототипированиеСоздание интерактивных макетов, работа с пользовательскими сценариями (User Flow), вайрфрейминг, визуальный дизайн.Исследования и аналитикаСбор и анализ пользовательских данных, проведение UX-исследований, юзабилити-тестирование, интерпретация метрик.Бизнес-мышлениеПонимание целей бизнеса, работа с продуктовой аналитикой, учет коммерческих KPI при проектировании интерфейсов.Взаимодействие с командойПрезентация и защита дизайн-решений, работа с разработчиками, продукт-менеджерами и маркетологами.Управление временемПланирование задач, работа в условиях дедлайнов, приоритизация фич и эффективное распределение ресурсов.Резюме: Дизайнер продуктов объединяет понимание бизнес-целей продукта, предпочтения пользователей, UX/UI-дизайн и аналитику чтобы создавать удобные и востребованные цифровые решения. UX-дизайнер: исследователь пользовательского опытаUX-дизайнеры должны уметь анализировать лучшие стороны конкурирующих продуктов, отклики на MVP и формулировать списки «болей» пользователей, которые сможет решить их продукт. Они изучают потребности аудитории, выявляют проблемные места во взаимодействии с продуктом и предлагают улучшения. UX-дизайнеры также работают со сценариями, связанными с юзабилити и совместимостью продукта с привычками и образом мышления пользователей.Например, еще лет 20-30 назад, когда понятие UX еще не было в почете, вполне обычным делом было послать пользователя изучать документацию, перед тем как что-то начать делать с помощью программного продукта в первый раз. Затем пришло понимание, что молодое поколение не будет так поступать, они хотят начинать пользоваться большинством функций с первого момента контакта с продуктом, а если что-то им непонятно или нелогично, то просто забрасывают такой продукт и переходят к конкурирующему софту. Итак, UX-дизайнеры отвечают за удобство, понятность и логику взаимодействия с продуктом. Они не принимают бизнес-решений, но активно работают с пользователями, тестируют гипотезы и совершенствуют интерфейсы.Основные обязанности UX-дизайнера:Проектирование пользовательского опыта (UX).Разработка интерактивных прототипов и макетов.Проведение юзабилити-тестирования и интервью с пользователями.Анализ пользовательских сценариев и выявление болевых точек.Улучшение навигации и логики взаимодействия.Взаимодействие с командами разработки и аналитики.Исследования, которые проводят UX-дизайнеры, бывают двух типов:Качественные — помогают понять мотивы пользователей: почему они совершают определенные действия, что вызывает у них трудности. Для этого UX-дизайнеры проводят опросы, тесты и анализируют поведенческие паттерны.Количественные — помогают понять что именно делают пользователи: какие кнопки они чаще нажимают и в какой последовательности, сколько времени проводят на веб-странице или в проложении и т. д. Такие данные фиксируются в аналитических системах.В практике многих компаний принято, что перед внедрением новой функции или иных улучшений, UX-дизайнеры представляют свою работу команде. Сюда входит показ макетов (mock-up) и объяснение того, какие проблемы пользователей новая фича решает, почему выбран именно такой подход и какие будут плюсы и минусы. Хорошая презентация помогает согласовать роадмап продукта с заказчиком (бизнес-подразделением) и убедить команду в правильности выбранных решений.Какие навыки нужны UX-дизайнеру?НавыкОписаниеПрототипирование и дизайнСоздание интерактивных прототипов, вайрфреймов, разработка UI-решений для улучшения пользовательского опыта.Пользовательские исследованияСбор обратной связи, проведение интервью, тестирование интерфейсов, анализ поведения пользователей.Юзабилити и аналитикаОценка удобства интерфейсов, работа с метриками, анализ пользовательских сценариев и pain points.Взаимодействие с командойРабота с разработчиками, продукт-менеджерами, аналитиками; презентация и защита дизайн-решений.Гибкость и адаптивностьУмение быстро вносить изменения, адаптироваться к новым данным и требованиям пользователей.Управление временемРабота в условиях сжатых сроков, приоритизации задач, соблюдения баланса между качеством и скоростью разработки.Резюме: UX-дизайнер — это не просто художник интерфейсов и тестировщик юзабилити, а специалист, который анализирует поведение пользователей, тестирует решения и делает продукт удобным. Тем самым, UX-дизайнер делает продукт понятным и эффективным, улучшая взаимодействие пользователей с цифровыми сервисами. Вывод: не путайте роли и не ведитесь на вакансии-ловушкиЕсли вы смотрите вакансии, отделяйте дизайнерские роли от управленческих. Если в вакансии Product Designer от вас требуют бизнес-аналитику и управление бэклогом — это уже ближе к Product Owner. Если UX-дизайнеру предлагают заниматься стратегией продукта — значит, компания не доросла до зрелости или HR не до конца понимает, кого ищет.Еще раз напоследок:Дизайнер продукта проектирует решение с учетом бизнеса и UX.UX-дизайнер делает это решение удобным для пользователей.Product Owner следит за реализацией идей.Product Manager определяет, какие идеи реализовывать согласно таймлайну разработки.И если HR-ы пишут в вакансии «... ищем UX/UI/Product/Business Analyst в одном лице», задайте им один простой вопрос: вы будете платить за все эти совмещения профессий? Немного HR-рекламы от нашего блога: мы занимаемся заказной разработкой ПО и будем рады резюме специалистов, готовых работать оффлайн в Москве и Томске, а также удаленно из любой точки России. Текущие вакансии на нашей странице на hh.ru. Если вашей специальности нет в списке вакансий, не стесняйтесь прислать нам резюме — в SSP SOFT новые позиции открываются регулярно. Резюме можно направить в Telegram или на почту job@ssp-soft.com.Успехов на техсобесах на вакансии дизайнера продуктов, UX-дизайнера, Product Owner и Product Manager!"
99,99,Нововведения Java 24,PVS-Studio,"Статический анализ кода для C, C++, C# и Java",329.24,"Программное обеспечение, Информационная безопасность",2025-03-12,"Уже скоро, 18 марта, выйдет новая версия Java. Поэтому предлагаю посмотреть, какие в ней будут новшества, включая финализацию давно ожидаемых Stream Gatherers!Порядок нововведений (JEP — JDK Enhancement Proposal) выбран по собственной оценке их ""интересности"", а не по их нумерации.JEP 485: Stream GatherersКак известно, операции Stream API разделяются на промежуточные (порождающие новый Stream) и терминирующие (создающие результат или имеющие побочный эффект). Однако у терминирующих есть collect(Collector), позволяющий создавать свои операции посредством реализации Collector. А вот список промежуточных расширить нельзя: ограничиваемся map, flatMap, filter, distinct, sorted, peek. Во всяком случае, так было до Java 24, ведь в ней введены Stream Gatherers.Ключевые моменты нововведения состоят в следующем:1. В java.util.stream.Stream добавлен метод gather(Gatherer).2. Добавлен интерфейс java.util.stream.Gatherer. Его спецификация заключается в четырёх методах:initializer — создание изначального промежуточного состояния. Использует Supplier;integrator — обработка элементов, опциональное использование промежуточного состояния, отправка результата далее в поток. Использует новый функциональный интерфейс Integrator;combiner — объединение состояний. Использует BinaryOperator;finisher — последняя операция использования промежуточного состояния и отправки результата далее в поток (после потребления всех элементов). Использует BiConsumer.3. Добавлен класс java.util.stream.Gatherers, который содержит несколько стандартных реализаций Gatherer:fold — подобная reduce операция;mapConcurrent — map с использованием Virtual Threads;scan — инкрементальная аккумулирующая операция;windowFixed — стандартная реализация Fixed Window;windowSliding — стандартная реализация Sliding Window.Объяснение Fixed Window и Sliding WindowSliding Window, или метод скользящего окна, подразумевает создание окна или диапазона размера N на входных данных и затем движение этого окна (смещение). Это легко показать на простой коллекции чисел (окно размера 3):Результатом такой операции станет Stream, содержащий все выделенные синей рамкой подколлекции.Теперь с использованием Gatherers мы можем вывести все подколлекции следующим образом:public static void main(String[] args) {     var list = List.of(         ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"",          ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14""     );     int k = 3;      list.stream()         .gather(Gatherers.windowSliding(k))         .forEach(sublist -> System.out.printf(""%s "", sublist));     System.out.println(); } Получаем вывод:[1, 2, 3] [2, 3, 4] [3, 4, 5] [4, 5, 6] [5, 6, 7] [6, 7, 8] [7, 8, 9] [8, 9, 10] [9, 10, 11] [10, 11, 12] [11, 12, 13] [12, 13, 14]Fixed Window имеет крайне схожую реализацию, но теперь смещение равно размеру окна:Использование идентичное:public static void main(String[] args) {     var list = List.of(         ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"",         ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14""     );     int k = 3;      list.stream()         .gather(Gatherers.windowFixed(k))         .forEach(sublist -> System.out.printf(""%s "", sublist));     System.out.println(); } Вывод:[1, 2, 3] [4, 5, 6] [7, 8, 9] [10, 11, 12] [13, 14]Помимо создания собственных классов, реализующих Gatherer, есть статические фабричные методы:Gatherer.of(integrator)Gatherer.ofSequential(integrator)Оба метода имеют вариации с различными дополнительными аргументами в виде функциональных интерфейсов, список которых был упомянут ранее (initializer, integrator, combiner, finisher).JEP 484: Class-File APIСоздание собственного API для работы с class-файлами было предложено ещё в JDK 22. Теперь в версии JDK 24 этот API финализируется.Активная разработка Java в последние годы привела к появлению частых и регулярных изменений в байткоде, с которым взаимодействуют стандартные инструменты jlink, jar и другие. Для этого взаимодействия они используют библиотеки вроде ASM. Из-за этого для поддержки новых версий байткода инструменты должны дожидаться поддержки библиотеками, которые, в свою очередь, ждут финализации версии JDK. Подобные зависимости сильно замедляют процесс разработки и добавления новых возможностей в class-файлы.Этот API едва ли найдёт применение для большинства разработчиков, но различные фреймворки и библиотеки (в том числе Spring, Hibernate) работают с байткодом и используют ASM. А старая версия ASM не может работать с новыми версиями JDK. При необходимости обновить в проекте версию JDK придётся обновить и ASM. А для обновления ASM нужно апдейтить всё, что от него зависит. Вот и получается, что обновить придётся практически всё... а хотелось просто поднять версию JDK.Посмотрим, как выглядит новый API. Немного покопавшись в нём, получилось сделать простой пример чтения статических константных примитивных полей (потребуется базовое понимание структуры class-файлов):public class ClassFileExample {    public static void main(String[] args) throws IOException {     var classFile = ClassFile.of().parse(Path.of(""./Main.class""));      for (var field : classFile.fields()) {       var flags = field.flags();       if (flags.has(AccessFlag.STATIC) && flags.has(AccessFlag.FINAL)) {         System.out.printf(""static final field %s = "", field.fieldName());         var value = field.attributes().stream()           .filter(ConstantValueAttribute.class::isInstance)           .map(ConstantValueAttribute.class::cast)           .findFirst()           .map(constant -> constant.constant().constantValue().toString())           .orElse(""null"");         System.out.printf(""%s%n"", value);       }     }   } } Решение подобной задачи, внезапно, имеет практическое применение, ведь использование рефлексии приводит к инициализации класса. Возможно, однажды я расскажу о последствиях такой непредумышленной инициализации.А знакомые с ASM разработчики могут заметить, что авторы решили не использовать паттерн Visitor. Это обосновывается нововведениями Java, в частности pattern matching.JEP 483: Ahead-of-Time Class Loading & LinkingЦелью нововведения является улучшение времени запуска приложений. Для этого добавляется возможность сохранения кэша загруженных классов. Генерация и использование кэша осуществляются в три шага:1. Генерация конфигурации AOT. Для этого необходимо запустить приложение с флагом -XX:AOTMode=record, и указать путь до итогового файла через -XX:AOTConfiguration=PATH:java -XX:AOTMode=record -XX:AOTConfiguration=app.aotconf -jar app.jar2. Генерация самого кэша с использованием конфигурации. Режим AOT меняется на create, а также добавляется флаг с указанием пути вывода кэша -XX:AOTCache=PATH:java -XX:AOTMode=create -XX:AOTConfiguration=app.aotconf -XX:AOTCache=app.aot -jar app.jar3. Запуск приложения с использованием кэша. Для этого оставляем лишь флаг -XX:AOTCache=PATH:java -XX:AOTCache=app.aot -jar app.jarВ тексте JEP указывают, что время запуска простой программы с использованием Stream API уменьшилось с 0.031 секунд до 0.018 (разница 42%). Время запуска проекта на Spring (Spring PetClinic) уменьшилось с 4.486 до 2.604 секунд.Я посмотрел разницу во времени запуска простого приложения с использованием Quarkus на примере проекта из недавно опубликованной книги ""Quarkus in Action"" (GitHub). Время запуска снизилось с 3.480 до 2.328 секунд, т.е. на 39.67%.JEP 491: Synchronize Virtual Threads without PinningЭтот JEP решает проблему блокировки платформенных потоков при использовании виртуальных потоков в synchronized блоках. Для понимания изменения необходимо ознакомиться с Project Loom, а конкретно — с виртуальными потоками.Ещё при введении виртуальных потоков (JEP 444) указывалось два случая, при которых они не освобождают несущий их платформенный поток в случае блокировки:Блокировка происходит в synchronized блоке;Блокировка происходит в нативных методах (будь это JNI или Foreign Functions).Отныне первый случай не является действительным. Теперь разработчики могут свободно выбирать между использованием ключевого слова synchronized и пакета java.util.concurrent.locks, ориентируясь лишь на требования решаемой задачи.JEP 490: ZGC: Remove the Non-Generational ModeZ Garbage Collector (ZGC) поддерживал два режима: Generational и Non-Generational. Поскольку Generational ZGC является лучшей опцией в большинстве случаев, было принято решение упростить дальнейшую поддержку ZGC, отключив один из режимов — Non-Generational. Теперь флаг ZGenerational является устаревшим, и при его использовании будет выведено соответствующее сообщение:JEP 498: Warn upon Use of Memory-Access Methods in sun.misc.UnsafeПри использовании методов из sun.misc.Unsafe, связанных с памятью, будет выдаваться предупреждение. Этот функционал был заменён VarHandle API и Foreign Function & Memory API, а само изменение приближает удаление связанных с памятью методов из sun.misc.Unsafe (которые уже были помечены как Deprecated for Removal) и мотивирует разработчиков библиотек переходить на использование новых альтернатив.JEP 472: Prepare to Restrict the Use of JNIИспользование Java Native Interface (JNI) и Foreign Function & Memory (FFM) теперь приводит к предупреждению:Это первый шаг к ограничению использования JNI и FFM: в будущем планируется выбрасывать исключение. Это делается не для удаления возможности использования JNI или FFM (что было бы очень иронично, ведь последний вышел в релиз в Java 22), а чтобы соблюсти политику целостности по умолчанию (the policy of integrity by default). По факту это означает лишь то, что разработчики должны явно указывать, что согласны на использование небезопасных возможностей JDK (нативный код).JEP 493: Linking Run-Time Images without JMODИспользование флага --enable-linkable-runtime при сборке JDK позволяет jlink создавать образы без использования JMOD файлов из JDK. Это сокращает итоговый размер образа на 25%.Забытые JMODJMOD-файлы появились ещё в Project Jigsaw, Java 9. Они используются в опциональной фазе линкования при использовании jlink, т.е. при создании оптимизированного по занимаемому месту JRE.Эти файлы выступают как альтернатива jar, позволяют хранить не только .class файлы и ресурсы, но и нативные библиотеки, лицензии, исполняемые файлы. Всё это затем попадает в итоговый JRE. Поскольку при разработке стандартного приложения файлов JAR более чем достаточно, то использовать JMOD и не приходится. А ещё для этого формата катастрофически мало документации.Хотя это и не затрагивает непосредственно разработчиков, но однозначно имеет эффект при работе с контейнерами или при необходимости создания минимальных образов. Однако такая сборка не включена изначально, и отдельные провайдеры JDK сами принимают решение об использовании этой оптимизации.Например, в Eclipse Temurin уже начали использовать этот флаг. Поддержку такой сборки затем добавили и в GraalVM.JEP 486: Permanently Disable the Security ManagerПодготовка к отключению java.lang.SecurityManager началась ещё в Java 17, когда он был помечен как Deprecated for Removal.Это связано с крайне редким использованием этого класса при высоких затратах на его поддержание. Теперь к изменениям.Флаг -Djava.security.manager в любых вариациях более не поддерживается и выдаёт ошибку:Исключением является -Djava.security.manager=disallow, а использование System::setSecurityManager приведёт к исключению UnsupportedOperationException.Помимо этого, системные свойства, связанные с SecurityManager отныне игнорируются, а также удалён файл conf/security/java.policy.Прочие изменения связаны с документацией: удаление упоминаний SecurityManager и SecurityException.Стоит отметить, что сами классы и методы не удалены, а деградированы до ""пустышек "", т.е. они либо возвращают null, false, либо сразу исполняют запрос, либо бросают SecurityException или UnsupportedOperationException.JEP 479: Remove the Windows 32-bit x86 PortПоддержка Windows 32-bit x86 наконец прекращается. Это упрощает инфраструктуру сборки и тестирования, а также позволяет перестать направлять ресурсы на поддержание этой платформы.Одной из причин удаления этого порта является отсутствие поддержки virtual threads, которые откатываются к классическим kernel threads. Помимо этого, поддержка последней 32-битной версии Windows 10 завершается в октябре 2025 года.JEP 501: Deprecate the 32-bit x86 Port for RemovalСудьба остальных 32-битных платформ также очевидна: их удалят, но не в этом релизе.Так случилось, что последней 32-битной поддерживаемой платформой является Linux. Для сборки 32-битной версии сейчас потребуется добавление флага --enable-deprecated-ports=yes:bash ./configure –enable-deprecated-ports=yes Однако полное удаление этого порта ожидается уже в Java 25.JEP 496: Quantum-Resistant Module-Lattice-Based Key Encapsulation MechanismЭтот и следующий JEP относятся к постквантовой криптографии.Постквантовая криптография относится к созданию криптографических алгоритмов, которые будут эффективны даже после появления квантовых компьютеров.Добавлена реализация ML-KEM для KeyPairGenerator, KEM, KeyFactory APIs, а именно ML-KEM-512, ML-KEM-768, ML-KEM-1024 согласно стандарту FIPS 203. Так, создание пары ключей можно осуществить следующим образом:KeyPairGenerator generator = KeyPairGenerator.getInstance(""ML-KEM-1024""); KeyPair keyPair = generator.generateKeyPair(); JEP 497: Quantum-Resistant Module-Lattice-Based Digital Signature AlgorithmВ продолжение предыдущего JEP.Добавлена реализация ML-DSA для KeyPairGenerator, Signature, KeyFactory APIs, а именно ML-DSA-44, ML-DSA-65, ML-DSA-87 согласно стандарту FIPS 204. Подобно предыдущему пункту, посмотрим пример получения соответствующей подписи:Signature signature = Signature.getInstance(""ML-DSA""); JEP 475: Late Barrier Expansion for G1Напоследок JEP, который не затрагивает напрямую разработчиков Java, это изменения в Garbage-first (G1) сборщике мусора, переносящие реализацию его барьеров на более поздний этап C2 JIT компиляции. Это должно помочь упростить понимание этих барьеров для будущих разработчиков, а также уменьшить время исполнения компиляции C2.In PreviewПомимо этого списка нововведений часть изменений все ещё находятся состоянии Preview или Experimental:JEP 404: Generational Shenandoah (Experimental)JEP 450: Compact Object Headers (Experimental)JEP 478: Key Derivation Function API (Preview)JEP 487: Scoped Values (Fourth Preview)JEP 488: Primitive Types in Patterns, instanceof, and switch (Second Preview)JEP 489: Vector API (Ninth Incubator)JEP 492: Flexible Constructor Bodies (Third Preview)JEP 494: Module Import Declarations (Second Preview)JEP 495: Simple Source Files and Instance Main Methods (Fourth Preview)JEP 499: Structured Concurrency (Fourth Preview)Рассчитываем, что в одном из будущих (и скорых) релизов получится рассмотреть и показать на практике эти нововведения.ЗаключениеПолный список ссылок JEP для подробного изучения можно прочитать по ссылке.Java продолжает развиваться активными темпами, а появление Class-File API уменьшает количество зависимостей, что ещё сильнее увеличивает скорость обновлений на всей платформе. Но на этом пока завершается релиз Java 24. Значит, можно возвращаться к долгому ожиданию Project Valhalla.Если хотите поделиться этой статьей с англоязычной аудиторией, то прошу использовать ссылку на перевод: Evgenii Slepyshkov. What's new in Java 24."
100,100,Сила воображения: контроль робота силой мысли,ua-hosting.company,Хостинг-провайдер: серверы в NL до 300 Гбит/с,149.8,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-03-12,"  В последние годы робототехника все дальше и дальше отдаляется от статуса чего-то фантастичного и неизведанного. Несмотря на то, что современные роботы все еще далеки от своих литературных или киношных собратьев, они стали неотъемлемой часть современного мира. Роботы в том или ином виде применяются в самых разных сферах жизни человека, от фабрик и заводов до уборки в доме и развлечения детей. Еще одним потенциальным и крайне важным применением роботов может стать их использование для парализованных людей. Ученые из Калифорнийского университета в Сан-Франциско (США) провели исследование, в котором парализованный человек смог силой мысли управлять роботизированным манипуляторов, просто представляя как он выполняет то или иное движение. Как именно была реализована связь человека и машины, какие принципы легли в основу разработки, и насколько она точна и эффективна? Ответы на эти вопросы мы найдем в докладе ученых.  Основа исследования Наша нервная система должна сбалансировать поддержание стабильного нейронного представления большого репертуара хорошо отрепетированных действий, а также способствовать новому обучению. В научном сообществе используется термин «представление» для обозначения распределения моделей активности во время повторного выполнения действия. Исследования на животных показали, что нейронные представления могут испытывать дрейф — изменения в корреляции между активностью и поведением с течением времени. Хотя работа на людях показала, что простые, хорошо отрепетированные движения, такие как сгибание пальцев и высовывание языка, имеют отчетливые соматотопические представления в определенный день, остается неясным, как представления таких простых действий меняются со временем. Более того, неясно, как новое обучение может повлиять на динамику представления.  Ключевой проблемой является точное отслеживание с течением времени и с высоким пространственно-временным разрешением представлений для репертуара действий. Ученые использовали внутрикорковые интерфейсы мозг-компьютер (BCI от brain-computer interface), основанные на мезомасштабной электрокортикографии (EcoG от electrocorticography), чтобы понять принципы репрезентативной стабильности и пластичности. Сначала ученые установили, что однополушарная сетка ECoG, покрывающая сенсомоторную кору, может представлять воображаемые движения частей тела. Это позволяет идентифицировать репрезентативную структуру на основе попарного разделения или относительных расстояний между действиями на сохраненном низкоразмерном мезомасштабном «многообразии»; затем можно отслеживать, как репрезентативная структура меняется в течение дней с помощью управления BCI.  В рассматриваемом нами сегодня труде ученые нашли доказательства как внутридневной, так и междневной пластичности при обучении BCI. В частности, в течение любого ежедневного сеанса BCI с замкнутым контуром (CL от closed-loop) — с обратной связью — наблюдалось быстрое увеличение парного разделения по всему репертуару воображаемых движений. В течение дней парное разделение во время контроля CL продолжало неуклонно увеличиваться. Напротив, ответы с открытым контуром (OL от open-loop) — без обратной связи — оставались стабильными. Это указывало на то, что наблюдаемые изменения во время контроля BCI были в высокой степени контекстуально специфичными.  Ученые также нашли доказательства репрезентативного «дрейфа»; местоположение многообразия смещалось по дням из-за дрейфа в центроидах нейронного распределения каждого дня. Однако этот междневной дистрибутивный дрейф и изменения во время контроля BCI были ограничены стабильным режимом, характеризующим мезомасштабную метаструктуру в представлениях с разделяемыми границами для репертуара, который обобщался по дням. Таким образом, выборка пластичности и дрейфа позволила осуществлять управление «plug and play» (PnP) — т. е. без дополнительных обновлений декодера — долгосрочного нейропротезного управления высокой степенью свободы (hDoF от high degree of freedom) роботизированной руки и кисти. Важно отметить, что ухудшение производительности в течение длительных периодов времени для задач, требующих высокой точности, можно устранить с помощью кратковременной перекалибровки. Данное исследование показывает, как ECoG может отслеживать репрезентативную статистику и обеспечивать долгосрочное сложное нейропротезное управление.  Теоретическая часть  Изображение №1  Были сделаны записи ECoG с левой сенсомоторной коры у двух участников-правшей (B1 и B3; 1A), у которых был тяжелый тетрапарез (двигательная дисфункция верхних и нижних конечностей) и анартрия (грубая иннервационная недостаточность речевого аппарата) из-за двустороннего инсульта ствола мозга. Необработанная активность была отфильтрована, а мощность вычислялась в трех диапазонах (δ — от 0.5 до 4 Гц; β — от 12 до 30 Гц; γH — от 70 до 150 Гц), когда участники представляли или пытались представить соответствующие движения в виде, связанном с событием (1B). Участники имеют серьезные нарушения; для подавляющего большинства «попыток движений» (далее также называемых «действием») не наблюдалось никаких фактических движений. Были собраны данные во время 30 действий в B1 (49 попыток/действие) и 32 действий в B3 (48 попыток/действие), уделяя особое внимание руке.  Сначала было определено, были ли существенные изменения γH, вызванные действием, коррелятом нейронной активности. Для каждого действия был проведен одновыборочный t-тест потенциала, связанного с событием (ERP от event related potential), по всем испытаниям для каждой пары канал-время (пространственно-временная кластерная коррекция на уровне α = 0.05). Пример существенных кластеров значений t показан на 1C. Все каналы, составляющие любой существенный пространственно-временной кластер, показаны на 1D вместе с репрезентативными ERP.  Определив такие существенные каналы, ученые смогли сформировать пространственные карты γH для каждого действия. Затем были сопоставлены карты γH, относящиеся к общей части тела, например, все каналы, участвующие в действиях правой руки (1E и 1F). Хотя имелись доказательства надежной нейронной реакции, визуальный осмотр 1E и 1F также указал на соматотопию, т. е. пространственно различные γH между частями тела. Например, карты действий правой руки были соматотопически более похожи друг на друга, чем на действия левой руки или другие действия всего тела (B1; 1G). Несмотря на эту перекрывающуюся соматотопию, внутриэффекторные действия, по-видимому, различались по уровню активации. Например, электрод в области ручеобразной извилины* (корковая область в первичной двигательной коре человека (М1)) имел значительно различающиеся γH между тремя действиями руки (1I). Аналогично, вентральный канал вдоль прецентральной извилины показал дифференциальные уровни реакции γH на орофациальные действия (1J).  Таким образом, были определены парные расстояния между нейронными распределениями по всем трем частотным диапазонам и каналам для репертуара (1K и 1L). Сначала ученые обнаружили, что почти все парные нейронные расстояния между действиями были значительно больше случайных (B1: 99.7% всех парных сравнений; B3: 100% всех сравнений). Для 30 общих действий парные расстояния были значительно коррелированы между B1 и B3, действительно раскрывая общую репрезентативную структуру. Относительные сравнения внутри и между эффекторами показаны на 1M и 1N для обоих участников с помощью иерархической кластеризации матрицы парных расстояний на 1K и 1L.  В совокупности данные результаты являются доказательством наличия надежных дифференциальных пространственных нейронных реакций на репертуар действий, характеризующихся уникальной репрезентативной структурой.  Экспериментальная часть  Изображение №2  Далее ученые хотели понять стабильность и пластичность представлений с использованием сложного управления BCI hDoF. Сначала ученые выбрали подмножество действий и сопоставили каждое из них, чтобы обеспечить управление виртуальной роботизированной рукой Jaco (2A). Были выбраны действия, основанные на интуитивной системе отсчета, ориентированной на тело (например, голова — вверх, нога — вниз, большой палец правой руки — вправо и большой палец левой руки — влево); они также основывались на предпочтениях участников и репрезентативной различимости. Ученые измерили репрезентативную структуру выбранных действий в трех контекстах.  Сначала были протестированы действия OL, подаваемые для каждого кардинального направления (зеленый куб на 2B) без визуальной обратной связи. Данные OL использовались для заполнения классификатора в реальном времени. Затем, в первом эксперименте CL (CL1), участник получал визуальную обратную связь в реальном времени о выходе классификатора (справа на 2C). Наконец, второй эксперимент CL (CL2) следовал той же процедуре, что и CL1, за исключением того, что веса декодера обновлялись данными CL1. Для обновления декодера с данными замкнутого цикла ученые предположили, что пользователь всегда намеревался направить сигнал/действовать. Вместе CL1 и CL2 предоставляют окно в то, как репрезентативная структура адаптируется к двум контекстам, специфичным для BCI. Каждый день опыты начинались с OL, а затем выполнялась оценка CL1 и CL2. В B1 и B3 проводилось 10 и 11 сеансов соответственно. Для каждого сеанса среднее количество испытаний OL составляло 89, испытаний CL1: 62 и CL2: 48. Ученые использовали многослойное восприятие (MLP от multilayer perception) в качестве ежедневного декодера.  Матрицы путаницы на уровне пробного сеанса в OL (перекрестная проверка), CL1 и CL2 для B1 показаны на 2D. Они предполагают, что действия были различимы и могли быть немедленно использованы для контроля BCI. Действительно, точность декодирования в испытаниях (2E) была значительно выше случайной для каждого отдельного сеанса в B1 и B3. Оффлайн-анализ показал, что, хотя все три нейронные характеристики (δ, β; γH) были важны для декодирования, δ и γH были наиболее информативными.  Более того, точность декодирования была выше во время CL1 и CL2 по сравнению с OL (2E). Примечательно, что точность OL оставалась стабильной и последовательно ниже между сеансами даже с обучением. CL2 имел значительно более высокую среднюю точность декодирования по сравнению с CL1. Были обнаружены схожие основные эффекты при изучении процентного изменения точности декодирования между тремя экспериментами. Затем ученые попытались понять нейронные корреляты поведения, в частности, как представления адаптировались к BCI, сохраняя при этом стабильность OL.  Чтобы визуализировать активность каждого сеанса для репертуара действий в OL, CL1 и CL2, ученые выполнили снижение размерности данных каждого дня с помощью автокодировщика. Нейронная активность первого и последнего дня, спроецированная на их соответствующие многообразия, показана на 2F и 2G. Проверка активности латентного пространства выявила немедленные внутрисеансовые сдвиги и увеличение разделения между распределениями активности при переходе от OL к CL. Сокращение нейронной дисперсии каждого движения было очевидным для CL по сравнению с OL. Чтобы количественно оценить эти эффекты, ученые измерили расстояние Махаланобиса (мера расстояния между векторами случайных величин, обобщающая понятие евклидова расстояния), попарное разделение между распределениями и дисперсию распределений в латентном пространстве (2H).  Подтверждая наблюдения, CL продемонстрировал большие средние расстояния Махаланобиса (все попарные сравнения), чем OL (2I). Этот основной эффект также присутствовал в B1 и B3 по отдельности. Из двух экспериментов CL, CL2 характеризовался большим средним расстоянием между действиями по сравнению с CL1 (2I). Аналогично, средняя нейронная дисперсия (по всем действиям) во время обоих экспериментов CL была ниже по сравнению с OL (2J), эффект, который присутствовал также в B1 и B3 по отдельности.  Сравнение двух экспериментов CL показало, что CL2 имел более низкие нейронные дисперсии между сессиями в среднем, чем CL1 в B1 и B3. Эти внутрисеансовые снижения дисперсии во время CL также отражали соответствующие снижения нейронной изменчивости на уровне многомерных признаков канала по сравнению с сохраненными нейронными картами OL, CL1 и CL2. Кроме того, основные выводы о различиях в латентном пространстве между CL и контролем OL воспроизводились даже с большими наборами действий.  Затем ученые сосредоточились на долгосрочной пластичности по дням. Сначала ученые хронологически построили график среднего расстояния Махаланобиса для каждой сессии для трех экспериментов BCI (2K и 2L для B1 и B3). Регрессионный анализ показал, что в то время как средние расстояния между действиями в OL оставались стабильными в течение дней (2L), CL характеризовался поразительным увеличением разделимости (2L). Важно отметить, что расстояния Махаланобиса в пространстве признаков высокой размерности совпали с наблюдениями в латентном пространстве. Эти увеличения средних расстояний в течение дня были значительным предиктором улучшенной точности декодирования в B1 и B3 (2M и 2N).   Изображение №3  Вышеописанные анализы до сих пор проводились в течение каждого дня, что ограничивало понимание изменений представлений в течение дня с практикой BCI. Чтобы оценить стабильность в течение дня в конструкции представлений, ученые сравнили многообразие каждого ежедневного сеанса с другими днями попарно, используя центрированное выравнивание ядра (CKA от Centered Kernal Alignment). CKA измеряет послойное сходство активации между двумя автоэнкодерами (3A). Пример значительного послойного сходства CKA показан на 3B для B1 (дни 2 и 9). Подавляющая доля всех парных сравнений между днями была значительно схожей (3C). В B1 80% всех парных сравнений имели все 5 слоев значительно схожими; оставшиеся 20% имели по крайней мере 4 из 5 схожих слоев. В B3 83.64% всех парных сравнений имели все слои схожими; 12.72% имели по крайней мере 4 из 5 схожих слоев, а оставшиеся 3.64% имели по крайней мере 3 из 5 схожих слоев. Это предполагает замечательную стабильность в многообразиях в течение дней, даже с новой инициализацией декодера и после адаптации.   Изображение №4  Анализ установил, что представления действий демонстрируют сохраненное многообразие и стабильную OL-репрезентативную структуру, которую можно гибко регулировать с помощью CL. Однако вычисление этой репрезентативной структуры было основано на относительных расстояниях между воображаемыми движениями. CKA также усредняет данные центров до вычисления сходств. Таким образом, они не зависят от потенциальных дрейфов в распределениях в течение дня, которые могут накладывать ограничения на управление без повторной калибровки декодера.  Для оценки дрейфа ученые построили общий коллектор, используя данные по дням, и изучили различия, характерные для разных дней, и схему распределения дрейфа в многодневных данных. Ученые сосредоточились на данных OL, учитывая стабильность их репрезентативной структуры. Например, на 4A показано латентное пространство большого пальца правой руки B1 OL по трем случайным дням. Даже при сохраненной нейронной карте и соматотопии наблюдалось небольшое перекрытие распределения между днями (4B). В целом, день записи можно было классифицировать по удерживаемым образцам на общем многообразии (4C и 4D). MLP в многомерном пространстве признаков канала показал схожие значимые точности декодирования на уровне проб. Напротив, центрирование и принижение данных каждого дня перед построением общего многообразия устранило различимость. В совокупности это подразумевало дрейф в течение дня в центроидах данных каждого дня, что свидетельствует о плохой обобщаемости ежедневного декодера на неучтенные дни.  Ученые задались вопросом, может ли выборка этого ежедневного дистрибутивного дрейфа раскрыть более стабильную репрезентативную структуру, которая позволяет реализовывать PnP. Чтобы проверить эту гипотезу в автономном режиме, ученые перестроили общее многообразие, используя данные OL из кумулятивно увеличивающегося числа дней. Затем они оценили, насколько хорошо оно улавливает дискриминируемость и репрезентативную структуру между действиями в отложенные дни. В качестве примера были спроецированы все нейронные данные B1 «движения руки» с 10-го дня на многообразие, построенное либо с использованием только первого дня (слева на 4E), либо в течение первых 9 дней (справа на 4E).  Паттерны активности оказались более различимыми на последнем многообразии. При рассмотрении всего репертуара было обнаружено, что парные расстояния между действиями в данный день линейно увеличивались в зависимости от количества предыдущих дней, использованных для построения общего многообразия (4F и 4G). Таким образом, даже несмотря на то, что имелся дрейф распределения по дням, он, по-видимому, был ограничен стабильным режимом, характеризующим метарепрезентативную структуру по дням (схема на 4H).  Эта метаструктура может иметь стабильные границы разделения для репертуара, которые обобщаются на неучтенные дни, тем самым позволяя PnP. Подтверждая эту гипотезу, ученые наблюдали сходимость стабильной метаграницы между двумя действиями рук на общем многообразии — что хорошо обобщалось с точки зрения производительности декодирования на новый отдельный день — как функцию количества предыдущих дней ежедневной инициализации, выбранных для обучения декодера (4I). Таким образом, для PnP, по-видимому, были преимущества использования многодневных данных.   Изображение №5  Таким образом, ученые использовали все данные OL и CL из всех предыдущих экспериментальных дней для затравки PnP MLP, выбирая как дистрибутивные дрейфы, так и увеличенные расстояния CL, т. е. репрезентативную пластичность. День 1 PnP — это первый день, когда были зафиксированы веса декодера. Средняя точность за весь период PnP составила 87.43%, а среднее время достижения цели составило 1.195 секунды (5A и 5B). Это привело к среднему битрейту 1.72 бит/с (5C; видео №1).  Видео №1  Затем ученые обновили веса MLP, используя данные первого эксперимента, и провели второй эксперимент, отличный от первого, в более длинном окне PnP (5D–5F). Средняя точность составила 80.71%, а среднее время достижения цели составило 0.875 с, что дало эффективную среднюю скорость передачи данных 1.848 бит/с. Существенных различий в производительности между двумя экспериментами PnP не было, а линейная подгонка к ежедневной медиане показала стабильную производительность по дням для обоих.  Примечательно, что точность PnP была аналогична новому ежедневному декодеру. В обоих экспериментах средняя эффективная скорость передачи данных составляла 1.8 бит/с, а максимальная — 3.351 бит/с. Ученым удалось провести один эксперимент PnP с B3 (5G–5I). Средняя точность составила 84.9%, а среднее время достижения цели составило 0.818 с при средней скорости передачи данных 2.22 бит/с (максимум 3.508 бит/с). Точность декодирования также оставалась стабильной в течение дней. Таким образом, у обоих испытуемых дрейф репрезентативности выборки и пластичность в течение дней привели к надежному декодеру PnP.   Изображение №6  Используя виртуальную трехмерную среду, ученые расширили результаты для проверки непрерывного управления роботом (только в B1, который был зарегистрирован в течение более длительного периода). В частности, дискретные представления всего тела использовались как импульсы скорости для модуляции непрерывного конечного трехмерного кинематического состояния робота (6A). Динамика робота (матрица A) следовала законам движения первого порядка с плавно затухающими скоростями при отсутствии нейронного привода. Представление, соответствующее началу координат («оба средних пальца»), использовалось как команда «схватить», которая замораживала положение робота и выполняла схватывание. Ученые назвали эту структуру управления BCI «интегрированным управлением непрерывной динамикой на основе ввода» (IBID от input-based integrated control of continuous dynamics). Таким образом, положение робота охватывает все трехмерное пространство с непрерывной динамикой первого порядка; именно нейронная скорость, поступающая в систему, является дискретной (с мгновенными интервалами). Сравнение IBID и ReFit-KF в один день тестирования (48 испытаний для IBID и 18 испытаний CL с ReFit-KF с общей матрицей динамики для обоих) показало, что первый характеризовался хорошо разделенными представлениями для осевых направлений. Это позволяло получать высокоточные трехмерные траектории «центр-наружу». IBID также допускал «движение по инерции», когда пользователь мог отключиться и позволить внутренней динамике робота взять верх. Эффективность IBID в точном управлении траекторией затем была протестирована в «задаче отслеживания пути», которая требовала от пользователя следовать по заранее определенному пути (вверху на 6B) и схватить виртуальный куб (внизу на 6B). Пример производительности показан на видео №2 и 6C.  Видео №2  Структура декодирования также не исключает диагональные траектории. Последовательно переключаясь между репрезентативными состояниями и используя интегрированную динамику IBID, B1 успешно выполнил диагональные траектории. На 6D и 6E показаны примеры 3D-траекторий с точной коартикуляцией осевых скоростей. Нулевая модель для этой задачи показана на 6F, характеризующаяся неинтегрированными шагами только вдоль основных осей. Неинтегрированные шаги приводят к постоянной ошибке 45 с идеальной скоростью. Отсутствие ковариации также подразумевает, что анализ главных компонент (PCA от principal-component analyses) скоростей будет повторять основные направления (справа на 6F).  Напротив, данные показали значительную ковариацию скорости из-за интеграции в IBID (слева и посередине на 6G). Полярная гистограмма ошибки между скоростями IBID и идеальной скоростью показана на 6G (справа). Среднее значение этого кругового распределения составило 1.885 и не сильно отличалось от нуля. В целом, начало испытания всегда имело бы ошибку 45 с идеальным вектором скорости, поскольку B1 может получить доступ только к основным направлениям с помощью дискретных входов. Однако с IBID участник достигал моментов нулевой ошибки между интегрированными скоростями и идеальной скоростью. B1 также мог получить доступ к внеосевым направлениям, коактивируя несколько представлений одновременно, например, представляя правый большой палец и голову для доступа к положительному квадранту XY.  Наконец, в отличие от подходов, которые имеют явную связь между нейронной активностью и кинематикой конечного эффектора (например, регрессионные подходы), рассматриваемая структура допускает более широкую генерализацию. Ученые обнаружили, что B1 мог немедленно и умело использовать декодер даже при значительных визуальных возмущениях, с точностью декодирования 86% и 94% при работе декодера PnP либо из аллоцентрической, либо из левосторонней системы отсчета соответственно (по 42 попытки каждая). Это еще раз подчеркнуло гибкость дискретных представлений в качестве входов для BCI.   Изображение №7  Затем ученые расширили фреймворк IBID для более сложных взаимодействий руки и объекта. Вместо того чтобы сосредоточиться только на транспортировке, ученые также оценили, можно ли гибко переназначить представления на управление вращением и захватом. Распространенным методом включения такого управления hDoF является команда переключения режима для перехода между транспортировкой и захватом (7A и 7B).  Для управления перемещением конечной точки ученые использовали описанную ранее структуру IBID. Однако теперь действие обоих средних пальцев использовалось в качестве переключателя режима. В режиме захвата шесть действий, используемых для управления декартовой досягаемостью, были переназначены на степени свободы захвата (7A и 7B). Это включало вращение руки вокруг фиксированной оси в соответствии с захватом и перемещение руки вперед и назад для дотягивания и извлечения объекта. Это позволяло достигать прямой линии и манипулировать объектом после того, как захват и объект были выровнены. Динамика захвата также была первого порядка и плавно затухала при отсутствии ввода. Наконец, для открытия и закрытия захвата использовались два отдельных действия. Чтобы оставаться интуитивно понятными, ученые использовали специфичные для руки действия для открытия/закрытия. Фрагменты из примера испытания комплексного управления hDoF на основе переключения режима показаны на 7C. Для выполнения задания требовалось, чтобы B1 схватил зеленый куб, переместил и поместил его на верхнюю часть белого ящика (видео №3).  Видео №3  Для объектных взаимодействий B1 успешно переключался с режима транспортировки на режим захвата и наоборот для транспортировки. Кроме того, нейронный привод был не постоянным, а мгновенным, при этом B1 выборочно и точно включал управление. B1 быстро освоил использование переключения режимов со 100% точностью после 1 недели практики (7D и 7E, среднее время выполнения задачи 45 секунд).  Убедившись, что представления могут быть успешно и гибко переназначены с помощью переключения режимов, ученые перевели структуру на реальный нейропротез hDoF (7F). В этой настройке ученые заменили прямую связь PnP MLP на глубокую рекуррентную нейронную сеть (RNN от recurrent neural network) для повышения производительности декодирования (также декодируя временную динамику).  Видео №4  Ученые протестировали долгосрочную производительность PnP (фиксированная RNN) в двух сложных задачах на дотягивание и схватывание и манипуляцию объектами с разным уровнем сложности и при полном сознательном контроле (без дополнительной помощи). Первая задача была на дотягивание и вращение, которая требовала точного дотягивания и схватывания объекта, а затем помещения его в целевое место после переориентации (7G). Медианный показатель успеха составил 90% со временем выполнения задачи 60.8 секунды (7I и 7J; видео №4).  Второе задание было связано с взаимодействием со стеной (7H), которое требовало дотянуться и схватить объект с центра стены и разместить его точно в одном из четырех целевых мест. Это задание сравнительно сложнее, поскольку конечное целевое место меньше по размеру. B1 смог выполнить первую часть (дотянуться и схватить цель с центра стены) успешно на протяжении всего окна тестирования PnP и для всех уровней сложности подхода (вверху на 7K; медианный показатель успешности 100%, среднее значение 94.6%; медианное время, чтобы дотянуться и схватить объект 38.053 секунды). Показатель успешности для полного задания, требующего точного размещения объекта, показан на рисунке 7K (внизу).  В этом случае наблюдалось падение производительности после 35-го дня PnP (медианная точность за первые 35 дней 88.19%; общее время выполнения задачи 75.52 с; медианная точность за дни 35–210 69.05%, общее время выполнения задачи 104.854 с). Это говорит о том, что особенно сложные фазы задачи — те, которые требуют высокой точности — могут демонстрировать долгосрочное снижение контроля PnP. Примечательно, что повторная калибровка RNN (данные, необходимые для обновления декодера, составляли 8 минут) немедленно повысила вероятность успеха до 100% со средним временем завершения 80.73 с (7K и 7L). В совокупности эти данные указывают на то, что 35 дней стабильного PnP могут быть легко достигнуты для сложных задач BCI, требующих высокой точности; однако после этого может быть очевидно ухудшение производительности PnP.  Для более детального ознакомления с нюансами исследования рекомендую заглянуть в доклад ученых и дополнительные материалы к нему.  Эпилог В рассмотренном нами сегодня труде ученым удалось достичь невероятного успеха в рамках исследования и реализации контроля и управления роботом силой мысли человека с парализованными конечностями. Человек мог схватить, переместить и бросить предмет, просто представляя как он это делает.  Устройство, известное как интерфейс мозг-компьютер (BCI от brain-computer interface), работало рекордные 7 месяцев без необходимости настройки. До сих пор такие устройства работали всего день или два. BCI опирается на модель ИИ, которая может подстраиваться под небольшие изменения, происходящие в мозге, когда человек повторяет движение — или, в данном случае, воображаемое движение — и учится делать это более утонченным образом. Важным моментом, позволившим реализовать данную систему, стало понимание того, как меняется активность мозга изо дня в день, когда человек выполняет (или представляет в голове как выполняет) одно и то же действие. Учет этих изменений в проектировании ИИ позволил системе работать длительное время без дополнительной настройки.  В исследование принимали участие люди с парализованными конечностями. К поверхности их мозга прикреплялся датчик, считывающий активность, когда человек представлял какое-то движение. Несмотря на то, что человек не мог реально выполнить движение, например, рукой, его мозг все же генерировал соответствующие данному движению сигналы. Данный процесс представление и считывания активности повторялся в течение двух недель. Однако во время практических тестов, когда участник должен был контролировать движения виртуального робота, они были неточными. Тогда ученые повторили процедуру представления и считывания активности, но уже с визуальной обратной связью. В результате это разительным образом увеличило точность движения робота, контролируемого участником опытов.  Успешность выполнения различных задач по захвату и перемещению объектов спустя месяц была такая же высокая, как и в первые дни опытов. Для этого требовалось всего 15 минут калибровки. В будущем ученые намерены усовершенствовать систему, сделав движения робота более точными, быстрыми и плавными.  Данная разработка является невероятно важной, особенно для людей, лишенных возможности двигаться. Они получат инструмент, позволяющий им самостоятельно выполнять различные действия без сторонней помощи.  Немного рекламы Спасибо, что остаётесь с нами. Вам нравятся наши статьи? Хотите видеть больше интересных материалов? Поддержите нас, оформив заказ или порекомендовав знакомым, облачные VPS для разработчиков от $4.99, уникальный аналог entry-level серверов, который был придуман нами для Вас: Вся правда о VPS (KVM) E5-2697 v3 (6 Cores) 10GB DDR4 480GB SSD 1Gbps от $19 или как правильно делить сервер? (доступны варианты с RAID1 и RAID10, до 24 ядер и до 40GB DDR4).  Dell R730xd в 2 раза дешевле в дата-центре Maincubes Tier IV в Амстердаме? Только у нас 2 х Intel TetraDeca-Core Xeon 2x E5-2697v3 2.6GHz 14C 64GB DDR4 4x960GB SSD 1Gbps 100 ТВ от $199 в Нидерландах! Dell R420 — 2x E5-2430 2.2Ghz 6C 128GB DDR3 2x960GB SSD 1Gbps 100TB — от $99! Читайте о том Как построить инфраструктуру корп. класса c применением серверов Dell R730xd Е5-2650 v4 стоимостью 9000 евро за копейки?"
101,101,Система управления проектированием API банка: от создания интерфейса до импорта спецификации,IBS,IBS – технологический партнер лидеров экономики,0.0,,2025-03-12,"Проектирование API для банка — это не просто техническая задача, а целая стратегия, которая определяет будущее цифровых продуктов финансовой организации. Эффективная система управления проектированием API — ключевой инструмент для обеспечения гибкости, быстроты выпуска новых сервисов и безупречной работы уже существующих. Эта статья — о том, как правильно организовать процесс разработки API в финтехе.Привет, Хабр! Меня зовут Алексей Тарасов, я ведущий аналитик IBS. При написании статьи я ориентировался именно на аналитиков, потому что чаще всего допиливание, валидация и публикация API для конечного потребителя — это наша ответственность. Ниже постарался осветить вариант решения задачи унификации и автоматизации процесса разработки API в крупном банке.Для чего нужна система управления проектированием API?Цифровая экосистема любой крупной организации сегодня состоит из большого количества разных информационных систем, а у этих систем есть множество пользователей, причем как внутри организации, так и за ее пределами. Современные информационные системы построены по принципу веб-сервера, с фронтом и бэком, которые общаются между собой при помощи API (Application Programming Interface) — определенного набор правил, инструкций, методов, структур данных и некоторых проверок, которые позволяют информационным системам обмениваться данными друг с другом.OpenAPI — формализованная спецификация и экосистема множества инструментов, которая предоставляет интерфейс между информационными системами, кодом библиотек низкого уровня и коммерческими решениями в виде API.Чтобы максимально упростить и унифицировать взаимодействие пользователей и систем, имеет смысл сделать единую точку, в которой все используемые API были бы описаны и настроены, а доступ к ним имели как клиенты (организации, стримы или информационные системы, которые могут подключаться к API), так и поставщики API (те, кто отвечают за создание, управление, обслуживание и актуализацию API для пользователей).Таким образом, назначение системы управления проектированием API банка можно сформулировать так:1. Обеспечение взаимодействия банковских систем между собой.2. Обеспечение требований информационной безопасности при межсистемном взаимодействии (в финтехе этот пункт по понятным причинам особенно актуален).3. Унификация процесса публикации API информационных систем банка.4. Единая точка администрирования API.5. Сбор аналитики и агрегация статистики по использованию API.Процессы управления — от проектирования до публикации APIПроцесс проектирования я представил в виде статусной модели. Как видно, она разбита на три блока по назначению компонентов — управление архитектурой, реестр API и управление API:Пользователь взаимодействует с информационной системой через API, которая является интерфейсом, поэтому проектирование начинается с создания нового интерфейса в системе управления архитектурой. Это единственный этап на схеме, которым, по идее, занимается не аналитик, а архитектор. Все, что требуется от аналитика, — сообщить архитектору, как должен называться будущий API. Созданный интерфейс затем должны согласовать ответственные лица: владельцы интерфейса, архитекторы и служба информационной безопасности. После того как все круги ада согласования пройдены и статус обновлен, автоматически формируются выгрузки по расписанию для реестра API.Реестр API — это то самое место, с которым уже работают поставщики API, где происходят, собственно, проектирование интерфейса и его настройка. На основании данных, полученных из файла с выгрузкой, в реестре создается новая спецификация, то есть сущность, в которой в дальнейшем мы и будем настраивать API. В созданной спецификации поставщик осуществляет проектирование в формате OpenAPI. Разрабатывать API можно как в самом реестре, так и подготовить его заранее через Swagger в формате YAML, загрузить в файл спецификации и провалидировать его.После этого можно переходить к этапу обогащения спецификации. Обогащение предполагает настройку шлюзов, на которых будет публиковаться API, дополнительные проверки, настройку дополнительных плагинов, выставление типов API, настройку поставщиков и прочее. Обогащение проводится либо как расширение кода OpenAPI, либо через специальные формы, которые предоставляет реестр API.Далее система проводит автоматическую валидацию созданной обогащенной спецификации. Ошибки, которые могут быть выявлены в процессе валидации, делятся на три вида: предупреждения, обычные и критические ошибки. Критические ошибки определяются системой на основе справочника обязательных проверок, зашитого в реестр API. Если критические ошибки присутствуют, то реестр просто не даст нам выгрузить API для публикации. Когда все исправлено, формируется файл выгрузки спецификации, который мы импортируем в систему управления API.Управление API — это несколько систем, которые распределены по контурам: стенд интеграционно-функционального тестирования, предпрод для проведения приемо-сдаточных испытаний и продуктивная среда. Это три отдельные системы, в которые можно загружать сформированные спецификации.Теперь рассмотрим ключевые этапы процесса в деталях.Создание интерфейса системыПри создании интерфейса системы ему присваивается наименование. В банковских системах оно строго типизировано для того, чтобы уже по наименованию можно было понять, к какой информационной системе относится API, какой у него тип и версия. Здесь же, в системе управления архитектурой, мы должны настроить контуры для публикации, связь с поставщиком и связь с клиентами API. Вот как все это выглядит:По центру — файлы с интерфейсами API. Их имя содержит код (1234) и мнемокод системы, указание на UI, синхронность (s), REST-взаимодействие, а также версию (v1). Слева отображена связь с платформой, которая обслуживает эту систему, а справа — с микросервисами, которые реализуют API. Если все клиенты и их системы известны на этапе проектирования, их также можно добавить, чтобы после выгрузки в реестр API можно было сразу настраивать их взаимодействие. В противном случае при появлении нового клиента нам придется возвращаться к этой схеме, вносить в нее изменения и обновлять API.Платформа внутренних APIКомплекс средств обслуживания разработанных API называют «платформой внутренних API».Какие функции она выполняет:авторизация вызова API;проектирование API;обработка вызовов;безопасный обмен файлами;ведение журнала трафика;ведение журнала операций — вызываемых методов, которые находятся внутри API;защита API — запрет на подключение без учетной записи или вызов неописанного метода;форматно-логический контроль.Tyk API GatewayВ качестве платформы внутренних API в банке используется система Tyk API Gateway. Эта система обеспечивает:проксирование трафика;контроль доступа к API;преобразование данных внутри настроенных API;ведение журналов использования;контроль параметров авторизации;логирование на уровне шлюза;маскирование данных.Tyk API MiddlewareВсе перечисленные выше задачи осуществляются с использованием middleware, или плагинов, которые позволяют:прерывать при необходимости выполнение запросов;модифицировать запросы и ответы;валидировать запросы и ответы;проводить размен токенов;настраивать CORS и вызов внешних сервисов.Обогащение APIПосле создания интерфейса мы «переезжаем» в реестр API, в котором уже осуществляются проектирование, валидация и обогащение API.К обогащению относятся:настройка шлюза и контура публикации;формирование базового пути;настройка типа аутентификации;настройка middleware, или плагинов;маскирование данных;настройка нефункциональных требований.Расскажу обо всех пунктах подробно.Выбор шлюза и контура публикации = типы API:публичные: для взаимодействия двух информационных систем в контуре банка.фронтальные: для взаимодействия фронта и бэка приложения.файловые: для отправки/получения файлов.внешние: для безопасного взаимодействия с системами банка внешних информационных систем за его контурами либо пользователей, живущих за пределами страны. Защищенные API в таком случае размещаются в специальном DMZ-контуре.Формирование базового путиБазовый путь — это тот путь, который добавляется к серверу вызова и по которому мы можем осуществлять вызовы методов. К базовому пути также предъявляются требования по унификации, чтобы по базовому пути было понятно, что это за API. Для каждого контура — интеграционно-функционального тестирования, предпрода и продуктивной среды — задаются свои URL. Потом это позволит нам выгрузить обогащенную спецификацию один раз и импортировать ее в любой контур, необходимые настройки для которого будут автоматически извлекаться из нужной строчки обогащенной версии файла.Базовый путь включает в себя тип API, код системы, код сервиса внутри этой системы и разрабатываемую версию. Вот как это выглядит:/ui-api-web/name/service/v1Настройка middleware, или плагиновПлагины в системе управления API бывают двух видов:Плагины вендора.Идут вместе с поставкой Tyk API Gateway «из коробки». Это классические модификации запросов и ответов (как тела, так и заголовков) и их валидация на соответствие стандарту OpenAPI.Собственные разработки.Поскольку шлюз открытый, для него существует возможность добавления сторонних плагинов, которые разрабатываются уже непосредственно сотрудниками банка под определенные нужды. К ним обычно относятся:расширенная валидация запросов и ответов по внутренним правилам — например, ограничение на размер тех или иных типов данных;настройка интеграции с другими системами — например, с антивирусом или системой хранения контента;файловые проверки: помимо классической антивирусной проверки, в банках к файловому обмену предъявляются дополнительные требования, поэтому файловые API выделены в отдельный шлюз;размен токенов;проверка anti-replay.Валидация также бывает двух видов:Автоматическая валидация при разработке внутри реестра API:валидация нотацией OpenAPI;валидация заполнения обязательных параметров;валидация по настроенным правилам.Настраиваемая валидация запросов и ответов на соответствие логической схеме — дополнительные плагины, которые позволяют включить валидацию при взаимодействии клиента с API. При включении такой валидации она также будет проходить автоматически. В случае поступления данных, которые не соответствуют описанной структуре данных, система просто вернет ошибку 422.Интеграции — настройка параметров, необходимых для вызова другой системы:заголовки, которые должны содержать, например, адрес сервиса, с которого передаются данные, имя передаваемого файла и учетные данные пользователя, который осуществляет запрос;URL сервиса;дополнительные передаваемые параметры;при необходимости — маскирование;ограничения — например, на типы и размер файлов при файловом обмене.МаскированиеВ финтехе мы неизбежно взаимодействуем с конфиденциальной информацией — банковской тайной и персональными данными пользователей, защищаемых соответствующим Федеральным законом № 152. По требованиям информационной безопасности, принятым и утвержденным архитектурным комитетом, эти данные необходимо маскировать при передаче с помощью настройки параметров заголовков и тел методов. Настроенные параметры не отображаются в вызовах методов и журнале событий системы управления API — вместо них мы видим «*».Проверка корректности маскирования всегда включается в протокол тестирования — без нее релиз не пройдет приемо-сдаточные испытания.Файловые проверкиНастройка на уровне плагинов параметров проверок при загрузке и выгрузке файлов:ограничения размера файлов;перечень разрешенных расширений;дополнительные параметры и адреса сервисов.Нефункциональные требованияЧтобы канал не забивался при подключении, для каждого клиента API выставляются требования к пропускной способности, ограничения на размер пакета и таймауты.Пример обогащения APIВот как выглядит обогащение API внутри YAML-файла:Выгрузка спецификации APIПосле того как мы все спроектировали, провели валидацию, настроили все необходимые параметры и устранили критические ошибки, система дает нам возможность выгрузить версию для публикации. Отмечу, что у нас есть возможность комплексной публикации, когда мы можем настроить публикацию одной и той же API сразу на несколько шлюзов.Импорт спецификации APIКак я уже говорил, импорт спецификации происходит в третьей системе — системе управления API. У каждого контура она своя.По классической схеме импорт спецификации осуществляется ответственными сотрудниками стрима OpenAPI банка. Однако поскольку шлюзов и контуров работы очень много, на практике управление шлюзом чаще всего отдается на откуп поставщику API. Это так называемая самопубликация, когда поставщик, разработав API, может самостоятельно осуществлять его публикацию и настройку.Исключение составляет продуктивный контур. Импорт спецификации на продуктив осуществляется ответственными сотрудниками стрима OpenAPI банка по специальной заявке от поставщика API после прохождения испытаний в плановый релиз.Использование APIПосле импорта спецификации API она активируется и становится доступной для подключения клиентов (при наличии сетевого доступа к шлюзу API). Каждый шлюз имеет свой адрес для подключения, который добавляется к базовому пути. Клиент может использовать API, вызывая его по адресу шлюза и базовому пути, которые сообщаются поставщиком.ВыводыПреимущества системы управления проектированием API банка проистекают из ее назначения: это единая точка для проектирования, настройки, публикации и администрирования API, которая позволяет унифицировать процессы и соблюдать требования информационной безопасности при межсистемном взаимодействии.К сожалению, у системы есть и свои недостатки:Сложный процесс (вы наверняка заметили это по длине статьи). В случае серьезных изменений (например, добавления нового поставщика) весь процесс придется пройти по новой. Ошибившись на одном из этапов, можно случайно сломать все, что было до этого.Отсутствие мониторинга выгрузки интерфейсов в Реестр API. Если по расписанию не произошла выгрузка спецификации, придется идти и разбираться вручную.Смена правил валидации API или настройке в реестре API без уведомлений пользователей. Бывает, что ты спокойно разрабатываешь API, а на следующий день приходишь в тот же самый реестр, а у него обновилась сборка, и уже все настройки находятся в другом месте. Никаких пояснений к релизу при этом нет.Отсутствие развернутого протокола импорта спецификаций API. Если при загрузке спецификации возникает ошибка, то также приходится вручную разбираться, что пошло не так.И тем не менее при работе на крупном финтех-проекте плюсы использования соответствующей системы сильно перевешивают минусы!"
102,102,Bobcat Miner 300 — жизнь после забвения. Как я получил мощный одноплатник почти бесплатно,МТС,Про жизнь и развитие в IT,2233.29,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-03-12,"Как лучше всего провести воскресное утро? Для меня вопрос давно решен, ведь в этот день работает моя любимая барахолка, где всегда можно найти что-то интересное. Здесь сочетается азарт и любопытство, когда, купив очередного «кота в мешке», пытаешься понять, стоил ли он потраченных денег. Сегодня расскажу, как случайно стал обладателем устройства Bobcat Miner 300, разобрался в особенностях работы сетей LoRaWAN и выяснил подводные камни, на которые можно нарваться, если купить такой девайс, например, на eBay. Нарезайте себе рулетик, заваривайте пуэр — и приятного чтения!О чем вообще речь?Покупки на барахолке давно превратились для меня в охоту за редкими девайсами. Часто продавцы сами не знают, что они продают, и уж тем более не в курсе особенностей тех или иных устройств. Когда я ковырялся в ящике с грудой старых роутеров, мое внимание привлекла коробка с единственным светодиодом и надписью BOBCAT:Герой рассказаСодержимое наклейки: Bobcat IoT hotspot;Model: Bobcat Miner 300 (G285);FREQ: EU868 MHz;INPUT: DC 12V — 1A.Там же была пара MAC-адресов и серийный номер устройства. Быстро выяснил, что подобный девайс на eBay продается от 100 $ до 500 $, и спросил у продавца стоимость. Тот назвал цену, соответствующую паре чашек кофе в хорошем заведении, обозвав Bobcat роутером. На этом и сошлись.В комплекте с ним не было блока питания. Но это дело поправимое, дома лежало несколько БП с нужными характеристиками. А вот антенну LongFi я решил заказать в Поднебесной, однако не успел. Коллега-радиолюбитель выделил мне одну из своих, лежавших в закромах. Вот теперь пришла пора познакомиться с устройством поближе.Внешний видСпереди нет никаких элементов, кроме светодиодного индикатора, демонстрирующего текущее состояние или готовность к беспроводному соединению.На задней части расположены:разъем RP-SMA M;скрытая кнопка сопряжения по Bluetooth;слот для MicroSD-карты;сетевой порт Ethernet;вход питания.Больше ничего примечательного снаружи я не нашел.НазначениеПрежде чем лезть внутрь, я решил узнать поподробнее, что это за устройство и как сочетается IoT hotspot и Miner. Для меня это было два несочетаемых понятия. Оказалось, что существует сеть Helium IoT, созданная для сбора данных с датчиков LoRaWAN. Пользователи приобретают устройства с поддержкой этого протокола и оставляют работать круглосуточно. Множество таких независимых хотспотов образует сеть, которая пригодна для передачи данных, текстовых сообщений, раздачи интернета и тому подобных задач.Чтобы мотивировать держателей хотспотов поддерживать круглосуточную работоспособность точек, создатели решили использовать криптовалюту. Каждый принятый и переданный пакет данных вознаграждался какой-то долей токена HNT. Helium Network продает свои услуги различным компаниям, использующим в работе беспроводные датчики. Сбор данных с них предполагает развертывание дорогостоящей инфраструктуры, что фактически и делает эта сеть.В ней IoT hotspot выполняет роль майнера, который можно приобрести, установить и поддерживать в рабочем состоянии 24/7, получая соответствующее вознаграждение. Казалось бы, ситуация Win-win, но дьявол, как всегда, кроется в деталях — об этом я узнал уже по факту.На граблиПервое, что я стал искать, это как восстановить настройки до дефолтных. Спустя пару часов пришел к выводу — такого способа нет. Это показалось максимально странным, но в процессе мне удалось попасть в веб-морду и сбросить настройки. После этого я скачал приложение Helium Hotspot, зарегистрировался в нем и попытался выполнить привязку устройства к своему аккаунту. В итоге получил сообщение об ошибке, поскольку оно уже привязано к учетке другого участника сети. И просто так отвязать его нельзя. Чтобы продать такой майнер, требуется выполнить виртуальную передачу прав на устройство от продавца к покупателю. Без этого оно бесполезно. А если подключить его к сети, то Bobcat радостно и весело продолжит «майнить» на своего предыдущего владельца.После вдумчивого гугления открылась еще более интересная информация. Официального сайта bobcatminer.com больше нет, хотя домен существует. Его регистрировала британская компания Com Laude в интересах гонконгской CATBOB HK LIMITED. Та формально «жива», но, вероятно, ведет уже другую деятельность. Домен истечет 2026-01-24, но сайт вряд ли заработает.Проблема в том, что с пропаданием вендора перестала обновляться и OTA-прошивка устройства. А после того как Helium перешел на блокчейн Solana, девайс вовсе потерял актуальность. Зато появились всякие разные сторонние проекты, которые решили воспользоваться ситуацией. Ими можно назвать https://bobber.com/ и https://bobcatminer.shop/. Интереса ради я написал запрос в саппорт contact@bobcatminer.shop и получил любопытный ответ:TL:DR; Заплатите 29 $ долларов, и мы поможем вам перепривязать устройствоЗаманчивое предложение, но слишком сильно похоже на скам. Да и майнить HNT уже не выйдет, сейчас сеть расплачивается токеном IOT, который почти ничего не стоит, так что смысла держать девайс фактически нет. Поэтому я решил исследовать аппаратную часть устройства и найти ему альтернативное применение.ПотрохаВсе самое интересное, увы, скрыто за металлическим экраномКак только я полез внутрь, стало ясно, что это достаточно мощный одноплатный компьютер с такими характеристиками:CPU — Rockchip RK3566;RAM — 2GB;Onboard eMMC Flash — 64GB;Wi-Fi — Azurewave AW-NM372SM (IEEE 802.11 b/g/n + BT + FM);Ethernet — MotorComm YT8512C (10/100 Mbps);LoRa Module — SX1302.Особое внимание стоит уделить модулю концентратора LoRa. Он выполнен в форм-факторе Mini PCI-e, а значит, при необходимости его легко можно интегрировать в любой ПК или ноутбук для исследовательских целей:Еще стало ясно, что этот одноплатник можно запустить с MicroSD-карты памяти, но это потребует определенных танцев с бубном. На этом этапе я наткнулся на проект crankk.io, ставший альтернативой сети Helium. Его авторы смекнули, что на руках у пользователей осталось множество бесполезных устройств, и решили вдохнуть в них новую жизнь в виде собственного образа операционной системы Crankk OS.Смена ОСПерепрошивка состоит из двух этапов. Первым нужно скачать образ патча (эта ссылка только для модели G285) на SD-карту, загрузить устройство и несколько минут подождать. После этого bootloader будет заменен, и теперь можно стартовать кастомную ОС.Стоит понимать, что после того как вы запустите процесс замены загрузчика, простого пути назад уже не будет. Дважды подумайте, а надо ли оно вам. Но если все же решитесь, то помните: процедура может окирпичить устройство. Изредка возможна ситуация, когда гаджет залипает в Maskrom-режиме (aka DFU), и поэтому авторы выложили еще один специальный образ M-VERSION, призванный помочь загрузиться. Плюс на самой плате есть кнопка Recovery, которая принудительно этот режим включает:Та самая кнопка для перехода в Maskrom и кнопка перезапуска платыАвторы позиционируют Crankk OS как бесплатную систему с открытым исходным кодом. Подразумевается, что она поможет использовать устройство в их собственной сети, но при этом обеспечит полноценный root-доступ. Захотели в сеть Crankk — есть простая процедура онбординга, после которой девайс начинает майнить токен CRKK в сети Kadena. Не хотите этим заниматься, удаляете контейнеры с ПО, отключаете механизм автоматического восстановления и ставите нужный вам софт.Я не буду рассказывать про то, что это за сеть и как с ней работать. Мне было интересно ответить на вопрос, смогу ли я использовать «железку» для запуска собственных приложений.После перепрошивкиПо умолчанию при загрузке нужно подключить Bobcat Miner к сети Ethernet-кабелем для получения настроек DHCP. Теперь можно постучаться по SSH. Дефолтные реквизиты: логин — crankk;пароль — B@tch0n3.Я поменял пароль на собственный и стал смотреть, что же внутри этой замечательной системы. Для начала то, как она себя определяет:# uname -a  Linux crankk-6fb3ed72 4.19.232 #2 SMP Thu Oct 3 18:28:41 UTC 2024 aarch64 GNU/LinuxДальше информация о процессоре (серийник затер):# cat /proc/cpuinfo  processor       : 0 BogoMIPS        : 48.00 Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdh                                                                                                                                                             p cpuid asimdrdm lrcpc dcpop asimddp CPU implementer : 0x41 CPU architecture: 8 CPU variant     : 0x2 CPU part        : 0xd05 CPU revision    : 0  processor       : 1 BogoMIPS        : 48.00 Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdh                                                                                                                                                             p cpuid asimdrdm lrcpc dcpop asimddp CPU implementer : 0x41 CPU architecture: 8 CPU variant     : 0x2 CPU part        : 0xd05 CPU revision    : 0  processor       : 2 BogoMIPS        : 48.00 Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdh                                                                                                                                                             p cpuid asimdrdm lrcpc dcpop asimddp CPU implementer : 0x41 CPU architecture: 8 CPU variant     : 0x2 CPU part        : 0xd05 CPU revision    : 0  processor       : 3 BogoMIPS        : 48.00 Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdh                                                                                                                                                             p cpuid asimdrdm lrcpc dcpop asimddp CPU implementer : 0x41 CPU architecture: 8 CPU variant     : 0x2 CPU part        : 0xd05 CPU revision    : 0  Hardware        : Rockchip RK3566 Tiansizhihui Board Serial          : XXXXXXXXXXXXXXXXОдним глазком взглянул на загружаемые модули:  # lsmod  Module                  Size  Used by    Not tainted cywdhd                901120  0  cywdhd — это драйвер Azurewave Wi-Fi. Затем посмотрел на точки монтирования:  Filesystem                Size      Used Available Use% Mounted on /dev/root               468.0M    285.5M    147.5M  66% / devtmpfs                977.0M         0    977.0M   0% /dev tmpfs                   985.8M         0    985.8M   0% /dev/shm tmpfs                   985.8M    496.0K    985.3M   0% /tmp /dev/mmcblk1p1           29.9M     27.3M      2.7M  91% /boot /dev/mmcblk1p3           57.5G      3.1G     51.4G   6% /data overlay                  57.5G      3.1G     51.4G   6% /usr overlay                  57.5G      3.1G     51.4G   6% /var/log overlay                  57.5G      3.1G     51.4G   6% /var/lib cgroup                  985.8M         0    985.8M   0% /sys/fs/cgroup overlay                  57.5G      3.1G     51.4G   6% /data/docker/overlay2/76aeccaae63474f69eb9d1e44b86600bcbeba3d6889d79d1b43f1a3c9f99e553/merged overlay                  57.5G      3.1G     51.4G   6% /data/docker/overlay2/8f8ab01d393f5e2cd42eb0b3df04ee7c4f2869d11ba81705fd58b91499bddf2e/merged overlay                  57.5G      3.1G     51.4G   6% /data/docker/overlay2/e6e74ba662e835c7c1da7fcc04468ee2f63d42bd3eb25bb7f3e1746f2f4cc198/merged overlay                  57.5G      3.1G     51.4G   6% /data/docker/overlay2/0dd8b2c3e3565ee0f38e1e1dea9ed7f7956c3ba882e7a7003a3430f280c1e242/merged overlay                  57.5G      3.1G     51.4G   6% /data/docker/overlay2/a7f7c721fd83a40eb1c4aa740dcfa847c955c0b088dc383308c1b37601ea7284/merged Хорошо видно, что тут весь софт работает в контейнерах. Для удобства накатил Portainer Community Edition. Вначале создал том, где хранить данные:# docker volume create portainer_dataСкачал и запустил нужный мне образ:# docker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:ltsКогда девайс настроен, внутри крутятся четыре контейнера, сохраняющие информацию в директорию /data/docker/volumes/crankk_data/_data. При необходимости все эти контейнеры и образы легко убираются, а следы вычищаются.Вместо заключенияПолучается, что итогом такой операции стал полный контроль над устройством, где я могу использовать предварительно настроенный Docker или вообще удалить все с этим связанное и ставить что-то свое с нуля. Вместо нерабочего майнера я получил вполне приятный, хоть и специфический одноплатный компьютер с поддержкой LoRa, который можно использовать в своих целях. Например, настроить на нем Meshtastic или вообще превратить в небольшой VPN-сервер.Я также хочу проверить LoRa-модуль вне этого устройства, подключив его с антенной к ноутбуку. Возможно, тоже хорошее начало для различных экспериментов. Если я бы покупал такой девайс отдельно, то это бы вышло в 10 раз дороже, чем внутри майнера.Мне нравится давать новую жизнь неактуальным и устаревшим устройствам. Так что моему Bobcat Miner 300 повезло: он точно не закончит свой путь в коробке со старыми китайскими роутерами, а продолжит трудиться, но уже в других проектах с LoRaWAN.Как вам девайс и что думаете по поводу Crankk OS? Жду вас в комментариях."
103,103,Неинтеграционное тестирование интеграционных потоков. Или интеграционное?,АШАН Тех,Генерируем идеи. Разрабатываем решения,61.25,"Программное обеспечение, Электронная коммерция, Информационная безопасность",2025-03-12,"Я работаю в тестировании 2 года – это не много и немало, но достаточно, чтобы понимать, что такое интеграционное тестирование и уметь с этим работать. Интеграция – это взаимодействие любых частей внутри целого. Бэкенд и фронтенд, бэкенд и база данных, два микросервиса между собой, процесс авторизации в приложении с помощью стороннего сервиса (почта, облачный сервис и так далее) – все это интеграции. И это база, которой учат/учатся (у кого как), чтобы дальше применять в своей работе. И вот, я пришел работать в АШАН ТЕХ и узнал об интеграционных потоках – сущности, которая заставляет задуматься о том, относится ли это к интеграции, имеет свои особенности тестирования и, что самое интересное, не встретилось мне ни в одной статье (но может где-то и есть).Попробуем описать: интеграционные потоки – это способы взаимодействия одной системы (в рамках складских перемещений, например) или различных систем бизнеса (поставщик – склад, витрина – каталоги провайдеров – заказы поставщикам, например) посредством передачи файлов с определенными масками и их маппинга (это не обязательное условие) в необходимые конечным получателям форматы. Проще говоря, в наших системах – это потоки файлов, которые содержат в себе информацию, необходимую для функционирования различных бизнес-подразделений (магазины – склады – поставщики), и передающиеся от Источника к Получателю.Здесь они делятся на два вида: Airflow потоки и Streamflow потоки.Airflow потоки – это все потоки, которые запускаются по триггеру при необходимости работы с ними. Для работы с ними мы используем Apache Airflow (что можно было понять по названию потоков) как самый удобный инструмент для работы с batch-процессами. Я не буду описывать как работает Apache Airflow – здесь много статей об этом, но если вы хотите понимать, что будет происходить дальше – советую их прочитать. Но по сути, наш поток — это просто Python-проект, который представлен в Apache Airflow в виде основной сущности DAG (некоторое смысловое объединение ваших задач, которые вы хотите выполнить в строго определенной последовательности по определенному расписанию). Если посмотреть детальнее, то заметно, что поток представляет собой здесь набор определенных задач, которые выполняются друг за другом при успешном запуске через play: Или же идут по продуманному в случае ошибок потока (нет соединения, нет файлов для передачи и так далее) флоу:Конфигурации потока в airflow задаются через импорт переменных:  Здесь прописывается откуда берутся данные, куда передаются, сколько раз переподключается поток при ошибке, какие БД (если есть с ними взаимодействие) подключаются по каким схемам, порты серверов источника и получателя и т.д. Данные по подключениям (в основном это пользователь, хост, порты и информация о том, какой ключ используется при подключении – не сам ключ или пароль ни в коем случае!!!) загружаются в раздел connections:Логирование происходит здесь же в Apache Airflow – все рядом близко и удобно.Streamflow потоки – это все потоки, которые работают непрерывно и обеспечивают постоянный обмен данными между системами в рамках интеграций. Их конфигурации можно задавать по-разному, указывать sleep timeoutы, итерации, но суть одна – работать они будут непрерывно 24 часа.Пишутся эти потоки также на Python, поднимаются из докера, разворачиваются в ci gitlab и отличаются тем, что после разработки и запуска в пайплайне (разработчиком или тестировщиком – бывает по-разному) больше не останавливаются (если нет критических ошибок).Логирование здесь мы отслеживаем через OpenSearch Dashboards, который дает полноценную информацию по всем шагам потока и возможность видеть, на каком этапе происходит передача данных и куда.Оба вида этих потоков можно разделить на степени сложности:1) Простые – передают файлы из папки одной системы в папку другой (обмен данными между поставщиками – магазинами), забирают данные из одной БД и складывают в другую перезаписью или обновлением, без маппинга данных.2) Среднесоставные – передают файлы в три или более систем, которые взаимодействуют между собой, где данные файлов должны проходить маппинг.3) Сложносоставные – файлы передаются в различные системы и БД, в том числе взаимодействуют с брокерами сообщений, маппинг может происходить несколько раз или же зависеть от системы к системе, в которую данные должны подаваться. Для передачи файлов используются протоколы FTP и SFTP.Этого достаточно, чтобы понять разницу в тестировании и его особенностях, о которых мы поговорим далее.Тестирование Streamflow потоков.Основными тестовыми артефактами для нас в тестировании потоков являются чек-листы для простых потоков, тест-кейсы для средних и сложных потоков (примеры будут приведены ниже), а также непосредственно инструменты для тестирования, такие как:1) PuTTy – используем ее для удаленного администрирования Linux серверов через Windows, если нужно создать папки/каталоги/файлы для потоков, а также, чтобы обмениваться файлами между локальным компьютером и удалённым. Важным преимуществом является поддержка разных версий SSH-протокола, что обеспечивает передачу данных через защищённое соединение, дистанционный запуск программ, сжатие файлов для быстрой передачи, передачу шифрованного трафика между портами разных машин. Также возможно перенаправление портов через протокол SSH.2) WinSCP — поддерживает передачу файлов между компьютером и сервером по протоколу SFTP на устройствах с ОС Windows. Утилита работает по защищённому протоколу SSH и не перегружена редко используемыми функциями, как некоторые аналоги. Это позволяет максимально быстро выполнять нужные задачи:- редактировать, копировать, загружать, удалять файлы на сервере;- создавать сразу несколько подключений к разным серверам;- пользоваться командной строкой.3) Любая СУБД – мы пользуемся Dbeaver – для селектов, инсертов, изменений данных в таблицах и проверки переданных данных в БД получателя.4) AKHQ графический интерфейс Kafka при передаче больших потоковых данных и проверке сообщений в брокере.Как выглядит основной флоу тестирования streamflow потоков у нас:1) Разработчик заканчивает поток, разворачивает его из докера с помощью ci gitlab и передает на тестирование;2) Тестировщик читает ТЗ потока, проверяет конфигурации потока в gitlab на соответствие с ТЗ – каталоги, сервера источника-получателя, установленные timeout’ы и итерации потока;3) Тестировщик подготавливает тестовые данные: при хорошо составленном ТЗ берет данные, которые подложили аналитики, если нет – сам готовит файлы;4) Проверяет соответствие маски файлов по ТЗ для потока;5) Далее необходимо подложить файлы на сервер источника через инструменты выше;6) Тестировщик отслеживает по логам действия потока и фиксирует данные;7) Далее переходит на сервер получателя данных и проверяет, что файл передан по необходимому каталогу в нужную систему;8) Если есть маппинг – проверяет соответствие данных ТЗ в файле источника и получателя;9) В OpenSearch Dashboard смотрит логи – проверяет их на соответствие с ТЗ; 10) Готовит отчет по тестированию и передает команде разработки и владельцам продукта.Примеры чек-листов для непрерывного потока: Логика работы потока1. Поток отрабатывает полный цикл;2. Подключение к SFTP источника;3. При неудачном подключении к источнику поток переподключается 3 раза с интервалом 30 секунд;4. Поиск исходной папки “Путь к папке на сервере источника”;5. Поиск входного файла по маске ""Маска файлов"";6. При некорректном имени входной файл игнорируется;7. Выходной файл скопирован в папку на SFTP приёмника “Путь к папке на сервере получателя”;8. При неудачной отправке входной файл перемещается в папку “Путь к каталогу при неудачной отправке""; 9. Удаление входного файла из исходной папки после отработки;10. Обработка нескольких входных файлов;11. Запуск потока при отсутствии прав у рабочих папок;12. Запуск при отсутствии доступа к рабочим серверам.Логирование потока1. Лог подключения к SFTP источника;2. Лог скачивания входного файла;3. Лог загрузки выходного файла в конечную папку;4. Лог удаления входного файла с SFTP источника;5. Ошибка подключения к SFTP источника;6. Ошибка подключения к SFTP приёмника;7. Ошибка доступа к исходной папке на SFTP источника;8. Ошибка доступа к конечной папке на SFTP приёмника.Примеры тест-кейсов для непрерывного потока:1. Проверка подключения к ИсточникуТест-кейс 1.1  Название: Повторное подключение к Источнику при неудаче  Предусловия: Источник недоступен  Шаги:  1. Проверить, что поток повторяет попытки подключения каждые 5 минут.Ожидаемый результат: Попытки подключения повторяются каждые 5 минут.2. Проверка определения количества файлов в каталоге ИсточникаТест-кейс 2.1  Название: Определение количества файлов  Предусловия: В каталоге Источника есть файлы с нужной маской  Шаги:  1. Проверить количество файлов.Ожидаемый результат: Количество файлов определено правильно.Тест-кейс 2.2  Название: Обработка отсутствия файлов в каталоге Источника  Предусловия: В каталоге Источника нет файлов с нужной маской  Шаги:  1. Проверить, что поток отключается от Источника.Ожидаемый результат: Поток отключается от Источника.При участии в тестировании БД – тестировщик проверяет БД источника. Таблицу, из которой берутся данные, и таблицы, куда данные перезаписываются. Тест-кейс:Название: Проверка обработки файла, по которому нет информации в DWHПредусловия: В каталог источника помещен файл, информации по файлу нет в DWH, нижеприведённые SELECT ничего не возвращаютШаги:  1. Добавить на источник файл с заказом Промежуточный ожидаемый результат: Поток не найдет данных по заказу в DWH, добавит информацию о заказе в файл2. После удаления файла с источника потоком выполнить 3 запроса INSERT в DWHОжидаемый результат: На следующей итерации поток удалит запись с номером заказа из файла, записи по другим заказам остаются в файле, сформирован файл на получателе.Тестирование Airflow потоков.При тестировании потоков, которые запускаются по триггеру основным инструментом для запуска потока и отслеживания логики, логов, всех шагов для нас является Apache Airflow. Все что касается добавления файлов, проверки таблиц – то инструментарий абсолютно такой же. Флоу тестирования airflow потоков:1) Разработчик заканчивает поток, разворачивает его и загружает необходимую конфигурацию в Apache Airflow и передает на тестирование;2) Тестировщик читает ТЗ потока, проверяет конфигурации потока в variables и connections в Apache Airflow на соответствие с ТЗ – каталоги, сервера источника-получателя, установленные timeoutы и итерации потока;3) Тестировщик подготавливает тестовые данные: при хорошо составленном ТЗ берет данные, которые подложили аналитики, если нет – сам готовит файлы; 4) Проверяет соответствие маски файлов по ТЗ для потока;5) Далее необходимо подложить файлы на сервер источника через инструменты выше или же запустить поток по триггеру, если он не использует файлы, а перезаписывает данные баз данных (потоки для изменения информации витрин);6) Тестировщик отслеживает по графике в Apache Airflow действия потока и фиксирует данные в логах здесь же; 7) Далее переходит на сервер получателя данных и проверяет, что файл передан по необходимому каталогу в нужную систему или в БД проверяя, что данные в таблице заместились новыми и есть timestamp отметка об этом;8) Если есть маппинг – проверяет соответствие данных ТЗ в файле источника и получателя;9) Готовит отчет по тестированию и передает команде разработки и владельцам продукта.Примеры чек-листов для непрерывного потока: Логика работы потока1. Поток отрабатывает полный цикл;2. Подключение к системе-источника;3. Запись новых данных в таблицу ""Название таблицы 1"";4. Запись новых данных в таблицу ""Название таблицы 2"";5. Запись новых данных в таблицу ""Название таблицы 3"";...10. Запуск при отсутствии доступа к БД;11. Запуск при отсутствии доступа к рабочим серверам;12. В случае невозможности подключения к системе-источнику поток должен сформировать лог ERROR;13. После восстановления доступа и получения новых или измененный записей, поток должен добавить записи в соответствующие таблицы в ods-слоя.В таком виде потоков мы практически не используем тест-кейсы, так как в основном они простые или средней сложности. Я не знаю, есть ли такие потоки где-то еще в бизнесе в сфере розничной торговли продуктами или иной, но это надежно. Это работает и не требует бешеных затрат на разработку, что сейчас немаловажно. Дальше я буду писать подробно про примеры, с тестированием которых я столкнулся в своей работе.Ну и самый главный вопрос: интеграционное это тестирование или же нет? Я считаю, что да – все признаки этого имеются, потому что наши потоки могут взаимодействовать внутри одной системы или же с другими системами, что в конечном итоге приводит к корректной работе бизнес-юнитов. А как считаете вы?"
104,104,Как Linux готовится ко сну,Флант,"DevOps-as-a-Service, Kubernetes, обслуживание 24×7",4.48,"Программное обеспечение, Консалтинг и поддержка, Веб-сервисы",2025-03-12,"Перевели для вас статью Джейкоба Адамса о том, что происходит перед тем, как Linux уходит в сон. Дальше идёт текст оригинала.Как Linux переходит в сон? Как ему потом удаётся восстановить первоначальное состояние? Пытаясь понять, где проходит граница между аппаратным и программным обеспечением, я с головой зарылся в глубины языка С.Мое исследование разделено на несколько частей. В первой речь пойдёт о периоде от вызова режима гибернации до синхронизации всех файловых систем на диск.Эта статья написана для Linux версии 6.9.9. Её исходники широко доступны, но проще всего их посмотреть на Bootlin Elixir Cross-Referencer. Каждый фрагмент кода будет начинаться со ссылки на вышеуказанный ресурс с указанием пути к файлу и номера строки, с которой начинается фрагмент.Содержание:Отправная точка: /sys/power/state и /sys/power/diskЭтапы перехода в гибернацию:Настройка спящего режима в ядреПроверка доступности спящего режимаПроверка сжатияЗахват блокировокПодготовка консолиУведомление цепочки вызовов управления питаниемСинхронизация файловых системКонец подготовкиОтправная точка: /sys/power/state и /sys/power/diskЭти два системных файла помогают отладить режим гибернации и, таким образом, непосредственно контролируют состояние. Записывая определённые значения в файл /sys/power/state, можно задавать конкретный тип энергопотребления (например, freeze, mem или disk), при этом disk предписывает системе перейти в режим гибернации. Файл /sys/power/disk контролирует, в какой именно режим гибернации перейдёт система  (например, в platform — см. ссылку выше).Разверни, чтобы узнать подробности о режимах энергопотребления в Linux.Режимы энергопотребления в LinuxСледует несколько слов сказать о режимах энергопотребления в Linux. Концептуально ядро поддерживает 4 основных состояния (см. System Sleep State):Suspend-to-idle. Процессоры переводятся в состояния простоя. Используется подсистема cpuidle. Замораживается пространство пользователя, приостанавливается отсчёт времени, все устройства ввода-вывода переводятся в состояние с низким энергопотреблением.Suspend-to-standby. В настоящее время этот режим почти не встречается.  См. доклад Лена Брауна «Is Linux Suspend ready for the next decade».Suspend-to-RAM. Отключаются все процессоры, кроме загрузочного (на машинах с несколькими процессорами). В зависимости от возможностей платформы могут выполняться дополнительные действия. В частности, в системах на базе ACPI ядро передаёт управление микропрограмме платформы (BIOS) в качестве последнего шага при переходе в S2RAM, что обычно приводит к отключению питания ещё нескольких низкоуровневых компонентов, которые не контролируются ядром напрямую. Состояние устройств и процессоров сохраняется в память и хранится в ней. Все устройства приостанавливаются и переводятся в состояние низкого энергопотребления. Во многих случаях при входе в S2RAM периферийные шины теряют питание, поэтому устройства должны уметь возвращаться во «включённое» состояние.Suspend-to-disk, также известный как гибернация. Создаётся образ текущего состояния системы. Образ сохраняется на диск, а система выключается. При следующем включении системы этот образ используется для восстановления состояния.Suspend-to-disk отличается от других состояний. Пробуждение после него больше похоже на перезагрузку, чем на возобновление работы в трёх других методах. Его не включают pm labels[] и mem_sleep_labels[] в kernel/power/suspend.c, а основные функции реализованы в kernel/power/hibernate.c вместо kernel/power/suspend.c.Неразбериха с названиямиLinux прокидывает вышеперечисленные состояния питания в различные интерфейсы. В итоге все перечисленные выше состояния питания (кроме suspend-to-disk) везде называются по-разному.suspend-to-{idle, standby, ram} — общие термины, относящиеся к реальным базовым механизмам.freeze, standby, mem относятся к интерфейсу sysfs. Важно: mem — это настраиваемое состояние, и оно может указывать на любое состояние из пункта 1.s2idle, shallow, deep предназначены для настройки mem. Они нужны, чтобы прокинуть состояние из пункта 1 в mem из пункта 2.Ещё раз:suspend-to-idle/standby/ram — общие термины для состояний питания;feeze, standby, mem — термины, используемые интерфейсом sysfs./sys/power/state в sysfs — интерфейс для управления состоянием энергопотребления системы в Linux. С помощью cat /sys/power/state можно посмотреть поддерживаемые состояния энергопотребления вашей системы (например, freeze, mem, disk). Если в этот файл записать одно из состояний, система в него перейдёт.В данном случае freeze означает переход в режим suspend-to-idle, а disk — переход в режим suspend-to-disk. С параметром mem сложнее, поскольку он настраивается пользователем. Он может переводить систему в suspend-to-ram (если этот режим поддерживается) или suspend-to-idle в качестве резервного механизма, особенно если suspend-to-ram не поддерживается. Конкретным поведением управляет /sys/power/mem_sleep, и у него свой персональный набор обозначений для этих состояний: s2idle, shallow, deep — термины для настройки поведения mem.Здесь s2idle (как вы догадались) означает suspend-to-idle; shallow — suspend-to-standby; а deep — suspend-to-ram. Файл /sys/power/mem_sleep определяет, в какой режим перейдёт система, когда mem запишется /sys/power/state. Посмотреть доступные опции можно с помощью команды cat /sys/power/mem_sleep.const char * const pm_labels[] = {  [PM_SUSPEND_TO_IDLE] = ""freeze"",  [PM_SUSPEND_STANDBY] = ""standby"",  [PM_SUSPEND_MEM] = ""mem"", }; const char *pm_states[PM_SUSPEND_MAX]; static const char * const mem_sleep_labels[] = {  [PM_SUSPEND_TO_IDLE] = ""s2idle"",  [PM_SUSPEND_STANDBY] = ""shallow"",  [PM_SUSPEND_MEM] = ""deep"", };Источники:https://hackmd.io/@0xff07/linux-pm/%2F%400xff07%2FrkmMQqbu6https://www.kernel.org/doc/html/latest/admin-guide/pm/sleep-states.htmlЭто удобно для понимания работы систем — можно отследить изменения после записи новых значений.Функции show и storeЭти две функции определяются с помощью макроса power_attr:kernel/power/power.h:80#define power_attr(_name) \ static struct kobj_attribute _name##_attr = {   \     .attr   = {             \         .name = __stringify(_name), \         .mode = 0644,           \     },                  \     .show   = _name##_show,         \     .store  = _name##_store,        \ }show вызывается при чтении, а store — при записи.state_show не подходит для наших целей, поскольку просто выводит все доступные состояния сна.kernel/power/main.c:657/*  * state контролирует состояния сна системы.  *  * show() возвращает доступные состояния сна: ""mem"", ""standby"",  * ""freeze"" и ""disk"" (гибернация).  * Описание режимов см. в Documentation/admin-guide/pm/sleep-states.rst  *   * store() принимает одно из этих строковых значений, преобразует его  * в соответствующее численное значение и запускает переход в сон.  */ static ssize_t state_show(struct kobject *kobj, struct kobj_attribute *attr,      char *buf) {  char *s = buf; #ifdef CONFIG_SUSPEND  suspend_state_t i;   for (i = PM_SUSPEND_MIN; i < PM_SUSPEND_MAX; i++)   if (pm_states[i])    s += sprintf(s,""%s "", pm_states[i]);  #endif  if (hibernation_available())   s += sprintf(s, ""disk "");  if (s != buf)   /* convert the last space to a newline */   *(s-1) = '\n';  return (s - buf); }С другой стороны, state_store обеспечивает нужную точку входа. Если в файл state записать значение «disk», вызывается hibernate(). Это наша точка входа.kernel/power/main.c:715static ssize_t state_store(struct kobject *kobj, struct kobj_attribute *attr,       const char *buf, size_t n) {  suspend_state_t state;  int error;   error = pm_autosleep_lock();  if (error)   return error;   if (pm_autosleep_state() > PM_SUSPEND_ON) {   error = -EBUSY;   goto out;  }   state = decode_state(buf, n);  if (state < PM_SUSPEND_MAX) {   if (state == PM_SUSPEND_MEM)    state = mem_sleep_current;    error = pm_suspend(state);  } else if (state == PM_SUSPEND_MAX) {   error = hibernate();  } else {   error = -EINVAL;  }   out:  pm_autosleep_unlock();  return error ? error : n; }kernel/power/main.c:688static suspend_state_t decode_state(const char *buf, size_t n) { #ifdef CONFIG_SUSPEND  suspend_state_t state; #endif  char *p;  int len;   p = memchr(buf, '\n', n);  len = p ? p - buf : n;   /* Сначала проверяем гибернацию. */  if (len == 4 && str_has_prefix(buf, ""disk""))   return PM_SUSPEND_MAX;  #ifdef CONFIG_SUSPEND  for (state = PM_SUSPEND_MIN; state < PM_SUSPEND_MAX; state++) {   const char *label = pm_states[state];    if (label && len == strlen(label) && !strncmp(buf, label, len))    return state;  } #endif   return PM_SUSPEND_ON; }Можно ли было догадаться просто по именам функций? Да, но так мы точно знаем, что до вызова этой функции ничего другого не происходит.AutosleepНаша первая остановка — система autosleep. Во фрагменте кода выше видно, что ядро блокирует pm_autosleep_lock перед проверкой текущего состояния.Механизм autosleep позаимствован у Android. Он отправляет всю систему в режим ожидания (suspend) или гибернации (hibernate), когда та неактивна. В большинстве десктопных конфигураций эта функция отключена, поскольку она предназначена в первую очередь для мобильных систем и переопределяет то, как режимы ожидания и гибернации работают в обычных условиях.Все реализовано в виде очереди workqueue, которая проверяет текущее количество событий пробуждения (wakeup), процессов и драйверов, которые должны быть запущены. И если их нет, система переводится в состояние autosleep (обычно в режим ожидания — suspend). Тем не менее это может быть и режим гибернации — достаточно настроить его через /sys/power/autosleep подобно тому, как /sys/power/state включает режим гибернации по команде пользователя.kernel/power/main.c:841static ssize_t autosleep_store(struct kobject *kobj,           struct kobj_attribute *attr,           const char *buf, size_t n) {  suspend_state_t state = decode_state(buf, n);  int error;   if (state == PM_SUSPEND_ON      && strcmp(buf, ""off"") && strcmp(buf, ""off\n""))   return -EINVAL;   if (state == PM_SUSPEND_MEM)   state = mem_sleep_current;   error = pm_autosleep_set_state(state);  return error ? error : n; }  power_attr(autosleep); #endif /* CONFIG_PM_AUTOSLEEP */kernel/power/autosleep.c:24 static DEFINE_MUTEX(autosleep_lock); static struct wakeup_source *autosleep_ws;  static void try_to_suspend(struct work_struct *work) {  unsigned int initial_count, final_count;   if (!pm_get_wakeup_count(&initial_count, true))   goto out;   mutex_lock(&autosleep_lock);   if (!pm_save_wakeup_count(initial_count) ||   system_state != SYSTEM_RUNNING) {   mutex_unlock(&autosleep_lock);   goto out;  }   if (autosleep_state == PM_SUSPEND_ON) {   mutex_unlock(&autosleep_lock);   return;  }  if (autosleep_state >= PM_SUSPEND_MAX)   hibernate();  else   pm_suspend(autosleep_state);   mutex_unlock(&autosleep_lock);   if (!pm_get_wakeup_count(&final_count, false))   goto out;   /*   * Если пробуждение произошло по неизвестной причине, ждём,   * чтобы избежать бесконечного цикла засыпаний и пробуждений.   */  if (final_count == initial_count)   schedule_timeout_uninterruptible(HZ / 2);   out:  queue_up_suspend_work(); }  static DECLARE_WORK(suspend_work, try_to_suspend);  void queue_up_suspend_work(void) {  if (autosleep_state > PM_SUSPEND_ON)   queue_work(autosleep_wq, &suspend_work); }Этапы перехода в гибернациюНастройка режима гибернации в ядреВажно отметить, что большинство функций, связанных с режимом гибернации, ничего не делают, если в конфигурации Kconfig не задано значение CONFIG_HIBERNATION. Например, вот как выглядит функция hibernate, если переменная CONFIG_HIBERNATE не установлена.include/linux/suspend.h:407static inline int hibernate(void) { return -ENOSYS; }Проверка доступности режима гибернацииПроверим с помощью функции hibernation_available, доступен ли режим.kernel/power/hibernate.c:742if (!hibernation_available()) {  pm_pr_dbg(""Hibernation not available.\n"");  return -EPERM; }kernel/power/hibernate.c:92 bool hibernation_available(void) {  return nohibernate == 0 &&   !security_locked_down(LOCKDOWN_HIBERNATION) &&   !secretmem_active() && !cxl_mem_active(); }Параметр nohibernate управляется флагами командной строки ядра. Его можно установить, задав либо nohibernate, либо hibernate=no.Хук security_locked_down для модулей безопасности Linux (Linux Security Modules) предотвращает переход в режим гибернации. Он не даёт перейти в него, если состояние записывается на незашифрованное устройство хранения (см. kernel_lockdown(7)). Любопытно, что любой уровень блокировки, будь то integrity или confidentiality (см. Linux kernel lockdown, integrity, and confidentiality. — Прим. пер.), запрещает переход в режим гибернации. Потому что в ином случае из сохранённого на диск образа ядра можно извлечь практически всё, что угодно, внести в него изменения и даже перезагрузиться с этим образом.secretmem_active проверяет, используется ли memfd_secret, и если да, то предотвращает переход в режим гибернации. memfd_secret возвращает файловый дескриптор, который можно отобразить в память процесса, но эта память отключена от адресного пространства ядра. Переход в гибернацию с памятью, к которой не имеет доступа даже ядро, открывает доступ к этой памяти тому, у кого есть доступ к образу гибернации. Эта особенность секретной памяти вызвала много споров, хотя и не таких, как опасения по поводу фрагментации при разворачивании памяти ядра (которая в итоге так и не стала реальной проблемой).cxl_mem_active просто проверяет, активна ли память CXL. Полное объяснение приведено в коммите, реализующем эту проверку. Но есть и сокращённое объяснение в cxl_mem_probe, которая устанавливает соответствующий флаг при инициализации устройства памяти CXL.drivers/cxl/mem.c:186Ядро может работать с CXL-памятью на этом устройстве.  * Не существует определённого в спецификации способа определить, * сохраняет ли это устройство содержимое при переходе  * в режим ожидания (suspend), и нет простого способа  * организовать suspend-образ таким образом, чтобы обойти CXL-память,  * но это создало бы циклическую зависимость:  * для восстановления состояния CXL-памяти нужно сначала  * восстановить работу PCI-шины, но для полного восстановления  * состояния системы нужно восстановить содержимое всей памяти,  * включая CXL.Проверка сжатияДалее необходимо проверить, включена ли поддержка сжатия, и если да, то включён ли требуемый алгоритм.kernel/power/hibernate.c:747/*  * Узнаём, какой алгоритм сжатия поддерживается, если сжатие включено.  */ if (!nocompress) {  strscpy(hib_comp_algo, hibernate_compressor, sizeof(hib_comp_algo));  if (crypto_has_comp(hib_comp_algo, 0, 0) != 1) {   pr_err(""%s compression is not available\n"", hib_comp_algo);   return -EOPNOTSUPP;  } }Флаг nocompress устанавливается с помощью параметра командной строки hibernate: hibernate=nocompress.Если сжатие включено, то hibernate_compressor копируется в hib_comp_algo. Так запрашиваемая настройка сжатия (hibernate_compressor) синхронизируется с текущей настройкой сжатия (hib_comp_algo).Оба значения являются символьными массивами размера CRYPTO_MAX_ALG_NAME (128 в данном ядре).kernel/power/hibernate.c:50static char hibernate_compressor[CRYPTO_MAX_ALG_NAME] = CONFIG_HIBERNATION_DEF_COMP;  /*  * Алгоритм сжатия/распаковки, который будет использоваться   * при сохранении/загрузке образа на диск. Позже он будет использован   * в файле 'kernel/power/swap.c' для распределения потоков   * сжатия.  */ char hib_comp_algo[CRYPTO_MAX_ALG_NAME];hibernate_compressor по умолчанию использует lzo, если этот алгоритм включён, иначе используется lz4. С помощью hibernate_compressor значение по умолчанию можно переопределить на lzo или lz4.kernel/power/Kconfig:95choice  prompt ""Default compressor""  default HIBERNATION_COMP_LZO  depends on HIBERNATION  config HIBERNATION_COMP_LZO  bool ""lzo""  depends on CRYPTO_LZO  config HIBERNATION_COMP_LZ4  bool ""lz4""  depends on CRYPTO_LZ4  endchoice  config HIBERNATION_DEF_COMP  string  default ""lzo"" if HIBERNATION_COMP_LZO  default ""lz4"" if HIBERNATION_COMP_LZ4  help    Default compressor to be used for hibernation.kernel/power/hibernate.c:1425static const char * const comp_alg_enabled[] = { #if IS_ENABLED(CONFIG_CRYPTO_LZO)  COMPRESSION_ALGO_LZO, #endif #if IS_ENABLED(CONFIG_CRYPTO_LZ4)  COMPRESSION_ALGO_LZ4, #endif };  static int hibernate_compressor_param_set(const char *compressor,   const struct kernel_param *kp) {  unsigned int sleep_flags;  int index, ret;   sleep_flags = lock_system_sleep();   index = sysfs_match_string(comp_alg_enabled, compressor);  if (index >= 0) {   ret = param_set_copystring(comp_alg_enabled[index], kp);   if (!ret)    strscpy(hib_comp_algo, comp_alg_enabled[index],     sizeof(hib_comp_algo));  } else {   ret = index;  }   unlock_system_sleep(sleep_flags);   if (ret)   pr_debug(""Cannot set specified compressor %s\n"",     compressor);   return ret; } static const struct kernel_param_ops hibernate_compressor_param_ops = {  .set    = hibernate_compressor_param_set,  .get    = param_get_string, };  static struct kparam_string hibernate_compressor_param_string = {  .maxlen = sizeof(hibernate_compressor),  .string = hibernate_compressor, };Затем с помощью crypto_has_comp проверяется, поддерживается ли запрашиваемый алгоритм. Если он не поддерживается, процесс гибернации прерывается с ошибкой EOPNOTSUPP.В рамках crypto_has_comp алгоритм инициализируется, загружаются нужные модули ядра и выполняется код инициализации.Захват блокировокСледующим шагом будет захват блокировок сна и гибернации с помощью lock_system_sleep и hibernate_acquire:kernel/power/hibernate.c:758sleep_flags = lock_system_sleep(); /* Snapshot-устройство не должно открываться, пока мы работаем */ if (!hibernate_acquire()) {  error = -EBUSY;  goto Unlock; }Сначала lock_system_sleep помечает текущий поток (thread) как незамораживаемый (frozen), что очень пригодится в дальнейшем. Затем он захватывает system_transistion_mutex, который блокирует создание снапшотов или изменение способа их создания, возобновление работы из образа гибернации, переход в любое suspend-состояние или перезагрузку.Маска GFPЯдро выдаёт предупреждение, если маска gfp изменяется с помощью pm_restore_gfp_mask или pm_restrict_gfp_mask без удержания system_transistion_mutex.Флаги GFP определяют, как ядру разрешено обрабатывать запрос на память.include/linux/gfp_types.h:12* Флаги GFP широко используются в Linux при распределении памяти. Аббревиатура GFP  * означает get_free_pages(), базовую функцию распределения памяти. Не все флаги GFP  * поддерживаются функциями, способными распределять память.В случае с гибернацией нас интересуют флаги IO и FS, которые являются операторами возврата (reclaim) памяти. Они указывают системе, что можно использовать операции ввода-вывода или файловую систему для освобождения памяти, если это необходимо для выполнения запроса на выделение памяти.include/linux/gfp_types.h:176* Модификаторы возврата  * Обратите внимание, что все последующие флаги применимы только  * к распределениям, поддерживающим спящий режим (например,  * %GFP_NOWAIT и  * %GFP_ATOMIC будут их игнорировать). *  * %__GFP_IO запускает физические операции ввода-вывода.  *  * %__GFP_FS работает с низкоуровневой ФС. Снятие флага позволяет  * аллокатору не обращаться к файловой системе, на которую уже  * навешены блокировки.gfp_allowed_mask определяет, какие флаги разрешено устанавливать в текущий момент.  gfp_allowed_mask — это маска, которая контролирует, какие флаги могут использоваться при выделении памяти. Во время приостановки или гибернации некоторые способы выделения памяти (например, те, которые могут вызвать операции ввода-вывода) становятся опасными. Поэтому система временно запрещает их использование. — Прим. пер.  Как говорится в комментарии ниже, предотвращение установки этих флагов позволяет избежать ситуаций, когда ядру необходимо выполнить операции ввода-вывода для распределения памяти, например чтение из / запись в swap. Но устройства, с которых нужно читать / в которые нужно писать, в данный момент недоступны.kernel/power/main.c:24/*  * Приведённые ниже функции используются кодом suspend/hibernate   * для временного изменения gfp_allowed_mask, чтобы избежать   * использования ввода-вывода при выделении памяти во время приостановки   * работы устройств. Чтобы избежать гонок с кодом suspend/hibernate, их   * всегда следует вызывать при захваченном system_transition_mutex   * (gfp_allowed_mask также следует изменять только при наличии   * system_transition_mutex, если только код suspend/hibernate   * гарантированно не будет выполняться параллельно с этой модификацией).  */ static gfp_t saved_gfp_mask;  void pm_restore_gfp_mask(void) {  WARN_ON(!mutex_is_locked(&system_transition_mutex));  if (saved_gfp_mask) {   gfp_allowed_mask = saved_gfp_mask;   saved_gfp_mask = 0;  } }  void pm_restrict_gfp_mask(void) {  WARN_ON(!mutex_is_locked(&system_transition_mutex));  WARN_ON(saved_gfp_mask);  saved_gfp_mask = gfp_allowed_mask;  gfp_allowed_mask &= ~(__GFP_IO | __GFP_FS); }Sleep-флагиЗаблокировав system_transition_mutex, ядро фиксирует предыдущее состояние флагов потоков в sleep_flags. Потом оно используется при удалении PF_NOFREEZE, если тот не был ранее установлен для текущего потока (то есть если PF_NOFREEZE не был установлен, то вернутся изначальные флаги из sleep_flags, если PF_NOFREEZE был установлен — он и останется. — Прим. пер.).kernel/power/main.c:52unsigned int lock_system_sleep(void) {  unsigned int flags = current->flags;  current->flags |= PF_NOFREEZE;  mutex_lock(&system_transition_mutex);  return flags; } EXPORT_SYMBOL_GPL(lock_system_sleep);include/linux/sched.h:1633#define PF_NOFREEZE  0x00008000 /* Этот поток не замораживать */Затем устанавливается семафор для гибернации, который не позволяет другим процессам открыть снапшот или возобновить работу из него, пока система переходит в режим гибернации. Эта блокировка также не дает вызвать функцию hibernate_quiet_exec, которая используется драйвером nvdimm для активации прошивки с замораживанием всех процессов и устройств. Это нужно, чтобы гарантировать, что в это время работает только сам драйвер.kernel/power/hibernate.c:82bool hibernate_acquire(void) {  return atomic_add_unless(&hibernate_atomic, -1, 0); }Подготовка консолиДалее ядро вызывает pm_prepare_console. Эта функция работает только в том случае, если установлено значение CONFIG_VT_CONSOLE_SLEEP.Она подготавливает виртуальный терминал (VT) к suspend, при необходимости переключаясь на консоль, используемую только для suspend.kernel/power/console.c:130void pm_prepare_console(void) {  if (!pm_vt_switch())   return;   orig_fgconsole = vt_move_to_console(SUSPEND_CONSOLE, 1);  if (orig_fgconsole < 0)   return;   orig_kmsg = vt_kmsg_redirect(SUSPEND_CONSOLE);  return; }Первое, что нужно сделать, — это проверить, действительно ли нужно переключать VT.kernel/power/console.c:94/*  * Есть три случая, когда требуется переключение VT при переходе  * в спящий режим / выходе из него:   * 1) ни один драйвер так или иначе не указал требования, поэтому   * сохраняем старое поведение;  * 2) консольный suspend (параметр no_console_suspend. — Прим. пер.)   * отключён, а мы хотим видеть отладочные сообщения во время перехода   * в спящий режим / выхода из него;   * 3) какой-либо зарегистрированный драйвер требует переключения VT.  *    * Если ни одно из этих условий не выполняется, то есть имеется хотя бы   * один драйвер, которому переключение не требуется, и нет ни одного,   * которому оно нужно, можно обойтись без него, чтобы выход из спящего   * режима выглядел чуть красивее (и переход в suspend тоже, но этого   * пользователь обычно не видит из-за того, например, что крышка его   * ноутбука уже закрыта).  */ static bool pm_vt_switch(void) {  struct pm_vt_switch *entry;  bool ret = true;   mutex_lock(&vt_switch_mutex);  if (list_empty(&pm_vt_switch_list))   goto out;   if (!console_suspend_enabled)   goto out;   list_for_each_entry(entry, &pm_vt_switch_list, head) {   if (entry->required)    goto out;  }   ret = false; out:  mutex_unlock(&vt_switch_mutex);  return ret; }В комментарии во фрагменте кода выше перечислены условия, при которых выполняется переключение. Давайте поговорим о них подробнее.Сначала блокируется vt_switch_mutex, чтобы ничто не могло изменить список, пока он изучается.Далее анализируется сам список pm_vt_switch_list. Этот список содержит драйверы, требующие переключения во время перехода в режим suspend. Они сообщают об этом с помощью pm_vt_switch_required.kernel/power/console.c:31/**  * pm_vt_switch_required — переключать VT при переходе в спящий режим   * @dev: устройство   * @required: если true, то вызывающему драйверу требуется переключение   * VT при переходе в спящий режим / выходе из него.   *   * Различные консольные драйверы могут требовать или не требовать   * переключения VT при переходе в спящий режим / выходе из него   * в зависимости от того, как обрабатывается восстановление графического режима   * и того, что запущено.   *   * Драйверы также могут указать, что переключение им не требуется, — это   * сэкономит время и устранит мерцание экрана, — передавая в качестве   * аргумента 'false'. Если какой-либо загруженный драйвер требует   * переключения VT или в командной строке был передан аргумент   * no_console_suspend, переключение VT произойдёт.  */ void pm_vt_switch_required(struct device *dev, bool required)Далее проверяется console_suspend_enabled. Это значение устанавливается в false параметром ядра no_console_suspend, но по умолчанию оно равно true.Наконец, если в списке pm_vt_switch_list есть какие-либо записи, то система проверяет, не требуют ли они переключения VT.Только если ни одно из этих условий не выполняется, возвращается false.Если переключение VT все-таки требуется, в SUSPEND_CONSOLE сначала перемещается активный в данный момент виртуальный терминал (консоль) (vt_move_to_console), а затем текущее местоположение сообщений ядра (vt_kmsg_redirect). SUSPEND_CONSOLE — последняя запись в списке возможных консолей, которая просто выполняет роль этакой черной дыры, в которую сбрасываются все сообщения.kernel/power/console.c:16#define SUSPEND_CONSOLE (MAX_NR_CONSOLES-1)Любопытно, что это отдельные функции. Хотя TIOCL_SETKMSGREDIRECT позволяет перенаправлять сообщения ядра на заданный виртуальный терминал, по умолчанию он совпадает с активной консолью.(Примечание: ioctl — это специальные операции ввода-вывода для конкретных устройств. Они позволяют выполнять действия, выходящие за рамки стандартных операций с файлами — чтения, записи, поиска и т. д.)Система сохраняет предыдущую активную консоль и место хранения сообщений ядра в переменных orig_fgconsole и orig_kmsg. После выхода из сна эти значения помогают восстановить состояние консоли и лога ядра.Важно: orig_fgconsole фиксирует не только номер консоли, но и ошибки. Перед тем как работать с журналом ядра при переходе в сон или пробуждении, необходимо проверить, что orig_fgconsole не меньше нуля. Иначе можно столкнуться с некорректным поведением системы.drivers/tty/vt/vt_ioctl.c:1268/* Выполняем инициированное ядром переключение VT для приостановки/возобновления работы */  static int disable_vt_switch;  int vt_move_to_console(unsigned int vt, int alloc) {  int prev;   console_lock();  /* Гарфический режим — вплоть до Х */  if (disable_vt_switch) {   console_unlock();   return 0;  }  prev = fg_console;   if (alloc && vc_allocate(vt)) {   /* Пока не можем освободить виртуальную консоль, ибо          * это может привести к проблемам с отображением на экран. */   console_unlock();   return -ENOSPC;  }   if (set_console(vt)) {   /*    * Не удалось переключиться на SUSPEND_CONSOLE.    * Сообщаем об этом вызывающей функции,                * пусть она решает, что делать.    */   console_unlock();   return -EIO;  }  console_unlock();  if (vt_waitactive(vt + 1)) {   pr_debug(""Suspend: Can't switch VCs."");   return -EINTR;  }  return prev; }В отличие от большинства функций блокировки, console_lock перед захватом семафора консоли проверяет, не паникует ли в этот момент другой процессор. (Если процессор паникует, он должен успеть вывести отладочную информацию в консоль до перезагрузки. Поэтому у него приоритет выше, а доступ других процессоров к терминалу временно блокируется. — Прим. пер.)ПаникиСистема отслеживает панику с помощью атомарного целого числа, которое хранит ID процессора, находящегося в состоянии паники.kernel/printk/printk.c:2649/**  * console_lock — блокирует печать в консольной субсистеме.  *  * Блокировка гарантирует, что ни одна консоль не выполняет   * или не будет выполнять коллбэк write().  *  * Может заснуть, ничего не возвращает.  */ void console_lock(void) {  might_sleep();   /* В случае паники console_lock должен оставаться у паникующего CPU.*/  while (other_cpu_in_panic())   msleep(1000);   down_console_sem();  console_locked = 1;  console_may_schedule = 1; } EXPORT_SYMBOL(console_lock);kernel/printk/printk.c:362/*  * Возвращает true, если паникует другой процессор.   *  * Если это так, текущий процессор должен немедленно освободить все ресурсы, связанные   * с выводом сообщений, чтобы они могли быть использованы паникующим процессором.  */ bool other_cpu_in_panic(void) {  return (panic_in_progress() && !this_cpu_in_panic()); }kernel/printk/printk.c:345static bool panic_in_progress(void) {  return unlikely(atomic_read(&panic_cpu) != PANIC_CPU_INVALID); }kernel/printk/printk.c:350/* Возвращает true, если паникует текущий процессор. */ bool this_cpu_in_panic(void) {  /*   * Здесь можно использовать raw_smp_processor_id(), потому    * что задача не может быть перенесена ни на panic_cpu, ни с него.    * Если panic_cpu уже установлен, и мы сейчас не выполняемся на нем   * то мы никогда на нем и не будем выполняться.   */  return unlikely(atomic_read(&panic_cpu) == raw_smp_processor_id()); }console_locked — отладочное значение. Используется для указания на необходимость удержания блокировки. Его наличие — первый признак того, что система виртуальных терминалов устроена сложнее, чем кажется на первый взгляд.kernel/printk/printk.c:373/*  * Используется для отладки бардака, которым является код VT,   * отслеживая, захвачен ли семафор консоли. Это определенно не идеальный   * инструмент отладки (неизвестно, удерживаем ли его _МЫ_ и участвуем   * ли в гонках), но он помогает отслеживать странности в консольном   * коде, когда мы оказываемся в местах, которые надо заблокировать   * без удержания консольного семафора.  */ static int console_locked;console_may_schedule показывает, можно ли перейти в спящий режим и запланировать другие задачи, пока удерживается блокировка. Как мы увидим позже, подсистема виртуального терминала не может безопасно обрабатывать повторные вызовы одних и тех же функций, поэтому есть всевозможные хаки, которые помогают гарантировать, что все важные куски кода, которые невозможно выполнить повторно, выполнены.Отключение переключателя VTКак говорится в приведённом ниже комментарии, когда графическим отображением занимается другая программа, нет необходимости в переключении виртуальных терминалов. Поэтому в ядре предусмотрен соответствующий выключатель. Интересно, что он используется только тремя драйверами — его аппаратная поддержка не слишком распространена.drivers/gpu/drm/omapdrm/dss drivers/video/fbdev/geode drivers/video/fbdev/omap2drivers/tty/vt/vt_ioctl.c:1308/*  * Обычно во время перехода в спящий режим мы выделяем новую консоль и   * переключаемся на неё. При возобновлении работы возвращаемся к исходной   * консоли. Такое переключение может проходить неспешно,   * поэтому в системах, где фреймбуфер и так справляется   * с восстановлением видеорегистров, в переключении нет смысла. Эта   * функция отключает переключение, передавая '0'.  */ void pm_set_vt_switch(int do_switch) {  console_lock();  disable_vt_switch = !do_switch;  console_unlock(); } EXPORT_SYMBOL(pm_set_vt_switch);Остальная часть функции vt_switch_console ничем не примечательна. Она просто выделяет место, если это необходимо, для создания требуемого виртуального терминала, а затем устанавливает текущий виртуальный терминал с помощью функции set_console.Set Console виртуального терминалаС помощью set_console мы начинаем входить в ту самую безумную подсистему виртуальных терминалов. Как уже говорилось, изменять её состояние следует крайне осторожно, поскольку параллельные процессы могут привести к полной неразберихе.При этом сам вызов set_console фактически не выполняет никакой работы по изменению состояния текущей консоли. Вместо этого он указывает, какие изменения необходимы, а затем планирует их (добавляет в workqueue. — Прим. пер.).drivers/tty/vt/vt.c:3153int set_console(int nr) {  struct vc_data *vc = vc_cons[fg_console].d;   if (!vc_cons_allocated(nr) || vt_dont_switch ||   (vc->vt_mode.mode == VT_AUTO && vc->vc_mode == KD_GRAPHICS)) {    /*    * Поскольку переключение консоли неизбежно приведет           * к ошибке в console_callback() или change_console(),    * отменяем планирование обратного вызова для оптимизации.          *    * Это безопасно, поскольку существующие пользователи           * функции set_console()    * игнорируют значение, возвращаемое ею.    */   return -EINVAL;  }   want_console = nr;  schedule_console_callback();   return 0; }Проверка vc->vc_mode == KD_GRAPHICS отменяет переключение в suspend-консоль, если система работает в графическом режиме.Флаг vt_dont_switch используется в ioctls VT_LOCKSWITCH и VT_UNLOCKSWITCH и не позволяет системе переключать виртуальный терминал, если пользователь явно заблокировал это.Флаг VT_AUTO означает, что автоматическое переключение виртуальных терминалов включено и, следовательно, преднамеренное переключение на suspend-терминал не требуется.Однако если вы работаете в виртуальном терминале, то механизм меняется. Переменная want_console говорит системе, что необходимо перейти на требуемый виртуальный терминал, а само переключение планируется с помощью schedule_console_callback.drivers/tty/vt/vt.c:315void schedule_console_callback(void) {  schedule_work(&console_work); }console_work — это workqueue, которая будет выполнять заданную задачу асинхронно.Коллбэк консолиdrivers/tty/vt/vt.c:3109/*  * Это коллбэк переключения консоли.   * Переключение консоли в контексте процесса позволяет   * выполнять переключения асинхронно (нужно, когда переключаемся   * по прерыванию клавиатуры). За синхронизацию с консольным кодом   * и предотвращение повторного входа в код переключения консоли   * отвечает console_lock.  */ static void console_callback(struct work_struct *ignored) {  console_lock();   if (want_console >= 0) {   if (want_console != fg_console &&       vc_cons_allocated(want_console)) {    hide_cursor(vc_cons[fg_console].d);    change_console(vc_cons[want_console].d);    /* Мы изменяем консоль, только если она уже выделена. Новая консоль не создается в обработчике прерывания. */   }   want_console = -1;  } ... console_callback сначала проверяет, есть ли консоль, которую хотят изменить через want_console, а затем переключается на неё, если она не является текущей и уже была выделена (allocated). Сначала с помощью hide_cursor удаляется состояние курсора.drivers/tty/vt/vt.c:841static void hide_cursor(struct vc_data *vc) {  if (vc_is_sel(vc))   clear_selection();   vc->vc_sw->con_cursor(vc, false);  hide_softcursor(vc); }Полное погружение в драйвер tty выходит за рамки этой статьи. Код выше даёт общее представление о том, как эта система взаимодействует с гибернацией.  Уведомление цепочки вызовов управления питаниемkernel/power/hibernate.c:767pm_notifier_call_chain_robust(PM_HIBERNATION_PREPARE, PM_POST_HIBERNATION)Вызов цепочки коллбэков управления питанием. Сначала передаётся PM_HIBERNATION_PREPARE, а затем PM_POST_HIBERNATION при запуске или ошибке с другим коллбэком.kernel/power/main.c:98int pm_notifier_call_chain_robust(unsigned long val_up, unsigned long val_down) {  int ret;   ret = blocking_notifier_call_chain_robust(&pm_chain_head, val_up, val_down, NULL);   return notifier_to_errno(ret); }Нотификатор управления питанием представляет собой блокирующую цепочку уведомлений, что означает, что он обладает следующими свойствами:include/linux/notifier.h:23 * Блокирующие цепочки уведомлений: коллбэки цепочек выполняются в контексте процесса.  * Функции обратного вызова (callouts) могут использовать блокирующие операции.Цепочка коллбэков представляет собой связанный список, каждая запись которого содержит приоритет и функцию для вызова. Технически функция принимает значение, но для цепи управления питанием оно всегда NULL.include/linux/notifier.h:49struct notifier_block;  typedef int (*notifier_fn_t)(struct notifier_block *nb,    unsigned long action, void *data);  struct notifier_block {  notifier_fn_t notifier_call;  struct notifier_block __rcu *next;  int priority; };Head связанного списка защищена семафором чтения-записи.include/linux/notifier.h:65struct blocking_notifier_head {  struct rw_semaphore rwsem;  struct notifier_block __rcu *head; };Поскольку список идёт с приоритетами, при добавлении его приходится перебирать до тех пор, пока не будет найден элемент с более низким приоритетом, перед которым можно вставить текущий элемент.kernel/notifier.c:252/*  * Блокирующие цепочки уведомлений. Весь доступ   *   к цепочке синхронизируется с помощью rwsem.  */  static int __blocking_notifier_chain_register(struct blocking_notifier_head *nh,            struct notifier_block *n,            bool unique_priority) {  int ret;   /*   * Этот код используется во время загрузки, когда   * переключение задач ещё не работает и прерывания   * должны оставаться отключёнными. В такие моменты   * нельзя вызывать down_write().   */  if (unlikely(system_state == SYSTEM_BOOTING))   return notifier_chain_register(&nh->head, n, unique_priority);   down_write(&nh->rwsem);  ret = notifier_chain_register(&nh->head, n, unique_priority);  up_write(&nh->rwsem);  return ret; }kernel/notifier.c:20/*  * Основные подпрограммы (routines) цепочки уведомлений.   * Экспортируемые подпрограммы накладываются   * поверх них, с добавлением соответствующей блокировки.  */  static int notifier_chain_register(struct notifier_block **nl,        struct notifier_block *n,        bool unique_priority) {  while ((*nl) != NULL) {   if (unlikely((*nl) == n)) {    WARN(1, ""notifier callback %ps already registered"",         n->notifier_call);    return -EEXIST;   }   if (n->priority > (*nl)->priority)    break;   if (n->priority == (*nl)->priority && unique_priority)    return -EBUSY;   nl = &((*nl)->next);  }  n->next = *nl;  rcu_assign_pointer(*nl, n);  trace_notifier_register((void *)n->notifier_call);  return 0; }Каждый коллбэк может возвращать одну из нескольких опций. include/linux/notifier.h:18#define NOTIFY_DONE      0x0000  /* Без разницы */ #define NOTIFY_OK      0x0001  /* Подходит */ #define NOTIFY_STOP_MASK 0x8000  /* Не вызываем дальше */ #define NOTIFY_BAD      (NOTIFY_STOP_MASK|0x0002)                       /* Плохое/Вето-действие */Если при уведомлении цепочки функция возвращает STOP или BAD, предыдущие части цепочки вызываются снова с PM_POST_HIBERNATION и возвращается ошибка.kernel/notifier.c:107/**  * notifier_call_chain_robust — информирование зарегистрированных   * уведомителей (notifiers) о событии и откат при ошибке.   * @nl:  указатель на голову цепочки блокирующих уведомителей.  * @val_up: значение, передаваемое в неизменном виде   * функции-уведомителю.  * @val_down: значение, передаваемое в неизменном виде  * функции-уведомителю при восстановлении после ошибки на @val_up.  * @v:  указатель, передаваемый в неизменном виде   * функции-уведомителю.  *  * ПРИМЕЧАНИЕ:  Важно, чтобы цепочка @nl не менялась между двумя  *   вызовами notifier_call_chain() так, чтобы перебирались  *    одни и те же коллбэки обработчиков; это исключает любое  *    использование RCU.  *  * Return: возвращаемое значение вызова @val_up.  */ static int notifier_call_chain_robust(struct notifier_block **nl,          unsigned long val_up, unsigned long val_down,          void *v) {  int ret, nr = 0;   ret = notifier_call_chain(nl, val_up, v, -1, &nr);  if (ret & NOTIFY_STOP_MASK)   notifier_call_chain(nl, val_down, v, nr-1, NULL);   return ret; }RCU, Read-Copy-Update — механизм синхронизации в Linux, который позволяет нескольким потокам одновременно читать данные (прим. пер.).  Каждый из этих коллбэков зависит от конкретного драйвера, поэтому воздержимся от дальнейшего обсуждения.Синхронизация файловых системСледующий шаг — убедиться, что все файловые системы синхронизированы на диск.Это реализуется с помощью простой вспомогательной функции, которая определяет, сколько времени занимает полная операция синхронизации ksys_sync.kernel/power/main.c:69void ksys_sync_helper(void) {  ktime_t start;  long elapsed_msecs;   start = ktime_get();  ksys_sync();  elapsed_msecs = ktime_to_ms(ktime_sub(ktime_get(), start));  pr_info(""Filesystems sync: %ld.%03ld seconds\n"",   elapsed_msecs / MSEC_PER_SEC, elapsed_msecs % MSEC_PER_SEC); } EXPORT_SYMBOL_GPL(ksys_sync_helper);ksys_sync запускает набор потоков сброса на диск для каждой файловой системы, даёт задачу синхронизировать их inodes, затем всю файловую систему и, наконец, все блочные устройства, чтобы гарантировать, что все страницы будут записаны на диск.fs/sync.c:87/*  * Синхронизируем всё. Начинаем с пробуждения потоков для   * сброса на диск, чтобы запись шла на всех устройствах параллельно.  * Затем синхронизируем все иноды, дожидаясь завершения записи   * всеми потоками. В этот момент все данные находятся на диске,   * поэтому метаданные не меняются. Файловым системам даётся задача  * синхронизировать метаданные с помощью вызовов ->sync_fs().   * Наконец, сохраняем все блочные устройства, потому что некоторые  * файловые системы (например, ext2) просто записывают метаданные   * (такие, как inodes или bitmaps) в кэш страниц блочных устройств   * и не синхронизируют их самостоятельно в ->sync_fs().  */ void ksys_sync(void) {  int nowait = 0, wait = 1;   wakeup_flusher_threads(WB_REASON_SYNC);  iterate_supers(sync_inodes_one_sb, NULL);  iterate_supers(sync_fs_one_sb, &nowait);  iterate_supers(sync_fs_one_sb, &wait);  sync_bdevs(false);  sync_bdevs(true);  if (unlikely(laptop_mode))   laptop_sync_completion(); }Здесь применяется интересная схема, когда iterate_supers запускает sync_inodes_one_sb, а затем sync_fs_one_sb для каждой известной файловой системы. (Каждая активная файловая система регистрируется в ядре с помощью структуры, известной как суперблок, который содержит ссылки на все иноды, а также указатели функций для выполнения различных необходимых операций, например синхронизации. — Прим. автора.) Кроме того, дважды вызываются sync_fs_one_sb и sync_bdevs, сначала без ожидания завершения операций, а затем с ожиданием завершения.Когда laptop_mode включён, система запускает дополнительные операции синхронизации файловой системы после указанной задержки без каких-либо операций записи.mm/page-writeback.c:111/*  * Флаг, переводящий машину в «режим ноутбука».   * Удваивается как таймаут в jiffies:  * если в течение этого времени диск оставался неактивен,  * запускается полная синхронизация.  */ int laptop_mode;  EXPORT_SYMBOL(laptop_mode);Jiffies — Интервал между двумя прерываниями системного таймера в ядре Linux (прим. пер.).   Однако при выполнении операции синхронизации файловой системы система добавит дополнительный таймер, чтобы запланировать больше операций записи после задержки laptop_mode. Нам не нужно, чтобы состояние системы менялось во время перехода в спящий режим, поэтому эти таймеры отменяются.mm/page-writeback.c:2198/*  * Мы в режиме ноутбука и только что провели синхронизацию.   * laptop_io_completion планирует очередную запись, однако   * больше ничего записывать не нужно, поэтому запланированная   * запись отменяется.  */ void laptop_sync_completion(void) {  struct backing_dev_info *bdi;   rcu_read_lock();   list_for_each_entry_rcu(bdi, &bdi_list, bdi_list)   del_timer(&bdi->laptop_mode_wb_timer);   rcu_read_unlock(); }В качестве примечания: функция ksys_sync просто вызывается, когда используется системный вызов sync.fs/sync.c:111SYSCALL_DEFINE0(sync) {  ksys_sync();  return 0; }Конец подготовительного этапаНа этом подготовка к переходу в спящий режим завершена. Я решил пока прерваться на этом моменте. Далее система начнет полное замораживание пользовательского пространства, чтобы затем сбросить память в образ и наконец перейти в режим гибернации. Но об этом мы расскажем в следующих статьях.P. S. Читайте также в нашем блоге:Как собрать Linux-контейнер с нуля и без DockerЧто не так с chroot: почему для контейнеров используется именно pivot_rootНеиспользуемые остатки образов в Docker: как удалить зомби-слои и защитить секреты"
105,105,Мобильная разработка: что происходит на рынке и что важно знать в 2025 году,Нетология,Источник знаний для роста в профессии,96.77,"Веб-разработка, Производство мультимедиа-контента, Веб-сервисы",2025-03-11,"Последние несколько лет мобильная разработка в России пережила серьёзные изменения. После роста в 2020–2021 годах рынок замедлился: компании сократили наём, а российские разработчики столкнулись с ограничениями в App Store и Google Play. Мы изучили исследования, посмотрели, как изменился спрос на специалистов, и пообщались с мобильными разработчиками. В статье разбираем, что происходит с iOS- и Android-разработкой в 2025 году, какие технологии в тренде и что нужно уметь, чтобы оставаться востребованным. Статья — не универсальная истина, а выжимка разных мнений, которые могут быть полезны тем, кто думает о смене карьеры или переходе в мобильную разработку из другого направления.Рынок оправился после падения в 2023: команды адаптировались, а RuStore стали пользоваться чащеСанкции, которые начали вводить в 2022 году, особенно сильно ударили по iOS-разработчикам. Компания Apple начала удалять из AppStore всё больше российских приложений. В результате многие крупные российские компании оказались в сложном положении, но смогли адаптироваться.Артём КалининiOS-разработчик в СбереВ 2022 году я работал в другом банке, и когда он попал под санкции, сразу стало понятно, что нас удалят из App Store. Переживал, что работы может не быть. Но в итоге ситуация не такая трагичная. У крупных компаний даже под санкциями есть инструменты и ресурсы, чтобы держать свои команды на плаву без убытка бизнесу. Сами команды за это время научились делать так, чтобы конечный пользователь получал продукт в виде приложений на своё устройство. Да, это дополнительная работа, но руки от этого не опускаются. Мелкие компании, которые не попадают под санкции, спокойно могут выкладывать свои приложения в App Store, как и раньше, правда, без монетизации. А вообще, такой сплочённости среди iOS-разработчиков я раньше не видел. Представьте: джун попадает в команду, а вокруг — мастодонты, которые пережили санкции, блокировки, удаление приложений и научились обходить любые ограничения. Apple вводит новые запреты — они находят новые решения. Работать в такой команде — мощная мотивация, потому что эти люди не сдаются и просто прут, как бульдозеры. У Android-разработчиков тоже были сложности: Google Play запретил им выкладывать приложения и получать выплаты на российские счета. Однако они могли регистрировать компании в других юрисдикциях и продолжать работу.Анатолий СпитченкоAndroid-разработчик в банкеПоявились и проблемы с безопасностью open source-инструментов. Были единичные случаи, когда в библиотеках находили вредоносный код. Мы отказались от большинства таких решений и перешли на внутренние разработки.В итоге разработчики нашли способы обходить ограничения App Store и Google Play. Появились и альтернативные площадки. RuStore стал основным магазином приложений для Android: по статистике за 2024 год он обогнал AppStore по количеству пользователей. Хотя сравнение не совсем справедливое, учитывая, что площадка актуальна только для Android-разработчиков. Надежда, что Apple разрешит устанавливать приложения из сторонних магазинов, как в ЕС, тоже есть, но пока это только разговоры. При этом в России по-прежнему покупают айфоны, а значит, iOS-разработчики не останутся без работы.Артём МостяевAndroid-разработчик в небольшой международной компанииУ нас бизнес маленький, но международный: есть юрлицо и в Америке, и в России. Поэтому используем и Play Store, и RuStore, но основная часть бизнеса ориентирована на западный рынок, поэтому с RuStore сотрудничаем, скорее, из интереса. Приложения выкладываем, но их SDK и библиотеку покупок не интегрировали. Работать с RuStore приятно: модерация проходит буквально за 10 минут. Возможно, система модерации ещё не такая строгая, как у Google Play, или приложений пока не так много. Платформа активно взаимодействует с разработчиками, присылает новости и обновления. Это совсем не похоже на формальный подход крупных зарубежных сторов.Анатолий СпитченкоAndroid-разработчик в банкеУ нас был запасной вариант на случай, если не взлетит RuStore — редизайнить приложение таким образом, чтобы убрать все упоминания банка. Но пользователям нравится RuStore, поэтому ничего не стали менять. На конференции Mobius послушал про их инструменты, и в нашей команде думаем перейти на их сервис Push и Crashlytics.Кросс-платформа и PWA помогают сэкономить, но есть нюансыC 2022 года компании стали развивать Progressive Web Apps (PWA) как альтернативу дорогим нативным приложениям. Однако этот тренд оказался временным: PWA не смогли заменить полноценные мобильные решения, а для Apple в 2024 году их поддержку и вовсе отключили.Кросс-платформенная разработка набирает популярность, особенно у небольших компаний, которым важна экономия. Но даже они по-возможности выбирают нативные приложения.Артём КалининiOS-разработчик в СбереКогда в 2022–2023 годах начались проблемы со сторами, многие компании задумались: может, ну его, эти приложения, оставим только веб?! Но браузером на смартфоне неудобно пользоваться, поэтому решили добавить PWA. Формально это похоже на приложение, но по факту — тот же браузер, только в другом оформлении.Компании, которые могут позволить себе нативные приложения, от них не откажутся — они просто дополняют их PWA, чтобы охватить больше пользователей. Человек зашёл на сайт с мобильного, получил PWA, разобрался в интерфейсе, а потом ему предложили скачать полноценное приложение. И он скачает, потому что так удобнее.Артём МостяевAndroid-разработчик в небольшой международной компанииЕсли бизнесу нужно простое приложение «на вчера», кросс-платформа подходит. У нас такие продукты были и до 2022 года. Но мы разработали своё решение по кросс-платформенности, оно позволяет на 50% использовать общий код для двух платформ. С PWA-приложениями я знаком только как пользователь. На мой взгляд, они работают хуже, чем нативные: интерфейсы iOS и Android разные, и универсальный дизайн не даёт хорошего опыта ни там, ни там. Зарплаты растут медленно, а офферы стали скромнее После роста в 2021 году зарплаты стали расходиться с ожиданиями кандидатов. Раньше смена работы могла принести +50% к доходу, но теперь это редкость. Рост в зарплатных исследованиях скорее похож на индексацию. Например, по данным Хабр Карьеры, оклад мидла за год вырос на 15 000 рублей. Анатолий СпитченкоAndroid-разработчик в банкеНаверно, мне повезло с компанией, но в прошлом году повышали зарплату — было точно выше инфляции. Другой банк пытался меня схантить на таких же условиях, и я отказался. Когда в прошлый раз переходил в банк, предложили в три раза больше, но сейчас таких офферов не встретишь, и в целом HR реже звонят. Джунам сложнее, но их всё равно нанимаютHh.ru по запросу «мобильный разработчик с опытом от 1 до 3 лет» выдаёт 756 вакансий, для Android-разработчиков с аналогичным опытом — 220, для iOS — 168. Компании чаще предпочитают опытных специалистов, но джуны всё ещё могут пробиться в профессию. Чаще — через стажировки, аутстаф, небольшие компании или студии. Статистика здесь суровая: из всех участников стажировки в штат могут взять одного-двух разработчиков. Артём МостяевAndroid-разработчик в небольшой международной компанииМы любим нанимать джунов и растить их внутри компании. У сеньоров, а порой и мидлов, требования сильно завышены. В прошлом году наняли одного джуна, и он уже подрос до мидла. Готовы нанять ещё одного.Анатолий СпитченкоAndroid-разработчик в банкеВ 2022 году мы проводили стажировку для ребят с последних курсов вузов, но это был скорее эксперимент. Сейчас вакансии открыты только для мидлов и сеньоров. У нас нет плана нанять конкретное количество людей в год. Если место в команде освобождается, открываем вакансию. Как раз ищем мидла — сеньоров стало сложно заинтересовать.У нас всего одно собеседование. Смотрим, что человек знает по теории и алгоритмам. Были случаи, когда кандидат подглядывал в Chat GPT. Не советую так делать — на проекте эти подсказки уже не помогут. Мы готовы поддерживать нового человека в команде, но и от него ждём самостоятельности, инициативы и помощи. Например, стажёр, который освоил Kotlin, может поучаствовать в миграции проекта с xml на Jetpack Compose.Аутстаф — это, в первую очередь, способ сэкономить на найме. Например, в 2024 году по такой схеме нанимали Android-разработчиков. Новички могут попробовать этот вариант, чтобы попасть в большие проекты без сложных собеседований. В некоторых компаниях по пять этапов интервью — не каждый это выдержит. Через аутстаф этот путь может быть короче.Артём КалининiOS-разработчик в СбереКому-то удобнее искать работу напрямую, но аутстаф — это реальная возможность поработать на крупных заказчиков. В 2021–2022 я сам работал по такой схеме. У клиентов компании были свои команды, но на отдельные задачи они привлекали специалистов через аутстаф. Это не плохо — так можно попасть в крупные компании без сложного отбора. Джунов на аутстаф тоже берут. Ты официально устроен в аутстаф-компанию, но работаешь на клиента. Контракт может закончиться, но тебя не оставят без работы: либо переведут в новый проект, либо задействуют во внутренних продуктах. Оптимизация — не равно кризисВ конце 2024 года крупные бизнесы объявили, что проводят оптимизацию — по факту сокращают штат. Но это произошло не потому, что рынок рушится, а потому что меняются приоритеты. Бум на импортозамещение прошёл, нейросети автоматизируют рутинные задачи, а бюджеты стали считать внимательнее. Работодатели оставляют ключевых специалистов, оптимизируют процессы и адаптируются к новым условиям. Артём КалининiOS-разработчик в СбереСокращения — это не про «всё пропало». Просто компании избавляются от лишних позиций. Если на проекте реально работают 20 человек, а числится 100, рано или поздно пройдёт ревизия. Всё это происходит не только в IT, но и в других сферах. Где-то прочитал, что Volkswagen накопил 170 тысяч невостребованных люксовых машин. Если сейчас рынок выпускает пар — это даже плюс для тех, кто только входит в профессию: пока учишься, ситуация стабилизируется, а конкуренция снизится.За трендами полезно следить, но не стоит гнатьсяСобрали ключевые тренды и спросили разработчиков, как их учитывать в развитии карьеры.🔵 Рынок mobile commerce продолжает расти, набирают популярность супераппы. Этим компаниям, скорее всего, будут нужны разработчики для поддержки и развития приложений со сложными UI.🔵 Sensor Tower выпустили исследование о международном рынке мобильных приложений и рассказали, какие направления будут активно развиваться: голосовые ассистенты, встроенные функции на основе ИИ, интеграция с умным домом. Также будет расти внимание к безопасности и шифрованию данных, а разработка будет упрощаться благодаря кросс-платформенным решениям.Артём КалининiOS-разработчик в СбереЕсли у тебя есть pet-проекты с AI — это всегда плюс. Никто не скажет: «Зачем ты это делал?». Наоборот, работодатели заинтересуются.🔵 В обзорах российских блогов встречаются тренды, схожие с международными. Например, в России тоже в фокусе безопасность данных — пользователи больше заботятся о конфиденциальности, а компании вынуждены соблюдать новые требования. Но есть и локальные тренды. Из-за санкций многие российские компании перешли на отечественные решения. Это касается не только сторов, но и инструментов аналитики, платёжных систем и облачных сервисов. Анатолий СпитченкоAndroid-разработчик в банкеНовые мобильные операционные системы типа «Авроры» — интересная тема. Возможно, я бы участвовал в таком проекте. Но проблема в том, что и приложений, и аудитории у них особо нет, хотя устройства начали продавать в рознице.Артём МостяевAndroid-разработчик в небольшой международной компанииНе советую гнаться за трендами — это быстро сойдёт на нет. Я в мобильной разработке уже больше десяти лет, и технологии меняются раз в 4–5 лет. Поэтому не так важно, какой фреймворк разработчик изучил. Важнее адаптироваться к технологиям, которые придут через пять лет. Например, мы полностью обновили курс по Android в 2021 году, и в этом году снова будем его перезапускать: добавим Jetpack Compose и новые библиотеки от Google (они часто их обновляют).Какие знания обязательны для мобильного разработчика в 2025 годуРынок мобильной разработки определился с ключевыми технологиями, которые работодатели ждут от специалистов. Это касается и iOS, и Android — причём у каждой платформы появились свои must-have навыки.SwiftUI и Jetpack Compose SwiftUI — это фреймворк от Apple для создания пользовательских интерфейсов на iOS, macOS, watchOS и tvOS. Он позволяет разработчикам быстро строить интерфейсы с помощью декларативного синтаксиса, который упрощает код и повышает производительность. SwiftUI интегрируется с остальными технологиями Apple и даёт возможность использовать мощь Swift для создания отзывчивых и гибких интерфейсов.Jetpack Compose — это аналогичный фреймворк от Google для создания интерфейсов на Android. Он также использует декларативный подход, позволяя строить UI с помощью Kotlin. Jetpack Compose упрощает разработку, уменьшает количество кода и улучшает производительность. С ним быстрее и легче создавать красивые и функциональные приложения для Android.Артём КалининiOS-разработчик в СбереНедавно просматривая вакансии на Хабре и hh.ru, я заметил, что почти в каждой встречалась позиция для разработчика SwiftUI. Раньше требовалось знание UIKit, а ещё раньше писали на Objective-C. Сейчас SwiftUI становится стандартом — это естественная смена эпох в мобильной разработке.Понимание работы с сервером и APIМобильные приложения редко работают автономно — чаще всего они взаимодействуют с бэкендом. Разработчикам важно понимать:что такое многопоточность;как работают REST API и GraphQL;как обрабатывать сеть и кешировать данные;как оптимизировать запросы и работу с базами данных.Умение решать задачи, а не просто писать кодТехнические навыки важны, но работодатели в первую очередь ценят мышление и способность обучаться.Артём КалининiOS-разработчик в СбереСпособность адаптироваться и находить решения, а также подход к задачам — многое говорят о разработчике не только как о специалисте, но и как о человеке. На собеседовании важно показать, как решаешь задачи. Даже если не знаешь точного ответа, нужно думать вслух.Анатолий СпитченкоAndroid-разработчик в банкеСмотрю сейчас на наши вопросы к собеседованиям, и кажется, что когда я начинал, было проще. Раньше могли спрашивать какие-нибудь основы языка: коллекции, многопоточность. Достаточно было просто написать приложение небольшое, сделать тестовое задание, и всё. Сейчас могут спросить, например, как работают архитектурные секции. Артём МостяевAndroid-разработчик в небольшой международной компанииЯ смотрю, насколько человек способен обучиться. Даже если он чего-то не знает, но адекватный и способный, то мы его научим. Что в итогеПока люди пользуются смартфонами, мобильная разработка никуда не денется. Разработчики продолжат поддерживать существующие продукты и создавать новые, заменяя ушедшие из России приложения.Android-разработчикам будет проще найти работу — рынок растёт. Но это не значит, что тем, что работает с iOS, стоит тут же переходить на Android. Однако знание стека этой платформы будет полезным дополнением и повысит шансы кандидата на рынке.Прикоснуться к магии мобильной разработки, начав с бесплатного интерактивного интенсива. А научиться разрабатывать приложения для мобильных устройств и открыть бóльшие перспективы в карьере позволит обучение с программой трудоустройства: «Android-разработчик с нуля» или «iOS-разработчик с нуля». С промокодом CODEHABR10 цена ещё приятнее. "
